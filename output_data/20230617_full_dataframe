,at,author,title,year,isbn,publisher,address,url,doi,abstract,booktitle,pages,numpages,keywords,location,series,issue_date,volume,number,issn,journal,month
0,inproceedings,"Zhang, Yiwen and Kumar, Gautam and Dukkipati, Nandita and Wu, Xian and Jha, Priyaranjan and Chowdhury, Mosharaf and Vahdat, Amin",Aequitas: Admission Control for Performance-Critical RPCs in Datacenters,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544271,10.1145/3544216.3544271,"With the increasing popularity of disaggregated storage and microservice architectures, high fan-out and fan-in Remote Procedure Calls (RPCs) now generate most of the traffic in modern datacenters. While the network plays a crucial role in RPC performance, traditional traffic classification categories cannot sufficiently capture their importance due to wide variations in RPC characteristics. As a result, meeting service-level objectives (SLOs), especially for performance-critical (PC) RPCs, remains challenging.We present Aequitas, a distributed sender-driven admission control scheme that uses commodity Weighted-Fair Queuing (WFQ) to guarantee RPC-level SLOs. In the presence of network overloads, it enforces cluster-wide RPC latency SLOs by limiting the amount of traffic admitted into any given QoS and downgrading the rest. We show analytically and empirically that this simple scheme works well. When the network demand spikes beyond provisioned capacity, Aequitas achieves a latency SLO that is 3.8\texttimes{",Proceedings of the ACM SIGCOMM 2022 Conference,1–18,18,"network overload, quality of service, RPC performance","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
1,inproceedings,"Chen, Shawn Shuoshuo and Wang, Weiyang and Canel, Christopher and Seshan, Srinivasan and Snoeren, Alex C. and Steenkiste, Peter",Time-Division TCP for Reconfigurable Data Center Networks,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544254,10.1145/3544216.3544254,"Recent proposals for reconfigurable data center networks have shown that providing multiple time-varying paths can improve network capacity and lower physical latency. However, existing TCP variants are ill-suited to utilize available capacity because their congestion control cannot react quickly enough to drastic variations in bandwidth and latency.We present Time-division TCP (TDTCP), a new TCP variant designed for reconfigurable data center networks. TDTCP recognizes that communication in these fabrics happens over a set of paths, each having its own physical characteristics and cross traffic. TDTCP multiplexes each connection across multiple independent congestion states---one for each distinct path---while managing connection-wide tasks in a shared fashion. It leverages network support to receive timely notification of path changes and promptly matches its local view to the current path. We implement TDTCP in the Linux kernel. Results on an emulated network show that TDTCP improves throughput over both traditional TCP variants, such as DCTCP and CUBIC, and multipath TCP by 24--41% without requiring significant in-network buffering to hide path variations.",Proceedings of the ACM SIGCOMM 2022 Conference,19–35,17,"congestion control, data center, transport protocol","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
2,inproceedings,"Addanki, Vamsi and Apostolaki, Maria and Ghobadi, Manya and Schmid, Stefan and Vanbever, Laurent",ABM: Active Buffer Management in Datacenters,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544252,10.1145/3544216.3544252,"Today's network devices share buffer across queues to avoid drops during transient congestion and absorb bursts. As the buffer-per-bandwidth-unit in datacenter decreases, the need for optimal buffer utilization becomes more pressing. Typical devices use a hierarchical packet admission control scheme: First, a Buffer Management (BM) scheme decides the maximum length per queue at the device level and then an Active Queue Management (AQM) scheme decides which packets will be admitted at the queue level. Unfortunately, the lack of cooperation between the two control schemes leads to (i) harmful interference across queues, due to the lack of isolation; (ii) increased queueing delay, due to the obliviousness to the per-queue drain time; and (iii) thus unpredictable burst tolerance. To overcome these limitations, we propose ABM, Active Buffer Management which incorporates insights from both BM and AQM. Concretely, ABM accounts for both total buffer occupancy (typically used by BM) and queue drain time (typically used by AQM). We analytically prove that ABM provides isolation, bounded buffer drain time and achieves predictable burst tolerance without sacrificing throughput. We empirically find that ABM improves the 99th percentile FCT for short flows by up to 94% compared to the state-of-the-art buffer management. We further show that ABM improves the performance of advanced datacenter transport protocols in terms of FCT by up to 76% compared to DCTCP, TIMELY and PowerTCP under bursty workloads even at moderate load conditions.",Proceedings of the ACM SIGCOMM 2022 Conference,36–52,17,"shared buffer, datacenter, queue management, buffer management","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
3,inproceedings,"Cai, Qizhe and Arashloo, Mina Tahmasbi and Agarwal, Rachit",DcPIM: Near-Optimal Proactive Datacenter Transport,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544235,10.1145/3544216.3544235,"Datacenter Parallel Iterative Matching (dcPIM) is a proactive data-center transport design that simultaneously achieves near-optimal tail latency for short flows and near-optimal network utilization, without requiring any specialized network hardware.dcPIM places its intellectual roots in the classical PIM protocol, variations of which are used in almost all switch fabrics. The key technical result in dcPIM is a new theoretical analysis of the PIM protocol for the datacenter context: we show that, unlike switch fabrics where PIM requires log(n) rounds of control plane messages (for an n-port switch fabric) to guarantee near-optimal network utilization, the datacenter context enables PIM to guarantee near-optimal utilization with constant number of rounds (independent of the number of hosts in the datacenter)! dcPIM design builds upon insights gained from this analysis, and extends the PIM design to overcome the unique challenges introduced by datacenter networks (much larger scales and round trip times when compared to switch fabrics). We demonstrate, both theoretically and empirically, the near-optimality of dcPIM performance.",Proceedings of the ACM SIGCOMM 2022 Conference,53–65,13,"datacenter networks, proactive transport, flow scheduling","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
4,inproceedings,"Poutievski, Leon and Mashayekhi, Omid and Ong, Joon and Singh, Arjun and Tariq, Mukarram and Wang, Rui and Zhang, Jianan and Beauregard, Virginia and Conner, Patrick and Gribble, Steve and Kapoor, Rishi and Kratzer, Stephen and Li, Nanfang and Liu, Hong and Nagaraj, Karthik and Ornstein, Jason and Sawhney, Samir and Urata, Ryohei and Vicisano, Lorenzo and Yasumura, Kevin and Zhang, Shidong and Zhou, Junlan and Vahdat, Amin",Jupiter Evolving: Transforming Google's Datacenter Network via Optical Circuit Switches and Software-Defined Networking,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544265,10.1145/3544216.3544265,"We present a decade of evolution and production experience with Jupiter datacenter network fabrics. In this period Jupiter has delivered 5x higher speed and capacity, 30% reduction in capex, 41% reduction in power, incremental deployment and technology refresh all while serving live production traffic. A key enabler for these improvements is evolving Jupiter from a Clos to a direct-connect topology among the machine aggregation blocks. Critical architectural changes for this include: A datacenter interconnection layer employing Micro-Electro-Mechanical Systems (MEMS) based Optical Circuit Switches (OCSes) to enable dynamic topology reconfiguration, centralized Software-Defined Networking (SDN) control for traffic engineering, and automated network operations for incremental capacity delivery and topology engineering. We show that the combination of traffic and topology engineering on direct-connect fabrics achieves similar throughput as Clos fabrics for our production traffic patterns. We also optimize for path lengths: 60% of the traffic takes direct path from source to destination aggregation blocks, while the remaining transits one additional block, achieving an average block-level path length of 1.4 in our fleet today. OCS also achieves 3x faster fabric reconfiguration compared to pre-evolution Clos fabrics that used a patch panel based interconnect.",Proceedings of the ACM SIGCOMM 2022 Conference,66–85,20,"datacenter network, traffic engineering, topology engineering, software-defined networking, optical circuit switches","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
5,inproceedings,"Hassan, Ahmad and Narayanan, Arvind and Zhang, Anlan and Ye, Wei and Zhu, Ruiyang and Jin, Shuowei and Carpenter, Jason and Mao, Z. Morley and Qian, Feng and Zhang, Zhi-Li",Vivisecting Mobility Management in 5G Cellular Networks,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544217,10.1145/3544216.3544217,"With 5G's support for diverse radio bands and different deployment modes, e.g., standalone (SA) vs. non-standalone (NSA), mobility management - especially the handover process - becomes far more complex. Measurement studies have shown that frequent handovers cause wild fluctuations in 5G throughput, and worst, service outages. Through a cross-country (6,200 km+) driving trip, we conduct in-depth measurements to study the current 5G mobility management practices adopted by three major U.S. carriers. Using this rich dataset, we carry out a systematic analysis to uncover the handover mechanisms employed by 5G carriers, and compare them along several dimensions such as (4G vs. 5G) radio technologies, radio (low-, mid- & high-)bands, and deployment (SA vs. NSA) modes. We further quantify the impact of mobility on application performance, power consumption, and signaling overheads. We identify key challenges facing today's NSA 5G deployments which result in unnecessary handovers and reduced coverage. Finally, we design a holistic handover prediction system Prognos and demonstrate its ability to improve QoE for two 5G applications 16K panoramic VoD and realtime volumetric video streaming. We have released the artifacts of our study at https://github.com/SIGCOMM22-5GMobility/artifact.",Proceedings of the ACM SIGCOMM 2022 Conference,86–100,15,"handover prediction, handover, 5G, energy, network measurement, mobility, application performance, coverage, mobility management, dataset","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
6,inproceedings,"Yuan, Xinjie and Wu, Mingzhou and Wang, Zhi and Zhu, Yifei and Ma, Ming and Guo, Junjian and Zhang, Zhi-Li and Zhu, Wenwu",Understanding 5G Performance for Real-World Services: A Content Provider's Perspective,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544219,10.1145/3544216.3544219,"5G has seen rapid growth recently, attracting several measurement studies on its coverage, connectivity and quality of service. However, there is still a lack of understanding of 5G's capabilities and potential impacts from a content provider (CP)'s perspective. This paper fills in this gap by studying 5G networks used by over 23 million users in one year in Kuaishou, a popular crowdsourced live streaming platform. Our measurements provide the following discoveries. i) Standalone (SA) 5G generally provides end-to-end performance improvement as compared with 4G or non-SA (NSA) 5G, but its advantage depends on both the number of cellular users and CP-level configurations. ii) In the radio access network, SA 5G is more sensitive to access density but has better handover tolerance. iii) Controlled experiments with 29 mobile device models on energy consumption refute some ""conventional wisdom,"" including that 5G always consumes more power. iv) Traceroute-based active experiments in over 300 cities show that although users are ""closer"" to the internet in SA 5G, their end-to-end latency may not benefit from that. Furthermore, we show new design space for 5G participants and provide a 5G-aware rebuffer strategy tested by 9 million viewers in Kuaishou, with a 7% reduction in rebuffer proportion.",Proceedings of the ACM SIGCOMM 2022 Conference,101–113,13,"content provider, core network, radio access network, 5G, SA 5G, crowdsourced live streaming, end-to-end performance","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
7,inproceedings,"Yang, Xinlei and Lin, Hao and Li, Zhenhua and Qian, Feng and Li, Xingyao and He, Zhiming and Wu, Xudong and Wang, Xianlong and Liu, Yunhao and Liao, Zhi and Hu, Daqiang and Xu, Tianyin","Mobile Access Bandwidth in Practice: Measurement, Analysis, and Implications",2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544237,10.1145/3544216.3544237,"Recent advances in mobile technologies such as 5G and WiFi 6E do not seem to deliver the promised mobile access bandwidth. To effectively characterize mobile access bandwidth in the wild, we work with a major commercial mobile bandwidth testing app to analyze mobile access bandwidths of 3.54M end users in China, based on fine-grained measurement and diagnostic information. Our analysis presents a surprising and frustrating fact---in the past two years, the average WiFi bandwidth remains largely unchanged, while the average 4G/5G bandwidth decreases remarkably. Our analysis further reveals the root causes---the bottlenecks in the underlying infrastructure (e.g., devices and wired Internet access) and side effects of aggressively migrating radio resources from 4G to 5G---with implications on closing the technology gaps. Additionally, our analysis provides insights on building ultra-fast, ultra-light bandwidth testing services (BTSes) at scale. Our new design dramatically reduces the test time of the commercial BTS from 10 seconds to 1 second on average, with a 15\texttimes{",Proceedings of the ACM SIGCOMM 2022 Conference,114–128,15,"bandwidth testing, access bandwidth, wifi network, LTE/5G network, mobile network","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
8,inproceedings,"Zhao, Jinghao and Tan, Zhaowei and Xu, Yifei and Zhang, Zhehui and Lu, Songwu",SEED: A SIM-Based Solution to 5G Failures,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544260,10.1145/3544216.3544260,"Failures in 5G mobile networks are becoming the norm with the ongoing global rollout. If left unattended, they affect mobile user experiences and the proper functioning of applications. In this work, we describe SEED, which offers a novel SIM-based solution to 5G failure diagnosis and handling. SEED infers failure causes by exploiting current standardized 5G error codes and decision-tree/online learning algorithms. It further takes corresponding multi-tier reset/redo actions (reset protocol operations, refresh outdated configurations, reload profiles, etc.) once the failure cause is inferred. SEED takes the operator's perspective in its design for fast deployment. SEED design works within the 5G standard framework and does not require changes on the device firmware or infrastructure hardware. Our evaluation has confirmed the viability of SEED.",Proceedings of the ACM SIGCOMM 2022 Conference,129–142,14,"mobile failure diagnosis, cellular failure, SIM-based failure diagnosis, 5G network","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
9,inproceedings,"Jain, Vivek and Chu, Hao-Tse and Qi, Shixiong and Lee, Chia-An and Chang, Hung-Cheng and Hsieh, Cheng-Ying and Ramakrishnan, K. K. and Chen, Jyh-Cheng",L25GC: A Low Latency 5G Core Network Based on High-Performance NFV Platforms,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544267,10.1145/3544216.3544267,"Cellular network control procedures (e.g., mobility, idle-active transition to conserve energy) directly influence data plane behavior, impacting user-experienced delay. Recognizing this control-data plane interdependence, L25GC re-architects the 5G Core (5GC) network, and its processing, to reduce latency of control plane operations and their impact on the data plane. Exploiting shared memory, L25GC eliminates message serialization and HTTP processing overheads, while being 3GPP-standards compliant. We improve data plane processing by factoring the functions to avoid control-data plane interference, and using scalable, flow-level packet classifiers for forwarding-rule lookups. Utilizing buffers at the 5GC, L25GC implements paging, and an intelligent handover scheme avoiding 3GPP's hairpin routing, and data loss caused by limited buffering at 5G base stations, reduces delay and unnecessary message processing. L25GC's integrated failure resiliency transparently recovers from failures of 5GC software network functions and hardware much faster than 3GPP's reattach recovery procedure. L25GC is built based on free5GC, an open-source kernel-based 5GC implementation. L25GC reduces event completion time by ~50% for several control plane events and improves data packet latency (due to improved control plane communication) by ~2\texttimes{",Proceedings of the ACM SIGCOMM 2022 Conference,143–157,15,"NFV, 5G cellular networks, low latency 5G core, cellular core","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
10,inproceedings,"Goyal, Prateesh and Narayan, Akshay and Cangialosi, Frank and Narayana, Srinivas and Alizadeh, Mohammad and Balakrishnan, Hari",Elasticity Detection: A Building Block for Internet Congestion Control,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544221,10.1145/3544216.3544221,"This paper introduces a new metric, ""elasticity,"" which characterizes the nature of cross-traffic competing with a flow. Elasticity captures whether the cross traffic reacts to changes in available bandwidth. We show that it is possible to robustly detect the elasticity of cross traffic at a sender without router support, and that elasticity detection can reduce delays in the Internet by enabling delay-controlling congestion control protocols to be deployed without hurting flow throughput. Our results show that the proposed method achieves more than 85% accuracy under a variety of network conditions, and that congestion control using elasticity detection achieves throughput comparable to Cubic but with delays that are 50--70 ms lower when cross traffic is inelastic.",Proceedings of the ACM SIGCOMM 2022 Conference,158–176,19,"elasticity detection, congestion control","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
11,inproceedings,"Arun, Venkat and Alizadeh, Mohammad and Balakrishnan, Hari",Starvation in End-to-End Congestion Control,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544223,10.1145/3544216.3544223,"To overcome weaknesses in traditional loss-based congestion control algorithms (CCAs), researchers have developed and deployed several delay-bounding CCAs that achieve high utilization without bloating delays (e.g., Vegas, FAST, BBR, PCC, Copa, etc.). When run on a path with a fixed bottleneck rate, these CCAs converge to a small delay range in equilibrium. This paper proves a surprising result: although designed to achieve reasonable inter-flow fairness, current methods to develop delay-bounding CCAs cannot always avoid starvation, an extreme form of unfairness. Starvation may occur when such a CCA runs on paths where non-congestive network delay variations due to real-world factors such as ACK aggregation and end-host scheduling exceed double the delay range that the CCA converges to in equilibrium. We provide experimental evidence for this result for BBR, PCC Vivace, and Copa with a link emulator. We discuss the implications of this result and posit that to guarantee no starvation an efficient delay-bounding CCA should design for a certain amount of non-congestive jitter and ensure that its equilibrium delay oscillations are at least one-half of this jitter.",Proceedings of the ACM SIGCOMM 2022 Conference,177–192,16,"delay-convergence, congestion control, starvation","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
12,inproceedings,"Meng, Zili and Guo, Yaning and Sun, Chen and Wang, Bo and Sherry, Justine and Liu, Hongqiang Harry and Xu, Mingwei",Achieving Consistent Low Latency for Wireless Real-Time Communications with the Shortest Control Loop,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544225,10.1145/3544216.3544225,"Real-time communication (RTC) applications like video conferencing or cloud gaming require consistent low latency to provide a seamless interactive experience. However, wireless networks including WiFi and cellular, albeit providing a satisfactory median latency, drastically degrade at the tail due to frequent and substantial wireless bandwidth fluctuations. We observe that the control loop for the sending rate of RTC applications is inflated when congestion happens at the wireless access point (AP), resulting in untimely rate adaption to wireless dynamics. Existing solutions, however, suffer from the inflated control loop and fail to quickly adapt to bandwidth fluctuations. In this paper, we propose Zhuge, a pure wireless AP based solution that reduces the control loop of RTC applications by separating congestion feedback from congested queues. We design a Fortune Teller to precisely estimate per-packet wireless latency upon its arrival at the wireless AP. To make Zhuge deployable at scale, we also design a Feedback Updater that translates the estimated latency to comprehensible feedback messages for various protocols and immediately delivers them back to senders for rate adaption. Trace-driven and real-world evaluation shows that Zhuge reduces the ratio of large tail latency and RTC performance degradation by 17% to 95%.",Proceedings of the ACM SIGCOMM 2022 Conference,193–206,14,"real-time communications, congestion control, wireless network","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
13,inproceedings,"Qureshi, Mubashir Adnan and Cheng, Yuchung and Yin, Qianwen and Fu, Qiaobin and Kumar, Gautam and Moshref, Masoud and Yan, Junhua and Jacobson, Van and Wetherall, David and Kabbani, Abdul",PLB: Congestion Signals Are Simple and Effective for Network Load Balancing,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544226,10.1145/3544216.3544226,"We present a new, host-based design for link load balancing and report the first experiences of link imbalance in datacenters. Our design, PLB (Protective Load Balancing), builds on transport protocols and ECMP/WCMP to reduce network hotspots. PLB randomly changes the paths of connections that experience congestion, preferring to repath after idle periods to minimize packet reordering. It repaths a connection by changing the IPv6 Flow Label on its packets, which switches include as part of ECMP/WCMP. Across hosts, this action drives down hotspots in the network, and lowers the latency of RPCs.PLB is used fleetwide at Google for TCP and Pony Express traffic. We could deploy it when other designs were infeasible because PLB requires only small transport modifications and switch configuration changes, and is backwards-compatible. It has produced excellent gains: the median utilization imbalance of highly-loaded ToR uplinks in Google datacenters fell by 60%, packet drops correspondingly fell by 33%, and the tail latency (99p) of small RPCs fell by 20%. PLB is also a general solution that works for settings from datacenters to backbone networks, as well as different transports.",Proceedings of the ACM SIGCOMM 2022 Conference,207–218,12,"datacenter fabric, congestion control, load balancing, distributed","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
14,inproceedings,"Yu, Liangcheng and Sonchack, John and Liu, Vincent",Cebinae: Scalable in-Network Fairness Augmentation,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544240,10.1145/3544216.3544240,"For public networks like the Internet and those of many clouds, end-host applications can use any congestion control protocol they wish. This protocol diversity and application autonomy are only increasing over time. While in-network support for fairness is an attractive solution for reigning in the inequity, existing solutions still have difficulty scaling to today's networks using today's devices. In this paper, we present Cebinae, a mechanism for augmenting existing networks of legacy hosts with penalties for flows that exceed their max-min fair share. Cebinae is compatible with all of the congestion control protocols in today's Internet, is deployable on commodity programmable switches, and scales orders of magnitude beyond existing alternatives.",Proceedings of the ACM SIGCOMM 2022 Conference,219–232,14,"congestion control, programmable networks, max-min fairness, P4","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
15,inproceedings,"Markovitch, Michael and Agarwal, Sharad and Fonseca, Rodrigo and Beckett, Ryan and Zhang, Chuanji and Atov, Irena and Chaturmohta, Somesh",TIPSY: Predicting Where Traffic Will Ingress a WAN,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544234,10.1145/3544216.3544234,"In addition to consumer workloads, public cloud providers host enterprise workloads such as video conferencing and AI+ML pipelines. Enterprise workloads can, at times, overwhelm the available ingress capacity on individual peering links. Traditional techniques to address this problem in the consumer setting do not always apply here, such as use of CDN caches in eyeball networks.Ingress congestion events necessitate shifting traffic to other peering links at short timescales. While content providers use such techniques in the egress direction, ingress is inherently a different and more challenging problem. Once a packet leaves an enterprise network, it is subject to opaque routing policies that influence the path to the cloud provider.We present TIPSY, a statistical-classification-based system for predicting the peering link through which a flow will enter a WAN. TIPSY's predictions are used to safely operate a congestion mitigation system that injects BGP withdrawal messages to redirect traffic away from congested peering links. We train TIPSY on traffic data from the Azure WAN, and we demonstrate 76% accuracy in predicting through which 3 peering links (out of thousands) a flow will enter the network after BGP withdrawals.",Proceedings of the ACM SIGCOMM 2022 Conference,233–249,17,"BGP, WAN, peering, statistical classification","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
16,inproceedings,"Ahuja, Satyajeet Singh and Dangui, Vinayak and Patil, Kirtesh and Somasundaram, Manikandan and Gupta, Varun and Sanchez, Mario and Yan, Guanqing and Noormohammadpour, Max and Razmjoo, Alaleh and Smith, Grace and Zhong, Hao and Triguna, Abhinav and Bali, Soshant and Xiang, Yuxiang and Chen, Yilun and Ganesan, Prabhakaran and Fernandez, Mikel Jimenez and Lapukhov, Petr and Liu, Guyue and Zhang, Ying",Network Entitlement: Contract-Based Network Sharing with Agility and SLO Guarantees,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544245,10.1145/3544216.3544245,"This paper presents Meta's Production Wide Area Network (WAN) Entitlement solution used by thousands of Meta's services to share the network safely and efficiently. We first introduce the Network Entitlement problem, i.e., how to share WAN bandwidth across services with flexibility and SLO guarantees. We present a new abstraction entitlement contract, which is stable, simple, and operationally friendly. The contract defines services' network quota and is set up between the network team and services teams to govern their obligations. Our framework includes two key parts: (1) an entitlement granting system that establishes an agile contract while achieving network efficiency and meeting long-term SLO guarantees, and (2) a large-scale distributed run-time enforcement system that enforces the contract on the production traffic. We demonstrate its effectiveness through extensive simulations and real-world end-to-end tests. The system has been deployed and operated for over two years in production. We hope that our years of experience provide a new angle to viewing WAN network sharing in production and will inspire follow-up research.",Proceedings of the ACM SIGCOMM 2022 Conference,250–263,14,"bandwidth sharing, network isolation, wide-area networks","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
17,inproceedings,"Uyeda, Frank and Alvidrez, Marc and Kline, Erik and Petrini, Bryce and Barritt, Brian and Mandle, David and Alexander, Aswin Chandy",SDN in the Stratosphere: Loon's Aerospace Mesh Network,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544231,10.1145/3544216.3544231,"The Loon project provided 4G LTE connectivity to under-served regions in emergency response and commercial mobile contexts using base stations carried by high-altitude balloons. To backhaul data, Loon orchestrated a moving mesh network of point-to-point radio links that interconnected balloons with each other and to ground infrastructure. This paper presents insights from 3 years of operational experience with Loon's mesh network above 3 continents.The challenging environment, comparable to many emerging non-terrestrial networks (NTNs), highlighted the design continuum between predictive optimization and reactive recovery. By forecasting the physical environment as a part of network planning, our novel Temporospatial SDN (TS-SDN) successfully moved from reactive to predictive recovery in many cases. We present insights on the following NTN concerns: connecting meshes of moving nodes using long distance, directional point-to-point links; employing a hybrid network control plane to balance performance and reliability; and understanding the behavior of a complex system spanning physical and logical domains in an inaccessible environment. The paper validates TS-SDN as a compelling architecture for orchestrating networks of moving platforms and steerable beams, and provides insights for those building similar networks in the future.",Proceedings of the ACM SIGCOMM 2022 Conference,264–280,17,"non-terrestrial network, balloon, project loon, minkowski, stratosphere, TS-SDN, temporospatial sdn","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
18,inproceedings,"Chen, Huangxun and Miao, Yukai and Chen, Li and Sun, Haifeng and Xu, Hong and Liu, Libin and Zhang, Gong and Wang, Wei",Software-Defined Network Assimilation: Bridging the Last Mile towards Centralized Network Configuration Management with NAssim,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544244,10.1145/3544216.3544244,"On-boarding new devices into an existing SDN network is a pain for network operations (NetOps) teams, because much expert effort is required to bridge the gap between the configuration models of the new devices and the unified data model in the SDN controller. In this work, we present an assistant framework NAssim, to help NetOps accelerate the process of assimilating a new device into a SDN network. Our solution features a unified parser framework to parse diverse device user manuals into preliminary configuration models, a rigorous validator that confirm the correctness of the models via formal syntax analysis, model hierarchy validation and empirical data validation, and a deep-learning-based mapping algorithm that uses state-of-the-art neural language processing techniques to produce human-comprehensible recommended mapping between the validated configuration model and the one in the SDN controller. In all, NAssim liberates the NetOps from most tedious tasks by learning directly from devices' manuals to produce data models which are comprehensible by both the SDN controller and human experts. Our evaluation shows, NAssim can accelerate the assimilation process by 9.1x. In this process, we also identify and correct 243 errors in four mainstream vendors' device manuals, and release a validated and expert-curated dataset of parsed manual corpus for future research.",Proceedings of the ACM SIGCOMM 2022 Conference,281–297,17,"multi-vendor networks, network configuration management, software-defined networks","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
19,inproceedings,"Li, Yuanjie and Li, Hewu and Liu, Wei and Liu, Lixin and Chen, Yimei and Wu, Jianping and Wu, Qian and Liu, Jun and Lai, Zeqi",A Case for Stateless Mobile Core Network Functions in Space,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544233,10.1145/3544216.3544233,"Is it worth and feasible to push mobile core network functions to low-earth-orbit (LEO) satellite mega-constellations? While this paradigm is being tested in space and promises new values, it also raises scalability, performance, and security concerns based on our study with datasets from operational satellites and 5G. A major challenge is today's stateful mobile core, which suffers from signaling storms in satellites' extreme mobility, intermittent failures in outer space, and attacks when unavoidably exposed to untrusted foreign locations. To this end, we make a case for a stateless mobile core in space. Our solution, SpaceCore, decouples states from orbital core functions, simplifies location states via geospatial addressing, eliminates unnecessary state migrations in satellite mobility by shifting to geospatial service areas, and localizes state retrievals with device-as-the-repository. Our evaluation with datasets from operational satellites and 5G shows SpaceCore's 17.5\texttimes{",Proceedings of the ACM SIGCOMM 2022 Conference,298–313,16,"5G and beyond, low-earth-orbit satellite mega-constellations, space network, mobile network, stateless core network functions","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
20,inproceedings,"Guo, Dong and Chen, Shenshen and Gao, Kai and Xiang, Qiao and Zhang, Ying and Yang, Y. Richard","Flash: Fast, Consistent Data Plane Verification for Large-Scale Network Settings",2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544246,10.1145/3544216.3544246,"Data plane verification can be an important technique to reduce network disruptions, and researchers have recently made significant progress in achieving fast data plane verification. However, as we apply existing data plane verification techniques to large-scale networks, two problems appear due to extremes. First, existing techniques cannot handle too-fast arrivals, which we call update storms, when a large number of data plane updates must be processed in a short time. Second, existing techniques cannot handle well too-slow arrivals, which we call long-tail update arrivals, when the updates from a number of switches take a long time to arrive.This paper presents Flash, a novel system that achieves fast, consistent data plane verification when update arrivals can include update storms, long-tail update arrivals, or both. In particular, Flash introduces a novel technique called fast inverse model transformation to swiftly transform a large block of rule updates to a block of conflict-free updates to efficiently handle update storms. Flash also introduces consistent, efficient, early detection, a systematic mechanism and associated novel algorithms to detect data plane violations with incomplete information, to avoid being delayed by long-tail arrivals. We fully implement Flash and conduct extensive evaluations under various settings. Using the data plane of a large-scale network, we show that compared with state-of-the-art sequential per-update verification systems, Flash is 9,000x faster.",Proceedings of the ACM SIGCOMM 2022 Conference,314–335,22,"network reliability, network monitoring, network verification","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
21,inproceedings,"Zhang, Peng and Wang, Dan and Gember-Jacobson, Aaron",Symbolic Router Execution,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544264,10.1145/3544216.3544264,"Network verification often requires analyzing properties across different spaces (header space, failure space, or their product) under different failure models (deterministic and/or probabilistic). Existing verifiers efficiently cover the header or failure space, but not both, and efficiently reason about deterministic or probabilistic failures, but not both. Consequently, no single verifier can support all analyses that require different space coverage and failure models. This paper introduces Symbolic Router Execution (SRE), a general and scalable verification engine that supports various analyses. SRE symbolically executes the network model to discover what we call packet failure equivalence classes (PFECs), each of which characterises a unique forwarding behavior across the product space of headers and failures. SRE enables various optimizations during the symbolic execution, while remaining agnostic of the failure model, so it scales to the product space in a general way. By using BDDs to encode symbolic headers and failures, various analyses reduce to graph algorithms (e.g., shortest-path) on the BDDs. Our evaluation using real and synthetic topologies show SRE achieves better or comparable performance when checking reachability, mining specifications, etc. compared to state-of-the-art methods.",Proceedings of the ACM SIGCOMM 2022 Conference,336–349,14,"network verification, symbolic execution, equivalence classes","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
22,inproceedings,"Zheng, Naiqian and Liu, Mengqi and Zhai, Ennan and Liu, Hongqiang Harry and Li, Yifan and Yang, Kaicheng and Liu, Xuanzhe and Jin, Xin",Meissa: Scalable Network Testing for Programmable Data Planes,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544247,10.1145/3544216.3544247,"Ensuring the correctness of programmable data planes is important. Testing offers comprehensive correctness checking, including detecting both code bugs and non-code bugs. However, scalability is a key challenge for testing production-scale data planes to achieve high coverage. This paper presents Meissa, a scalable network testing system for programmable data planes with full path coverage. The core of Meissa is a domain-specific code summary technique that simplifies the control flow graph of a data plane program for scalable testing without sacrificing coverage. Code summary decomposes a data plane program into individual pipelines, and summarizes each pipeline with a succinct representation. We formally prove that Meissa with code summary achieves 100% path coverage. We use both open-source and production-scale data plane programs to evaluate Meissa. The evaluation shows that (i) Meissa is able to test production-scale data plane programs that cannot be supported by state-of-the-art efforts, and (ii) besides P4 code bugs, Meissa is able to not only identify known non-code bugs, but also detect previously-unknown non-code bugs. We also share in this paper several real cases tested by Meissa in a production programmable data plane.",Proceedings of the ACM SIGCOMM 2022 Conference,350–364,15,"P4 testing, formal methods, programmable switches","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
23,inproceedings,"Albab, Kinan Dak and DiLorenzo, Jonathan and Heule, Stefan and Kheradmand, Ali and Smolka, Steffen and Weitz, Konstantin and Timarzi, Muhammad and Gao, Jiaqi and Yu, Minlan",SwitchV: Automated SDN Switch Validation with P4 Models,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544220,10.1145/3544216.3544220,"Increasing demand on computer networks continuously pushes manufacturers to incorporate novel features and capabilities into their switches at an ever-accelerating pace. However, the traditional approach to switch development relies on informal specifications and handcrafted tests to ensure reliability, which are tedious and slow to maintain and update, effectively putting feature velocity at odds with reliability.This work describes our experiences following a new approach during the development of switch software stacks that extend fixed-function ASICs with SDN capabilities. Specifically, we focus on SwitchV, our system for automated end-to-end switch validation using fuzzing and symbolic analysis, that evolves effortlessly with the switch specification. Our approach is centered around using the P4 language to model the data plane behavior of the switch as well as its control plane API. Such P4 models are then used as a formal specification by SwitchV, as well as a switch-agnostic contract by SDN controllers, and a living documentation by engineers.SwitchV found a total of 154 bugs spanning all switch layers. The majority of bugs were highly relevant and fixed within 14 days.",Proceedings of the ACM SIGCOMM 2022 Conference,365–379,15,"fuzzing, SDN switch validation, symbolic execution, PINS, automated test generation, SAI, P4 modeling, P4","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
24,inproceedings,"Li, Hejing and Li, Jialin and Kaufmann, Antoine",SimBricks: End-to-End Network System Evaluation with Modular Simulation,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544253,10.1145/3544216.3544253,"Full system ""end-to-end"" measurements in physical testbeds are the gold standard for network systems evaluation but are often not feasible. When physical testbeds are not available we frequently turn to simulation for evaluation. Unfortunately, existing simulators are insufficient for end-to-end evaluation, as they either cannot simulate all components, or simulate them with inadequate detail.We address this through modular simulation, flexibly combining and connecting multiple existing simulators for different components, including processor and memory, devices, and network, into virtual end-to-end testbeds tuned for each use-case. Our architecture, SimBricks, combines well-defined component interfaces for extensibility and modularity, efficient communication channels for local and distributed simulation, and a co-designed efficient synchronization mechanism for accurate timing across simulators. We demonstrate SimBricks scales to 1000 simulated hosts, each running a full software stack including Linux, and that it can simulate testbeds with existing NIC and switch RTL implementations. We also reproduce key findings from prior work in congestion control, NIC architecture, and in-network computing in SimBricks.",Proceedings of the ACM SIGCOMM 2022 Conference,380–396,17,"end-to-end evaluation, modular simulation, network systems","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
25,inproceedings,"Xia, Zhengxu and Zhou, Yajie and Yan, Francis Y. and Jiang, Junchen",Genet: Automatic Curriculum Generation for Learning Adaptation in Networking,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544243,10.1145/3544216.3544243,"As deep reinforcement learning (RL) showcases its strengths in networking, its pitfalls are also coming to the public's attention. Training on a wide range of network environments leads to suboptimal performance, whereas training on a narrow distribution of environments results in poor generalization.This work presents Genet, a new training framework for learning better RL-based network adaptation algorithms. Genet is built on curriculum learning, which has proved effective against similar issues in other RL applications. At a high level, curriculum learning gradually feeds more ""difficult"" environments to the training rather than choosing them uniformly at random. However, applying curriculum learning in networking is nontrivial since the ""difficulty"" of a network environment is unknown. Our insight is to leverage traditional rule-based (non-RL) baselines: If the current RL model performs significantly worse in a network environment than the rule-based baselines, then further training it in this environment tends to bring substantial improvement. Genet automatically searches for such environments and iteratively promotes them to training. Three case studies---adaptive video streaming, congestion control, and load balancing---demonstrate that Genet produces RL policies that outperform both regularly trained RL policies and traditional baselines.",Proceedings of the ACM SIGCOMM 2022 Conference,397–413,17,"reinforcement learning, curriculum learning, adaptive bitrate video, network adaptation, congestion control","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
26,inproceedings,"Zhang, Junxue and Zeng, Chaoliang and Zhang, Hong and Hu, Shuihai and Chen, Kai",LiteFlow: Towards High-Performance Adaptive Neural Networks for Kernel Datapath,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544229,10.1145/3544216.3544229,"Adaptive neural networks (NN) have been used to optimize OS kernel datapath functions because they can achieve superior performance under changing environments. However, how to deploy these NNs remains a challenge. One approach is to deploy these adaptive NNs in the userspace. However, such userspace deployments suffer from either high cross-space communication overhead or low responsiveness, significantly compromising the function performance. On the other hand, pure kernel-space deployments also incur a large performance degradation because the computation logic of model tuning algorithm is typically complex, interfering with the performance of normal datapath execution.This paper presents LiteFlow, a hybrid solution to build high-performance adaptive NNs for kernel datapath. At its core, LiteFlow decouples the control path of adaptive NNs into: (1) a kernel-space fast path for efficient model inference, and (2) a userspace slow path for effective model tuning. We have implemented LiteFlow with Linux kernel datapath and evaluated it with three popular datapath functions including congestion control, flow scheduling, and load balancing. Compared to prior works, LiteFlow achieves 44.4% better goodput for congestion control, and improves the completion time for long flows by 33.7% and 56.7% for flow scheduling and load balancing, respectively.",Proceedings of the ACM SIGCOMM 2022 Conference,414–427,14,"adaptive neural network, kernel datapath, deployment","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
27,inproceedings,"Zhao, Yihao and Liu, Yuanqiang and Peng, Yanghua and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin",Multi-Resource Interleaving for Deep Learning Training,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544224,10.1145/3544216.3544224,"Training Deep Learning (DL) model requires multiple resource types, including CPUs, GPUs, storage IO, and network IO. Advancements in DL have produced a wide spectrum of models that have diverse usage patterns on different resource types. Existing DL schedulers focus on only GPU allocation, while missing the opportunity of packing jobs along multiple resource types.We present Muri, a multi-resource cluster scheduler for DL workloads. Muri exploits multi-resource interleaving of DL training jobs to achieve high resource utilization and reduce job completion time (JCT). DL jobs have a unique staged, iterative computation pattern. In contrast to multi-resource schedulers for big data workloads that pack jobs in the space dimension, Muri leverages this unique pattern to interleave jobs on the same set of resources in the time dimension. Muri adapts Blossom algorithm to find the perfect grouping plan for single-GPU jobs with two resource types, and generalizes the algorithm to handle multi-GPU jobs with more than two types. We build a prototype of Muri and integrate it with PyTorch. Experiments on a cluster with 64 GPUs demonstrate that Muri improves the average JCT by up to 3.6\texttimes{",Proceedings of the ACM SIGCOMM 2022 Conference,428–440,13,"resource sharing, deep learning","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
28,inproceedings,"Yang, Qingqing and Peng, Xi and Chen, Li and Liu, Libin and Zhang, Jingze and Xu, Hong and Li, Baochun and Zhang, Gong",DeepQueueNet: Towards Scalable and Generalized Network Performance Estimation with Packet-Level Visibility,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544248,10.1145/3544216.3544248,"Network simulators are an essential tool for network operators, and can assist important tasks such as capacity planning, topology design, and parameter tuning. Popular simulators are all based on discrete event simulation, and their performance does not scale with the size of modern networks. Recently, deep-learning-based techniques are introduced to solve the scalability problem, but, as we show with experiments, they have poor visibility in their simulation results, and cannot generalize to diverse scenarios. In this work, we combine scalable and generalized continuous simulation techniques with discrete event simulation to achieve high scalability, while providing packet-level visibility. We start from a solid queueing-theoretic modeling of modern networks, and carefully identify the mathematically-intractable or computationally-expensive parts, only which are then modeled using deep neural networks (DNN). Dubbed DeepQueueNet, our approach combines prior knowledge of networks, and supports arbitrary topology and device traffic management mechanisms (given sufficient training data). Our extensive experiments show that DeepQueueNet achieves near-linear speedup in the number of GPUs, and its estimation accuracy for average and 99th percentile round-trip time outperforms existing end-to-end DNN-based performance estimators in all scenarios.",Proceedings of the ACM SIGCOMM 2022 Conference,441–457,17,"network simulation, machine learning, network performance estimation, network modeling, queueing theory","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
29,inproceedings,"Yin, Yucheng and Lin, Zinan and Jin, Minhao and Fanti, Giulia and Sekar, Vyas",Practical GAN-Based Synthetic IP Header Trace Generation Using NetShare,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544251,10.1145/3544216.3544251,"We explore the feasibility of using Generative Adversarial Networks (GANs) to automatically learn generative models to generate synthetic packet- and flow header traces for networking tasks (e.g., telemetry, anomaly detection, provisioning). We identify key fidelity, scalability, and privacy challenges and tradeoffs in existing GAN-based approaches. By synthesizing domain-specific insights with recent advances in machine learning and privacy, we identify design choices to tackle these challenges. Building on these insights, we develop an end-to-end framework, NetShare. We evaluate NetShare on six diverse packet header traces and find that: (1) across all distributional metrics and traces, it achieves 46% more accuracy than baselines and (2) it meets users' requirements of downstream tasks in evaluating accuracy and rank ordering of candidate approaches.",Proceedings of the ACM SIGCOMM 2022 Conference,458–472,15,"network flows, privacy, network packets, generative adversarial networks, synthetic data generation","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
30,inproceedings,"Sengupta, Satadal and Kim, Hyojoon and Rexford, Jennifer",Continuous In-Network Round-Trip Time Monitoring,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544222,10.1145/3544216.3544222,"Round-trip time (RTT) is a central metric that influences end-user QoE and can expose traffic-interception attacks. Many popular RTT monitoring techniques either send active probes (that do not capture application-level RTTs) or passively monitor only the TCP handshake (which can be inaccurate, especially for long-lived flows). High-speed programmable switches present a unique opportunity to monitor the RTTs continuously and react in real time to improve performance and security. In this paper, we present Dart, an inline, real-time, and continuous RTT measurement system that can enable automated detection of network events and adapt (e.g., routing, scheduling, marking, or dropping traffic) inside the network. However, designing Dart is fraught with challenges, due to the idiosyncrasies of the TCP protocol and the resource constraints in high-speed switches. Dart overcomes these challenges by strategically limiting the tracking of packets to only those that can generate useful RTT samples, and by identifying the synergy between per-flow state and per-packet state for efficient memory use. We present a P4 prototype of Dart for the Tofino switch, as well our experiments on a campus testbed and simulations using anonymized campus traces. Dart, running in real time and with limited data-plane memory, is able to collect 99% of the RTT samples of an offline, software baseline---a variant of the popular tcptrace tool that has access to unlimited memory.",Proceedings of the ACM SIGCOMM 2022 Conference,473–485,13,"highspeed programmable switch, passive measurement, network monitoring, round-trip time","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
31,inproceedings,"Zheng, Hao and Tian, Chen and Yang, Tong and Lin, Huiping and Liu, Chang and Zhang, Zhaochen and Dou, Wanchun and Chen, Guihai",FlyMon: Enabling on-the-Fly Task Reconfiguration for Network Measurement,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544239,10.1145/3544216.3544239,"Network measurement is important to data center operators. Most existing efforts focus on developing new implementation schemes for measurement tasks. Little attention is paid to on-the-fly task reconfiguration. Due to resource constraints, it is impossible to configure all needed tasks at start-up and dynamically turn on/of them. To support real-time reconfiguration of many different tasks, a key observation is that it is unnecessary to bind a task and its implementation at the compilation phase. We design FlyMon, the first sketch-based measurement system that can make on-the-fly reconfigurations on a large set of measurement tasks. FlyMon introduces the concept of Composable Measurement Units (CMUs), which are general operation units that support reconfigurable implementation for measurement tasks combined from different flow keys and flow attributes. FlyMon maps the design of CMUs to programmable switches' data planes so that the number of compacted CMUs can be maximized. FlyMon also provides dynamic memory management. We prototype FlyMon on Tofino and currently enable four frequently used flow attributes. Each CMU Group (with 3 CMUs) can concurrently perform up to 96 isolated measurement tasks with less than 8.3% hardware resources. The tasks can be deployed with configurable memory size at the millisecond level. By cross-stacking, FlyMon can deploy up to 27 CMUs in one pipeline of Tofino.",Proceedings of the ACM SIGCOMM 2022 Conference,486–502,17,"network monitoring, programmable switch, sketching","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
32,inproceedings,"Izhikevich, Liz and Teixeira, Renata and Durumeric, Zakir",Predicting IPv4 Services across All Ports,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544249,10.1145/3544216.3544249,"Internet-wide scanning is commonly used to understand the topology and security of the Internet. However, IPv4 Internet scans have been limited to scanning only a subset of services---exhaustively scanning all IPv4 services is too costly and no existing bandwidth-saving frameworks are designed to scan IPv4 addresses across all ports. In this work we introduce GPS, a system that efficiently discovers Internet services across all ports. GPS runs a predictive framework that learns from extremely small sample sizes and is highly parallelizable, allowing it to quickly find patterns between services across all 65K ports and a myriad of features. GPS computes service predictions in 13 minutes (four orders of magnitude faster than prior work) and finds 92.5% of services across all ports with 131\texttimes{",Proceedings of the ACM SIGCOMM 2022 Conference,503–515,13,"measurement, internet scanning, prediction, IPv4","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
33,inproceedings,"Lei, Yiran and Yu, Liangcheng and Liu, Vincent and Xu, Mingwei",PrintQueue: Performance Diagnosis via Queue Measurement in the Data Plane,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544257,10.1145/3544216.3544257,"When diagnosing performance anomalies, it is often useful to reason about why a packet experienced the queuing that it did. To that end, we observe that queuing is both a result of historical effects and the current state of the network. Further, both factors involve short and long timescales by nature. Existing work fails to provide insight that satisfies all of these needs.This paper presents PrintQueue, a practical data-plane monitoring system for tracking the provenance of packet-level delays at both small and large timescales. We propose a set of metrics for describing 'congestion regimes' and present a set of novel data-plane data structures that accurately track those metrics over arbitrary time spans. We implement PrintQueue on a Tofino switch and evaluate it with multiple network traces. Our evaluation shows that the accuracy of PrintQueue is up to 3\texttimes{",Proceedings of the ACM SIGCOMM 2022 Conference,516–529,14,"queue measurement, data plane, programmable networks","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
34,inproceedings,"Wan, Gerry and Gong, Fengchen and Barbette, Tom and Durumeric, Zakir",Retina: Analyzing 100GbE Traffic on Commodity Hardware,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544227,10.1145/3544216.3544227,"As network speeds have increased to over 100 Gbps, operators and researchers have lost the ability to easily ask complex questions of reassembled and parsed network traffic. In this paper, we introduce Retina, a software framework that lets users analyze over 100 Gbps of real-world traffic on a single server with no specialized hardware. Retina supports running arbitrary user-defined analysis functions on a wide variety of extensible data representations ranging from raw packets to parsed application-layer handshakes. We introduce a novel filtering mechanism and subscription interface to safely and efficiently process high-speed traffic. Under the hood, Retina implements an efficient data pipeline that strategically discards unneeded traffic and defers expensive processing operations to preserve computation for complex analyses. We present the framework architecture, evaluate its performance on production traffic, and explore several applications. Our experiments show that Retina is capable of running sophisticated analyses at over 100 Gbps on a single commodity server and can support 5--100\texttimes{",Proceedings of the ACM SIGCOMM 2022 Conference,530–544,15,"100 GbE, traffic analysis, internet measurement","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
35,inproceedings,"Chen, Tuochao and Chan, Justin and Gollakota, Shyamnath",Underwater Messaging Using Mobile Devices,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544258,10.1145/3544216.3544258,"Since its inception, underwater digital acoustic communication has required custom hardware that neither has the economies of scale nor is pervasive. We present the first acoustic system that brings underwater messaging capabilities to existing mobile devices like smartphones and smart watches. Our software-only solution leverages audio sensors, i.e., microphones and speakers, ubiquitous in today's devices to enable acoustic underwater communication between mobile devices. To achieve this, we design a communication system that in real-time adapts to differences in frequency responses across mobile devices, changes in multipath and noise levels at different locations and dynamic channel changes due to mobility. We evaluate our system in six different real-world underwater environments with depths of 2--15 m in the presence of boats, ships and people fishing and kayaking. Our results show that our system can in real-time adapt its frequency band and achieve bit rates of 100 bps to 1.8 kbps and a range of 30 m. By using a lower bit rate of 10--20 bps, we can further increase the range to 100 m. As smartphones and watches are increasingly being used in underwater scenarios, our software-based approach has the potential to make underwater messaging capabilities widely available to anyone with a mobile device.Project page with open-source code and data can be found here: https://underwatermessaging.cs.washington.edu/",Proceedings of the ACM SIGCOMM 2022 Conference,545–559,15,"smart watches, underwater exploration, underwater communication, SOS beacons, mobile phones, ocean sciences","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
36,inproceedings,"Gong, Zheng and Han, Lubing and An, Zhenlin and Yang, Lei and Ding, Siqi and Xiang, Yu",Empowering Smart Buildings with Self-Sensing Concrete for Structural Health Monitoring,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544270,10.1145/3544216.3544270,"Given the increasing number of building collapse tragedies nowadays (e.g., Florida condo collapse), people gradually recognize that long-term and persistent structural health monitoring (SHM) becomes indispensable for civilian buildings. However, current SHM techniques suffer from high cost and deployment difficulty caused by the wired connection. Traditional wireless sensor networks fail to serve in-concrete communication for SHM because of the complexity of battery replacement and the concrete Faraday cage. In this work, we collaborate with experts from civil engineering to create a type of promising self-sensing concrete by introducing a novel functional filler, called EcoCapsule- a battery-free and miniature piezoelectric backscatter node. We overcome the fundamental challenges in in-concrete energy harvesting and wireless communication to achieve SHM via EcoCapsules. We prototype EcoCapsules and mix them with other raw materials (such as cement, sand, water, etc) to cast the self-sensing concrete, into which EcoCapsules are implanted permanently. We tested EcoCapsules regarding real-world buildings comprehensively. Our results demonstrate single link throughputs of up to 13 kbps and power-up ranges of up to 6 m. Finally, we demonstrate a long-term pilot study on the structural health monitoring of a real-life footbridge.",Proceedings of the ACM SIGCOMM 2022 Conference,560–575,16,"structural health monitoring, ultrasonics, backscatter communication","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
37,inproceedings,"Oppermann, Peter and Renner, Christian",Higher-Order Modulation for Acoustic Backscatter Communication in Metals,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544261,10.1145/3544216.3544261,"Backscatter communication enables miniature, batteryless, and low-cost wireless sensors. Since electromagnetic waves are strongly attenuated in several scenarios, backscatter communication in metals via acoustic waves can leverage various applications, e. g., in structural health monitoring. When backscattering, the Tag has little control over the modulation it performs on the carrier wave. Therefore, existing approaches commonly employ differential binary modulation schemes, limiting the achievable data rates. To overcome this limitation, we derive a channel model that accurately describes the modulation in an acoustic backscatter channel---as, e. g., found in steel beams---and leverage it to achieve higher-order load modulation. We present an open-source Reader and Tag pair prototype based on COTS components that we have developed for communication and on-the-fly channel characterization. We explore the influence of various parameters on communication performance on different channels. Moreover, (i) we are the first to demonstrate that acoustic backscatter is feasible in guided-wave channels, covering up to 3 meters, and that (ii) our modulation scheme achieves up to 211% higher data rates than binary modulation schemes, and (iii) provides reliable communication through channel coding.",Proceedings of the ACM SIGCOMM 2022 Conference,576–587,12,"wireless communication, through-metal, battery-free, acoustic backscatter","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
38,inproceedings,"Shenoy, Jayanth and Liu, Zikun and Tao, Bill and Kabelac, Zachary and Vasisht, Deepak",RF-Protect: Privacy against Device-Free Human Tracking,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544256,10.1145/3544216.3544256,"The advent of radio sensing that works through walls & obstacles challenges the notion of indoor privacy. An eavesdropper can deploy such sensing to snoop on their neighbors and a smart sensor embedded with such sensing capabilities can perform large scale behavioral and health data mining. We present RF-Protect, a new framework that enables privacy by injecting fake humans in the sensed data. RF-Protect consists of a novel hardware reflector design that modifies radio waves to create reflections at arbitrary locations in the environment and a new generative mechanism to create realistic human trajectories. RF-Protect's design doesn't require any high bandwidth hardware or physical motion. We implement RF-Protect using commodity hardware and validate its ability to generate fake human trajectories.",Proceedings of the ACM SIGCOMM 2022 Conference,588–600,13,"IoT, wireless, sensing, RF, embedded systems, privacy","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
39,inproceedings,"Gupta, Himanshu and Curran, Max and Longtin, Jon and Rockwell, Torin and Zheng, Kai and Dasari, Mallesham",Cyclops: An FSO-Based Wireless Link for VR Headsets,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544255,10.1145/3544216.3544255,"The ultimate goal of virtual reality (VR) is to create an experience indistinguishable from actual reality. To provide such a ""life-like"" experience, (i) the VR headset (VRH) should be wireless so that the user can move around freely, and (ii) the wireless link, connecting the VRH to a high-performance renderer, should support high data rates (tens to hundreds of Gbps). Industry is already pushing towards such wireless VRHs; however, these wireless links can only support a few Gbps rates. In general, current radio-frequency (RF) links (including mmWave) are not able to provide desired data rates. In this paper, we build a system, we call Cyclops, which uses free-space optical (FSO) technology to create a high-bandwidth VR wireless link. FSO links are capable of very high data rates (up to Tbps) due to the high frequencies of light waves and narrow beams. The main challenges in developing an effective FSO link are: (i) designing a link with sufficient movement tolerance, and (ii) developing a viable tracking and pointing (TP) mechanism which maintains the link while the VRH moves. As traditional TP approaches seem infeasible in our context, we develop a novel TP approach based on learning techniques, leveraging the VRH's inbuilt tracking system. We build robust 10 Gbps and 25Gbps link prototypes from commodity components, demonstrate their viability for expected movement speeds of a VRH, and show that, with certain custom-built components, we can support much higher movement speeds and bandwidths.",Proceedings of the ACM SIGCOMM 2022 Conference,601–614,14,"wireless link, tracking and pointing, virtual reality, free-space optics","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
40,inproceedings,"Wang, Shuai and Gao, Kaihui and Qian, Kun and Li, Dan and Miao, Rui and Li, Bo and Zhou, Yu and Zhai, Ennan and Sun, Chen and Gao, Jiaqi and Zhang, Dai and Fu, Binzhang and Kelly, Frank and Cai, Dennis and Liu, Hongqiang Harry and Zhang, Ming",Predictable VFabric on Informative Data Plane,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544241,10.1145/3544216.3544241,"In multi-tenant data centers, each tenant desires reassuring predictability from the virtual network fabric - bandwidth guarantee, work conservation, and bounded tail latency. Achieving these goals simultaneously relies on rapid and precise traffic admission. However, the slow convergence (tens of milliseconds) of prior works can hardly satisfy the increasingly rigorous performance demand under dynamic traffic patterns. Further, state-of-the-art load balance schemes are all guarantee-agnostic and bring great risks on breaking bandwidth guarantee, which is overlooked in prior works.In this paper, we propose μFab, a predictable virtual fabric solution which can (1) explicitly select proper paths for all flows and (2) converge to ideal bandwidth allocation at sub-millisecond timescales. The core idea of μFab is to leverage the programmable data plane to build a fusion of an active edge (e.g., NIC) and an informative core (e.g., switch), where the core sends link status and tenant information to the edge via telemetry to help the latter make a timely and accurate decision on path selection and traffic admission. We fully implement μFab with commodity SmartNICs and programmable switches. Evaluations show that μFab can keep minimum bandwidth guarantee with high bandwidth utilization and near-optimal transmission latency in various network situations with limited probing bandwidth overhead. Application-level experiments, e.g., compute and storage scenarios, show that μFab can improve QPS by 2.5\texttimes{",Proceedings of the ACM SIGCOMM 2022 Conference,615–632,18,"programmable data plane, performance isolation","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
41,inproceedings,"Yang, Mingran and Baban, Alex and Kugel, Valery and Libby, Jeff and Mackie, Scott and Kananda, Swamy Sadashivaiah Renu and Wu, Chang-Hong and Ghobadi, Manya",Using Trio: Juniper Networks' Programmable Chipset - for Emerging in-Network Applications,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544262,10.1145/3544216.3544262,"This paper describes Trio, a programmable chipset used in Juniper Networks' MX-series routers and switches. Trio's architecture is based on a multi-threaded programmable packet processing engine and a hierarchy of high-capacity memory systems, making it fundamentally different from pipeline-based architectures. Trio gracefully handles non-homogeneous packet processing rates for a wide range of networking use cases and protocols, making it an ideal platform for emerging in-network applications. We begin by describing the Trio chipset's fundamental building blocks, including its multi-threaded Packet Forwarding and Packet Processing Engines. We then discuss Trio's programming language, called Microcode. To showcase Trio's flexible Microcode-based programming environment, we describe two use cases. First, we demonstrate Trio's ability to perform in-network aggregation for distributed machine learning. Second, we propose and design an in-network straggler mitigation technique using Trio's timer threads. We prototype both use cases on a testbed using three real DNN models (ResNet50, DenseNet161, and VGG11) to demonstrate Trio's ability to mitigate stragglers while performing in-network aggregation. Our evaluations show that when stragglers occur in the cluster, Trio outperforms today's pipeline-based solutions by up to 1.8x.",Proceedings of the ACM SIGCOMM 2022 Conference,633–648,16,"network hardware design, network support for machine learning, programmable dataplanes","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
42,inproceedings,"Shrivastav, Vishal",Programmable Multi-Dimensional Table Filters for Line Rate Network Functions,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544266,10.1145/3544216.3544266,"The ability to filter entries in the data plane from a set or table of resources (e.g., network paths, servers, switch ports) based on multi-dimensional policies over stateful resource-specific metrics (e.g., filter paths with utilization < 0.6 and latency < 3us) is critical for several key network functions, such as performance-aware routing, resource-aware load balancing, network diagnosis, security and firewall. However, current generation of programmable switches do not support table-wide stateful filtering at line rate. We present Thanos, which augments the existing programmable switch pipeline with support for programmable multi-dimensional filtering over a set of resources. Thanos seamlessly integrates with multi-terabit programmable switch pipelines at nominal chip area overhead. Our evaluation, based on an FPGA prototype and a simulator, shows that policies expressed in Thanos can improve the performance of key network functions by up to 1.7\texttimes{",Proceedings of the ACM SIGCOMM 2022 Conference,649–662,14,"switch architecture, programmable networks","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
43,inproceedings,"Shrivastav, Vishal",Stateful Multi-Pipelined Programmable Switches,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544269,10.1145/3544216.3544269,"Given the clock rate of a single packet processing pipeline has saturated due to slowdown in transistor scaling, today's programmable switches employ multiple parallel pipelines to meet high packet processing rates. However, parallel processing poses a challenge for stateful packet processing, where it becomes hard to guarantee functional correctness while maintaining line rate processing. This paper presents the design and implementation of MP5, which is a new switch architecture, compiler, and runtime for multi-pipelined programmable switches that is functionally equivalent to a logical single pipelined switch while also processing packets close to the ideal processing rate, for all packet processing programs.",Proceedings of the ACM SIGCOMM 2022 Conference,663–676,14,"switch architecture, programmable networks","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
44,inproceedings,"Molero, Edgar Costa and Vissicchio, Stefano and Vanbever, Laurent",FAst In-Network GraY Failure Detection for ISPs,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544242,10.1145/3544216.3544242,"Avoiding packet loss is crucial for ISPs. Unfortunately, malfunctioning hardware at ISPs can cause long-lasting packet drops, also known as gray failures, which are undetectable by existing monitoring tools.In this paper, we describe the design and implementation of FANcY, an ISP-targeted system that detects and localizes gray failures quickly and accurately. FANcY complements previous monitoring approaches, which are mainly tailored for low-delay networks such as data center networks and do not work at ISP scale. We experimentally confirm FANcY's capability to accurately detect gray failures in seconds, as long as only tiny fractions of traffic experience losses. We also implement FANcY in an Intel Tofino switch, demonstrating how it enables fine-grained fast rerouting.",Proceedings of the ACM SIGCOMM 2022 Conference,677–692,16,"measurements, network hardware, programmable data planes, failure detection","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
45,inproceedings,"Alcoz, Albert Gran and Strohmeier, Martin and Lenders, Vincent and Vanbever, Laurent",Aggregate-Based Congestion Control for Pulse-Wave DDoS Defense,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544263,10.1145/3544216.3544263,"Pulse-wave DDoS attacks are a new type of volumetric attack formed by short, high-rate traffic pulses. Such attacks target the Achilles' heel of state-of-the-art DDoS defenses: their reaction time. By continuously adapting their attack vectors, pulse-wave attacks manage to render existing defenses ineffective.In this paper, we leverage programmable switches to build an in-network DDoS defense effective against pulse-wave attacks. To do so, we revisit Aggregate-based Congestion Control (ACC): a mechanism proposed two decades ago to manage congestion events caused by high-bandwidth traffic aggregates. While ACC proved efficient in inferring and controlling DDoS attacks, it cannot keep up with the speed requirements of pulse-wave attacks.We propose ACC-Turbo, a renewed version of ACC that infers attack patterns by applying online-clustering techniques in the network and mitigates them by leveraging programmable packet scheduling. By doing so, ACC-Turbo identifies attacks at line rate and in real-time, and rate-limits attack traffic on a per-packet basis.We fully implement ACC-Turbo in P4 and evaluate it on a wide range of attack scenarios. Our evaluation shows that ACC-Turbo autonomously identifies DDoS attack vectors in an unsupervised manner and rapidly mitigates pulse-wave DDoS attacks. We also show that ACC-Turbo runs on existing hardware (Intel Tofino).",Proceedings of the ACM SIGCOMM 2022 Conference,693–706,14,"aggregate-based congestion control, ACC, programmable scheduling, network defenses, pulse-wave DDoS, DDoS, network security","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
46,inproceedings,"Wichtlhuber, Matthias and Strehle, Eric and Kopp, Daniel and Prepens, Lars and Stegmueller, Stefan and Rubina, Alina and Dietzel, Christoph and Hohlfeld, Oliver",IXP Scrubber: Learning from Blackholing Traffic for ML-Driven DDoS Detection at Scale,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544268,10.1145/3544216.3544268,"Distributed Denial of Service (DDoS) attacks are among the most critical cybersecurity threats, jeopardizing the stability of even the largest networks and services. The existing range of mitigation services predominantly filters at the edge of the Internet, thus creating unnecessary burden for network infrastructures. Consequently, we present IXP Scrubber, a Machine Learning (ML) based system for detecting and filtering DDoS traffic at the core of the Internet at Internet Exchange Points (IXPs) which see large volumes and varieties of DDoS. IXP Scrubber continuously learns DDoS traffic properties from neighboring Autonomous Systems (ASes). It utilizes BGP signals to drop traffic for certain routes (blackholing) to sample DDoS and can thus learn new attack vectors without the operator's intervention and on unprecedented amounts of training data. We present three major contributions: i) a method to semi-automatically generate arbitrarily large amounts of labeled DDoS training data from IXPs' sampled packet traces, ii) the novel, controllable, locally explainable and highly precise two-step IXP Scrubber ML model, and iii) an evaluation of the IXP Scrubber ML model, including its temporal and geographical drift, based on data from 5 IXPs covering a time span of up to two years.",Proceedings of the ACM SIGCOMM 2022 Conference,707–722,16,"traffic classification, denial of service, machine learning","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
47,inproceedings,"Atre, Nirav and Sadok, Hugo and Chiang, Erica and Wang, Weina and Sherry, Justine",SurgeProtector: Mitigating Temporal Algorithmic Complexity Attacks Using Adversarial Scheduling,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544250,10.1145/3544216.3544250,"Denial-of-Service (DoS) attacks are the bane of public-facing network deployments. Algorithmic complexity attacks (ACAs) are a class of DoS attacks where an attacker uses a small amount of adversarial traffic to induce a large amount of work in the target system, pushing the system into overload and causing it to drop packets from innocent users. ACAs are particularly dangerous because, unlike volumetric DoS attacks, ACAs don't require a significant network bandwidth investment from the attacker Today, network functions (NFs) on the Internet must be designed and engineered on a case-by-case basis to mitigate the debilitating impact of ACAs. Further, the resulting designs tend to be overly conservative in their attack mitigation strategy, limiting the innocent traffic that the NF can serve under common-case operation.In this work, we propose a more general framework to make NFs resilient to ACAs. Our framework, SurgeProtector, uses the NF's scheduler to mitigate the impact of ACAs using a very traditional scheduling algorithm: Weighted Shortest Job First (WSJF). To evaluate SurgeProtector, we propose a new metric of vulnerability called the Displacement Factor (DF), which quantifies the 'harm per unit effort' that an adversary can inflict on the system. We provide novel, adversarial analysis of WSJF and show that any system using this policy has a worst-case DF of only a small constant, where traditional schedulers place no upper bound on the DF. Illustrating that SurgeProtector is not only theoretically, but practically robust, we integrate SurgeProtector into an open source intrusion detection system (IDS). Under simulated attack, the SurgeProtector-augmented IDS suffers 90--99% lower innocent traffic loss than the original system.",Proceedings of the ACM SIGCOMM 2022 Conference,723–738,16,"algorithmic complexity attacks, network security, pigasus, surgeprotector, WSJF, adversarial scheduling","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
48,inproceedings,"Trautwein, Dennis and Raman, Aravindh and Tyson, Gareth and Castro, Ignacio and Scott, Will and Schubotz, Moritz and Gipp, Bela and Psaras, Yiannis",Design and Evaluation of IPFS: A Storage Layer for the Decentralized Web,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544232,10.1145/3544216.3544232,"Recent years have witnessed growing consolidation of web operations. For example, the majority of web traffic now originates from a few organizations, and even micro-websites often choose to host on large pre-existing cloud infrastructures. In response to this, the ""Decentralized Web"" attempts to distribute ownership and operation of web services more evenly. This paper describes the design and implementation of the largest and most widely used Decentralized Web platform --- the InterPlanetary File System (IPFS) --- an open-source, content-addressable peer-to-peer network that provides distributed data storage and delivery. IPFS has millions of daily content retrievals and already underpins dozens of third-party applications. This paper evaluates the performance of IPFS by introducing a set of measurement methodologies that allow us to uncover the characteristics of peers in the IPFS network. We reveal presence in more than 2700 Autonomous Systems and 152 countries, the majority of which operate outside large central cloud providers like Amazon or Azure. We further evaluate IPFS performance, showing that both publication and retrieval delays are acceptable for a wide range of use cases. Finally, we share our datasets, experiences and lessons learned.",Proceedings of the ACM SIGCOMM 2022 Conference,739–752,14,"interplanetary file system, content addressable storage, libp2p, content addressing, decentralized web","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
49,inproceedings,"Miao, Rui and Zhu, Lingjun and Ma, Shu and Qian, Kun and Zhuang, Shujun and Li, Bo and Cheng, Shuguang and Gao, Jiaqi and Zhuang, Yan and Zhang, Pengcheng and Liu, Rong and Shi, Chao and Fu, Binzhang and Zhu, Jiaji and Wu, Jiesheng and Cai, Dennis and Liu, Hongqiang Harry",From Luna to Solar: The Evolutions of the Compute-to-Storage Networks in Alibaba Cloud,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544238,10.1145/3544216.3544238,"This paper presents the two generations of storage network stacks that reduced the average I/O latency of Alibaba Cloud's EBS service by 72% in the last five years: Luna, a user-space TCP stack that corresponds the latency of network to the speed of SSD; and Solar, a storage-oriented UDP stack that enables both storage and network hardware accelerations.Luna is our first step towards a high-speed compute-to-storage network in the ""storage disaggregation"" architecture. Besides the tremendous performance gains and CPU savings compared with the legacy kernel TCP stack, more importantly, it teaches us the necessity of offloading both network and storage into hardware and the importance of recovering instantaneously from network failures.Solar provides a highly reliable and performant storage network running on hardware. For avoiding hardware's resource limitations and offloading storage's entire data path, Solar eliminates the superfluous complexity and the overfull states from the traditional architecture of the storage network. The core design of Solar is unifying the concepts of network packet and storage data block - each network packet is a self-contained storage data block. There are three remarkable advantages to doing so. First, it merges the packet processing and storage virtualization pipelines to bypass the CPU and PCIe; Second, since the storage processes data blocks independently, the packets in Solar become independent. Therefore, the storage (in hardware) does not need to maintain receiving buffers for assembling packets into blocks or handling packet reordering. Finally, due to the low resource requirement and the resilience to packet reordering, Solar inherently supports large-scale multi-path transport for fast failure recovery. Facing the future, Solar demonstrates that we can formalize the storage virtualization procedure into a P4-compatible packet processing pipeline. Hence, SOLAR's design perfectly applies to commodity DPUs (data processing units).",Proceedings of the ACM SIGCOMM 2022 Conference,753–766,14,"data processing unit, storage network, in-network acceleration","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
50,inproceedings,"Cai, Qizhe and Vuppalapati, Midhul and Hwang, Jaehyun and Kozyrakis, Christos and Agarwal, Rachit",Towards μs Tail Latency and Terabit Ethernet: Disaggregating the Host Network Stack,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544230,10.1145/3544216.3544230,"Dedicated, tightly integrated, and static packet processing pipelines in today's most widely deployed network stacks preclude them from fully exploiting capabilities of modern hardware.We present NetChannel, a disaggregated network stack architecture for μs-scale applications running atop Terabit Ethernet. NetChannel's disaggregated architecture enables independent scaling and scheduling of resources allocated to each layer in the packet processing pipeline. Using an end-to-end NetChannel realization within the Linux network stack, we demonstrate that NetChannel enables new operating points---(1) enabling a single application thread to saturate multi-hundred gigabit access link bandwidth; (2) enabling near-linear scalability for small message processing with number of cores, independent of number of application threads; and, (3) enabling isolation of latency-sensitive applications, allowing them to maintain μs-scale tail latency even when competing with throughput-bound applications operating at near-line rate.",Proceedings of the ACM SIGCOMM 2022 Conference,767–779,13,"operating system, network stack, terabit ethernet","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
51,inproceedings,"Qi, Shixiong and Monis, Leslie and Zeng, Ziteng and Wang, Ian-chin and Ramakrishnan, K. K.","SPRIGHT: Extracting the Server from Serverless Computing! High-Performance EBPF-Based Event-Driven, Shared-Memory Processing",2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544259,10.1145/3544216.3544259,"Serverless computing promises an efficient, low-cost compute capability in cloud environments. However, existing solutions, epitomized by open-source platforms such as Knative, include heavyweight components that undermine this goal of serverless computing. Additionally, such serverless platforms lack dataplane optimizations to achieve efficient, high-performance function chains that facilitate the popular microservices development paradigm. Their use of unnecessarily complex and duplicate capabilities for building function chains severely degrades performance. 'Cold-start' latency is another deterrent.We describe SPRIGHT, a lightweight, high-performance, responsive serverless framework. SPRIGHT exploits shared memory processing and dramatically improves the scalability of the dataplane by avoiding unnecessary protocol processing and serialization-deserialization overheads. SPRIGHT extensively leverages event-driven processing with the extended Berkeley Packet Filter (eBPF). We creatively use eBPF's socket message mechanism to support shared memory processing, with overheads being strictly load-proportional. Compared to constantly-running, polling-based DPDK, SPRIGHT achieves the same dataplane performance with 10\texttimes{",Proceedings of the ACM SIGCOMM 2022 Conference,780–794,15,"serverless, eBPF, function chain, event-driven","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
52,inproceedings,"Yeo, Hyunho and Lim, Hwijoon and Kim, Jaehong and Jung, Youngmok and Ye, Juncheol and Han, Dongsu",NeuroScaler: Neural Video Enhancement at Scale,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544218,10.1145/3544216.3544218,"High-definition live streaming has experienced tremendous growth. However, the video quality of live video is often limited by the streamer's uplink bandwidth. Recently, neural-enhanced live streaming has shown great promise in enhancing the video quality by running neural super-resolution at the ingest server. Despite its benefit, it is too expensive to be deployed at scale. To overcome the limitation, we present NeuroScaler, a framework that delivers efficient and scalable neural enhancement for live streams. First, to accelerate end-to-end neural enhancement, we propose novel algorithms that significantly reduce the overhead of video super-resolution, encoding, and GPU context switching. Second, to maximize the overall quality gain, we devise a resource scheduler that considers the unique characteristics of the neural-enhancing workload. Our evaluation on a public cloud shows NeuroScaler reduces the overall cost by 22.3\texttimes{",Proceedings of the ACM SIGCOMM 2022 Conference,795–811,17,"super-resolution, deep neural networks, live streaming","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
53,inproceedings,"Li, Jinyang and Li, Zhenyu and Lu, Ri and Xiao, Kai and Li, Songlin and Chen, Jufeng and Yang, Jingyu and Zong, Chunli and Chen, Aiyun and Wu, Qinghua and Sun, Chen and Tyson, Gareth and Liu, Hongqiang Harry",LiveNet: A Low-Latency Video Transport Network for Large-Scale Live Streaming,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544236,10.1145/3544216.3544236,"Low-latency live streaming has imposed stringent latency requirements on video transport networks. In this paper we report on the design and operation of the Alibaba low-latency video transport network, LiveNet. LiveNet builds on a flat CDN overlay with a centralized controller for global optimization. As part of this, we present our design of the global routing computation and path assignment, as well as our fast data transmission architecture with fine-grained control of video frames. The performance results obtained from three years of operation demonstrate the effectiveness of LiveNet in improving CDN performance and QoE metrics. Compared with our prior state-of-the-art hierarchical CDN deployment, LiveNet halves the CDN delay and ensures 98% of views do not experience stalls and that 95% can start playback within 1 second. We further report our experiences of running LiveNet over the last 3 years.",Proceedings of the ACM SIGCOMM 2022 Conference,812–825,14,"live streaming, CDN, low latency transmission","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
54,inproceedings,"Lin, Xianshang and Ma, Yunfei and Zhang, Junshao and Cui, Yao and Li, Jing and Bai, Shi and Zhang, Ziyue and Cai, Dennis and Liu, Hongqiang Harry and Zhang, Ming",GSO-Simulcast: Global Stream Orchestration in Simulcast Video Conferencing Systems,2022,9781450394208,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3544216.3544228,10.1145/3544216.3544228,"We present GSO-Simulcast, a new architecture designed for large-scale multi-party video-conferencing systems. GSO-Simulcast is currently deployed at full-scale in Alibaba's Dingtalk video conferencing that serves more than 500 million users. It marks a fundamental shift from today's Simulcast, where a media server locally decides how to switch and forward video streams based on a fragmented network view. Instead, GSO-Simulcast globally orchestrates the publishing, subscribing, as well as the resolution and bitrate of video streams for each participant using a centralized controller that is aware of all network constraints in a meeting. The controller automatically modifies stream configurations to meet the participants' real-time network changes and updates. In doing so, GSO-Simulcast achieves multiple goals: (1) reducing video and network mismatch, (2) less path congestion, and (3) automated stream policy management. With the deployment of GSO-Simulcast, we observed more than a 35% reduction in the average video stall, 50% reduction in the average voice stall, and 6% improvement in the average video framerate. We describe the principle, design, deployment, and lessons learned.",Proceedings of the ACM SIGCOMM 2022 Conference,826–839,14,"teleconferencing, mobile transport, simulcast, video conferencing","Amsterdam, Netherlands",SIGCOMM '22,,,,,,
55,inproceedings,"Arun, Venkat and Arashloo, Mina Tahmasbi and Saeed, Ahmed and Alizadeh, Mohammad and Balakrishnan, Hari",Toward Formally Verifying Congestion Control Behavior,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472912,10.1145/3452296.3472912,"The diversity of paths on the Internet makes it difficult for designers and operators to confidently deploy new congestion control algorithms (CCAs) without extensive real-world experiments, but such capabilities are not available to most of the networking community. And even when they are available, understanding why a CCA underperforms by trawling through massive amounts of statistical data from network connections is challenging. The history of congestion control is replete with many examples of surprising and unanticipated behaviors unseen in simulation but observed on real-world paths. In this paper, we propose initial steps toward modeling and improving our confidence in a CCA's behavior. We have developed CCAC, a tool that uses formal verification to establish certain properties of CCAs. It is able to prove hypotheses about CCAs or generate counterexamples for invalid hypotheses. With CCAC, a designer can not only gain greater confidence prior to deployment to avoid unpleasant surprises, but can also use the counterexamples to iteratively improvetheir algorithm. We have modeled additive-increase/multiplicative-decrease (AIMD), Copa, and BBR with CCAC, and describe some surprising results from the exercise.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,1–16,16,"congestion control, formal verification, WAN transport","Virtual Event, USA",SIGCOMM '21,,,,,,
56,inproceedings,"Tian, Bingchuan and Gao, Jiaqi and Liu, Mengqi and Zhai, Ennan and Chen, Yanqing and Zhou, Yu and Dai, Li and Yan, Feng and Ma, Mengjing and Tang, Ming and Lu, Jie and Wei, Xionglie and Liu, Hongqiang Harry and Zhang, Ming and Tian, Chen and Yu, Minlan",Aquila: A Practically Usable Verification System for Production-Scale Programmable Data Planes,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472937,10.1145/3452296.3472937,"This paper presents Aquila, the first practically usable verification system for Alibaba's production-scale programmable data planes. Aquila addresses four challenges in building a practically usable verification: (1) specification complexity; (2) verification scalability; (3) bug localization; and (4) verifier self validation. Specifically, first, Aquila proposes a high-level language that facilitates easy expression of specifications, reducing lines of specification codes by tenfold compared to the state-of-the-art. Second, Aquila constructs a sequential encoding algorithm to circumvent the exponential growth of states associated with the upscaling of data plane programs to production level. Third, Aquila adopts an automatic and accurate bug localization approach that can narrow down suspects based on reported violations and pinpoint the culprit by simulating a fix for each suspect. Fourth and finally, Aquila can perform self validation based on refinement proof, which involves the construction of an alternative representation and subsequent equivalence checking. To this date, Aquila has been used in the verification of our production-scale programmable edge networks for over half a year, and it has successfully prevented many potential failures resulting from data plane bugs.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,17–32,16,"P4 verification, formal methods, programmable switches","Virtual Event, USA",SIGCOMM '21,,,,,,
57,inproceedings,"Schneider, Tibor and Birkner, R\""{u",Snowcap: Synthesizing Network-Wide Configuration Updates,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472915,10.1145/3452296.3472915,"Large-scale reconfiguration campaigns tend to be nerve-racking for network operators as they can lead to significant network downtimes, decreased performance, and policy violations. Unfortunately, existing reconfiguration frameworks often fall short in practice as they either only support a small set of reconfiguration scenarios or simply do not scale.We address these problems with Snowcap, the first network reconfiguration framework which can synthesize configuration updates that comply with arbitrary hard and soft specifications, and involve arbitrary routing protocols. Our key contribution is an efficient search procedure which leverages counter-examples to efficiently navigate the space of configuration updates. Given a reconfiguration ordering which violates the desired specifications, our algorithm automatically identifies the problematic commands so that it can avoid this particular order in the next iteration.We fully implemented Snowcap and extensively evaluated its scalability and effectiveness on real-world topologies and typical, large-scale reconfiguration scenarios. Even for large topologies, Snowcap finds a valid reconfiguration ordering with minimal side-effects (i.e., traffic shifts) within a few seconds at most.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,33–49,17,"network analysis, configuration, migration","Virtual Event, USA",SIGCOMM '21,,,,,,
58,inproceedings,"Xu, Qiongwen and Wong, Michael D. and Wagle, Tanvi and Narayana, Srinivas and Sivaraman, Anirudh",Synthesizing Safe and Efficient Kernel Extensions for Packet Processing,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472929,10.1145/3452296.3472929,"Extended Berkeley Packet Filter (BPF) has emerged as a powerful method to extend packet-processing functionality in the Linux operating system. BPF allows users to write code in high-level languages (like C or Rust) and execute them at specific hooks in the kernel, such as the network device driver. To ensure safe execution of a user-developed BPF program in kernel context, Linux uses an in-kernel static checker. The checker allows a program to execute only if it can prove that the program is crash-free, always accesses memory within safe bounds, and avoids leaking kernel data.BPF programming is not easy. One, even modest-sized BPF programs are deemed too large to analyze and rejected by the kernel checker. Two, the kernel checker may incorrectly determine that a BPF program exhibits unsafe behaviors. Three, even small performance optimizations to BPF code (e.g., 5% gains) must be meticulously hand-crafted by expert developers. Traditional optimizing compilers for BPF are often inadequate since the kernel checker's safety constraints are incompatible with rule-based optimizations.We present K2, a program-synthesis-based compiler that automatically optimizes BPF bytecode with formal correctness and safety guarantees. K2 produces code with 6--26% reduced size, 1.36%--55.03% lower average packet-processing latency, and 0--4.75% higher throughput (packets per second per core) relative to the best clang-compiled program, across benchmarks drawn from Cilium, Facebook, and the Linux kernel. K2 incorporates several domain-specific techniques to make synthesis practical by accelerating equivalence-checking of BPF programs by 6 orders of magnitude.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,50–64,15,"synthesis, BPF, stochastic optimization, endpoint packet processing","Virtual Event, USA",SIGCOMM '21,,,,,,
59,inproceedings,"Cai, Qizhe and Chaudhary, Shubham and Vuppalapati, Midhul and Hwang, Jaehyun and Agarwal, Rachit",Understanding Host Network Stack Overheads,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472888,10.1145/3452296.3472888,"Traditional end-host network stacks are struggling to keep up with rapidly increasing datacenter access link bandwidths due to their unsustainable CPU overheads. Motivated by this, our community is exploring a multitude of solutions for future network stacks: from Linux kernel optimizations to partial hardware offload to clean-slate userspace stacks to specialized host network hardware. The design space explored by these solutions would benefit from a detailed understanding of CPU inefficiencies in existing network stacks.This paper presents measurement and insights for Linux kernel network stack performance for 100Gbps access link bandwidths. Our study reveals that such high bandwidth links, coupled with relatively stagnant technology trends for other host resources (e.g., CPU speeds and capacity, cache sizes, NIC buffer sizes, etc.), mark a fundamental shift in host network stack bottlenecks. For instance, we find that a single core is no longer able to process packets at line rate, with data copy from kernel to application buffers at the receiver becoming the core performance bottleneck. In addition, increase in bandwidth-delay products have outpaced the increase in cache sizes, resulting in inefficient DMA pipeline between the NIC and the CPU. Finally, we find that traditional loosely-coupled design of network stack and CPU schedulers in existing operating systems becomes a limiting factor in scaling network stack performance across cores. Based on insights from our study, we discuss implications to design of future operating systems, network protocols, and host hardware.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,65–77,13,"datacenter networks, network hardware, host network stacks","Virtual Event, USA",SIGCOMM '21,,,,,,
60,inproceedings,"Li, Bojie and Zuo, Gefei and Bai, Wei and Zhang, Lintao",1Pipe: Scalable Total Order Communication in Data Center Networks,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472909,10.1145/3452296.3472909,"This paper proposes 1Pipe, a novel communication abstraction that enables different receivers to process messages from senders in a consistent total order. More precisely, 1Pipe provides both unicast and scattering (i.e., a group of messages to different destinations) in a causally and totally ordered manner. 1Pipe provides a best effort service that delivers each message at most once, as well as a reliable service that guarantees delivery and provides restricted atomic delivery for each scattering. 1Pipe can simplify and accelerate many distributed applications, e.g., transactional key-value stores, log replication, and distributed data structures.We propose a scalable and efficient method to implement 1Pipe inside data centers. To achieve total order delivery in a scalable manner, 1Pipe separates the bookkeeping of order information from message forwarding, and distributes the work to each switch and host. 1Pipe aggregates order information using in-network computation at switches. This forms the “control plane” of the system. On the “data plane”, 1Pipe forwards messages in the network as usual and reorders them at the receiver based on the order information.Evaluation on a 32-server testbed shows that 1Pipe achieves scalable throughput (80M messages per second per host) and low latency (10𝜇s) with little CPU and network overhead. 1Pipe achieves linearly scalable throughput and low latency in transactional key-value store, TPC-C, remote data structures, and replication that outperforms traditional designs by 2∼20x.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,78–92,15,"data center networks, total order communication, CATOCS, in-network processing","Virtual Event, USA",SIGCOMM '21,,,,,,
61,inproceedings,"Singhvi, Arjun and Akella, Aditya and Anderson, Maggie and Cauble, Rob and Deshmukh, Harshad and Gibson, Dan and Martin, Milo M. K. and Strominger, Amanda and Wenisch, Thomas F. and Vahdat, Amin",CliqueMap: Productionizing an RMA-Based Distributed Caching System,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472934,10.1145/3452296.3472934,"Distributed in-memory caching is a key component of modern Internet services. Such caches are often accessed via remote procedure call (RPC), as RPC frameworks provide rich support for productionization, including protocol versioning, memory efficiency, auto-scaling, and hitless upgrades. However, full-featured RPC limits performance and scalability as it incurs high latencies and CPU overheads. Remote Memory Access (RMA) offers a promising alternative, but meeting productionization requirements can be a significant challenge with RMA-based systems due to limited programmability and narrow RMA primitives.This paper describes the design, implementation, and experience derived from CliqueMap, a hybrid RMA/RPC caching system. CliqueMap has been in production use in Google's datacenters for over three years, currently serves more than 1PB of DRAM, and underlies several end-user visible services. CliqueMap makes use of performant and efficient RMAs on the critical serving path and judiciously applies RPCs toward other functionality. The design embraces lightweight replication, client-based quoruming, self-validating server responses, per-operation client-side retries, and co-design with the network layers. These foci lead to a system resilient to the rigors of production and frequent post deployment evolution.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,93–105,13,"key-value caching system, remote procedure call, remote memory access","Virtual Event, USA",SIGCOMM '21,,,,,,
62,inproceedings,"Min, Jaehong and Liu, Ming and Chugh, Tapan and Zhao, Chenxingyu and Wei, Andrew and Doh, In Hwan and Krishnamurthy, Arvind",Gimbal: Enabling Multi-Tenant Storage Disaggregation on SmartNIC JBOFs,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472940,10.1145/3452296.3472940,"Emerging SmartNIC-based disaggregated NVMe storage has become a promising storage infrastructure due to its competitive IO performance and low cost. These SmartNIC JBOFs are shared among multiple co-resident applications, and there is a need for the platform to ensure fairness, QoS, and high utilization. Unfortunately, given the limited computing capability of the SmartNICs and the non-deterministic nature of NVMe drives, it is challenging to provide such support on today's SmartNIC JBOFs.This paper presents Gimbal, a software storage switch that orchestrates IO traffic between Ethernet ports and NVMe drives for co-located tenants. It enables efficient multi-tenancy on SmartNIC JBOFs using the following techniques: a delay-based SSD congestion control algorithm, dynamic estimation of SSD write costs, a fair scheduler that operates at the granularity of a virtual slot, and an end-to-end credit-based flow control channel. Our prototyped system not only achieves up to x6.6 better utilization and 62.6% less tail latency but also improves the fairness for complex workloads. It also improves a commercial key-value store performance in a multi-tenant environment with x1.7 better throughput and 35.0% less tail latency on average.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,106–122,17,"congestion control, fairness, SSD, disaggregated storage","Virtual Event, USA",SIGCOMM '21,,,,,,
63,inproceedings,"Zelaya, R. Ivan and Sussman, William and Gummeson, Jeremy and Jamieson, Kyle and Hu, Wenjun",LAVA: Fine-Grained 3D Indoor Wireless Coverage for Small IoT Devices,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472890,10.1145/3452296.3472890,"Small IoT devices deployed in challenging locations suffer from uneven 3D coverage in complex environments. This work optimizes indoor coverage with LAVA, a Large Array of Vanilla Amplifiers. LAVA is a standard-agnostic cooperative mesh of elements, i.e., RF devices each consisting of several switched input and output antennas connected to fixed-gain amplifiers. Each LAVA element is further equipped with rudimentary power sensing to detect nearby transmissions. The elements report power readings to the LAVA control plane, which then infers active link sessions without explicitly interacting with the endpoint transmitter or receiver. With simple on-off control of amplifiers and antenna switching, LAVA boosts passing signals via multi hop amplify-and-forward. LAVA explores a middle ground between smart surfaces and physical-layer relays. Multi-hopping over short inter-hop distances exerts more control over the end-to-end trajectory, supporting fine-grained coverage and spatial reuse. Ceiling testbed results show throughput improvements to individual Wi-Fi links by 50% on average and up to 100% at 15 dBm transmit power (193% on average, up to 8x at 0 dBm). ZigBee links see up to 17 dB power gain. For pairs of co-channel concurrent links, LAVA provides average per-link throughput improvements of 517% at 0 dBm and 80% at 15 dBm.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,123–136,14,"programmable radio environments, non-uniform 3D coverage, multi-hop amplify-and-forward, smart surfaces","Virtual Event, USA",SIGCOMM '21,,,,,,
64,inproceedings,"Yang, Zhijian and Choudhury, Romit Roy",Personalizing Head Related Transfer Functions for Earables,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472907,10.1145/3452296.3472907,"Head related transfer functions (HRTF) describe how sound signals bounce, scatter, and diffract when they arrive at the head, and travel towards the ears. HRTFs produce distinct sound patterns that ultimately help the brain infer the spatial properties of the sound, such as its direction of arrival, 𝜃. If an earphone can learn the HRTF, it could apply the HRTF to any sound and make that sound appear directional to the user. For instance, a directional voice guide could help a tourist navigate a new city. While past works have estimated human HRTFs, an important gap lies in personalization. Today's HRTFs are global templates that are used in all products; since human HRTFs are unique, a global HRTF only offers a coarse-grained experience. This paper shows that by moving a smartphone around the head, combined with mobile acoustic communications between the phone and the earbuds, it is possible to estimate a user's personal HRTF. Our personalization system, UNIQ, combines techniques from channel estimation, motion tracking, and signal processing, with a focus on modeling signal diffraction on the curvature of the face. The results are promising and could open new doors into the rapidly growing space of immersive AR/VR, earables, smart hearing aids, etc.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,137–150,14,"virtual acoustics, spatial audio, earables, AR, VR, head related transfer function (HRTF), HRTF personalization","Virtual Event, USA",SIGCOMM '21,,,,,,
65,inproceedings,"Vasisht, Deepak and Shenoy, Jayanth and Chandra, Ranveer",L2D2: Low Latency Distributed Downlink for LEO Satellites,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472932,10.1145/3452296.3472932,"Large constellations of Low Earth Orbit satellites promise to provide near real-time high-resolution Earth imagery. Yet, getting this large amount of data back to Earth is challenging because of their low orbits and fast motion through space. Centralized architectures with few multi-million dollar ground stations incur large hour-level data download latency and are hard to scale. We propose a geographically distributed ground station design, L2D2, that uses low-cost commodity hardware to offer low latency robust downlink. L2D2 is the first system to use a hybrid ground station model, where only a subset of ground stations are uplink-capable. We design new algorithms for scheduling and rate adaptation that enable low latency and high robustness despite the limitations of the receive-only ground stations. We evaluate L2D2 through a combination of trace-driven simulations and real-world satellite-ground station measurements. Our results demonstrate that L2D2's geographically distributed design can reduce data downlink latency from 90 minutes to 21 minutes.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,151–164,14,"earth observation, satellite networking, distributed ground station, ground station architecture","Virtual Event, USA",SIGCOMM '21,,,,,,
66,inproceedings,"Nolan, John and Qian, Kun and Zhang, Xinyu",RoS: Passive Smart Surface for Roadside-to-Vehicle Communication,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472896,10.1145/3452296.3472896,"Modern autonomous vehicles are commonly instrumented with radars for all-weather perception. Yet the radar functionality is limited to identifying the positions of reflectors in the environment. In this paper, we investigate the feasibility of smartening transportation infrastructure for the purpose of conveying richer information to automotive radars. We propose RoS, a passive PCB-fabricated smart surface which can be reconfigured to embed digital bits, and inform the radar much like visual road signs do to cameras. We design the RoS signage to act as a retrodirective reflector which can reflect signals back to the radar from wide viewing angles. We further introduce a spatial encoding scheme, which piggybacks information in the reflected analog signals based on the geometrical layout of the retroreflective elements. Our prototype fabrication and experimentation verifies the effectiveness of RoS as an RF ''barcode'' which is readable by radar in practical transportation environment.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,165–178,14,"V2X, intelligent reflecting surface, Van Atta array, smart surface, millimeter wave radar","Virtual Event, USA",SIGCOMM '21,,,,,,
67,inproceedings,"Yu, Zhuolong and Hu, Chuheng and Wu, Jingfeng and Sun, Xiao and Braverman, Vladimir and Chowdhury, Mosharaf and Liu, Zhenhua and Jin, Xin",Programmable Packet Scheduling with a Single Queue,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472887,10.1145/3452296.3472887,"Programmable packet scheduling enables scheduling algorithms to be programmed into the data plane without changing the hardware. Existing proposals either have no hardware implementations for switch ASICs or require multiple strict-priority queues.We present Admission-In First-Out (AIFO) queues, a new solution for programmable packet scheduling that uses only a emph{single",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,179–193,15,"in-network processing, packet scheduling, data center networks, programmable networks","Virtual Event, USA",SIGCOMM '21,,,,,,
68,inproceedings,"Pan, Tian and Yu, Nianbing and Jia, Chenhao and Pi, Jianwen and Xu, Liang and Qiao, Yisong and Li, Zhiguo and Liu, Kun and Lu, Jie and Lu, Jianyuan and Song, Enge and Zhang, Jiao and Huang, Tao and Zhu, Shunmin",Sailfish: Accelerating Cloud-Scale Multi-Tenant Multi-Service Gateways with Programmable Switches,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472889,10.1145/3452296.3472889,"The cloud gateway is essential in the public cloud as the central hub of cloud traffic. We show that horizontal scaling of software gateways, once sustainable for years, is no longer future-proof facing the massive scale and rapid growth of today's cloud. The root cause is the stagnant performance of the CPU core, which is prone to be overloaded by heavy hitters as traffic growth goes far beyond Moore's law. To address this, we propose emph{Sailfish",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,194–206,13,"forwarding table compression, programmable data plane, cloud gateways, virtual private cloud","Virtual Event, USA",SIGCOMM '21,,,,,,
69,inproceedings,"Zhang, Yinda and Liu, Zaoxing and Wang, Ruixin and Yang, Tong and Li, Jizhou and Miao, Ruijie and Liu, Peng and Zhang, Ruwen and Jiang, Junchen",CocoSketch: High-Performance Sketch-Based Measurement over Arbitrary Partial Key Query,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472892,10.1145/3452296.3472892,"Sketch-based measurement has emerged as a promising alternative to the traditional sampling-based network measurement approaches due to its high accuracy and resource efficiency. While there have been various designs around sketches, they focus on measuring one particular flow key, and it is infeasible to support many keys based on these sketches. In this work, we take a significant step towards supporting arbitrary partial key queries, where we only need to specify a full range of possible flow keys that are of interest before measurement starts, and in query time, we can extract the information of any key in that range. We design CocoSketch, which casts arbitrary partial key queries to the subset sum estimation problem and makes the theoretical tools for subset sum estimation practical. To realize desirable resource-accuracy tradeoffs in software and hardware platforms, we propose two techniques: (1) stochastic variance minimization to significantly reduce per-packet update delay, and (2) removing circular dependencies in the per-packet update logic to make the implementation hardware-friendly. We implement CocoSketch on four popular platforms (CPU, Open vSwitch, P4, and FPGA) and show that compared to baselines that use traditional single-key sketches, CocoSketch improves average packet processing throughput by 27.2x and accuracy by 10.4x when measuring six flow keys.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,207–222,16,"FPGA, P4, sketch, arbitrary partial key query","Virtual Event, USA",SIGCOMM '21,,,,,,
70,inproceedings,"Kim, Daehyeok and Nelson, Jacob and Ports, Dan R. K. and Sekar, Vyas and Seshan, Srinivasan",RedPlane: Enabling Fault-Tolerant Stateful in-Switch Applications,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472905,10.1145/3452296.3472905,"Many recent efforts have demonstrated the performance benefits of running datacenter functions (emph{e.g.,",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,223–244,22,"state replication, fault tolerance, programmable switches, programmable networks","Virtual Event, USA",SIGCOMM '21,,,,,,
71,inproceedings,"Tu, William and Wei, Yi-Hung and Antichi, Gianni and Pfaff, Ben",Revisiting the Open VSwitch Dataplane Ten Years Later,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472914,10.1145/3452296.3472914,"This paper shares our experience in supporting and running the Open vSwitch (OVS) software switch, as part of the NSX product for enterprise data center virtualization used by thousands of VMware customers. Starting in 2009, the OVS design split its code between tightly coupled kernel and userspace components. This split was necessary at the time for performance, but it caused maintainability problems that persist today. In addition, in-kernel packet processing is now much slower than newer options.To solve the problems caused by the user/kernel split, OVS must adopt a new architecture. We describe two possibilities that we explored, but did not adopt, one because it gives up compatibility with drivers and tools that are important to virtual data center operators, the other because it performs poorly. Instead, we endorse a third approach, based on a new Linux socket type called AF_XDP, which solves the maintainability problem in a compatible, performant way. The new code is already merged into the mainstream OVS repository. We include a thorough performance evaluation and a collection of lessons learned.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,245–257,13,"OVS, virtual switch, XDP, eBPF","Virtual Event, USA",SIGCOMM '21,,,,,,
72,inproceedings,"Zhu, Hang and Gupta, Varun and Ahuja, Satyajeet Singh and Tian, Yuandong and Zhang, Ying and Jin, Xin",Network Planning with Deep Reinforcement Learning,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472902,10.1145/3452296.3472902,"Network planning is critical to the performance, reliability and cost of web services. This problem is typically formulated as an Integer Linear Programming (ILP) problem. Today's practice relies on hand-tuned heuristics from human experts to address the scalability challenge of ILP solvers.In this paper, we propose NeuroPlan, a deep reinforcement learning (RL) approach to solve the network planning problem. This problem involves multi-step decision making and cost minimization, which can be naturally cast as a deep RL problem. We develop two important domain-specific techniques. First, we use a graph neural network (GNN) and a novel domain-specific node-link transformation for state encoding, in order to handle the dynamic nature of the evolving network topology during planning decision making. Second, we leverage a two-stage hybrid approach that first uses deep RL to prune the search space and then uses an ILP solver to find the optimal solution. This approach resembles today's practice, but avoids human experts with an RL agent in the first stage. Evaluation on real topologies and setups from large production networks demonstrates that NeuroPlan scales to large topologies beyond the capability of ILP solvers, and reduces the cost by up to 17% compared to hand-tuned heuristics.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,258–271,14,"network planning, graph neural network, reinforcement learning","Virtual Event, USA",SIGCOMM '21,,,,,,
73,inproceedings,"Yen, Jane and L\'{e",Semi-Automated Protocol Disambiguation and Code Generation,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472910,10.1145/3452296.3472910,"For decades, Internet protocols have been specified using natural language. Given the ambiguity inherent in such text, it is not surprising that protocol implementations have long exhibited bugs. In this paper, we apply natural language processing (NLP) to effect semi-automated generation of protocol implementations from specification text. Our system, Sage, can uncover ambiguous or under-specified sentences in specifications; once these are clarified by the author of the protocol specification, Sage can generate protocol code automatically.Using Sage, we discover 5 instances of ambiguity and 6 instances of under-specification in the ICMP RFC; after fixing these, Sage is able to automatically generate code that interoperates perfectly with Linux implementations. We show that Sage generalizes to sections of BFD, IGMP, and NTP and identify additional conceptual components that Sage needs to support to generalize to complete, complex protocols like BGP and TCP.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,272–286,15,"natural language, protocol specifications","Virtual Event, USA",SIGCOMM '21,,,,,,
74,inproceedings,"Zhang, Qizhen and Ng, Kelvin K. W. and Kazer, Charles and Yan, Shen and Sedoc, Jo\~{a",MimicNet: Fast Performance Estimates for Data Center Networks with Machine Learning,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472926,10.1145/3452296.3472926,"At-scale evaluation of new data center network innovations is becoming increasingly intractable. This is true for testbeds, where few, if any, can afford a dedicated, full-scale replica of a data center. It is also true for simulations, which while originally designed for precisely this purpose, have struggled to cope with the size of today's networks. This paper presents an approach for quickly obtaining accurate performance estimates for large data center networks. Our system,MimicNet, provides users with the familiar abstraction of a packet-level simulation for a portion of the network while leveraging redundancy and recent advances in machine learning to quickly and accurately approximate portions of the network that are not directly visible. MimicNet can provide over two orders of magnitude speedup compared to regular simulation for a data center with thousands of servers. Even at this scale, MimicNet estimates of the tail FCT, throughput, and RTT are within 5% of the true results.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,287–304,18,"network simulation, machine learning, approximation, network modeling, data center networks","Virtual Event, USA",SIGCOMM '21,,,,,,
75,inproceedings,"Eliyahu, Tomer and Kazak, Yafim and Katz, Guy and Schapira, Michael",Verifying Learning-Augmented Systems,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472936,10.1145/3452296.3472936,"The application of deep reinforcement learning (DRL) to computer and networked systems has recently gained significant popularity. However, the obscurity of decisions by DRL policies renders it hard to ascertain that learning-augmented systems are safe to deploy, posing a significant obstacle to their real-world adoption. We observe that specific characteristics of recent applications of DRL to systems contexts give rise to an exciting opportunity: applying formal verification to establish that a given system provably satisfies designer/user-specified requirements, or to expose concrete counter-examples. We present whiRL, a platform for verifying DRL policies for systems, which combines recent advances in the verification of deep neural networks with scalable model checking techniques. To exemplify its usefulness, we employ whiRL to verify natural equirements from recently introduced learning-augmented systems for three real-world environments: Internet congestion control, adaptive video streaming, and job scheduling in compute clusters. Our evaluation shows that whiRL is capable of guaranteeing that natural requirements from these systems are satisfied, and of exposing specific scenarios in which other basic requirements are not.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,305–318,14,"resource scheduling, formal verification, deep learning, networked systems, deep reinforcement learning, adaptive bitrate algorithms, congestion control, neural networks","Virtual Event, USA",SIGCOMM '21,,,,,,
76,inproceedings,"Ros-Giralt, Jordi and Amsel, Noah and Yellamraju, Sruthi and Ezick, James and Lethin, Richard and Jiang, Yuang and Feng, Aosong and Tassiulas, Leandros and Wu, Zhenguo and Teh, Min Yee and Bergman, Keren",Designing Data Center Networks Using Bottleneck Structures,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472898,10.1145/3452296.3472898,"This paper provides a mathematical model of data center performance based on the recently introduced Quantitative Theory of Bottleneck Structures (QTBS). Using the model, we prove that if the traffic pattern is textit{interference-free",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,319–348,30,"design, data center, max-min, model, bottleneck structure","Virtual Event, USA",SIGCOMM '21,,,,,,
77,inproceedings,"Namyar, Pooria and Supittayapornpong, Sucha and Zhang, Mingyang and Yu, Minlan and Govindan, Ramesh",A Throughput-Centric View of the Performance of Datacenter Topologies,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472913,10.1145/3452296.3472913,"While prior work has explored many proposed datacenter designs, only two designs, Clos-based and expander-based, are generally considered practical because they can scale using commodity switching chips. Prior work has used two different metrics, bisection bandwidth and throughput, for evaluating these topologies at scale. Little is known, theoretically or practically, how these metrics relate to each other. Exploiting characteristics of these topologies, we prove an upper bound on their throughput, then show that this upper bound better estimates worst-case throughput than all previously proposed throughput estimators and scales better than most of them. Using this upper bound, we show that for expander-based topologies, unlike Clos, beyond a certain size of the network, no topology can have full throughput, even if it has full bisection bandwidth; in fact, even relatively small expander-based topologies fail to achieve full throughput. We conclude by showing that using throughput to evaluate datacenter performance instead of bisection bandwidth can alter conclusions in prior work about datacenter cost, manageability, and reliability.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,349–369,21,"Clos topologies, data centers, throughput, network management","Virtual Event, USA",SIGCOMM '21,,,,,,
78,inproceedings,"Zhang, Yiran and Liu, Yifan and Meng, Qingkai and Ren, Fengyuan",Congestion Detection in Lossless Networks,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472899,10.1145/3452296.3472899,"Congestion detection is the cornerstone of end-to-end congestion control. Through in-depth observations and understandings, we reveal that existing congestion detection mechanisms in mainstream lossless networks (i.e., Converged Enhanced Ethernet and InfiniBand) are improper, due to failing to cognize the interaction between hop-by-hop flow controls and congestion detection behaviors in switches. We define ternary states of switch ports and present Ternary Congestion Detection (TCD) for mainstream lossless networks. Testbed and extensive simulations demonstrate that TCD can detect congestion ports accurately and identify flows contributing to congestion as well as flows only affected by hop-by-hop flow controls. Meanwhile, we shed light on how to incorporate TCD with rate control. Case studies show that existing congestion control algorithms can achieve 3.3x and 2.0x better median and 99th-percentile FCT slowdown by combining with TCD.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,370–383,14,"congestion detection, flow control, lossless networks","Virtual Event, USA",SIGCOMM '21,,,,,,
79,inproceedings,"Yan, Siyu and Wang, Xiaoliang and Zheng, Xiaolong and Xia, Yinben and Liu, Derui and Deng, Weishan",ACC: Automatic ECN Tuning for High-Speed Datacenter Networks,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472927,10.1145/3452296.3472927,"For the widely deployed ECN-based congestion control schemes, the marking threshold is the key to deliver high bandwidth and low latency. However, due to traffic dynamics in the high-speed production networks, it is difficult to maintain persistent performance by using the static ECN setting. To meet the operational challenge, in this paper we report the design and implementation of an automatic run-time optimization scheme, ACC, which leverages the multi-agent reinforcement learning technique to dynamically adjust the marking threshold at each switch. The proposed approach works in a distributed fashion and combines offline and online training to adapt to dynamic traffic patterns. It can be easily deployed based on the common features supported by major commodity switching chips. Both testbed experiments and large-scale simulations have shown that ACC achieves low flow completion time (FCT) for both mice flows and elephant flows at line-rate. Under heterogeneous production environments with 300 machines, compared with the well-tuned static ECN settings, ACC achieves up to 20% improvement on IOPS and 30% lower FCT for storage service. ACC has been applied in high-speed datacenter networks and significantly simplifies the network operations.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,384–397,14,"congestion control, ECN, AQM, datacenter network","Virtual Event, USA",SIGCOMM '21,,,,,,
80,inproceedings,"Koch, Thomas and Katz-Bassett, Ethan and Heidemann, John and Calder, Matt and Ardi, Calvin and Li, Ke",Anycast In Context: A Tale of Two Systems,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472891,10.1145/3452296.3472891,"Anycast is used to serve content including web pages and DNS, and anycast deployments are growing. However, prior work examining root DNS suggests anycast deployments incur significant inflation, with users often routed to suboptimal sites. We reassess anycast performance, first extending prior analysis on inflation in the root DNS. We show that inflation is very common in root DNS, affecting more than 95% of users. However, we then show root DNS latency emph{hardly matters",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,398–417,20,"CDN, Anycast, routing, latency, root DNS","Virtual Event, USA",SIGCOMM '21,,,,,,
81,inproceedings,"Zheng, Zhilong and Ma, Yunfei and Liu, Yanmei and Yang, Furong and Li, Zhenyu and Zhang, Yuanbo and Zhang, Jiuhai and Shi, Wei and Chen, Wentao and Li, Ding and An, Qing and Hong, Hai and Liu, Hongqiang Harry and Zhang, Ming",XLINK: QoE-Driven Multi-Path QUIC Transport in Large-Scale Video Services,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472893,10.1145/3452296.3472893,"We report XLINK, a multi-path QUIC video transport solution with experiments in Taobao short videos. XLINK is designed to meet two operational challenges at the same time: (1) Optimized user-perceived quality of experience (QoE) in terms of robustness, smoothness, responsiveness, and mobility and (2) Minimized cost overhead for service providers (typically CDNs). The core of XLINK is to take the opportunity of QUIC as a user-space protocol and directly capture user-perceived video QoE intent to control multi-path scheduling and management. We overcome major hurdles such as multi-path head-of-line blocking, network heterogeneity, and rapid link variations and balance cost and performance.To the best of our knowledge, XLINK is the first large-scale experimental study of multi-path QUIC video services in production environments. We present the results of over 3 million e-commerce product short-video plays from consumers who upgraded to Taobao android app with XLINK. Our study shows that compared to single-path QUIC, XLINK achieved 19 to 50% improvement in the 99-th percentile video-chunk request completion time, 32% improvement in the 99-th percentile first-video-frame latency, 23 to 67% improvement in the re-buffering rate at the expense of 2.1% redundant traffic.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,418–432,15,"scheduling, video, QUIC, wireless transport, multi-path, QoE","Virtual Event, USA",SIGCOMM '21,,,,,,
82,inproceedings,"Fayed, Marwan and Bauer, Lorenz and Giotsas, Vasileios and Kerola, Sami and Majkowski, Marek and Odintsov, Pavel and Sitnicki, Jakub and Chung, Taejoong and Levin, Dave and Mislove, Alan and Wood, Christopher A. and Sullivan, Nick",The Ties That Un-Bind: Decoupling IP from Web Services and Sockets for Robust Addressing Agility at CDN-Scale,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472922,10.1145/3452296.3472922,"The couplings between IP addresses, names of content or services, and socket interfaces, are too tight. This impedes system manageability, growth, and overall provisioning. In turn, large-scale content providers are forced to use staggering numbers of addresses, ultimately leading to address exhaustion (IPv4) and inefficiency (IPv6).In this paper, we revisit IP bindings, entirely. We attempt to evolve addressing conventions by decoupling IP in DNS and from network sockets. Alongside technologies such as SNI and ECMP, a new architecture emerges that ``unbinds'' IP from services and servers, thereby returning IP's role to merely that of reachability. The architecture is under evaluation at a major CDN in multiple datacenters. We show that addresses can be generated randomly emph{per-query",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,433–446,14,"content distribution, provisioning, programmable sockets, addressing","Virtual Event, USA",SIGCOMM '21,,,,,,
83,inproceedings,"Zhang, Xiao and Sen, Tanmoy and Zhang, Zheyuan and April, Tim and Chandrasekaran, Balakrishnan and Choffnes, David and Maggs, Bruce M. and Shen, Haiying and Sitaraman, Ramesh K. and Yang, Xiaowei",AnyOpt: Predicting and Optimizing IP Anycast Performance,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472935,10.1145/3452296.3472935,"The key to optimizing the performance of an anycast-based system (e.g., the root DNS or a CDN) is choosing the right set of sites to announce the anycast prefix. One challenge here is predicting catchments. A na\""{\i",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,447–462,16,"routing, Anycast, BGP, performance optimization","Virtual Event, USA",SIGCOMM '21,,,,,,
84,inproceedings,"Mazaheri, Mohammad Hossein and Chen, Alex and Abari, Omid",MmTag: A Millimeter Wave Backscatter Network,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472917,10.1145/3452296.3472917,"Recent advances in IoT, machine learning and cloud computing have placed a huge strain on wireless networks. In particular, many emerging applications require streaming rich content (such as videos) in real time, while they are constrained by energy sources. A wireless network which supports high data-rate while consuming low-power would be very attractive for these applications. Unfortunately, existing wireless networks do not satisfy this requirement. For example, WiFi backscatter and Bluetooth networks have very low power consumption, but their data-rate is very limited (less than a Mbps). On the other hand, modern WiFi and mmWave networks support high throughput, but have a high power consumption (more than a watt).To address this problem, we present mmTag, a novel mmWave backscatter network which enables low-power high-throughput wireless links for emerging applications. mmTag is a backscatter system which operates in the mmWave frequency bands. mmTag addresses the key challenges that prevent existing backscatter networks from operating at mmWave bands. We implemented mmTag and evaluated its performance empirically. Our results show that mmTag is capable of achieving 1 Gbps and 100 Mbps at 4.6 m and 8 m, respectively, while consuming only 2.4 nJ/bit.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,463–474,12,"wireless, low power, internet of things, IoT, mmWave, backscatter","Virtual Event, USA",SIGCOMM '21,,,,,,
85,inproceedings,"Cho, Hsun-Wei and Shin, Kang G.",BlueFi: Bluetooth over WiFi,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472920,10.1145/3452296.3472920,"Bluetooth and WiFi are the two dominant technologies enabling the communication of mobile and IoT devices. Built with specific design goals and principles, they are vastly different, each using its own hardware and software. Thus, they are not interoperable and require different hardware.One may, therefore, ask a simple, yet seemingly impossible question: “Can we transmit Bluetooth packets on commercial off-the-shelf (COTS) WiFi hardware?” We answer this question positively by designing, implementing and demonstrating a novel system called BlueFi. It can readily run on existing, widely-deployed WiFi devices without modifying NIC firmware/hardware. BlueFi works by reversing the signal processing of WiFi hardware and finds special 802.11n packets that are decodable by unmodified Bluetooth devices. With BlueFi, every 802.11n device can be used simultaneously as a Bluetooth device, which instantly increases the coverage of Bluetooth, thanks to the omnipresence of WiFi devices. BlueFi is particularly useful for WiFi-only devices or environments.We implement and evaluate BlueFi on devices with widely-adopted WiFi chips. We also construct two prevalent end-to-end apps — Bluetooth beacon and audio — to showcase the practical use of BlueFi. The former allows ordinary APs to send location beacons; the latter enables WiFi chips to stream Bluetooth audio in real time.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,475–487,13,"WiFi, cross-technology communication, Bluetooth","Virtual Event, USA",SIGCOMM '21,,,,,,
86,inproceedings,"Jain, Ish Kumar and Subbaraman, Raghav and Bharadia, Dinesh",Two Beams Are Better than One: Towards Reliable and High Throughput MmWave Links,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472924,10.1145/3452296.3472924,"Millimeter-wave communication with high throughput and high reliability is poised to be a gamechanger for V2X and VR applications. However, mmWave links are notorious for low reliability since they suffer from frequent outages due to blockage and user mobility. We build mmReliable, a reliable mmWave system that implements multi-beamforming and user tracking to handle environmental vulnerabilities. It creates constructive multi-beam patterns and optimizes their angle, phase, and amplitude to maximize the signal strength at the receiver. Multi-beam links are reliable since they are resilient to occasional blockages of few constituent beams compared to a single-beam system. We implement mmReliable on a 28 GHz testbed with 400 MHz bandwidth, and a 64 element phased array supporting 5G NR waveforms. Rigorous indoor and outdoor experiments demonstrate that mmReliable achieves close to 100% reliability providing 2.3x improvement in the throughput-reliability product than single-beam systems.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,488–502,15,"tracking, mobility, throughput, multi-beam, 5G NR, reliability, phased arrays, analog beamforming, millimeter-wave, blockage","Virtual Event, USA",SIGCOMM '21,,,,,,
87,inproceedings,"Shahid, Muhammad Osama and Philipose, Millan and Chintalapudi, Krishna and Banerjee, Suman and Krishnaswamy, Bhuvana",Concurrent Interference Cancellation: Decoding Multi-Packet Collisions in LoRa,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472931,10.1145/3452296.3472931,"LoRa has seen widespread adoption as a long range IoT technology. As the number of LoRa deployments grow, packet collisions undermine its overall network throughput. In this paper, we propose a novel interference cancellation technique -- Concurrent Interference Cancellation (CIC), that enables concurrent decoding of multiple collided LoRa packets. CIC fundamentally differs from existing approaches as it demodulates symbols by canceling out all other interfering symbols. It achieves this cancellation by carefully selecting a set of sub-symbols -- pieces of the original symbol such that no interfering symbol is common across all sub-symbols in this set. Thus, after demodulating each sub-symbol, an intersection across their spectra cancels out all the interfering symbols. Through LoRa deployments using COTS devices, we demonstrate that CIC can increase the network capacity of standard LoRa by up to 10x and up to 4x over the state-of-the-art research. While beneficial across all scenarios, CIC has even more significant benefits under low SNR conditions that are common to LoRa deployments, in which prior approaches appear to perform quite poorly.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,503–515,13,"multi-packet collisions, interference cancellation, LoRa","Virtual Event, USA",SIGCOMM '21,,,,,,
88,inproceedings,"Gigis, Petros and Calder, Matt and Manassakis, Lefteris and Nomikos, George and Kotronis, Vasileios and Dimitropoulos, Xenofontas and Katz-Bassett, Ethan and Smaragdakis, Georgios",Seven Years in the Life of Hypergiants' off-Nets,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472928,10.1145/3452296.3472928,"Content Hypergiants deliver the vast majority of Internet traffic to end users. In recent years, some have invested heavily in deploying services and servers inside end-user networks. With several dozen Hypergiants and thousands of servers deployed inside networks, these off-net (meaning outside the Hypergiant networks) deployments change the structure of the Internet. Previous efforts to study them have relied on proprietary data or specialized per-Hypergiant measurement techniques that neither scale nor generalize, providing a limited view of content delivery on today's Internet.In this paper, we develop a generic and easy to implement methodology to measure the expansion of Hypergiants' off-nets. Our key observation is that Hypergiants increasingly encrypt their traffic to protect their customers' privacy. Thus, we can analyze publicly available Internet-wide scans of port 443 and retrieve TLS certificates to discover which IP addresses host Hypergiant certificates in order to infer the networks hosting off-nets for the corresponding Hypergiants. Our results show that the number of networks hosting Hypergiant off-nets has tripled from 2013 to 2021, reaching 4.5k networks. The largest Hypergiants dominate these deployments, with almost all of these networks hosting an off-net for at least one -- and increasingly two or more -- of Google, Netflix, Facebook, or Akamai. These four Hypergiants have off-nets within networks that provide access to a significant fraction of end user population.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,516–533,18,"Hypergiants, TLS, content delivery networks, server deployment","Virtual Event, USA",SIGCOMM '21,,,,,,
89,inproceedings,"Singh, Rachee and Bjorner, Nikolaj and Shoham, Sharon and Yin, Yawei and Arnold, John and Gaudette, Jamie",Cost-Effective Capacity Provisioning in Wide Area Networks with Shoofly,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472895,10.1145/3452296.3472895,"In this work we propose Shoofly, a network design tool that minimizes hardware costs of provisioning long-haul capacity by optically bypassing network hops where conversion of signals from optical to electrical domain is unnecessary and uneconomical. Shoofly leverages optical signal quality and traffic demand telemetry from a large commercial cloud provider to identify optical bypasses in the cloud WAN that reduce the hardware cost of long-haul capacity by 40%. A key challenge is that optical bypasses cause signals to travel longer distances on fiber before re-generation, potentially reducing link capacities and resilience to optical link failures. Despite these challenges, Shoofly provisions bypass-enabled topologies that meet 8X the present-day demands using existing network hardware. Even under aggressive stochastic and deterministic link failure scenarios, these topologies save 32% of the cost of long-haul capacity.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,534–546,13,"traffic engineering, backbone design, optical bypass","Virtual Event, USA",SIGCOMM '21,,,,,,
90,inproceedings,"Ahuja, Satyajeet Singh and Gupta, Varun and Dangui, Vinayak and Bali, Soshant and Gopalan, Abishek and Zhong, Hao and Lapukhov, Petr and Xia, Yiting and Zhang, Ying",Capacity-Efficient and Uncertainty-Resilient Backbone Network Planning with Hose,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472918,10.1145/3452296.3472918,"This paper presents Facebook's design and operational experience of a Hose-based backbone network planning system. This initial adoption of the Hose model in network planning is driven by the capacity and demand uncertainty pressure of backbone expansion. Since the Hose model abstracts the aggregated traffic demand per site, peak traffic flows at different times can be multiplexed to save capacity and buffer traffic spikes. Our core design involves heuristic algorithms to select Hose-compliant traffic matrices and cross-layer optimization between the optical and IP networks. We evaluate the system performance in production and share insights from years of production experience. Hose-based network planning can save 17.4% capacity and drops 75% less traffic under fiber cuts. As the first study of Hose in network planning, our work has the potential to inspire follow-up research.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,547–559,13,"network modeling, network planning, network optimization, wide-area networks","Virtual Event, USA",SIGCOMM '21,,,,,,
91,inproceedings,"Zhong, Zhizhen and Ghobadi, Manya and Khaddaj, Alaa and Leach, Jonathan and Xia, Yiting and Zhang, Ying",ARROW: Restoration-Aware Traffic Engineering,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472921,10.1145/3452296.3472921,"Fiber cut events reduce the capacity of wide-area networks (WANs) by several Tbps. In this paper, we revive the lost capacity by reconfiguring the wavelengths from cut fibers into healthy fibers. We highlight two challenges that made prior solutions impractical and propose a system called Arrow to address them. First, our measurements show that contrary to common belief, in most cases, the lost capacity is only partially restorable. This poses a cross-layer challenge from the Traffic Engineering (TE) perspective that has not been considered before: “Which IP links should be restored and by how much to best match the TE objective?” To address this challenge, Arrow's restoration-aware TE system takes a set of partial restoration candidates (that we call LotteryTickets) as input and proactively finds the best restoration plan. Second, prior work has not considered the reconfiguration latency of amplifiers. However, in practical settings, amplifiers add tens of minutes of reconfiguration delay. To enable fast and practical restoration, Arrow leverages optical noise loading and bypasses amplifier reconfiguration altogether. We evaluate Arrow using large-scale simulations and a testbed. Our testbed demonstrates Arrow's end-to-end restoration latency is eight seconds. Our large-scale simulations compare Arrow to the state-of-the-art TE schemes and show it can support 2.0x--2.4x more demand without compromising 99.99% availability.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,560–579,20,"traffic engineering, network optimization, wide-area networks, optical restoration, randomized rounding","Virtual Event, USA",SIGCOMM '21,,,,,,
92,inproceedings,"Foukas, Xenofon and Radunovic, Bozidar",Concordia: Teaching the 5G VRAN to Share Compute,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472894,10.1145/3452296.3472894,"Virtualized Radio Access Network (vRAN) offers a cost-efficient solution for running the 5G RAN as a virtualized network function (VNF) on commodity hardware. The vRAN is more efficient than traditional RANs, as it multiplexes several base station workloads on the same compute hardware. Our measurements show that, whilst this multiplexing provides efficiency gains, more than 50% of the CPU cycles in typical vRAN settings still remain unused. A way to further improve CPU utilization is to collocate the vRAN with general-purpose workloads. However, to maintain performance, vRAN tasks have sub-millisecond latency requirements that have to be met 99.999% of times. We show that this is difficult to achieve with existing systems. We propose Concordia, a userspace deadline scheduling framework for the vRAN on Linux. Concordia builds prediction models using quantile decision trees to predict the worst case execution times of vRAN signal processing tasks. The Concordia scheduler is fast (runs every 20 us) and the prediction models are accurate, enabling the system to reserve a minimum number of cores required for vRAN tasks, leaving the rest for general-purpose workloads. We evaluate Concordia on a commercial-grade reference vRAN platform. We show that it meets the 99.999% reliability requirements and reclaims more than 70% of idle CPU cycles without affecting the RAN performance.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,580–596,17,"mobile networks, machine learning, real-time scheduling, edge computing, prediction model, NFV, 5G, vRAN","Virtual Event, USA",SIGCOMM '21,,,,,,
93,inproceedings,"Li, Yang and Lin, Hao and Li, Zhenhua and Liu, Yunhao and Qian, Feng and Gong, Liangyi and Xin, Xianlong and Xu, Tianyin","A Nationwide Study on Cellular Reliability: Measurement, Analysis, and Enhancements",2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472908,10.1145/3452296.3472908,"With recent advances on cellular technologies (such as 5G) that push the boundary of cellular performance, cellular reliability has become a key concern of cellular technology adoption and deployment. However, this fundamental concern has never been addressed due to the challenges of measuring cellular reliability on mobile devices and the cost of conducting large-scale measurements. This paper closes the knowledge gap by presenting the first large-scale, in-depth study on cellular reliability with more than 70 million Android phones across 34 different hardware models. Our study identifies the critical factors that affect cellular reliability and clears up misleading intuitions indicated by common wisdom. In particular, our study pinpoints that software reliability defects are among the main root causes of cellular data connection failures. Our work provides actionable insights for improving cellular reliability at scale. More importantly, we have built on our insights to develop enhancements that effectively address cellular reliability issues with remarkable real-world impact---our optimizations on Android's cellular implementations have effectively reduced 40% cellular connection failures for 5G phones and 36% failure duration across all phones.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,597–609,13,"cellular network, cellular connection management, reliability measurement, mobile operating system, 5G network","Virtual Event, USA",SIGCOMM '21,,,,,,
94,inproceedings,"Narayanan, Arvind and Zhang, Xumiao and Zhu, Ruiyang and Hassan, Ahmad and Jin, Shuowei and Zhu, Xiao and Zhang, Xiaoxuan and Rybkin, Denis and Yang, Zhengxuan and Mao, Zhuoqing Morley and Qian, Feng and Zhang, Zhi-Li","A Variegated Look at 5G in the Wild: Performance, Power, and QoE Implications",2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472923,10.1145/3452296.3472923,"Motivated by the rapid deployment of 5G, we carry out an in-depth measurement study of the performance, power consumption, and application quality-of-experience (QoE) of commercial 5G networks in the wild. We examine different 5G carriers, deployment schemes (Non-Standalone, NSA vs. Standalone, SA), radio bands (mmWave and sub 6-GHz), protocol configurations (_e.g._ Radio Resource Control state transitions), mobility patterns (stationary, walking, driving), client devices (_i.e._ User Equipment), and upper-layer applications (file download, video streaming, and web browsing). Our findings reveal key characteristics of commercial 5G in terms of throughput, latency, handover behaviors, radio state transitions, and radio power consumption under the above diverse scenarios, with detailed comparisons to 4G/LTE networks. Furthermore, our study provides key insights into how upper-layer applications should best utilize 5G by balancing the critical tradeoff between performance and energy consumption, as well as by taking into account the availability of both network and computation resources. We have released the datasets and tools of our study at https://github.com/SIGCOMM21-5G/artifact.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,610–625,16,"5G, power characteristics, mmWave, energy efficiency, video streaming, dataset, power model, network measurement, latency","Virtual Event, USA",SIGCOMM '21,,,,,,
95,inproceedings,"Luo, Zhihong and Fu, Silvery and Theis, Mark and Hasan, Shaddi and Ratnasamy, Sylvia and Shenker, Scott",Democratizing Cellular Access with CellBricks,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3473336,10.1145/3452296.3473336,"Markets in which competition thrives are good for both consumers and innovation but, unfortunately, competition is not thriving in the increasingly important cellular market. We propose CellBricks, a novel cellular architecture that lowers the barrier to entry for new operators by enabling users to consume access on-demand from any available cellular operator — small or large, trusted or untrusted. CellBricks achieves this by moving support for mobility and user management (authentication and billing) out of the network and into end hosts. These changes, we believe, bring valuable benefits beyond enabling competition: they lead to a cellular infrastructure that is simpler and more efficient.We design, build, and evaluate CellBricks, showing that its benefits come at little-to-no cost in performance, with application performance overhead between -1.6% to 3.1% of that achieved by current cellular infrastructure.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,626–640,15,"host-driven mobility, cellular architecture, democratization","Virtual Event, USA",SIGCOMM '21,,,,,,
96,inproceedings,"Zhuang, Siyuan and Li, Zhuohan and Zhuo, Danyang and Wang, Stephanie and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Stoica, Ion",Hoplite: Efficient and Fault-Tolerant Collective Communication for Task-Based Distributed Systems,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472897,10.1145/3452296.3472897,"Task-based distributed frameworks (e.g., Ray, Dask, Hydro) have become increasingly popular for distributed applications that contain asynchronous and dynamic workloads, including asynchronous gradient descent, reinforcement learning, and model serving. As more data-intensive applications move to run on top of task-based systems, collective communication efficiency has become an important problem. Unfortunately, traditional collective communication libraries (e.g., MPI, Horovod, NCCL) are an ill fit, because they require the communication schedule to be known before runtime and they do not provide fault tolerance.We design and implement Hoplite, an efficient and fault-tolerant collective communication layer for task-based distributed systems. Our key technique is to compute data transfer schedules on the fly and execute the schedules efficiently through fine-grained pipelining. At the same time, when a task fails, the data transfer schedule adapts quickly to allow other tasks to keep making progress. We apply Hoplite to a popular task-based distributed framework, Ray. We show that Hoplite speeds up asynchronous stochastic gradient descent, reinforcement learning, and serving an ensemble of machine learning models that are difficult to execute efficiently with traditional collective communication by up to 7.8x, 3.9x, and 3.3x, respectively.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,641–656,16,"collective communication, distributed systems","Virtual Event, USA",SIGCOMM '21,,,,,,
97,inproceedings,"Khani, Mehrdad and Ghobadi, Manya and Alizadeh, Mohammad and Zhu, Ziyi and Glick, Madeleine and Bergman, Keren and Vahdat, Amin and Klenk, Benjamin and Ebrahimi, Eiman",SiP-ML: High-Bandwidth Optical Network Interconnects for Machine Learning Training,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472900,10.1145/3452296.3472900,"This paper proposes optical network interconnects as a key enabler for building high-bandwidth ML training clusters with strong scaling properties. Our design, called SiP-ML, accelerates the training time of popular DNN models using silicon photonics links capable of providing multiple terabits-per-second of bandwidth per GPU. SiP-ML partitions the training job across GPUs with hybrid data and model parallelism while ensuring the communication pattern can be supported efficiently on the network interconnect. We develop task partitioning and device placement methods that take the degree and reconfiguration latency of optical interconnects into account. Simulations using real DNN models show that, compared to the state-of-the-art electrical networks, our approach improves training time by 1.3--9.1x.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,657–675,19,"optical networks, reconfigurable networks, distributed machine learning, silicon photonics","Virtual Event, USA",SIGCOMM '21,,,,,,
98,inproceedings,"Fei, Jiawei and Ho, Chen-Yu and Sahu, Atal N. and Canini, Marco and Sapio, Amedeo",Efficient Sparse Collective Communication and Its Application to Accelerate Distributed Deep Learning,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472904,10.1145/3452296.3472904,"Efficient collective communication is crucial to parallel-computing applications such as distributed training of large-scale recommendation systems and natural language processing models. Existing collective communication libraries focus on optimizing operations for dense inputs, resulting in transmissions of many zeros when inputs are sparse. This counters current trends that see increasing data sparsity in large models.We propose OmniReduce, an efficient streaming aggregation system that exploits sparsity to maximize effective bandwidth use by sending only non-zero data blocks. We demonstrate that this idea is beneficial and accelerates distributed training by up to 8.2x. Even at 100 Gbps, OmniReduce delivers 1.4--2.9x better performance for network-bottlenecked DNNs.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,676–691,16,"deep learning, distributed training","Virtual Event, USA",SIGCOMM '21,,,,,,
99,inproceedings,"Jyothi, Sangeetha Abdu",Solar Superstorms: Planning for an Internet Apocalypse,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472916,10.1145/3452296.3472916,"Black swan events are hard-to-predict rare events that can significantly alter the course of our lives. The Internet has played a key role in helping us deal with the coronavirus pandemic, a recent black swan event. However, Internet researchers and operators are mostly blind to another black swan event that poses a direct threat to Internet infrastructure. In this paper, we investigate the impact of solar superstorms that can potentially cause large-scale Internet outages covering the entire globe and lasting several months. We discuss the challenges posed by such activity and currently available mitigation techniques. Using real-world datasets, we analyze the robustness of the current Internet infrastructure and show that submarine cables are at greater risk of failure compared to land cables. Moreover, the US has a higher risk for disconnection compared to Asia. Finally, we lay out steps for improving the Internet's resiliency.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,692–704,13,"internet topology, internet resilience, solar storms","Virtual Event, USA",SIGCOMM '21,,,,,,
100,inproceedings,"Ding, Yi and Yang, Yu and Jiang, Wenchao and Liu, Yunhuai and He, Tian and Zhang, Desheng",Nationwide Deployment and Operation of a Virtual Arrival Detection System in the Wild,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472911,10.1145/3452296.3472911,"We report a 30-month nationwide deployment and operation study of an indoor arrival detection system based on Bluetooth Low Energy called VALID in 364 Chinese cities. VALID is pilot-studied, deployed, and operated in the wild to infer real-time indoor arrival status of couriers, and improve their status reporting behavior based on the detection. During its full nationwide operation (2018/12- 2021/01), VALID consists of virtual devices at 3 million shops and restaurants, where 530,859 of them are in multi-story malls and markets to infer and influence 1 million couriers' behavior, and assist the scheduling of 3.9 billion orders for 186 million customers. Although indoor arrival detection is straightforward in controlled environments, the scale of our platform makes the cost prohibitively high. In this work, we explore to use merchants' smartphones under their consent as a virtual infrastructure to design, build, deploy, and operate VALID from in-lab conception to nationwide operation in three phases for 30 months. We consider metrics including system evolution, reliability, utility, participation, energy, privacy, monetary benefits, along with couriers' behavior changes. We share three lessons and their implications for similar wireless sensing or communication systems with large geospatial operations.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,705–717,13,"Bluetooth sensing, arrival detection, nationwide deployment, operational system","Virtual Event, USA",SIGCOMM '21,,,,,,
101,inproceedings,"Lutu, Andra and Perino, Diego and Bagnulo, Marcelo and Bustamante, Fabi\'{a",Insights from Operating an IP Exchange Provider,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472930,10.1145/3452296.3472930,"IP Exchange Providers (IPX-Ps) offer to their customers (e.g., mobile or IoT service providers) global data roaming and support for a variety of emerging services. They peer to other IPX-Ps and form the IPX network, which interconnects 800 MNOs worldwide offering their customers access to mobile services in any other country. Despite the importance of IPX-Ps, little is known about their operations and performance. In this paper, we shed light on these opaque providers by analyzing a large IPX-P with more than 100 PoPs in 40+ countries, with a particularly strong presence in America and Europe. Specifically, we characterize the traffic and performance of the main infrastructures of the IPX-P (i.e., 2-3-4G signaling and GTP tunneling), and provide implications for its operation, as well as for the IPX-P's customers. Our analysis is based on statistics we collected during two time periods (i.e., prior and during COVID-19 pandemic) and includes insights on the main service the platform supports (i.e., IoT and data roaming), traffic breakdown and geographical/temporal distribution, communication performance (e.g., tunnel setup time, RTTs). Our results constitute a step towards advancing the understanding of IPX-Ps at their core, and provide guidelines for their operations and customer satisfaction.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,718–730,13,"mobile networks, international mobile roaming, performance analysis, measurements, IPX provider","Virtual Event, USA",SIGCOMM '21,,,,,,
102,inproceedings,"Sonchack, John and Loehr, Devon and Rexford, Jennifer and Walker, David",Lucid: A Language for Control in the Data Plane,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472903,10.1145/3452296.3472903,"Programmable switch hardware makes it possible to move fine-grained control logic inside the network data plane, improving performance for a wide range of applications. However, applications with integrated control are inherently hard to write in existing data-plane programming languages such as P4. This paper presents Lucid, a language that raises the level of abstraction for putting control functionality in the data plane. Lucid introduces abstractions that make it easy to write sophisticated data-plane applications with interleaved packet-handling and control logic, specialized type and syntax systems that prevent programmer bugs related to data-plane state, and an open-sourced compiler that translates Lucid programs into P4 optimized for the Intel Tofino. These features make Lucid general and easy to use, as we demonstrate by writing a suite of ten different data-plane applications in Lucid. Working prototypes take well under an hour to write, even for a programmer without prior Tofino experience, have around 10x fewer lines of code compared to P4, and compile efficiently to real hardware. In a stateful firewall written in Lucid, we find that moving control from a switch's CPU to its data-plane processor using Lucid reduces the latency of performance-sensitive operations by over 300X.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,731–747,17,"ordered type-and-effect-system, data plane programming abstractions, syntactic constraints, network control","Virtual Event, USA",SIGCOMM '21,,,,,,
103,inproceedings,"Tang, Alan and Kakarla, Siva Kesava Reddy and Beckett, Ryan and Zhai, Ennan and Brown, Matt and Millstein, Todd and Tamir, Yuval and Varghese, George",Campion: Debugging Router Configuration Differences,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472925,10.1145/3452296.3472925,"We present a new approach for debugging two router configurations that are intended to be behaviorally equivalent. Existing router verification techniques cannot identify all differences or localize those differences to relevant configuration lines. Our approach addresses these limitations through a _modular_ analysis, which separately analyzes pairs of corresponding configuration components. It handles all router components that affect routing and forwarding, including configuration for BGP, OSPF, static routes, route maps and ACLs. Further, for many configuration components our modular approach enables simple _structural equivalence_ checks to be used without additional loss of precision versus modular semantic checks, aiding both efficiency and error localization. We implemented this approach in the tool Campion and applied it to debugging pairs of backup routers from different manufacturers and validating replacement of critical routers. Campion analyzed 30 proposed router replacements in a production cloud network and proactively detected four configuration bugs, including a route reflector bug that could have caused a severe outage. Campion also found multiple differences between backup routers from different vendors in a university network. These were undetected for three years, and depended on subtle semantic differences that the operators said they were ""highly unlikely"" to detect by ""just eyeballing the configs.""",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,748–761,14,"error localization, modular reasoning, network verification, equivalence checking","Virtual Event, USA",SIGCOMM '21,,,,,,
104,inproceedings,"Ferreira, Tiago and Brewton, Harrison and D'Antoni, Loris and Silva, Alexandra",Prognosis: Closed-Box Analysis of Network Protocol Implementations,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472938,10.1145/3452296.3472938,"We present Prognosis, a framework offering automated closed-box learning and analysis of models of network protocol implementations. Prognosis can learn models that vary in abstraction level from simple deterministic automata to models containing data operations, such as register updates, and can be used to unlock a variety of analysis techniques -- model checking temporal properties, computing differences between models of two implementations of the same protocol, or improving testing via model-based test generation. Prognosis is modular and easily adaptable to different protocols (e.g. TCP and QUIC) and their implementations. We use Prognosis to learn models of (parts of) three QUIC implementations -- Quiche (Cloudflare), Google QUIC, and Facebook mvfst -- and use these models to analyse the differences between the various implementations. Our analysis provides insights into different design choices and uncovers potential bugs. Concretely, we have found critical bugs in multiple QUIC implementations, which have been acknowledged by the developers.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,762–774,13,"model learning, varied abstraction modelling, protocol state machines, bug finding, synthesis","Virtual Event, USA",SIGCOMM '21,,,,,,
105,inproceedings,"Xu, Xieyang and Beckett, Ryan and Jayaraman, Karthick and Mahajan, Ratul and Walker, David",Test Coverage Metrics for the Network,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472941,10.1145/3452296.3472941,"Testing and verification have emerged as key tools in the battle to improve the reliability of networks and the services they provide. However, the success of even the best technology of this sort is limited by how effectively it is applied, and in today's enormously complex industrial networks, it is surprisingly easy to overlook particular interfaces, routes, or flows when creating a test suite. Moreover, network engineers, unlike their software counterparts, have no help to battle this problem—there are no metrics or systems to compute the quality of their test suites or the extent to which their networks have been verified.To address this gap, we develop a general framework to define and compute network coverage for stateless network data planes. It computes coverage for a range of network components (EG, interfaces, devices, paths) and supports many types of tests (e.g., concrete versus symbolic; local versus end-to-end; tests that check network state versus those that analyze behavior). Our framework is based on the observation that any network dataplane component can be decomposed into forwarding rules and all types of tests ultimately exercise these rules using one or more packets.We build a system called Yardstick based on this framework and deploy it in Microsoft Azure. Within the first month of its deployment inside one of the production networks, it uncovered several testing gaps and helped improve testing by covering 89% more forwarding rules and 17% more network interfaces.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,775–787,13,"coverage metrics, reliability, network verification","Virtual Event, USA",SIGCOMM '21,,,,,,
106,inproceedings,"Mahimkar, Ajay and de Andrade, Carlos Eduardo and Sinha, Rakesh and Rana, Giritharan",A Composition Framework for Change Management,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472901,10.1145/3452296.3472901,"Change management has been a long-standing challenge for network operations. The large scale and diversity of networks, their complex dependencies, and continuous evolution through technology and software updates combined with the risk of service impact create tremendous challenges to effectively manage changes. In this paper, we use data from a large service provider and experiences of their operations teams to highlight the need for quick and easy adaptation of change management capabilities and keep up with the continuous network changes. We propose a new framework CORNET (COmposition fRamework for chaNge managEmenT) with key ideas of modularization of changes into building blocks, flexible composition into change workflows, change plan optimization, change impact verification, and automated translation of high-level change management intent into low-level implementations and mathematical models. We demonstrate the effectiveness of CORNET using real-world data collected from 4G and 5G cellular networks and virtualized services such as VPN and SDWAN running in the cloud as well as experiments conducted on a testbed of virtualized network functions. We also share our operational experiences and lessons learned from successfully using CORNET within a large service provider network over the last three years.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,788–806,19,"impact verification, change plan optimization, network change management, composition framework","Virtual Event, USA",SIGCOMM '21,,,,,,
107,inproceedings,"Mahimkar, Ajay and Sivakumar, Ashiwan and Ge, Zihui and Pathak, Shomik and Biswas, Karunasish",Auric: Using Data-Driven Recommendation to Automatically Generate Cellular Configuration,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472906,10.1145/3452296.3472906,"Cellular service providers add carriers in the network in order to support the increasing demand in voice and data traffic and provide good quality of service to the users. Addition of new carriers requires the network operators to accurately configure their parameters for the desired behaviors. This is a challenging problem because of the large number of parameters related to various functions like user mobility, interference management and load balancing. Furthermore, the same parameters can have varying values across different locations to manage user and traffic behaviors as planned and respond appropriately to different signal propagation patterns and interference. Manual configuration is time-consuming, tedious and error-prone, which could result in poor quality of service. In this paper, we propose a new data-driven recommendation approach Auric to automatically and accurately generate configuration parameters for new carriers added in cellular networks. Our approach incorporates new algorithms based on collaborative filtering and geographical proximity to automatically determine similarity across existing carriers. We conduct a thorough evaluation using real-world LTE network data and observe a high accuracy (96%) across a large number of carriers and configuration parameters. We also share experiences from our deployment and use of Auric in production environments.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,807–820,14,"collaborative filtering, carrier addition, recommendation algorithms, cellular network configuration","Virtual Event, USA",SIGCOMM '21,,,,,,
108,inproceedings,"Reininger, Michael and Arora, Arushi and Herwig, Stephen and Francino, Nicholas and Hurst, Jayson and Garman, Christina and Levin, Dave",Bento: Safely Bringing Network Function Virtualization to Tor,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472919,10.1145/3452296.3472919,"Tor is a powerful and important tool for providing anonymity and censorship resistance to users around the world. Yet it is surprisingly difficult to deploy new services in Tor—it is largely relegated to proxies and hidden services—or to nimbly react to new forms of attack. Conversely, “non-anonymous” Internet services are thriving like never before because of recent advances in programmable networks, such as Network Function Virtualization (NFV) which provides programmable in-network middleboxes.This paper seeks to close this gap by introducing programmable middleboxes into the Tor network. In this architecture, users can install and run sophisticated “functions” on willing Tor routers. We demonstrate a wide range of functions that improve anonymity, resilience to attack, performance of hidden services, and more. We present the design and implementation of an architecture, Bento, that protects middlebox nodes from the functions they run—and protects the functions from the middleboxes they run on.We evaluate Bento by running it on the live Tor network. We show that, with just a few lines of Python, we can significantly extend the capabilities of Tor to meet users' anonymity needs and nimbly react to new threats. We will be making our code and data publicly available.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,821–835,15,"Intel SGX, Tor, network function virtualization (NFV)","Virtual Event, USA",SIGCOMM '21,,,,,,
109,inproceedings,"Dai, Tianxiang and Jeitner, Philipp and Shulman, Haya and Waidner, Michael",From IP to Transport and beyond: Cross-Layer Attacks against Applications,2021,9781450383837,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3452296.3472933,10.1145/3452296.3472933,"We perform the first analysis of methodologies for launching DNS cache poisoning: manipulation at the IP layer, hijack of the inter-domain routing and probing open ports via side channels. We evaluate these methodologies against DNS resolvers in the Internet and compare them with respect to effectiveness, applicability and stealth. Our study shows that DNS cache poisoning is a practical and pervasive threat.We then demonstrate cross-layer attacks that leverage DNS cache poisoning for attacking popular systems, ranging from security mechanisms, such as RPKI, to applications, such as VoIP. In addition to more traditional adversarial goals, most notably impersonation and Denial of Service, we show for the first time that DNS cache poisoning can even enable adversaries to bypass cryptographic defences: we demonstrate how DNS cache poisoning can facilitate BGP prefix hijacking of networks protected with RPKI even when all the other networks apply route origin validation to filter invalid BGP announcements. Our study shows that DNS plays a much more central role in the Internet security than previously assumed.We recommend mitigations for securing the applications and for preventing cache poisoning.",Proceedings of the 2021 ACM SIGCOMM 2021 Conference,836–849,14,"BGP hijacking, DNS cache poisoning, fragmentation, side channels","Virtual Event, USA",SIGCOMM '21,,,,,,
110,inproceedings,"He, Zhiqiang and Wang, Dongyang and Fu, Binzhang and Tan, Kun and Hua, Bei and Zhang, Zhi-Li and Zheng, Kai",MasQ: RDMA for Virtual Private Cloud,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405849,10.1145/3387514.3405849,"RDMA communication in virtual private cloud (VPC) networks is still a challenging job due to the difficulty in fulfilling all virtualization requirements without sacrificing RDMA communication performance. To address this problem, this paper proposes a software-defined solution, namely, MasQ, which is short for ""queue masquerade"". The core insight of MasQ is that all RDMA communications should associate with at least one queue pair (QP). Thus, the requirements of virtualization, such as network isolation and the application of security rules, can be easily fulfilled if QP's behavior is properly defined. In particular, MasQ exploits the virtio-based paravirtualization technique to realize the control path. Moreover, to avoid performance overhead, MasQ leaves all data path operations, such as sending and receiving, to the hardware. We have implemented MasQ in the OpenFabrics Enterprise Distribution (OFED) framework and proved its scalability and performance efficiency by evaluating it against typical applications. The results demonstrate that MasQ achieves almost the same performance as bare-metal RDMA for data communication.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",1–14,14,"Network virtualization, Datacenter network, RDMA","Virtual Event, USA",SIGCOMM '20,,,,,,
111,inproceedings,"Li, Tong and Zheng, Kai and Xu, Ke and Jadhav, Rahul Arvind and Xiong, Tao and Winstein, Keith and Tan, Kun",TACK: Improving Wireless Transport Performance by Taming Acknowledgments,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405850,10.1145/3387514.3405850,"The shared nature of the wireless medium induces contention between data transport and backward signaling, such as acknowledgement. The current way of TCP acknowledgment induces control overhead which is counter-productive for TCP performance especially in wireless local area network (WLAN) scenarios.In this paper, we present a new acknowledgement called TACK (""Tame ACK""), as well as its TCP implementation TCP-TACK. TCP-TACK works on top of commodity WLAN, delivering high wireless transport goodput with minimal control overhead in the form of ACKs, without any hardware modification. To minimize ACK frequency, TACK abandons the legacy received-packet-driven ACK. Instead, it balances byte-counting ACK and periodic ACK so as to achieve a controlled ACK frequency. Evaluation results show that TCP-TACK achieves significant advantages over legacy TCP in WLAN scenarios due to less contention between data packets and ACKs. Specifically, TCP-TACK reduces over 90% of ACKs and also obtains an improvement of ~ 28% on good-put. We further find it performs equally well as high-speed TCP variants in wide area network (WAN) scenarios, this is attributed to the advancements of the TACK-based protocol design in loss recovery, round-trip timing, and send rate control.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",15–30,16,"periodic ACK, instant ACK, acknowledgement mechanism, ACK frequency","Virtual Event, USA",SIGCOMM '20,,,,,,
112,inproceedings,"Fang, Chongrong and Liu, Haoyu and Miao, Mao and Ye, Jie and Wang, Lei and Zhang, Wansheng and Kang, Daxiang and Lyv, Biao and Cheng, Peng and Chen, Jiming",VTrace: Automatic Diagnostic System for Persistent Packet Loss in Cloud-Scale Overlay Network,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405851,10.1145/3387514.3405851,"Persistent packet loss in the cloud-scale overlay network severely compromises tenant experiences. Cloud providers are keen to automatically and quickly determine the root cause of such problems. However, existing work is either designed for the physical network or insufficient to present the concrete reason of packet loss. In this paper, we propose to record and analyze the on-site forwarding condition of packets during packet-level tracing. The cloud-scale overlay network presents great challenges to achieve this goal with its high network complexity, multi-tenant nature, and diversity of root causes. To address these challenges, we present VTrace, an automatic diagnostic system for persistent packet loss over the cloud-scale overlay network. Utilizing the ""fast path-slow path"" structure of virtual forwarding devices (VFDs), e.g., vSwitches, VTrace installs several ""coloring, matching and logging"" rules in VFDs to selectively track the packets of interest and inspect them in depth. The detailed forwarding situation at each hop is logged and then assembled to perform analysis with an efficient path reconstruction scheme. Experiments are conducted to demonstrate VTrace's low overhead and quick responsiveness. We share experiences of how VTrace efficiently resolves persistent packet loss issues after deploying it in Alibaba Cloud for over 20 months.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",31–43,13,"Cloud-scale overlay network, Network diagnosis","Virtual Event, USA",SIGCOMM '20,,,,,,
113,inproceedings,"Gao, Xiangyu and Kim, Taegyun and Wong, Michael D. and Raghunathan, Divya and Varma, Aatish Kishan and Kannan, Pravein Govindan and Sivaraman, Anirudh and Narayana, Srinivas and Gupta, Aarti",Switch Code Generation Using Program Synthesis,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405852,10.1145/3387514.3405852,"Writing packet-processing programs for programmable switch pipelines is challenging because of their all-or-nothing nature: a program either runs at line rate if it can fit within pipeline resources, or does not run at all. It is the compiler's responsibility to fit programs into pipeline resources. However, switch compilers, which use rewrite rules to generate switch machine code, often reject programs because the rules fail to transform programs into a form that can be mapped to a pipeline's limited resources---even if a mapping actually exists.This paper presents a compiler, Chipmunk, which formulates code generation as a program synthesis problem. Chipmunk uses a program synthesis engine, SKETCH, to transform high-level programs down to switch machine code. However, naively formulating code generation as program synthesis can lead to long compile times. Hence, we develop a new domain-specific synthesis technique, slicing, which reduces compile times by 1-387x and 51x on average.Using a switch hardware simulator, we show that Chipmunk compiles many programs that a previous rule-based compiler, Domino, rejects. Chipmunk also produces machine code with fewer pipeline stages than Domino. A Chipmunk backend for the Tofino programmable switch shows that program synthesis can produce machine code for high-speed switches.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",44–61,18,"program synthesis, code generation, Programmable switches, slicing, packet processing pipelines","Virtual Event, USA",SIGCOMM '20,,,,,,
114,inproceedings,"Shi, Shouqian and Qian, Chen",Concurrent Entanglement Routing for Quantum Networks: Model and Designs,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405853,10.1145/3387514.3405853,"Quantum entanglement enables important computing applications such as quantum key distribution. Based on quantum entanglement, quantum networks are built to provide long-distance secret sharing between two remote communication parties. Establishing a multi-hop quantum entanglement exhibits a high failure rate, and existing quantum networks rely on trusted repeater nodes to transmit quantum bits. However, when the scale of a quantum network increases, it requires end-to-end multi-hop quantum entanglements in order to deliver secret bits without letting the repeaters know the secret bits. This work focuses on the entanglement routing problem, whose objective is to build long-distance entanglements via untrusted repeaters for concurrent source-destination pairs through multiple hops. Different from existing work that analyzes the traditional routing techniques on special network topologies, we present a comprehensive entanglement routing model that reflects the differences between quantum networks and classical networks as well as a new entanglement routing algorithm that utilizes the unique properties of quantum networks. Evaluation results show that the proposed algorithm Q-CAST increases the number of successful long-distance entanglements by a big margin compared to other methods. The model and simulator developed by this work may encourage more network researchers to study the entanglement routing problem.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",62–75,14,"Network Modeling, Entanglement Routing, Quantum Networks, Quantum Internet","Virtual Event, USA",SIGCOMM '20,,,,,,
115,inproceedings,"Zhou, Yu and Sun, Chen and Liu, Hongqiang Harry and Miao, Rui and Bai, Shi and Li, Bo and Zheng, Zhilong and Zhu, Lingjun and Shen, Zhen and Xi, Yongqing and Zhang, Pengcheng and Cai, Dennis and Zhang, Ming and Xu, Mingwei",Flow Event Telemetry on Programmable Data Plane,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3406214,10.1145/3387514.3406214,"Network performance anomalies (NPAs), e.g. long-tailed latency, bandwidth decline, etc., are increasingly crucial to cloud providers as applications are getting more sensitive to performance. The fundamental difficulty to quickly mitigate NPAs lies in the limitations of state-of-the-art network monitoring solutions --- coarse-grained counters, active probing, or packet telemetry either cannot provide enough insights on flows or incur too much overhead. This paper presents NetSeer, a flow event telemetry (FET) monitor which aims to discover and record all performance-critical data plane events, e.g. packet drops, congestion, path change, and packet pause. NetSeer is efficiently realized on the programmable data plane. It has a high coverage on flow events including inter-switch packet drop/corruption which is critical but also challenging to retrieve the original flow information, with novel intra- and inter-switch event detection algorithms running on data plane; NetSeer also achieves high scalability and accuracy with innovative designs of event aggregation, information compression, and message batching that mainly run on data plane, using switch CPU as complement. NetSeer has been implemented on commodity programmable switches and NICs. With real case studies and extensive experiments, we show NetSeer can reduce NPA mitigation time by 61%-99% with only 0.01% overhead of monitoring traffic.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",76–89,14,"programmable data plane, Flow event telemetry, monitoring","Virtual Event, USA",SIGCOMM '20,,,,,,
116,inproceedings,"Kim, Daehyeok and Liu, Zaoxing and Zhu, Yibo and Kim, Changhoon and Lee, Jeongkeun and Sekar, Vyas and Seshan, Srinivasan",TEA: Enabling State-Intensive Network Functions on Programmable Switches,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405855,10.1145/3387514.3405855,"Programmable switches have been touted as an attractive alternative for deploying network functions (NFs) such as network address translators (NATs), load balancers, and firewalls. However, their limited memory capacity has been a major stumbling block that has stymied their adoption for supporting state-intensive NFs such as cloud-scale NATs and load balancers that maintain millions of flow-table entries. In this paper, we explore a new approach that leverages DRAM on servers available in typical NFV clusters. Our new system architecture, called TEA (Table Extension Architecture), provides a virtual table abstraction that allows NFs on programmable switches to look up large virtual tables built on external DRAM. Our approach enables switch ASICs to access external DRAM purely in the data plane without involving CPUs on servers. We address key design and implementation challenges in realizing this idea. We demonstrate its feasibility and practicality with our implementation on a Tofino-based programmable switch. Our evaluation shows that NFs built with TEA can look up table entries on external DRAM with low and predictable latency (1.8-2.2 μs) and the lookup throughput can be linearly scaled with additional servers (138 million lookups per seconds with 8 servers).","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",90–106,17,"Programmable networks, Programmable switches, Network Function Virtualization, Data centers, Remote Direct Memory Access","Virtual Event, USA",SIGCOMM '20,,,,,,
117,inproceedings,"Kim, Jaehong and Jung, Youngmok and Yeo, Hyunho and Ye, Juncheol and Han, Dongsu",Neural-Enhanced Live Streaming: Improving Live Video Ingest via Online Learning,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405856,10.1145/3387514.3405856,"Live video accounts for a significant volume of today's Internet video. Despite a large number of efforts to enhance user quality of experience (QoE) both at the ingest and distribution side of live video, the fundamental limitations are that streamer's upstream bandwidth and computational capacity limit the quality of experience of thousands of viewers.To overcome this limitation, we design LiveNAS, a new live video ingest framework that enhances the origin stream's quality by leveraging computation at ingest servers. Our ingest server applies neural super-resolution on the original stream, while imposing minimal overhead on ingest clients. LiveNAS employs online learning to maximize the quality gain and dynamically adjusts the resource use to the real-time quality improvement. LiveNAS delivers high-quality live streams up to 4K resolution, outperforming WebRTC by 1.96 dB on average in Peak-Signal-to-Noise-Ratio on real video streams and network traces, which leads to 12%-69% QoE improvement for live stream viewers.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",107–125,19,"video delivery, super-resolution, online learning, live streaming, deep neural networks","Virtual Event, USA",SIGCOMM '20,,,,,,
118,inproceedings,"Yu, Zhuolong and Zhang, Yiwen and Braverman, Vladimir and Chowdhury, Mosharaf and Jin, Xin","NetLock: Fast, Centralized Lock Management Using Programmable Switches",2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405857,10.1145/3387514.3405857,"Lock managers are widely used by distributed systems. Traditional centralized lock managers can easily support policies between multiple users using global knowledge, but they suffer from low performance. In contrast, emerging decentralized approaches are faster but cannot provide flexible policy support. Furthermore, performance in both cases is limited by the server capability.We present NetLock, a new centralized lock manager that co-designs servers and network switches to achieve high performance without sacrificing flexibility in policy support. The key idea of NetLock is to exploit the capability of emerging programmable switches to directly process lock requests in the switch data plane. Due to the limited switch memory, we design a memory management mechanism to seamlessly integrate the switch and server memory. To realize the locking functionality in the switch, we design a custom data plane module that efficiently pools multiple register arrays together to maximize memory utilization We have implemented a NetLock prototype with a Barefoot Tofino switch and a cluster of commodity servers. Evaluation results show that NetLock improves the throughput by 14.0-18.4x, and reduces the average and 99% latency by 4.7-20.3x and 10.4-18.7x over DSLR, a state-of-the-art RDMA-based solution, while providing flexible policy support.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",126–138,13,"Centralized, Data plane, Lock Management, Programmable Switches","Virtual Event, USA",SIGCOMM '20,,,,,,
119,inproceedings,"Jiang, Chuan and Rao, Sanjay and Tawarmalani, Mohit",PCF: Provably Resilient Flexible Routing,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405858,10.1145/3387514.3405858,"Recently, traffic engineering mechanisms have been developed that guarantee that a network (cloud provider WAN, or ISP) does not experience congestion under failures. In this paper, we show that existing congestion-free mechanisms, notably FFC, achieve performance far short of the network's intrinsic capability. We propose PCF, a set of novel congestion-free mechanisms to bridge this gap. PCF achieves these goals by better modeling network structure, and by carefully enhancing the flexibility of network response while ensuring that the performance under failures can be tractably modeled. All of PCF's schemes involve relatively light-weight operations on failures, and many of them can be realized using a local proportional routing scheme similar to FFC. We show PCF's effectiveness through formal theoretical results, and empirical experiments over 21 Internet topologies. PCF's schemes provably out-perform FFC, and in practice, can sustain higher throughput than FFC by a factor of 1.11X to 1.5X on average across the topologies, while providing a benefit of 2.6X in some cases.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",139–153,15,"network optimization, network resilience","Virtual Event, USA",SIGCOMM '20,,,,,,
120,inproceedings,"Meng, Zili and Wang, Minhu and Bai, Jiasong and Xu, Mingwei and Mao, Hongzi and Hu, Hongxin",Interpreting Deep Learning-Based Networking Systems,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405859,10.1145/3387514.3405859,"While many deep learning (DL)-based networking systems have demonstrated superior performance, the underlying Deep Neural Networks (DNNs) remain blackboxes and stay uninterpretable for network operators. The lack of interpretability makes DL-based networking systems prohibitive to deploy in practice. In this paper, we propose Metis, a framework that provides interpretability for two general categories of networking problems spanning local and global control. Accordingly, Metis introduces two different interpretation methods based on decision tree and hypergraph, where it converts DNN policies to interpretable rule-based controllers and highlight critical components based on analysis over hypergraph. We evaluate Metis over two categories of state-of-the-art DL-based networking systems and show that Metis provides human-readable interpretations while preserving nearly no degradation in performance. We further present four concrete use cases of Metis, showcasing how Metis helps network operators to design, debug, deploy, and ad-hoc adjust DL-based networking systems.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",154–171,18,"Interpretability, decision tree, DL-based networking systems, hypergraph","Virtual Event, USA",SIGCOMM '20,,,,,,
121,inproceedings,"Chi, Zicheng and Liu, Xin and Wang, Wei and Yao, Yao and Zhu, Ting",Leveraging Ambient LTE Traffic for Ubiquitous Passive Communication,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405861,10.1145/3387514.3405861,"To support ubiquitous computing for various applications (such as smart health, smart homes, and smart cities), the communication system requires to be ubiquitously available, ultra-low-power, high throughput, and low-latency. A passive communication system such as backscatter is desirable. However, existing backscatter systems cannot achieve all of the above requirements. In this paper, we present the first LTE backscatter (LScatter) system that leverages the continuous LTE ambient traffic for ubiquitous, high throughput and low latency backscatter communication. Our design is motivated by our observation that LTE ambient traffic is continuous (v.s. bursty and intermittent WiFi/LoRa traffic), which makes LTE ambient traffic a perfect signal source of a backscatter system. Our design addresses practical issues such as time synchronization, phase modulation, as well as phase offset elimination. We extensively evaluated our design using a testbed of backscatter hardware and USRPs in multiple real-world scenarios. Results show that our LScatter's performance is consistently orders of magnitude better than WiFi backscatter in all the above scenarios. For example, LScatter's throughput is 13.63Mbps, which is 368 times higher than the latest ambient WiFi backscatter system [54]. We also demonstrate the effectiveness of our system using two real-world applications.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",172–185,14,"LTE, Internet of things, Backscatter","Virtual Event, USA",SIGCOMM '20,,,,,,
122,inproceedings,"Wu, Yue and Wang, Purui and Xu, Kenuo and Feng, Lilei and Xu, Chenren",Turboboosting Visible Light Backscatter Communication,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3406229,10.1145/3387514.3406229,"Visible light backscatter communication (VLBC) presents an emerging low power IoT connectivity solution with spatial reuse and interference immunity advantages over RF-based (backscatter) technologies. State-of-the-art VLBC systems employ COTS LCD shutter as optical modulator, whose slow response fundamentally throttles its data rate to sub-Kbps, and limits its deployment at scale for use cases where higher rate and/or low latency is a necessity.We design and implement RetroTurbo, a VLBC system dedicated for turboboosting data rate. At the heart of RetroTurbo design is a pair of novel modulation schemes, namely delayed superimposition modulation (DSM) and polarization-based QAM (PQAM), to push the rate limit by strategically coordinating the state of a liquid crystal modulator (LCM) pixel array in time and polarization domains. Specifically, DSM ensures we fully exploit the available SNR for high order modulation in the LCM-imposed nonlinear channel; PQAM is based on polarized light communication that creates a QAM design in polarization domain with flexible angular misalignment between two ends. A real-time near-optimal demodulation algorithm is designed to ensure system's robustness to heterogeneous signal distortion. Based on our prototyped system, RetroTurbo demonstrates 32x and 128x rate gain via experiments and emulation respectively in practical real-world indoor setting.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",186–197,12,"Polarization-based QAM, Delayed Superimposed Modulation, Visible Light Backscatter Communication","Virtual Event, USA",SIGCOMM '20,,,,,,
123,inproceedings,"Ghaznavi, Milad and Jalalpour, Elaheh and Wong, Bernard and Boutaba, Raouf and Mashtizadeh, Ali Jos\'{e",Fault Tolerant Service Function Chaining,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405863,10.1145/3387514.3405863,"Network traffic typically traverses a sequence of middleboxes forming a service function chain, or simply a chain. Tolerating failures when they occur along chains is imperative to the availability and reliability of enterprise applications. Making a chain fault-tolerant is challenging since, in the event of failures, the state of faulty middleboxes must be correctly and quickly recovered while providing high throughput and low latency.In this paper, we introduce FTC, a system design and protocol for fault-tolerant service function chaining. FTC provides strong consistency with up to f middlebox failures for chains of length f + 1 or longer without requiring dedicated replica nodes. In FTC, state updates caused by packet processing at a middlebox are collected, piggybacked onto the packet, and sent along the chain to be replicated. Our evaluation shows that compared with the state of art [51], FTC improves throughput by 2-3.5X for a chain of two to five middleboxes.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",198–210,13,"Service Function Chain Fault Tolerance, Middlebox Reliability","Virtual Event, USA",SIGCOMM '20,,,,,,
124,inproceedings,"Sobrinho, Jo\~{a",Routing on Multiple Optimality Criteria,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405864,10.1145/3387514.3405864,"Standard vectoring protocols, such as EIGRP, BGP, DSDV, or Babel, only route on optimal paths when the total order on path attributes that substantiates optimality is consistent with the extension operation that calculates path attributes from link attributes, leaving out many optimality criteria of practical interest. We present a solution to this problem and, more generally, to the problem of routing on multiple optimality criteria. A key idea is the derivation of a partial order on path attributes that is consistent with the extension operation and respects every optimality criterion of a designated collection of such criteria. We design new vectoring protocols that compute on partial orders, with every node capable of electing multiple attributes per destination rather than a single attribute as in standard vectoring protocols. Our evaluation over publicly available network topologies and attributes shows that the proposed protocols converge fast and enable optimal path routing concurrently for many optimality criteria with only a few elected attributes at each node per destination. We further show how predicating computations on partial orders allows incorporation of service chain constraints on optimal path routing.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",211–225,15,"routing protocols, partial orders, routing algebras, Routing, optimal path routing, optimality criteria","Virtual Event, USA",SIGCOMM '20,,,,,,
125,inproceedings,"Chen, Xiaoqi and Landau-Feibish, Shir and Braverman, Mark and Rexford, Jennifer","BeauCoup: Answering Many Network Traffic Queries, One Memory Update at a Time",2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405865,10.1145/3387514.3405865,"Network administrators constantly monitor network traffic for congestion and attacks. They need to perform a large number of measurements on the traffic simultaneously, to detect different types of anomalies such as heavy hitters or super-spreaders. Existing techniques often focus on a single statistic (e.g., traffic volume) or traffic attribute (e.g., destination IP). However, performing numerous heterogeneous measurements within the constrained memory architecture of modern network devices poses significant challenges, due to the limited number of memory accesses allowed per packet. We propose BeauCoup, a system based on the coupon collector problem, that supports multiple distinct counting queries simultaneously while making only a small constant number of memory accesses per packet. We implement BeauCoup on PISA commodity programmable switches, satisfying the strict memory size and access constraints while using a moderate portion of other data-plane hardware resources. Evaluations show BeauCoup achieves the same accuracy as other sketch-based or sampling-based solutions using 4x fewer memory access.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",226–239,14,"Streaming Algorithm, Programmable Switch, Distinct Counting, Sketching, Network Measurement, Data Plane","Virtual Event, USA",SIGCOMM '20,,,,,,
126,inproceedings,"Abedi, Ali and Dehbashi, Farzan and Mazaheri, Mohammad Hossein and Abari, Omid and Brecht, Tim",WiTAG: Seamless WiFi Backscatter Communication,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405866,10.1145/3387514.3405866,"WiFi backscatter communication has the potential to enable battery-free sensors which can transmit data using a WiFi network. In order for WiFi backscatter systems to be practical they should be compatible with existing WiFi networks without any hardware or software modifications. Moreover, they should work with networks that use encryption. In this paper, we present WiTAG which achieves these requirements, making the implementation and deployment of WiFi backscatter communication more practical. In contrast with existing systems which utilize the physical layer for backscatter communication, we take a different approach by leveraging features of the MAC layer to communicate. WiTAG is designed to send data by selectively interfering with subframes (MPDUs) in an aggregated frame (A-MPDU). This enables standard compliant communication using modern, open or encrypted 802.11n and 802.11ac networks without requiring hardware or software modifications to any devices. We implement WiTAG using off-the-shelf components and evaluate its performance in line-of-sight and non-line-of-sight scenarios. We show that WiTAG achieves a throughput of up to 4 Kbps without impacting other devices in the network.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",240–252,13,"WiFi Backscatter, 802.11 Networks, Battery-free communication, Sensors, Internet of Things (IoT)","Virtual Event, USA",SIGCOMM '20,,,,,,
127,inproceedings,"Gao, Jiaqi and Yaseen, Nofel and MacDavid, Robert and Frujeri, Felipe Vieira and Liu, Vincent and Bianchini, Ricardo and Aditya, Ramaswamy and Wang, Xiaohang and Lee, Henry and Maltz, David and Yu, Minlan and Arzani, Behnaz",Scouts: Improving the Diagnosis Process Through Domain-Customized Incident Routing,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405867,10.1145/3387514.3405867,"Incident routing is critical for maintaining service level objectives in the cloud: the time-to-diagnosis can increase by 10x due to mis-routings. Properly routing incidents is challenging because of the complexity of today's data center (DC) applications and their dependencies. For instance, an application running on a VM might rely on a functioning host-server, remote-storage service, and virtual and physical network components. It is hard for any one team, rule-based system, or even machine learning solution to fully learn the complexity and solve the incident routing problem. We propose a different approach using per-team Scouts. Each teams' Scout acts as its gate-keeper --- it routes relevant incidents to the team and routes-away unrelated ones. We solve the problem through a collection of these Scouts. Our PhyNet Scout alone --- currently deployed in production --- reduces the time-to-mitigation of 65% of mis-routed incidents in our dataset.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",253–269,17,"Machine learning, Diagnosis, Data center networks","Virtual Event, USA",SIGCOMM '20,,,,,,
128,inproceedings,"Manousis, Antonis and Sharma, Rahul Anand and Sekar, Vyas and Sherry, Justine",Contention-Aware Performance Prediction For Virtualized Network Functions,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405868,10.1145/3387514.3405868,"At the core of Network Functions Virtualization lie Network Functions (NFs) that run co-resident on the same server, contend over its hardware resources and, thus, might suffer from reduced performance relative to running alone on the same hardware. Therefore, to efficiently manage resources and meet performance SLAs, NFV orchestrators need mechanisms to predict contention-induced performance degradation. In this work, we find that prior performance prediction frameworks suffer from poor accuracy on modern architectures and NFs because they treat memory as a monolithic whole. In addition, we show that, in practice, there exist multiple components of the memory subsystem that can separately induce contention. By precisely characterizing (1) the pressure each NF applies on the server's shared hardware resources (contentiousness) and (2) how susceptible each NF is to performance drop due to competing contentiousness (sensitivity), we develop SLOMO, a multivariable performance prediction framework for Network Functions. We show that relative to prior work SLOMO reduces prediction error by 2-5x and enables 6-14% more efficient cluster utilization. SLOMO's codebase can be found at https://github.com/cmu-snap/SLOMO.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",270–282,13,"Packet Processing Software, Network Functions Performance","Virtual Event, USA",SIGCOMM '20,,,,,,
129,inproceedings,"Zhang, Kaiyuan and Zhuo, Danyang and Krishnamurthy, Arvind",Gallium: Automated Software Middlebox Offloading to Programmable Switches,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405869,10.1145/3387514.3405869,"Researchers have shown that offloading software middleboxes (e.g., NAT, firewall, load balancer) to programmable switches can yield orders-of-magnitude performance gains. However, it requires manually selecting the middle-box components to offload and rewriting the offloaded code in P4, a domain-specific language for programmable switches. We design and implement Gallium, a compiler that transforms an input software middlebox into two parts---a P4 program that runs on a programmable switch and an x86 non-offloaded program that runs on a regular middlebox server. Gallium ensures that (1) the combined effect of the P4 program and the non-offloaded program is functionally equivalent to the input middlebox program, (2) the P4 program respects the resource constraints in the programmable switch, and (3) the run-to-completion semantics are met under concurrent execution. Our evaluations show that Gallium saves 21-79% of processing cycles and reduces latency by about 31% across various software middleboxes.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",283–295,13,"Middleboxes, Protocol offload","Virtual Event, USA",SIGCOMM '20,,,,,,
130,inproceedings,"Yu, Liangcheng and Sonchack, John and Liu, Vincent",Mantis: Reactive Programmable Switches,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405870,10.1145/3387514.3405870,"For modern data center switches, the ability to---with minimum latency and maximum flexibility--- react to current network conditions is important for managing increasingly dynamic networks. The traditional approach to implementing this type of behavior is through a control plane that is orders of magnitude slower than the speed at which typical data center congestion events occur. More recent alternatives like programmable switches can remember statistics about passing traffic and adjust behavior accordingly, but unfortunately, their capabilities severely limit what can be done.In this paper, we present Mantis, a framework for implementing fine-grained reactive behavior on today's programmable switches with the help of a specialized reactive control plane architecture. Mantis is, thus, a combination of language for specifying dynamic components of packet processing and an optimized, general, and safe control loop for implementing them. Mantis provides a simple-to-reason-about set of abstractions for users, and the Mantis control plane can react to changes in the network in 10s of μs.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",296–309,14,"Reconfiguration, Control plane, Programmable networks, P4","Virtual Event, USA",SIGCOMM '20,,,,,,
131,inproceedings,"Kakarla, Siva Kesava Reddy and Beckett, Ryan and Arzani, Behnaz and Millstein, Todd and Varghese, George",GRooT: Proactive Verification of DNS Configurations,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405871,10.1145/3387514.3405871,"The Domain Name System (DNS) plays a vital role in today's Internet but relies on complex distributed management of records. DNS misconfiguration related outages have rendered popular services like GitHub, HBO, LinkedIn, and Azure inaccessible for extended periods. This paper introduces GRoot, the first verifier that performs static analysis of DNS configuration files, enabling proactive and exhaustive checking for common DNS bugs; by contrast, existing solutions are reactive and incomplete. GRoot uses a new, fast verification algorithm based on generating and enumerating DNS query equivalence classes. GRoot symbolically executes the set of queries in each equivalence class to efficiently find (or prove the absence of) any bugs such as rewrite loops. To prove the correctness of our approach, we develop a formal semantic model of DNS resolution. Applied to the configuration files from a campus network with over a hundred thousand records, GRoot revealed 109 bugs within seconds. When applied to internal zone files consisting of over 3.5 million records from a large infrastructure service provider, GRoot revealed around 160k issues of blackholing, initiating a cleanup. Finally, on a synthetic dataset with over 65 million real records, we find GRoot can scale to networks with tens of millions of records.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",310–328,19,"Static Analysis, Formal Methods, Verification, DNS","Virtual Event, USA",SIGCOMM '20,,,,,,
132,inproceedings,"Soni, Hardik and Rifai, Myriana and Kumar, Praveen and Doenges, Ryan and Foster, Nate",Composing Dataplane Programs with μP4,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405872,10.1145/3387514.3405872,"Dataplane languages like P4 enable flexible and efficient packet-processing using domain-specific primitives such as programmable parsers and match-action tables. Unfortunately, P4 programs tend to be monolithic and tightly coupled to the hardware architecture, which makes it hard to write programs in a portable and modular way---e.g., by composing reusable libraries of standard protocols.To address this challenge, we present the design and implementation of a novel framework (μP4) comprising a lightweight logical architecture that abstracts away from the structure of the underlying hardware pipelines and naturally supports powerful forms of program composition. Using examples, we show how enables modular programming. We present a prototype of the compiler that generates code for multiple lower-level architectures, including Barefoot's Tofino Native Architecture. We evaluate the overheads induced by our compiler on realistic examples.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",329–343,15,"P4, Modularity, Programmable dataplanes, Composition","Virtual Event, USA",SIGCOMM '20,,,,,,
133,inproceedings,"Li, Yuanjie and Li, Qianru and Zhang, Zhehui and Baig, Ghufran and Qiu, Lili and Lu, Songwu",Beyond 5G: Reliable Extreme Mobility Management,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405873,10.1145/3387514.3405873,"Extreme mobility has become a norm rather than an exception. However, 4G/5G mobility management is not always reliable in extreme mobility, with non-negligible failures and policy conflicts. The root cause is that, existing mobility management is primarily based on wireless signal strength. While reasonable in static and low mobility, it is vulnerable to dramatic wireless dynamics from extreme mobility in triggering, decision, and execution. We devise REM, Reliable Extreme Mobility management for 4G, 5G, and beyond. REM shifts to movement-based mobility management in the delay-Doppler domain. Its signaling overlay relaxes feedback via cross-band estimation, simplifies policies with provable conflict freedom, and stabilizes signaling via scheduling-based OTFS modulation. Our evaluation with operational high-speed rail datasets shows that, REM reduces failures comparable to static and low mobility, with low signaling and latency cost.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",344–358,15,"extreme mobility management, reliability, cross-band estimation, delay-Doppler domain, Mobile network, beyond 5G, policy conflicts","Virtual Event, USA",SIGCOMM '20,,,,,,
134,inproceedings,"Li, Yuanqi and Padmanabhan, Arthi and Zhao, Pengzhan and Wang, Yufei and Xu, Guoqing Harry and Netravali, Ravi",Reducto: On-Camera Filtering for Resource-Efficient Real-Time Video Analytics,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405874,10.1145/3387514.3405874,"To cope with the high resource (network and compute) demands of real-time video analytics pipelines, recent systems have relied on frame filtering. However, filtering has typically been done with neural networks running on edge/backend servers that are expensive to operate. This paper investigates on-camera filtering, which moves filtering to the beginning of the pipeline. Unfortunately, we find that commodity cameras have limited compute resources that only permit filtering via frame differencing based on low-level video features. Used incorrectly, such techniques can lead to unacceptable drops in query accuracy. To overcome this, we built Reducto, a system that dynamically adapts filtering decisions according to the time-varying correlation between feature type, filtering threshold, query accuracy, and video content. Experiments with a variety of videos and queries show that Reducto achieves significant (51-97% of frames) filtering benefits, while consistently meeting the desired accuracy.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",359–376,18,"deep neural networks, object detection, video analytics","Virtual Event, USA",SIGCOMM '20,,,,,,
135,inproceedings,"Harchol, Yotam and Bergemann, Dirk and Feamster, Nick and Friedman, Eric and Krishnamurthy, Arvind and Panda, Aurojit and Ratnasamy, Sylvia and Schapira, Michael and Shenker, Scott",A Public Option for the Core,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405875,10.1145/3387514.3405875,"This paper is focused not on the Internet architecture - as defined by layering, the narrow waist of IP, and other core design principles - but on the Internet infrastructure, as embodied in the technologies and organizations that provide Internet service. In this paper we discuss both the challenges and the opportunities that make this an auspicious time to revisit how we might best structure the Internet's infrastructure. Currently, the tasks of transit-between-domains and last-mile-delivery are jointly handled by a set of ISPs who interconnect through BGP. In this paper we propose cleanly separating these two tasks. For transit, we propose the creation of a ""public option"" for the Internet's core backbone. This public option core, which complements rather than replaces the backbones used by large-scale ISPs, would (i) run an open market for backbone bandwidth so it could leverage links offered by third-parties, and (ii) structure its terms-of-service to enforce network neutrality so as to encourage competition and reduce the advantage of large incumbents.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",377–389,13,"Internet infrastructure, Network neutrality, Internet transit","Virtual Event, USA",SIGCOMM '20,,,,,,
136,inproceedings,"Gong, Junzhi and Li, Yuliang and Anwer, Bilal and Shaikh, Aman and Yu, Minlan",Microscope: Queue-Based Performance Diagnosis for Network Functions,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405876,10.1145/3387514.3405876,"By moving monolithic network appliances to software running on commodity hardware, network function virtualization allows flexible resource sharing among network functions and achieves scalability with low cost. However, due to resource contention, network functions can suffer from performance problems that are hard to diagnose. In particular, when many flows traverse a complex topology of NF instances, it is hard to pinpoint root causes for a flow experiencing performance issues such as low throughput or high latency. Simply maintaining resource counters at individual NFs is not sufficient since the effect of resource contention can propagate across NFs and over time. In this paper, we introduce Microscope, a performance diagnosis tool, for network functions that leverages queuing information at NFs to identify the root causes (i.e., resources, NFs, traffic patterns of flows etc.). Our evaluation on realistic NF chains and traffic shows that we can correctly capture root causes behind 89.7% of performance impairments, up to 2.5 times more than the state-of-the-art tools with low overhead.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",390–403,14,"performance, diagnosis, NFV","Virtual Event, USA",SIGCOMM '20,,,,,,
137,inproceedings,"Huang, Qun and Sun, Haifeng and Lee, Patrick P. C. and Bai, Wei and Zhu, Feng and Bao, Yungang",OmniMon: Re-Architecting Network Telemetry with Resource Efficiency and Full Accuracy,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405877,10.1145/3387514.3405877,"Network telemetry is essential for administrators to monitor massive data traffic in a network-wide manner. Existing telemetry solutions often face the dilemma between resource efficiency (i.e., low CPU, memory, and bandwidth overhead) and full accuracy (i.e., error-free and holistic measurement). We break this dilemma via a network-wide architectural design OmniMon, which simultaneously achieves resource efficiency and full accuracy in flow-level telemetry for large-scale data centers. OmniMon carefully coordinates the collaboration among different types of entities in the whole network to execute telemetry operations, such that the resource constraints of each entity are satisfied without compromising full accuracy. It further addresses consistency in network-wide epoch synchronization and accountability in error-free packet loss inference. We prototype OmniMon in DPDK and P4. Testbed experiments on commodity servers and Tofino switches demonstrate the effectiveness of OmniMon over state-of-the-art telemetry designs.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",404–421,18,"Distributed systems, Network measurement","Virtual Event, USA",SIGCOMM '20,,,,,,
138,inproceedings,"Hu, Shuihai and Bai, Wei and Zeng, Gaoxiong and Wang, Zilong and Qiao, Baochen and Chen, Kai and Tan, Kun and Wang, Yi",Aeolus: A Building Block for Proactive Transport in Datacenters,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405878,10.1145/3387514.3405878,"As datacenter network bandwidth keeps growing, proactive transport becomes attractive, where bandwidth is proactively allocated as ""credits"" to senders who then can send ""scheduled packets"" at a right rate to ensure high link utilization, low latency, and zero packet loss. While promising, a fundamental challenge is that proactive transport requires at least one-RTT for credits to be computed and delivered. In this paper, we show such one-RTT ""pre-credit"" phase could carry a substantial amount of flows at high link-speeds, but none of existing proactive solutions treats it appropriately. We present Aeolus, a solution focusing on ""pre-credit"" packet transmission as a building block for proactive transports. Aeolus contains unconventional design principles such as scheduled-packet-first (SPF) that de-prioritizes the first-RTT packets, instead of prioritizing them as prior work. It further exploits the preserved, deterministic nature of proactive transport as a means to recover lost first-RTT packets efficiently. We have integrated Aeolus into ExpressPass[14], NDP[18] and Homa[29], and shown, through both implementation and simulations, that the Aeolus-enhanced solutions deliver signiicant performance or deployability advantages. For example, it improves the average FCT of ExpressPass by 56%, cuts the tail FCT of Homa by 20x, while achieving similar performance as NDP without switch modifications.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",422–434,13,"Selective Dropping, First RTT, Data Center Networks, Proactive Transport","Virtual Event, USA",SIGCOMM '20,,,,,,
139,inproceedings,"Gao, Jiaqi and Zhai, Ennan and Liu, Hongqiang Harry and Miao, Rui and Zhou, Yu and Tian, Bingchuan and Sun, Chen and Cai, Dennis and Zhang, Ming and Yu, Minlan",Lyra: A Cross-Platform Language and Compiler for Data Plane Programming on Heterogeneous ASICs,2020,9781450379557,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3387514.3405879,10.1145/3387514.3405879,"Programmable data plane has been moving towards deployments in data centers as mainstream vendors of switching ASICs enable programmability in their newly launched products, such as Broadcom's Trident-4, Intel/Barefoot's Tofino, and Cisco's Silicon One. However, current data plane programs are written in low-level, chip-specific languages (e.g., P4 and NPL) and thus tightly coupled to the chip-specific architecture. As a result, it is arduous and error-prone to develop, maintain, and composite data plane programs in production networks. This paper presents Lyra, the first cross-platform, high-level language & compiler system that aids the programmers in programming data planes efficiently. Lyra offers a one-big-pipeline abstraction that allows programmers to use simple statements to express their intent, without laboriously taking care of the details in hardware; Lyra also proposes a set of synthesis and optimization techniques to automatically compile this ""big-pipeline"" program into multiple pieces of runnable chip-specific code that can be launched directly on the individual programmable switches of the target network. We built and evaluated Lyra. Lyra not only generates runnable real-world programs (in both P4 and NPL), but also uses up to 87.5% fewer hardware resources and up to 78% fewer lines of code than human-written programs.","Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication",435–450,16,"Compiler, P4 Synthesis, Programming Language, Programmable Networks, Programmable switching ASIC","Virtual Event, USA",SIGCOMM '20,,,,,,
140,inproceedings,"Leng, Xue and Juang, Tzung-Han and Chen, Yan and Liu, Han",AOMO: An AI-Aided Optimizer for Microservices Orchestration,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342287,10.1145/3342280.3342287,,Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,1–2,2,"Orchestration Optimization, Microservices","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
141,inproceedings,"Li, Xing and Chen, Yan and Lin, Zhiqiang",Towards Automated Inter-Service Authorization for Microservice Applications,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342288,10.1145/3342280.3342288,,Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,3–5,3,"Policy-Based Access Control, Microservice, Automation","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
142,inproceedings,"Sakic, Ermin and Serna, Cristian Bermudez and Goshi, Endri and Deric, Nemanja and Kellerer, Wolfgang",P4BFT: A Demonstration of Hardware-Accelerated BFT in Fault-Tolerant Network Control Plane,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342289,10.1145/3342280.3342289,,Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,6–8,3,,"Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
143,inproceedings,"Chen, Gonglong and Wang, Yihui and Li, Huikang and Dong, Wei","TinyNet: A Lightweight, Modular, and Unified Network Architecture for The Internet of Things",2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342290,10.1145/3342280.3342290,,Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,9–11,3,"modular, Internet of Things, network architectures","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
144,inproceedings,"Huang, Tianchi and Zhang, Rui-Xiao and Yao, Xin and Wu, Chenglei and Sun, Lifeng",Being More Effective and Interpretable: Bridging the Gap Between Heuristics and AI for ABR Algorithms,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342291,10.1145/3342280.3342291,"In this poster, we propose several novel ABR approaches, namely BBA+ and MPC+, which are the fusion of heuristics and AI-based schemes. Results indicate that the proposed methods perform better than recent heuristic ABR methods. Meanwhile, such methods have also become more interpretable compared with AI-based schemes.",Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,12–14,3,"Heuristics, Adaptive Bitrate Streaming, Explainable-AI","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
145,inproceedings,"Lu, Bo and Xu, Ling and Song, Yuezhong and Dai, Longfei and Liu, Min and Zhou, Tianran and Li, Zhenbin and Song, Haoyu",IFIT: Intelligent Flow Information Telemetry,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342292,10.1145/3342280.3342292,,Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,15–17,3,,"Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
146,inproceedings,"Yang, Jiadi and Zhang, Shuhang and Zhang, Hongliang and Song, Lingyang",Cooperative Trajectory Optimization for a Cellular Internet of UAVs,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342293,10.1145/3342280.3342293,"In this poster, we study a cellular Internet of unmanned aerial vehicles (UAVs), in which one UAV collects data from ground sensors (GSs), and one UAV relays the sensory data to the base station (BS). Since the trajectories of these two UAVs have impact on each other, we formulate a cooperative trajectory optimization problem to minimize the completion time for all the tasks. To solve this non-convex problem, we propose an algorithm that solves the trajectories of the two UAVs iteratively. Simulation results show that the completion time for all the tasks in the proposed scheme is significantly improved than the non-cooperative scheme.",Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,18–20,3,"cellular Internet of UAVs, cooperative trajectory optimization, UAV sensing","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
147,inproceedings,"Yu, Yinbo and Li, You and Hou, Kaiyu and Chen, Yan and Zhou, Hai and Yang, Jianfeng",CellScope: Automatically Specifying and Verifying Cellular Network Protocols,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342294,10.1145/3342280.3342294,,Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,21–23,3,"protocol specification and verification, model checking, Cellular network","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
148,inproceedings,"Zhang, Che and Zhang, Shiwei and Jin, Bo and Li, Weichao and Wang, Zhen and Wang, Yi",A3: An Automatic Malfunction Detection and Fixation System in FatTree Data Center Networks,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342295,10.1145/3342280.3342295,,Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,24–26,3,"graph isomorphism, data center networks, failure detection","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
149,inproceedings,"Li, Zhaogeng and Liu, Ning and Wu, Jiaoren",Toward a Production-Ready General-Purpose RDMA-Enabled RPC,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342296,10.1145/3342280.3342296,,Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,27–29,3,,"Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
150,inproceedings,"Huet, Alexis and Ben Houidi, Zied and Cai, Shengming and Shi, Hao and Xu, Jinchun and Rossi, Dario",Web Quality of Experience from Encrypted Packets,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342297,10.1145/3342280.3342297,"Pervasive encryption makes it hard for ISPs to manage their network. Yet, to avoid user churn at times of shrinking revenues, ISPs must be able to assess the quality of experience they are delivering to their customers. The case of the Web is particularly complex, with a plethora of recently proposed in-browser metrics that aim at capturing the page visual rendering quality (e.g. Above the Fold and SpeedIndex). In this demo, we showcase that such metrics can be estimated quite accurately just from streams of encrypted packets, using classic supervised learning techniques.",Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,30–32,3,,"Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
151,inproceedings,"Gong, Junzhi and Li, Yuliang and Anwer, Bilal and Shaikh, Aman and Yu, Minlan",DeepDiag: Detailed NFV Performance Diagnosis,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342298,10.1145/3342280.3342298,,Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,33–35,3,"performance, diagnosis, NFV","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
152,inproceedings,"Yerabolu, Susham and Gomena, Samuel and Aryafar, Ehsan and Joe-Wong, Carlee",An Edge Computing Marketplace for Distributed Machine Learning,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342299,10.1145/3342280.3342299,"There is an increasing demand among machine learning researchers for powerful computational resources to train their machine learning models. In order to train these models in a reasonable amount of time, the training is often distributed among multiple machines; yet paying for such machines is costly. DeepMarket attempts to reduce these costs by creating a marketplace that integrates multiple computational resources over a distributed tensorflow framework. Instead of requiring users to rent expensive resources from a third party cloud provider, DeepMarket will allow users to lend their computing resources to each other when they are available. Such a marketplace, however, requires a credit mechanism that ensures users receive resources in proportion to the resources they lend to others. Moreover, DeepMarket must respect users' needs to use their own resources and the resulting limits on when resources can be lent to others. This Demo will introduce the audience to PLUTO: DeepMarket's intuitive graphical user interface. The audience will be able to see how PLUTO in coordination with DeepMarket servers tracks the performance of each user's training jobs, matches jobs to resources made available by other users, and tracks the resulting credits that regulate the exchange of resources.",Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,36–38,3,"TensorFlow, Marketplace Design, Network Economics","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
153,inproceedings,"Rodrigues, Bruno and Stiller, Burkhard",Cooperative Signaling of DDoS Attacks in a Blockchain-Based Network,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342300,10.1145/3342280.3342300,"Driven by the increasing number of stationary and portable devices, Distributed Denial-of-Service (DDoS) attacks pose a major threat to Internet availability. While advantages of cooperative defenses have been widely recognized over traditional on-premise defenses, there is not a widespread deployment of such cooperative defenses. This work demonstrates the Blockchain Signaling System (BloSS), a modular, network-agnostic and cooperative DDoS defense system consisting of independent instances working together to mitigate attacks targeted at any member of this alliance.",Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,39–41,3,"Blockchain, Cooperative Defense, Distributed Denial-of-Service (DDoS), Security","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
154,inproceedings,"Xi, Zhaowei and Zhou, Yu and Zhang, Dai and Wang, Jinqiu and Chen, Sun and Wang, Yangyang and Li, Xinrui and Wang, HaoMing and Wu, Jianping",HyperGen: High-Performance Flexible Packet Generator Using Programmable Switching ASIC,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342301,10.1145/3342280.3342301,,Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,42–44,3,"Packet Generator, Programmable Switching ASIC, P4","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
155,inproceedings,"Liu, Libin and Xu, Hong",Tyrus: PHY-Assisted Neural Adaptive Congestion Control for Cellular Networks,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342302,10.1145/3342280.3342302,,Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,45–47,3,"Cellular Networks, Congestion Control, Reinforcement Learning, Physical Layer Information","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
156,inproceedings,"Kalmbach, Patrick and Hock, David and Lipp, Fabian and Kellerer, Wolfgang and Blenk, Andreas",NOracle: Who is Communicating with Whom in My Network?,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342303,10.1145/3342280.3342303,"This demo presents NOracle: a system using Stochastic Block Models (SBMs) to infer structural roles of hosts and communication patterns of services in networks. NOracle can be used with existing monitoring systems to analyze and visualize networks in an online manner or be used to analyze stored traces. Network operators can use SBMs to monitor and verify network operation, detect possible security issues and change-points. To showcase this, NOracle combines the production-grade network management solution StableNet with an SBM based anomaly detection and network visualization module. StableNet provides network flow statistics in real-time from actual devices. The SBM extracts roles and communication patterns live from the data provided by StableNet. The result can help to reason about communication behaviors, detect anomalous hosts and indicate changes in the large scale-structure of network communication.",Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,48–50,3,"Anomaly Detection, Stochastic Block Model, Network Monitoring","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
157,inproceedings,"Fernandes, Eder Le\~{a",Faster Control Plane Experimentation with Horse,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342304,10.1145/3342280.3342304,"Simulation and emulation are popular approaches for experimentation in Computer Networks. However, due to their respective inherent drawbacks, existing solutions cannot perform both fast and realistic control plane experiments. To close this gap, we introduce Horse. Horse is a hybrid solution with an emulated control plane, for realism, and simulated data plane, for speed. Our decoupling of the control and data plane allows us to speed up the experiments without sacrificing control plane realism.",Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,51–53,3,"Traffic Engineering, Network Simulation, Network Emulation","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
158,inproceedings,"Fu, Yongquan and Li, Dongsheng and Shen, Siqi and Zhang, Yiming and Chen, Kai",Resilient Disaggregated Network Flow Monitoring,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342305,10.1145/3342280.3342305,"Sketch has been extensively used for scalable network monitoring, which unfortunately, is sensitive to hash collisions. Deploying the sketch involves fine-grained performance control and instrumentation. This paper presents a new class of sketch structure that proactively minimizes the estimation error and reduces the error variance. We develop a disaggregated monitoring application that natively scales the sketching deployment. Testbed and real-world trace-driven simulations show that LSS achieves close-to-optimal performance under hash collisions.",Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,54–56,3,"Network flow, hash function, disaggregated application, sketch","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
159,inproceedings,"Bao, Jinzhen and Dong, Dezun and Zhao, Baokang and Huang, Shan",HyFabric: Minimizing FCT in Optical and Electrical Hybrid Data Center Networks,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342306,10.1145/3342280.3342306,"In this paper, we propose HyFabric, jointing the flow and circuit scheduling to minimize flow completion times (FCT) in optical and electrical hybrid data center networks. Our extensive evaluation results show that HyFabric reduces 93% FCT and 32% energy consumption compared with Solstice[4] and ProjecToR[3]. And it delivers comparable FCT and reduces 5% ~ 14% energy consumptions compared with pFabric[1].",Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,57–59,3,"hybrid data center networks, flow scheduling, circuit scheduling","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
160,inproceedings,"Mok, Ricky K. P. and Kawaguti, Ginga and claffy, kc",QUINCE: A Unified Crowdsourcing-Based QoE Measurement Platform,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342307,10.1145/3342280.3342307,"Assessing QoE in situ is a challenging task. Researchers have employed crowdsourcing-based approaches to achieve scale and diversity of subjects, but not without confounding factors. Apart from subjective bias of users, environmental factors introduce variance to QoE measurements. We propose QUINCE, a QoE measurement platform, which uses a gamified approach to enable longitudinal study with repeated and varying measurements in a single platform. We leveraged existing Internet measurement data and infrastructures to integrate three different types of network and QoE measurements to yield a more comprehensive view of subjects. Our preliminarily results show that QUINCE achieves a high level of engagement from subjects and collects data that is useful for correlating network performance and YouTube video streaming QoE.",Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,60–62,3,"network measurement, crowdsourcing, QoE","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
161,inproceedings,"Yan, Liu and Li, Zhuo and Liu, Kaihua",Learning Tree: Neural Network-Based Index for NDN Forwarding Plane,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342308,10.1145/3342280.3342308,"Named Data Networking (NDN) is a recent promising future Internet architecture, which forwards packets based on the named data. To realize this paradigm, a quick enough index with high capacity for forwarding processes has to be utilized in NDN forwarding plane. Unfortunately, all indexes proposed are based on the traditional data structures, which have to make a trade-off between the memory consumption and the lookup speed. In view of this, a neural network-based index, called Learning Tree, is proposed in NDN forwarding plane. Preliminary evaluations indicate that Learning Tree can minimize the memory consumption to 20% compared with traditional hash table and has a practicable high name lookup speed.",Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,63–65,3,"forwarding plane, index, neural networks, Named Data Networking","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
162,inproceedings,"Chen, Lien-Wu and Chen, Tsung-Ping and Weng, Chia-Chun",IBaby: A Mobile Children Monitoring and Finding System with Stranger Holding Detection Based on IoT Technologies,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342309,10.1145/3342280.3342309,"This paper designs and implements a mobile children monitoring and finding system, called iBaby, using wearable devices and nearby smartphones to detect unexpected holding and find missing children through Internet of Things (IoT) technologies, respectively. In the monitoring mode, the iBaby system can prevent young children from taking away by strangers/people with bad intentions. In the finding mode, the iBaby system can cooperatively find missing children equipped with hand wearable devices consisting of the mobile iBeacon and 3-axis accelermeter through crowdsourced sensing networks formed by smartphone users with outdoor GPS and indoor IoT localization. To accurately detect stranger holding behaviors, multi-feature based, artificial neural network based, and convolutional neural network based posture recognition methods are designed to improve recognition success rates of iBaby as much as possible. In particular, an iOS-based prototype with Arduino wearable devices and static iBeacon nodes is implemented to verify the feasibility and correctness of our iBaby system.",Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,66–68,3,"Internet of Things, Wearable Device, Crowdsourced Sensing, Smart Handheld Device, iBeacon","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
163,inproceedings,"Joshi, Raj and Leong, Ben and Chan, Mun Choon",TimerTasks: Towards Time-Driven Execution in Programmable Dataplanes,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342310,10.1145/3342280.3342310,,Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,69–71,3,,"Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
164,inproceedings,"Tirmazi, Muhammad and Ben Basat, Ran and Gao, Jiaqi and Yu, Minlan",Cheetah: Accelerating Database Queries with Switch Pruning,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342311,10.1145/3342280.3342311,"Modern database systems are growing increasingly distributed and struggle to reduce the query completion time with a large volume of data. In this poster, we propose to leverage programmable switches in the network to offload part of the query computation to the switch. While switches provide high performance, they also have many resource and programming constraints that make it hard to implement diverse database queries. To fit in these constraints, we introduce the concept of data pruning - filtering out entries which are guaranteed not to affect the output. The database system then runs the same query, but on the pruned data, which significantly reduces the processing time. We propose a set of pruning algorithms for a variety of queries. We implement our system, Cheetah, on a Barefoot Tofino switch and Spark. Our evaluation on the Berkeley AMPLab benchmark shows up to 3x improvement in the query completion time compared to Apache Spark.",Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,72–74,3,"P4, Databases, Algorithms, Pruning, Programmable Switches","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
165,inproceedings,"Han, Jiangping and Xue, Kaiping and Xing, Yitao and Hong, Peilin and Wei, David S.L.",Measurement and Redesign of BBR-Based MPTCP,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342312,10.1145/3342280.3342312,"BBR is a congestion-based congestion control algorithm proposed to promote the performance of TCP. The interesting and vital question is thus: How would MPTCP perform on BBR? We first conduct extensive experimental evaluation of BBR-based MPTCP, and show that it provides several times throughput higher and more stable sending rate than that of others. We also observe that to replace the congestion control in MPTCP, some important algorithms in MPTCP need to be re-designed: 1) To achieve fairness with other flows, we propose a BBR based coupled congestion control algorithm, called Coupled BBR; 2) To further prevent performance degradation in highly dynamic nature, we also propose a scheduler to take advantage of dynamic redundancy.",Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,75–77,3,,"Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
166,inproceedings,"Wu, Chenshu and Zhang, Feng and Fan, Yusen and Liu, K. J. Ray",RF-Based Inertial Measurement,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342313,10.1145/3342280.3342313,,Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,78–79,2,,"Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
167,inproceedings,"Saquetti, Mateus and Bueno, Guilherme and Cordeiro, Weverton and Azambuja, Jos\'{e",Hard Virtualization of P4-Based Switches with VirtP4,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342314,10.1145/3342280.3342314,"We present VirtP4, a conceptual architecture for hard (type-1) virtualization of P4-based switch programs in a NetFPGA SUME board. In contrast to existing (type-2) solutions like Hyper4, HyperVDP, and P4Visor, our design does not require access to switch source code (either for merging or custom compilation). We thus preserve the intellectual property of switch programs, while providing near line-rate performance for virtual switch instances.",Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,80–81,2,"Programmable Forwarding Planes, P4, NetFPGA-SUME, Virtualization, Simple Sume Switch","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
168,inproceedings,"Jacobs, Arthur S. and Pfitscher, Ricardo J. and Ribeiro, Rafael H. and Ferreira, Ronaldo A. and Granville, Lisandro Z. and Rao, Sanjay G.",Deploying Natural Language Intents with Lumi,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342315,10.1145/3342280.3342315,,Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,82–84,3,"Intent-based Networking, Natural Language Processing, Machine Learning","Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
169,inproceedings,"Ivkin, Nikita and Ben Basat, Ran and Liu, Zaoxing and Einziger, Gil and Friedman, Roy and Braverman, Vladimir",Attack Time Localization Using Interval Queries,2019,9781450368865,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3342280.3342316,10.1145/3342280.3342316,"Modern telemetry systems require advanced analytic capabilities such as drill down queries. These queries can be used to detect the beginning and end of a network anomaly by efficiently refining the search space. We present the first integral solution that (i) enables multiple measurement tasks inside the same data structure, (ii) supports specifying the time frame of interest as part of its queries, and (iii) is sketch-based and thus space efficient. Namely, our approach allows the user to define both the measurement task (e.g., heavy hitters, entropy estimation, cardinality estimation) and the time frame of relevance (e.g., 5PM-6PM) at query time. Our approach provides accuracy guarantees and is the only space-efficient solution that offers such capabilities. Finally, we demonstrate how the algorithm can be used to accurately pinpoint the beginning of a realistic DDoS attack.",Proceedings of the ACM SIGCOMM 2019 Conference Posters and Demos,85–87,3,,"Beijing, China",SIGCOMM Posters and Demos '19,,,,,,
170,inproceedings,"Dhamdhere, Amogh and Clark, David D. and Gamero-Garrido, Alexander and Luckie, Matthew and Mok, Ricky K. P. and Akiwate, Gautam and Gogia, Kabir and Bajpai, Vaibhav and Snoeren, Alex C. and Claffy, Kc",Inferring Persistent Interdomain Congestion,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230549,10.1145/3230543.3230549,"There is significant interest in the technical and policy communities regarding the extent, scope, and consumer harm of persistent interdomain congestion. We provide empirical grounding for discussions of interdomain congestion by developing a system and method to measure congestion on thousands of interdomain links without direct access to them. We implement a system based on the Time Series Latency Probes (TSLP) technique that identifies links with evidence of recurring congestion suggestive of an under-provisioned link. We deploy our system at 86 vantage points worldwide and show that congestion inferred using our lightweight TSLP method correlates with other metrics of interconnection performance impairment. We use our method to study interdomain links of eight large U.S. broadband access providers from March 2016 to December 2017, and validate our inferences against ground-truth traffic statistics from two of the providers. For the period of time over which we gathered measurements, we did not find evidence of widespread endemic congestion on interdomain links between access ISPs and directly connected transit and content providers, although some such links exhibited recurring congestion patterns. We describe limitations, open challenges, and a path toward the use of this method for large-scale third-party monitoring of the Internet interconnection ecosystem.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,1–15,15,"internet topology, internet congestion, performance","Budapest, Hungary",SIGCOMM '18,,,,,,
171,inproceedings,"Agarwal, Saksham and Rajakrishnan, Shijin and Narayan, Akshay and Agarwal, Rachit and Shmoys, David and Vahdat, Amin",Sincronia: Near-Optimal Network Design for Coflows,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230569,10.1145/3230543.3230569,"We present Sincronia, a near-optimal network design for coflows that can be implemented on top on any transport layer (for flows) that supports priority scheduling. Sincronia achieves this using a key technical result --- we show that given a ""right"" ordering of coflows, any per-flow rate allocation mechanism achieves average coflow completion time within 4X of the optimal as long as (co)flows are prioritized with respect to the ordering.Sincronia uses a simple greedy mechanism to periodically order all unfinished coflows; each host sets priorities for its flows using corresponding coflow order and offloads the flow scheduling and rate allocation to the underlying priority-enabled transport layer. We evaluate Sincronia over a real testbed comprising 16-servers and commodity switches, and using simulations across a variety of workloads. Evaluation results suggest that Sincronia not only admits a practical, near-optimal design but also improves upon state-of-the-art network designs for coflows (sometimes by as much as 8X).",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,16–29,14,"datacenter networks, approximation algorithms, coflow","Budapest, Hungary",SIGCOMM '18,,,,,,
172,inproceedings,"Narayan, Akshay and Cangialosi, Frank and Raghavan, Deepti and Goyal, Prateesh and Narayana, Srinivas and Mittal, Radhika and Alizadeh, Mohammad and Balakrishnan, Hari",Restructuring Endpoint Congestion Control,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230553,10.1145/3230543.3230553,"This paper describes the implementation and evaluation of a system to implement complex congestion control functions by placing them in a separate agent outside the datapath. Each datapath---such as the Linux kernel TCP, UDP-based QUIC, or kernel-bypass transports like mTCP-on-DPDK---summarizes information about packet round-trip times, receptions, losses, and ECN via a well-defined interface to algorithms running in the off-datapath Congestion Control Plane (CCP). The algorithms use this information to control the datapath's congestion window or pacing rate. Algorithms written in CCP can run on multiple datapaths. CCP improves both the pace of development and ease of maintenance of congestion control algorithms by providing better, modular abstractions, and supports aggregation capabilities of the Congestion Manager, all with one-time changes to datapaths. CCP also enables new capabilities, such as Copa in Linux TCP, several algorithms running on QUIC and mTCP/DPDK, and the use of signal processing algorithms to detect whether cross-traffic is ACK-clocked. Experiments with our user-level Linux CCP implementation show that CCP algorithms behave similarly to kernel algorithms, and incur modest CPU overhead of a few percent.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,30–43,14,"operating systems, congestion control","Budapest, Hungary",SIGCOMM '18,,,,,,
173,inproceedings,"Akhtar, Zahaib and Nam, Yun Seong and Govindan, Ramesh and Rao, Sanjay and Chen, Jessica and Katz-Bassett, Ethan and Ribeiro, Bruno and Zhan, Jibin and Zhang, Hui",Oboe: Auto-Tuning Video ABR Algorithms to Network Conditions,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230558,10.1145/3230543.3230558,"Most content providers are interested in providing good video delivery QoE for all users, not just on average. State-of-the-art ABR algorithms like BOLA and MPC rely on parameters that are sensitive to network conditions, so may perform poorly for some users and/or videos. In this paper, we propose a technique called Oboe to auto-tune these parameters to different network conditions. Oboe pre-computes, for a given ABR algorithm, the best possible parameters for different network conditions, then dynamically adapts the parameters at run-time for the current network conditions. Using testbed experiments, we show that Oboe significantly improves BOLA, MPC, and a commercially deployed ABR. Oboe also betters a recently proposed reinforcement learning based ABR, Pensieve, by 24% on average on a composite QoE metric, in part because it is able to better specialize ABR behavior across different network states.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,44–58,15,"adaptive bitrate algorithms, video delivery","Budapest, Hungary",SIGCOMM '18,,,,,,
174,inproceedings,"Li, Zhihao and Levin, Dave and Spring, Neil and Bhattacharjee, Bobby","Internet Anycast: Performance, Problems, & Potential",2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230547,10.1145/3230543.3230547,"Internet anycast depends on inter-domain routing to direct clients to their ""closest"" sites. Using data collected from a root DNS server for over a year (400M+ queries/day from 100+ sites), we characterize the load balancing and latency performance of global anycast. Our analysis shows that site loads are often unbalanced, and that most queries travel longer than necessary, many by over 5000 km.Investigating the root causes of these inefficiencies, we can attribute path inflation to two causes. Like unicast, anycast routes are subject to inter-domain routing topology and policies that can increase path length compared to theoretical shortest (e.g., great-circle distance). Unlike unicast, anycast routes are also affected by poor route selection when paths to multiple sites are available, subjecting anycast routes to an additional, unnecessary, penalty.Unfortunately, BGP provides no information about the number or goodness of reachable anycast sites. We propose an additional hint in BGP advertisements for anycast routes that can enable ISPs to make better choices when multiple ""equally good"" routes are available. Our results show that use of such routing hints can eliminate much of the anycast path inflation, enabling anycast to approach the performance of unicast routing.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,59–73,15,,"Budapest, Hungary",SIGCOMM '18,,,,,,
175,inproceedings,"Hong, Chi-Yao and Mandal, Subhasree and Al-Fares, Mohammad and Zhu, Min and Alimi, Richard and B., Kondapa Naidu and Bhagat, Chandan and Jain, Sourabh and Kaimal, Jay and Liang, Shiyu and Mendelev, Kirill and Padgett, Steve and Rabe, Faro and Ray, Saikat and Tewari, Malveeka and Tierney, Matt and Zahn, Monika and Zolla, Jonathan and Ong, Joon and Vahdat, Amin","B4 and after: Managing Hierarchy, Partitioning, and Asymmetry for Availability and Scale in Google's Software-Defined WAN",2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230545,10.1145/3230543.3230545,"Private WANs are increasingly important to the operation of enterprises, telecoms, and cloud providers. For example, B4, Google's private software-defined WAN, is larger and growing faster than our connectivity to the public Internet. In this paper, we present the five-year evolution of B4. We describe the techniques we employed to incrementally move from offering best-effort content-copy services to carrier-grade availability, while concurrently scaling B4 to accommodate 100x more traffic. Our key challenge is balancing the tension introduced by hierarchy required for scalability, the partitioning required for availability, and the capacity asymmetry inherent to the construction and operation of any large-scale network. We discuss our approach to managing this tension: i) we design a custom hierarchical network topology for both horizontal and vertical software scaling, ii) we manage inherent capacity asymmetry in hierarchical topologies using a novel traffic engineering algorithm without packet encapsulation, and iii) we re-architect switch forwarding rules via two-stage matching/hashing to deal with asymmetric network failures at scale.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,74–87,14,"software-defined WAN, traffic engineering","Budapest, Hungary",SIGCOMM '18,,,,,,
176,inproceedings,"Gvozdiev, Nikola and Vissicchio, Stefano and Karp, Brad and Handley, Mark","On Low-Latency-Capable Topologies, and Their Impact on the Design of Intra-Domain Routing",2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230575,10.1145/3230543.3230575,"An ISP's customers increasingly demand delivery of their traffic without congestion and with low latency. The ISP's topology, routing, and traffic engineering, often over multiple paths, together determine congestion and latency within its backbone. We first consider how to measure a topology's capacity to route traffic without congestion and with low latency. We introduce low-latency path diversity (LLPD), a metric that captures a topology's flexibility to accommodate traffic on alternative low-latency paths. We explore to what extent 116 real backbone topologies can, regardless of routing system, keep latency low when demand exceeds the shortest path's capacity. We find, perhaps surprisingly, that topologies with good LLPD are precisely those where routing schemes struggle to achieve low latency without congestion. We examine why these schemes perform poorly, and offer an existence proof that a practical routing scheme can achieve a topology's potential for congestion-free, low-delay routing. Finally we examine implications for the design of backbone topologies amenable to achieving high capacity and low delay.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,88–102,15,,"Budapest, Hungary",SIGCOMM '18,,,,,,
177,inproceedings,"Daggitt, Matthew L. and Gurney, Alexander J. T. and Griffin, Timothy G.",Asynchronous Convergence of Policy-Rich Distributed Bellman-Ford Routing Protocols,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230561,10.1145/3230543.3230561,"We present new results in the theory of asynchronous convergence for the Distributed Bellman-Ford (DBF) family of routing protocols which includes distance-vector protocols (e.g. RIP) and path-vector protocols (e.g. BGP). We take the strictly increasing conditions of Sobrinho and make three main new contributions.First, we show that the conditions are sufficient to guarantee that the protocols will converge to a unique solution, preventing the possibility of BGP wedgies. Second, we decouple the computation from the asynchronous context in which it occurs, allowing us to reason about a more relaxed model of asynchronous computation in which routing messages can be lost, reordered, and duplicated. Third, our theory and results have been fully formalised in the Agda theorem prover and the resulting library is publicly available for others to use and extend. This is in line with the increasing emphasis on formal verification of software for critical infrastructure.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,103–116,14,,"Budapest, Hungary",SIGCOMM '18,,,,,,
178,inproceedings,"Tonolini, Francesco and Adib, Fadel",Networking across Boundaries: Enabling Wireless Communication through the Water-Air Interface,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230580,10.1145/3230543.3230580,"We consider the problem of wireless communication across medium boundaries, specifically across the water-air interface. In particular, we are interested in enabling a submerged underwater sensor to directly communicate with an airborne node. Today's communication technologies cannot enable such a communication link. This is because no single type of wireless signal can operate well across different media and most wireless signals reflect back at media boundaries.We present a new communication technology, translational acoustic-RF communication (TARF). TARF enables underwater nodes to directly communicate with airborne nodes by transmitting standard acoustic signals. TARF exploits the fact that underwater acoustic signals travel as pressure waves, and that these waves cause displacements of the water surface when they impinge on the water-air boundary. To decode the transmitted signals, TARF leverages an airborne radar which measures and decodes these surface displacements.We built a prototype of TARF that incorporates algorithms for dealing with the constraints of this new communication modality. We evaluated TARF in controlled and uncontrolled environments and demonstrated that it enables the first practical communication link across the water-air interface. Our results show that TARF can achieve standard underwater bitrates up to 400bps, and that it can operate correctly in the presence of surface waves with amplitudes up to 16 cm peak-to-peak, i.e., 100,000X larger than the surface perturbations caused by TARF's underwater acoustic transmitter.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,117–131,15,"wireless, subsea internet of things, cross-medium communications","Budapest, Hungary",SIGCOMM '18,,,,,,
179,inproceedings,"Vasisht, Deepak and Zhang, Guo and Abari, Omid and Lu, Hsiao-Ming and Flanz, Jacob and Katabi, Dina",In-Body Backscatter Communication and Localization,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230565,10.1145/3230543.3230565,"Backscatter requires zero transmission power, making it a compelling technology for in-body communication and localization. It can significantly reduce the battery requirements (and hence the size) of micro-implants and smart capsules, and enable them to be located on-the-move inside the body. The problem however is that the electrical properties of human tissues are very different from air and vacuum. This creates new challenges for both communication and localization. For example, signals no longer travel along straight lines, which destroys the geometric principles underlying many localization algorithms. Furthermore, the human skin backscatters the signal creating strong interference to the weak in-body backscatter transmission. These challenges make deep-tissue backscatter intrinsically different from backscatter in air or vacuum. This paper introduces ReMix, a new backscatter design that is particularly customized for deep tissue devices. It overcomes interference from the body surface, and localizes the in-body backscatter devices even though the signal travels along crooked paths. We have implemented our design and evaluated it in animal tissues and human phantoms. Our results demonstrate that ReMix delivers efficient communication at an average SNR of 15.2 dB at 1 MHz bandwidth, and has an average localization accuracy of 1.4cm in animal tissues.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,132–146,15,,"Budapest, Hungary",SIGCOMM '18,,,,,,
180,inproceedings,"Peng, Yao and Shangguan, Longfei and Hu, Yue and Qian, Yujie and Lin, Xianshang and Chen, Xiaojiang and Fang, Dingyi and Jamieson, Kyle",PLoRa: A Passive Long-Range Data Network from Ambient LoRa Transmissions,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230567,10.1145/3230543.3230567,"This paper presents PLoRa, an ambient backscatter design that enables long-range wireless connectivity for batteryless IoT devices. PLoRa takes ambient LoRa transmissions as the excitation signals, conveys data by modulating an excitation signal into a new standard LoRa ""chirp"" signal, and shifts this new signal to a different LoRa channel to be received at a gateway faraway. PLoRa achieves this by a holistic RF front-end hardware and software design, including a low-power packet detection circuit, a blind chirp modulation algorithm and a low-power energy management circuit. To form a complete ambient LoRa backscatter network, we integrate a light-weight backscatter signal decoding algorithm with a MAC-layer protocol that work together to make coexistence of PLoRa tags and active LoRa nodes possible in the network. We prototype PLoRa on a four-layer printed circuit board, and test it in various outdoor and indoor environments. Our experimental results demonstrate that our prototype PCB PLoRa tag can backscatter an ambient LoRa transmission sent from a nearby LoRa node (20 cm away) to a gateway up to 1.1 km away, and deliver 284 bytes data every 24 minutes indoors, or every 17 minutes outdoors. We also simulate a 28-nm low-power FPGA based prototype whose digital baseband processor achieves 220 μW power consumption.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,147–160,14,"wireless networks, backscatter, long-range, LoRa","Budapest, Hungary",SIGCOMM '18,,,,,,
181,inproceedings,"Li, Li and Xu, Ke and Li, Tong and Zheng, Kai and Peng, Chunyi and Wang, Dan and Wang, Xiangxiang and Shen, Meng and Mijumbi, Rashid",A Measurement Study on Multi-Path TCP with Multiple Cellular Carriers on High Speed Rails,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230556,10.1145/3230543.3230556,"Recent advances in high speed rails (HSRs) are propelling the need for acceptable network service in high speed mobility environments. However, previous studies show that the performance of traditional single-path transmission degrades significantly during high speed mobility due to frequent handoff. Multi-path transmission with multiple carriers is a promising way to enhance the performance, because at any time, there is possibly at least one path not suffering a handoff. In this paper, for the first time, we measure multi-path TCP (MPTCP) with two cellular carriers on HSRs with a peak speed of 310km/h. We find a significant difference in handoff time between the two carriers. Moreover, we observe that MPTCP can provide much better performance than TCP in the poorer of the two paths. This indicates that MPTCP's robustness to handoff is much higher than TCP's. However, the efficiency of MPTCP is far from satisfactory. MPTCP performs worse than TCP in the better path most of the time. We find that the low efficiency can be attributed to poor adaptability to frequent handoff by MPTCP's key operations in sub-flow establishment, congestion control and scheduling. Finally, we discuss possible directions for improving MPTCP for such scenarios.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,161–175,15,"multi-path TCP, high speed rails, measurement, cellular networks","Budapest, Hungary",SIGCOMM '18,,,,,,
182,inproceedings,"Wu, Dingming and Xia, Yiting and Sun, Xiaoye Steven and Huang, Xin Sunny and Dzinamarira, Simbarashe and Ng, T. S. Eugene",Masking Failures from Application Performance in Data Center Networks with Shareable Backup,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230577,10.1145/3230543.3230577,"Shareable backup is an economical and effective way to mask failures from application performance. A small number of backup switches are shared network-wide for repairing failures on demand so that the network quickly recovers to its full capacity without applications noticing the failures. This approach avoids complications and ineffectiveness of rerouting. We propose ShareBackup as a prototype architecture to realize this concept and present the detailed design. We implement ShareBackup on a hardware testbed. Its failure recovery takes merely 0.73ms, causing no disruption to routing; and it accelerates Spark and Tez jobs by up to 4.1X under failures. Large-scale simulations with real data center traffic and failure model show that ShareBackup reduces the percentage of job flows prolonged by failures from 47.2% to as little as 0.78%. In all our experiments, the results for ShareBackup have little difference from the no-failure case.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,176–190,15,"circuit switching, failure recovery, data center network","Budapest, Hungary",SIGCOMM '18,,,,,,
183,inproceedings,"Chen, Li and Lingys, Justinas and Chen, Kai and Liu, Feng",AuTO: Scaling Deep Reinforcement Learning for Datacenter-Scale Automatic Traffic Optimization,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230551,10.1145/3230543.3230551,"Traffic optimizations (TO, e.g. flow scheduling, load balancing) in datacenters are difficult online decision-making problems. Previously, they are done with heuristics relying on operators' understanding of the workload and environment. Designing and implementing proper TO algorithms thus take at least weeks. Encouraged by recent successes in applying deep reinforcement learning (DRL) techniques to solve complex online control problems, we study if DRL can be used for automatic TO without human-intervention. However, our experiments show that the latency of current DRL systems cannot handle flow-level TO at the scale of current datacenters, because short flows (which constitute the majority of traffic) are usually gone before decisions can be made.Leveraging the long-tail distribution of datacenter traffic, we develop a two-level DRL system, AuTO, mimicking the Peripheral & Central Nervous Systems in animals, to solve the scalability problem. Peripheral Systems (PS) reside on end-hosts, collect flow information, and make TO decisions locally with minimal delay for short flows. PS's decisions are informed by a Central System (CS), where global traffic information is aggregated and processed. CS further makes individual TO decisions for long flows. With CS&PS, AuTO is an end-to-end automatic TO system that can collect network information, learn from past decisions, and perform actions to achieve operator-defined goals. We implement AuTO with popular machine learning frameworks and commodity servers, and deploy it on a 32-server testbed. Compared to existing approaches, AuTO reduces the TO turn-around time from weeks to ~100 milliseconds while achieving superior performance. For example, it demonstrates up to 48.14% reduction in average flow completion time (FCT) over existing solutions.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,191–205,15,"traffic optimization, reinforcement learning, datacenter networks","Budapest, Hungary",SIGCOMM '18,,,,,,
184,inproceedings,"Wohlfart, Florian and Chatzis, Nikolaos and Dabanoglu, Caglar and Carle, Georg and Willinger, Walter",Leveraging Interconnections for Performance: The Serving Infrastructure of a Large CDN,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230576,10.1145/3230543.3230576,"Today's large content providers (CP) are busy building out their service infrastructures or ""peering edges"" to satisfy the insatiable demand for content created by an ever-expanding Internet edge. One component of these serving infrastructures that features prominently in this build-out is their connectivity fabric; i.e., the set of all Internet interconnections that content has to traverse en route from the CP's various ""deployments"" or ""serving sites"" to end users. However, these connectivity fabrics have received little attention in the past and remain largely ill-understood.In this paper, we describe the results of an in-depth study of the connectivity fabric of Akamai. Our study reveals that Akamai's connectivity fabric consists of some 6,100 different ""explicit"" peerings (i.e., Akamai is one of the two involved peers) and about 28,500 different ""implicit"" peerings (i.e., Akamai is neither of the two peers). Our work contributes to a better understanding of real-world serving infrastructures by providing an original account of implicit peerings and demonstrating the performance benefits that Akamai can reap from leveraging its rich connectivity fabric for serving its customers' content to end users.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,206–220,15,"content providers, peering, content delivery networks","Budapest, Hungary",SIGCOMM '18,,,,,,
185,inproceedings,"Montazeri, Behnam and Li, Yilong and Alizadeh, Mohammad and Ousterhout, John",Homa: A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230564,10.1145/3230543.3230564,"Homa is a new transport protocol for datacenter networks. It provides exceptionally low latency, especially for workloads with a high volume of very short messages, and it also supports large messages and high network utilization. Homa uses in-network priority queues to ensure low latency for short messages; priority allocation is managed dynamically by each receiver and integrated with a receiver-driven flow control mechanism. Homa also uses controlled overcommitment of receiver downlinks to ensure efficient bandwidth utilization at high load. Our implementation of Homa delivers 99th percentile round-trip times less than 15 μs for short messages on a 10 Gbps network running at 80% load. These latencies are almost 100x lower than the best published measurements of an implementation. In simulations, Homa's latency is roughly equal to pFabric and significantly better than pHost, PIAS, and NDP for almost all message sizes and workloads. Homa can also sustain higher network loads than pFabric, pHost, or PIAS.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,221–235,15,"data centers, low latency, transport protocols, network stacks","Budapest, Hungary",SIGCOMM '18,,,,,,
186,inproceedings,"Zhang, Ben and Jin, Xin and Ratnasamy, Sylvia and Wawrzynek, John and Lee, Edward A.",AWStream: Adaptive Wide-Area Streaming Analytics,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230554,10.1145/3230543.3230554,"The emerging class of wide-area streaming analytics faces the challenge of scarce and variable WAN bandwidth. Non-adaptive applications built with TCP or UDP suffer from increased latency or degraded accuracy. State-of-the-art approaches that adapt to network changes require developer writing sub-optimal manual policies or are limited to application-specific optimizations.We present AWStream, a stream processing system that simultaneously achieves low latency and high accuracy in the wide area, requiring minimal developer efforts. To realize this, AWStream uses three ideas: (i) it integrates application adaptation as a first-class programming abstraction in the stream processing model; (ii) with a combination of offline and online profiling, it automatically learns an accurate profile that models accuracy and bandwidth trade-off; and (iii) at runtime, it carefully adjusts the application data rate to match the available bandwidth while maximizing the achievable accuracy. We evaluate AWStream with three real-world applications: augmented reality, pedestrian detection, and monitoring log analysis. Our experiments show that AWStream achieves sub-second latency with only nominal accuracy drop (2-6%).",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,236–252,17,"adaptation, profiling, learning, wide area network","Budapest, Hungary",SIGCOMM '18,,,,,,
187,inproceedings,"Jiang, Junchen and Ananthanarayanan, Ganesh and Bodik, Peter and Sen, Siddhartha and Stoica, Ion",Chameleon: Scalable Adaptation of Video Analytics,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230574,10.1145/3230543.3230574,"Applying deep convolutional neural networks (NN) to video data at scale poses a substantial systems challenge, as improving inference accuracy often requires a prohibitive cost in computational resources. While it is promising to balance resource and accuracy by selecting a suitable NN configuration (e.g., the resolution and frame rate of the input video), one must also address the significant dynamics of the NN configuration's impact on video analytics accuracy. We present Chameleon, a controller that dynamically picks the best configurations for existing NN-based video analytics pipelines. The key challenge in Chameleon is that in theory, adapting configurations frequently can reduce resource consumption with little degradation in accuracy, but searching a large space of configurations periodically incurs an overwhelming resource overhead that negates the gains of adaptation. The insight behind Chameleon is that the underlying characteristics (e.g., the velocity and sizes of objects) that affect the best configuration have enough temporal and spatial correlation to allow the search cost to be amortized over time and across multiple video feeds. For example, using the video feeds of five traffic cameras, we demonstrate that compared to a baseline that picks a single optimal configuration offline, Chameleon can achieve 20-50% higher accuracy with the same amount of resources, or achieve the same accuracy with only 30--50% of the resources (a 2-3X speedup).",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,253–266,14,"deep neural networks, object detection, video analytics","Budapest, Hungary",SIGCOMM '18,,,,,,
188,inproceedings,"Zhao, Mingmin and Tian, Yonglong and Zhao, Hang and Alsheikh, Mohammad Abu and Li, Tianhong and Hristov, Rumen and Kabelac, Zachary and Katabi, Dina and Torralba, Antonio",RF-Based 3D Skeletons,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230579,10.1145/3230543.3230579,"This paper introduces RF-Pose3D, the first system that infers 3D human skeletons from RF signals. It requires no sensors on the body, and works with multiple people and across walls and occlusions. Further, it generates dynamic skeletons that follow the people as they move, walk or sit. As such, RF-Pose3D provides a significant leap in RF-based sensing and enables new applications in gaming, healthcare, and smart homes.RF-Pose3D is based on a novel convolutional neural network (CNN) architecture that performs high-dimensional convolutions by decomposing them into low-dimensional operations. This property allows the network to efficiently condense the spatio-temporal information in RF signals. The network first zooms in on the individuals in the scene, and crops the RF signals reflected off each person. For each individual, it localizes and tracks their body parts - head, shoulders, arms, wrists, hip, knees, and feet. Our evaluation results show that RF-Pose3D tracks each keypoint on the human body with an average error of 4.2 cm, 4.0 cm, and 4.9 cm along the X, Y, and Z axes respectively. It maintains this accuracy even in the presence of multiple people, and in new environments that it has not seen in the training set. Demo videos are available at our website: http://rfpose3d.csail.mit.edu.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,267–281,15,"localization, neural networks, smart homes, 3D human pose estimation, RF sensing, machine learning","Budapest, Hungary",SIGCOMM '18,,,,,,
189,inproceedings,"Shen, Sheng and Roy, Nirupam and Guan, Junfeng and Hassanieh, Haitham and Choudhury, Romit Roy",MUTE: Bringing IoT to Noise Cancellation,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230550,10.1145/3230543.3230550,"Active Noise Cancellation (ANC) is a classical area where noise in the environment is canceled by producing anti-noise signals near the human ears (e.g., in Bose's noise cancellation headphones). This paper brings IoT to active noise cancellation by combining wireless communication with acoustics. The core idea is to place an IoT device in the environment that listens to ambient sounds and forwards the sound over its wireless radio. Since wireless signals travel much faster than sound, our ear-device receives the sound in advance of its actual arrival. This serves as a glimpse into the future, that we call lookahead, and proves crucial for real-time noise cancellation, especially for unpredictable, wide-band sounds like music and speech. Using custom IoT hardware, as well as lookahead-aware cancellation algorithms, we demonstrate MUTE, a fully functional noise cancellation prototype that outperforms Bose's latest ANC headphone. Importantly, our design does not need to block the ear - the ear canal remains open, making it comfortable (and healthier) for continuous use.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,282–296,15,"acoustics, earphone, internet of things, adaptive filter, wearables, edge computing, smart home, noise cancellation","Budapest, Hungary",SIGCOMM '18,,,,,,
190,inproceedings,"Kim, Daehyeok and Memaripour, Amirsaman and Badam, Anirudh and Zhu, Yibo and Liu, Hongqiang Harry and Padhye, Jitu and Raindel, Shachar and Swanson, Steven and Sekar, Vyas and Seshan, Srinivasan",Hyperloop: Group-Based NIC-Offloading to Accelerate Replicated Transactions in Multi-Tenant Storage Systems,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230572,10.1145/3230543.3230572,"Storage systems in data centers are an important component of large-scale online services. They typically perform replicated transactional operations for high data availability and integrity. Today, however, such operations suffer from high tail latency even with recent kernel bypass and storage optimizations, and thus affect the predictability of end-to-end performance of these services. We observe that the root cause of the problem is the involvement of the CPU, a precious commodity in multi-tenant settings, in the critical path of replicated transactions. In this paper, we present HyperLoop, a new framework that removes CPU from the critical path of replicated transactions in storage systems by offloading them to commodity RDMA NICs, with non-volatile memory as the storage medium. To achieve this, we develop new and general NIC offloading primitives that can perform memory operations on all nodes in a replication group while guaranteeing ACID properties without CPU involvement. We demonstrate that popular storage applications can be easily optimized using our primitives. Our evaluation results with microbenchmarks and application benchmarks show that HyperLoop can reduce 99th percentile latency ≈ 800X with close to 0% CPU consumption on replicas.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,297–312,16,"distributed storage systems, replicated transactions, NIC-offloading, RDMA","Budapest, Hungary",SIGCOMM '18,,,,,,
191,inproceedings,"Mittal, Radhika and Shpiner, Alexander and Panda, Aurojit and Zahavi, Eitan and Krishnamurthy, Arvind and Ratnasamy, Sylvia and Shenker, Scott",Revisiting Network Support for RDMA,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230557,10.1145/3230543.3230557,"The advent of RoCE (RDMA over Converged Ethernet) has led to a significant increase in the use of RDMA in datacenter networks. To achieve good performance, RoCE requires a lossless network which is in turn achieved by enabling Priority Flow Control (PFC) within the network. However, PFC brings with it a host of problems such as head-of-the-line blocking, congestion spreading, and occasional deadlocks. Rather than seek to fix these issues, we instead ask: is PFC fundamentally required to support RDMA over Ethernet?We show that the need for PFC is an artifact of current RoCE NIC designs rather than a fundamental requirement. We propose an improved RoCE NIC (IRN) design that makes a few simple changes to the RoCE NIC for better handling of packet losses. We show that IRN (without PFC) outperforms RoCE (with PFC) by 6-83% for typical network scenarios. Thus not only does IRN eliminate the need for PFC, it improves performance in the process! We further show that the changes that IRN introduces can be implemented with modest overheads of about 3-10% to NIC resources. Based on our results, we argue that research and industry should rethink the current trajectory of network support for RDMA.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,313–326,14,"PFC, RoCE, RDMA, iWARP, datacenter transport","Budapest, Hungary",SIGCOMM '18,,,,,,
192,inproceedings,"Neugebauer, Rolf and Antichi, Gianni and Zazo, Jos\'{e",Understanding PCIe Performance for End Host Networking,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230560,10.1145/3230543.3230560,"In recent years, spurred on by the development and availability of programmable NICs, end hosts have increasingly become the enforcement point for core network functions such as load balancing, congestion control, and application specific network offloads. However, implementing custom designs on programmable NICs is not easy: many potential bottlenecks can impact performance.This paper focuses on the performance implication of PCIe, the de-facto I/O interconnect in contemporary servers, when interacting with the host architecture and device drivers. We present a theoretical model for PCIe and pcie-bench, an open-source suite, that allows developers to gain an accurate and deep understanding of the PCIe substrate. Using pcie-bench, we characterize the PCIe subsystem in modern servers. We highlight surprising differences in PCIe implementations, evaluate the undesirable impact of PCIe features such as IOMMUs, and show the practical limits for common network cards operating at 40Gb/s and beyond. Furthermore, through pcie-bench we gained insights which guided software and future hardware architectures for both commercial and research oriented network cards and DMA engines.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,327–341,15,"reconfigurable hardware, operating system, PCIe","Budapest, Hungary",SIGCOMM '18,,,,,,
193,inproceedings,"Choi, Sean and Burkov, Boris and Eckert, Alex and Fang, Tian and Kazemkhani, Saman and Sherwood, Rob and Zhang, Ying and Zeng, Hongyi",FBOSS: Building Switch Software at Scale,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230546,10.1145/3230543.3230546,"The conventional software running on network devices, such as switches and routers, is typically vendor-supplied, proprietary and closed-source; as a result, it tends to contain extraneous features that a single operator will not most likely fully utilize. Furthermore, cloud-scale data center networks often times have software and operational requirements that may not be well addressed by the switch vendors.In this paper, we present our ongoing experiences on overcoming the complexity and scaling issues that we face when designing, developing, deploying and operating an in-house software built to manage and support a set of features required for data center switches of a large scale Internet content provider. We present FBOSS, our own data center switch software, that is designed with the basis on our switch-as-a-server and deploy-early-and-iterate principles. We treat software running on data center switches as any other software services that run on a commodity server. We also build and deploy only a minimal number of features and iterate on it. These principles allow us to rapidly iterate, test, deploy and manage FBOSS at scale. Over the last five years, our experiences show that FBOSS's design principles allow us to quickly build a stable and scalable network. As evidence, we have successfully grown the number of FBOSS instances running in our data center by over 30x over a two year period.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,342–356,15,"switch software design, data center networks, FBOSS, network management, Facebook, network monitoring","Budapest, Hungary",SIGCOMM '18,,,,,,
194,inproceedings,"Gupta, Arpit and Harrison, Rob and Canini, Marco and Feamster, Nick and Rexford, Jennifer and Willinger, Walter",Sonata: Query-Driven Streaming Network Telemetry,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230555,10.1145/3230543.3230555,"Managing and securing networks requires collecting and analyzing network traffic data in real time. Existing telemetry systems do not allow operators to express the range of queries needed to perform management or scale to large traffic volumes and rates. We present Sonata, an expressive and scalable telemetry system that coordinates joint collection and analysis of network traffic. Sonata provides a declarative interface to express queries for a wide range of common telemetry tasks; to enable real-time execution, Sonata partitions each query across the stream processor and the data plane, running as much of the query as it can on the network switch, at line rate. To optimize the use of limited switch memory, Sonata dynamically refines each query to ensure that available resources focus only on traffic that satisfies the query. Our evaluation shows that Sonata can support a wide range of telemetry tasks while reducing the workload for the stream processor by as much as seven orders of magnitude compared to existing telemetry systems.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,357–371,15,"analytics, programmable switches, stream processing","Budapest, Hungary",SIGCOMM '18,,,,,,
195,inproceedings,"Pedrosa, Luis and Iyer, Rishabh and Zaostrovnykh, Arseniy and Fietz, Jonas and Argyraki, Katerina",Automated Synthesis of Adversarial Workloads for Network Functions,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230573,10.1145/3230543.3230573,"Software network functions promise to simplify the deployment of network services and reduce network operation cost. However, they face the challenge of unpredictable performance. Given this performance variability, it is imperative that during deployment, network operators consider the performance of the NF not only for typical but also adversarial workloads. We contribute a tool that helps solve this challenge: it takes as input the LLVM code of a network function and outputs packet sequences that trigger slow execution paths. Under the covers, it combines directed symbolic execution with a sophisticated cache model to look for execution paths that incur many CPU cycles and involve adversarial memory-access patterns. We used our tool on 11 network functions that implement a variety of data structures and discovered workloads that can in some cases triple latency and cut throughput by 19% relative to typical testing workloads.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,372–385,14,"network function performance, adversarial inputs","Budapest, Hungary",SIGCOMM '18,,,,,,
196,inproceedings,"Gao, Kai and Nojima, Taishi and Yang, Y. Richard",Trident: Toward a Unified SDN Programming Framework with Automatic Updates,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230562,10.1145/3230543.3230562,"Software-defined networking (SDN) and network functions (NF) are two essential technologies that need to work together to achieve the goal of highly programmable networking. Unified SDN programming, which integrates states of network functions into SDN control plane programming, brings these two technologies together. In this paper, we conduct the first systematic study of unified SDN programming. We first show that integrating asynchronous, continuously changing states of network functions into SDN can introduce basic complexities. We then present Trident, a novel, unified SDN programming framework that introduces programming primitives including stream attributes, route algebra and live variables to remove these complexities. We demonstrate the expressiveness of Trident using realistic use cases and conduct an extensive evaluation of its efficiency.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,386–401,16,"network functions, route algebra, SDN, network programming, stream attributes, live variables","Budapest, Hungary",SIGCOMM '18,,,,,,
197,inproceedings,"Yaseen, Nofel and Sonchack, John and Liu, Vincent",Synchronized Network Snapshots,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230552,10.1145/3230543.3230552,"When monitoring a network, operators rarely have a finegrained and complete view of the network's state. Instead, today's network monitoring tools generally only measure a single device or path at a time; whole-network metrics are a composition of these independent measurements, i.e., an afterthought. Such tools fail to fully answer a wide range of questions. Is my load balancing algorithm taking advantage of all available paths evenly? How much of my network is concurrently loaded? Is application traffic synchronized? These types of concurrent network behavior are challenging to capture at fine granularity as they involve coordination across the entire network. At the same time, understanding them is essential to the design of network switches, architectures, and protocols.This paper presents the design of a Synchronized Network Snapshot protocol. The goal of our primitive is the collection of a network-wide set of measurements. To ensure that the measurements are meaningful, our design guarantees they are both causally consistent and approximately synchronous. We demonstrate with a Wedge100BF implementation the feasibility of our approach as well as its many potential uses.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,402–416,15,"whole-network measurement, network snapshots","Budapest, Hungary",SIGCOMM '18,,,,,,
198,inproceedings,"Ma, Yunfei and Luo, Zhihong and Steiger, Christoph and Traverso, Giovanni and Adib, Fadel",Enabling Deep-Tissue Networking for Miniature Medical Devices,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230566,10.1145/3230543.3230566,"We present IVN (In-Vivo Networking), a system that enables powering up and communicating with miniature sensors implanted or injected in deep tissues. IVN overcomes fundamental challenges which have prevented past systems from powering up miniature sensors beyond superficial depths. These challenges include the significant signal attenuation caused by bodily tissues and the miniature antennas of the implantable sensors.IVN's key contribution is a novel beamforming algorithm that can focus its energy toward an implantable device, despite its inability to estimate its channel or its location. We implement a multi-antenna prototype of IVN, and perform extensive evaluations via in-vitro, ex-vivo, and in-vivo tests in a pig. Our results demonstrate that it can power up and communicate with millimeter-sized sensors at over 10 cm depths in fluids, as well as battery-free tags placed in a central organ of a swine.The implications of our new beamforming technology extend beyond miniature implantables. In particular, our results demonstrate that IVN can power up off-the-shelf passive RFIDs at distances of 38 m, i.e., 7.6X larger than the operation range of the same RFIDs.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,417–431,15,"medical implants, power delivery, battery-free, wireless sensors, deep-tissues, RFID","Budapest, Hungary",SIGCOMM '18,,,,,,
199,inproceedings,"Hassanieh, Haitham and Abari, Omid and Rodriguez, Michael and Abdelghany, Mohammed and Katabi, Dina and Indyk, Piotr",Fast Millimeter Wave Beam Alignment,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230581,10.1145/3230543.3230581,"There is much interest in integrating millimeter wave radios (mmWave) into wireless LANs and 5G cellular networks to benefit from their multi-GHz of available spectrum. Yet, unlike existing technologies, e.g., WiFi, mmWave radios require highly directional antennas. Since the antennas have pencil-beams, the transmitter and receiver need to align their beams before they can communicate. Existing systems scan the space to find the best alignment. Such a process has been shown to introduce up to seconds of delay, and is unsuitable for wireless networks where an access point has to quickly switch between users and accommodate mobile clients.This paper presents Agile-Link, a new protocol that can find the best mmWave beam alignment without scanning the space. Given all possible directions for setting the antenna beam, Agile-Link provably finds the optimal direction in logarithmic number of measurements. Further, Agile-Link works within the existing 802.11ad standard for mmWave LAN, and can support both clients and access points. We have implemented Agile-Link in a mmWave radio and evaluated it empirically. Our results show that it reduces beam alignment delay by orders of magnitude. In particular, for highly directional mmWave devices operating under 802.11ad, the delay drops from over a second to 2.5 ms.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,432–445,14,"5G, millimeter wave, sparse recovery, beam alignment","Budapest, Hungary",SIGCOMM '18,,,,,,
200,inproceedings,"Rostami, Mohammad and Gummeson, Jeremy and Kiaghadi, Ali and Ganesan, Deepak",Polymorphic Radios: A New Design Paradigm for Ultra-Low Power Communication,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230571,10.1145/3230543.3230571,"Duty-cycling has emerged as the predominant method for optimizing power consumption of low-power radios, particularly for sensors that transmit sporadically in small bursts. But duty-cycling is a poor fit for applications involving high-rate sensor data from wearable sensors such as IMUs, microphones, and imagers that need to stream data to the cloud to execute sophisticated machine learning models.We argue that there is significant room to optimize low-power radios if we can take advantage of channel dynamics in short-range settings. However, we face challenges in designing radios that are efficient at power levels between μWs and mWs to take advantage of periods of good signal strength and nimble to deal with highly dynamic channels resulting from body movements. To achieve this, we propose radio polymorphism, a radio architecture with tightly integrated passive and active components that allows us to turn high channel dynamics to our advantage. We leverage passive modes in myriad ways within the network stack, from minimizing data transfer and control overheads to improving rate selection and enabling channel-aware opportunistic transmission. We instantiate our design in a full hardware-software prototype, Morpho, and demonstrate up to an order of improvement in efficiency across diverse scenarios and applications.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,446–460,15,,"Budapest, Hungary",SIGCOMM '18,,,,,,
201,inproceedings,"Hamed, Ezzeldin and Rahul, Hariharan and Partov, Bahar",Chorus: Truly Distributed Distributed-MIMO,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230578,10.1145/3230543.3230578,"Distributed MIMO has long been known theoretically to bring large throughput gains to wireless networks. Recent years have seen significant interest and progress in developing practical distributed MIMO systems. However, these systems only distribute the transmission function across the multiple nodes. The control fabric that synchronizes the nodes to a common reference phase still fundamentally requires a single leader that all nodes in the network are capable of hearing.This paper presents Chorus, a truly distributed distributed-MIMO system. Chorus is leaderless - all nodes are peers, and jointly transmit the synchronization signal used by other nodes to synchronize to a common reference phase. The participation of all nodes in the network in the synchronization signal enables Chorus to scale to large networks, while being resilient to node failures or changes in network connectivity, and without imposing onerous management burdens on network administrators. We implement and evaluate Chorus and demonstrate that it can synchronize effectively without the need for a single leader, scale to large networks where no leader node can be heard by all others, and provide 2.7X throughput improvement over traditional leader-based systems.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,461–475,15,"distributed MIMO, LTE, wireless networks, synchronization, multi-user MIMO","Budapest, Hungary",SIGCOMM '18,,,,,,
202,inproceedings,"Beckett, Ryan and Gupta, Aarti and Mahajan, Ratul and Walker, David",Control Plane Compression,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230583,10.1145/3230543.3230583,"We develop an algorithm capable of compressing large networks into smaller ones with similar control plane behavior: For every stable routing solution in the large, original network, there exists a corresponding solution in the compressed network, and vice versa. Our compression algorithm preserves a wide variety of network properties including reachability, loop freedom, and path length. Consequently, operators may speed up network analysis, based on simulation, emulation, or verification, by analyzing only the compressed network. Our approach is based on a new theory of control plane equivalence. We implement these ideas in a tool called Bonsai and apply it to real and synthetic networks. Bonsai can shrink real networks by over a factor of 5 and speed up analysis by several orders of magnitude.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,476–489,14,"stable routing problem, network verification","Budapest, Hungary",SIGCOMM '18,,,,,,
203,inproceedings,"Liu, Jed and Hallahan, William and Schlesinger, Cole and Sharif, Milad and Lee, Jeongkeun and Soul\'{e",P4v: Practical Verification for Programmable Data Planes,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230582,10.1145/3230543.3230582,"We present the design and implementation of p4v, a practical tool for verifying data planes described using the P4 programming language. The design of p4v is based on classic verification techniques but adds several key innovations including a novel mechanism for incorporating assumptions about the control plane and domain-specific optimizations which are needed to scale to large programs. We present case studies showing that p4v verifies important properties and finds bugs in real-world programs. We conduct experiments to quantify the scalability of p4v on a wide range of additional examples. We show that with just a few hundred lines of control-plane annotations, p4v is able to verify critical safety properties for switch.p4, a program that implements the functionality of on a modern data center switch, in under three minutes.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,490–503,14,"programmable data planes, verification, P4","Budapest, Hungary",SIGCOMM '18,,,,,,
204,inproceedings,"Liu, Guyue and Ren, Yuxin and Yurchenko, Mykola and Ramakrishnan, K. K. and Wood, Timothy","Microboxes: High Performance NFV with Customizable, Asynchronous TCP Stacks and Dynamic Subscriptions",2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230563,10.1145/3230543.3230563,"Existing network service chaining frameworks are based on a ""packet-centric"" model where each NF in a chain is given every packet for processing. This approach becomes both inefficient and inconvenient for more complex network functions that operate at higher levels of the protocol stack. We propose Microboxes, a novel service chaining abstraction designed to support transport- and application-layer middle-boxes, or even end-system like services. Simply including a TCP stack in an NFV platform is insufficient because there is a wide spectrum of middlebox types-from NFs requiring only simple TCP bytestream reconstruction to full endpoint termination. By exposing a publish/subscribe-based API for NFs to access packets or protocol events as needed, Microboxes eliminates redundant processing across a chain and enables a modular design. Our implementation on a DPDK-based NFV framework can double throughput by consolidating stack operations and provide a 51% throughput gain by customizing TCP processing to the appropriate level.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,504–517,14,"middleboxes, service chain, NFV, networking stack","Budapest, Hungary",SIGCOMM '18,,,,,,
205,inproceedings,"Stoenescu, Radu and Dumitrescu, Dragos and Popovici, Matei and Negreanu, Lorina and Raiciu, Costin",Debugging P4 Programs with Vera,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230548,10.1145/3230543.3230548,"We present Vera, a tool that verifies P4 programs using symbolic execution. Vera automatically uncovers a number of common bugs including parsing/deparsing errors, invalid memory accesses, loops and tunneling errors, among others. Vera can also be used to verify user-specified properties in a novel language we call NetCTL.To enable scalable, exhaustive verification of P4 program snapshots, Vera automatically generates all valid header layouts and uses a novel data-structure for match-action processing optimized for verification. These techniques allow Vera to scale very well: it only takes between 5s-15s to track the execution of a purely symbolic packet in the largest P4 program currently available (6KLOC) and can compute SEFL model updates in milliseconds. Vera can also explore multiple concrete dataplanes at once by allowing the programmer to insert symbolic table entries; the resulting verification highlights possible control plane errors.We have used Vera to analyze many P4 programs including the P4 tutorials, P4 programs in the research literature and the switch code from https://p4.org. Vera has found several bugs in each of them in seconds/minutes.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,518–532,15,,"Budapest, Hungary",SIGCOMM '18,,,,,,
206,inproceedings,"Nisar, Aqib and Kashaf, Aqsa and Qazi, Ihsan Ayyub and Uzmi, Zartash Afzal",Incentivizing Censorship Measurements via Circumvention,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230568,10.1145/3230543.3230568,"We present C-Saw, a system that measures Internet censorship by offering data-driven censorship circumvention to users. The adaptive circumvention capability of C-Saw incentivizes users to opt-in by offering small page load times (PLTs). As users crowdsource, the measurement data gets richer, offering greater insights into censorship mechanisms over a wider region, and in turn leading to even better circumvention capabilities. C-Saw incorporates user consent in its design by measuring only those URLs that a user actually visits. Using a cross-platform implementation of C-Saw, we show that it is effective at collecting and disseminating censorship measurements, selecting circumvention approaches, and optimizing user experience. C-Saw improves the average PLT by up to 48% and 63% over Lantern and Tor, respectively. We demonstrate the feasibility of a large-scale deployment of C-Saw with a pilot study.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,533–546,14,,"Budapest, Hungary",SIGCOMM '18,,,,,,
207,inproceedings,"Singh, Rachee and Ghobadi, Manya and Foerster, Klaus-Tycho and Filer, Mark and Gill, Phillipa",RADWAN: Rate Adaptive Wide Area Network,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230570,10.1145/3230543.3230570,"Fiber optic cables connecting data centers are an expensive but important resource for large organizations. Their importance has driven a conservative deployment approach, with redundancy and reliability baked in at multiple layers. In this work, we take a more aggressive approach and argue for adapting the capacity of fiber optic links based on their signal-to-noise ratio (SNR). We investigate this idea by analyzing the SNR of over 8,000 links in an optical backbone for a period of three years. We show that the capacity of 64% of 100 Gbps IP links can be augmented by at least 75 Gbps, leading to an overall capacity gain of over 134 Tbps. Moreover, adapting link capacity to a lower rate can prevent up to 25% of link failures. Our analysis shows that using the same links, we get higher capacity, better availability, and 32% lower cost per gigabit per second. To accomplish this, we propose RADWAN, a traffic engineering system that allows optical links to adapt their rate based on the observed SNR to achieve higher throughput and availability while minimizing the churn during capacity reconfigurations. We evaluate RADWAN using a testbed consisting of 1,540 km fiber with 16 amplifiers and attenuators. We then simulate the throughput gains of RADWAN at scale and compare them to the gains of state-of-the-art traffic engineering systems. Our data-driven simulations show that RADWAN improves the overall network throughput by 40% while also improving the average link availability.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,547–560,14,"wide area networks, optical backbone, traffic engineering","Budapest, Hungary",SIGCOMM '18,,,,,,
208,inproceedings,"Yang, Tong and Jiang, Jie and Liu, Peng and Huang, Qun and Gong, Junzhi and Zhou, Yang and Miao, Rui and Li, Xiaoming and Uhlig, Steve",Elastic Sketch: Adaptive and Fast Network-Wide Measurements,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230544,10.1145/3230543.3230544,"When network is undergoing problems such as congestion, scan attack, DDoS attack, etc., measurements are much more important than usual. In this case, traffic characteristics including available bandwidth, packet rate, and flow size distribution vary drastically, significantly degrading the performance of measurements. To address this issue, we propose the Elastic sketch. It is adaptive to currently traffic characteristics. Besides, it is generic to measurement tasks and platforms. We implement the Elastic sketch on six platforms: P4, FPGA, GPU, CPU, multi-core CPU, and OVS, to process six typical measurement tasks. Experimental results and theoretical analysis show that the Elastic sketch can adapt well to traffic characteristics. Compared to the state-of-the-art, the Elastic sketch achieves 44.6 ∼ 45.2 times faster speed and 2.0 ∼ 273.7 smaller error rate.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,561–575,15,"generic, sketches, elastic, network measurements, compression","Budapest, Hungary",SIGCOMM '18,,,,,,
209,inproceedings,"Huang, Qun and Lee, Patrick P. C. and Bao, Yungang",Sketchlearn: Relieving User Burdens in Approximate Measurement with Automated Statistical Inference,2018,9781450355674,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3230543.3230559,10.1145/3230543.3230559,"Network measurement is challenged to fulfill stringent resource requirements in the face of massive network traffic. While approximate measurement can trade accuracy for resource savings, it demands intensive manual efforts to configure the right resource-accuracy trade-offs in real deployment. Such user burdens are caused by how existing approximate measurement approaches inherently deal with resource conflicts when tracking massive network traffic with limited resources. In particular, they tightly couple resource configurations with accuracy parameters, so as to provision sufficient resources to bound the measurement errors. We design SketchLearn, a novel sketch-based measurement framework that resolves resource conflicts by learning their statistical properties to eliminate conflicting traffic components. We prototype SketchLearn on OpenVSwitch and P4, and our testbed experiments and stress-test simulation show that SketchLearn accurately and automatically monitors various traffic statistics and effectively supports network-wide measurement with limited resources.",Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication,576–590,15,"network measurement, sketch","Budapest, Hungary",SIGCOMM '18,,,,,,
210,inproceedings,"Chole, Sharad and Fingerhut, Andy and Ma, Sha and Sivaraman, Anirudh and Vargaftik, Shay and Berger, Alon and Mendelson, Gal and Alizadeh, Mohammad and Chuang, Shang-Tse and Keslassy, Isaac and Orda, Ariel and Edsall, Tom",DRMT: Disaggregated Programmable Switching,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098823,10.1145/3098822.3098823,"We present dRMT (disaggregated Reconfigurable Match-Action Table), a new architecture for programmable switches. dRMT overcomes two important restrictions of RMT, the predominant pipeline-based architecture for programmable switches: (1) table memory is local to an RMT pipeline stage, implying that memory not used by one stage cannot be reclaimed by another, and (2) RMT is hardwired to always sequentially execute matches followed by actions as packets traverse pipeline stages. We show that these restrictions make it difficult to execute programs efficiently on RMT.dRMT resolves both issues by disaggregating the memory and compute resources of a programmable switch. Specifically, dRMT moves table memories out of pipeline stages and into a centralized pool that is accessible through a crossbar. In addition, dRMT replaces RMT's pipeline stages with a cluster of processors that can execute match and action operations in any order.We show how to schedule a P4 program on dRMT at compile time to guarantee deterministic throughput and latency. We also present a hardware design for dRMT and analyze its feasibility and chip area. Our results show that dRMT can run programs at line rate with fewer processors compared to RMT, and avoids performance cliffs when there are not enough processors to run a program at line rate. dRMT's hardware design incurs a modest increase in chip area relative to RMT, mainly due to the crossbar.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,1–14,14,"packet processing, RMT, Programmable switching, disagreggation","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
211,inproceedings,"Miao, Rui and Zeng, Hongyi and Kim, Changhoon and Lee, Jeongkeun and Yu, Minlan",SilkRoad: Making Stateful Layer-4 Load Balancing Fast and Cheap Using Switching ASICs,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098824,10.1145/3098822.3098824,"In this paper, we show that up to hundreds of software load balancer (SLB) servers can be replaced by a single modern switching ASIC, potentially reducing the cost of load balancing by over two orders of magnitude. Today, large data centers typically employ hundreds or thousands of servers to load-balance incoming traffic over application servers. These software load balancers (SLBs) map packets destined to a service (with a virtual IP address, or VIP), to a pool of servers tasked with providing the service (with multiple direct IP addresses, or DIPs). An SLB is stateful, it must always map a connection to the same server, even if the pool of servers changes and/or if the load is spread differently across the pool. This property is called per-connection consistency or PCC. The challenge is that the load balancer must keep track of millions of connections simultaneously.Until recently, it was not possible to implement a load balancer with PCC in a merchant switching ASIC, because high-performance switching ASICs typically can not maintain per-connection states with PCC. Newer switching ASICs provide resources and primitives to enable PCC at a large scale. In this paper, we explore how to use switching ASICs to build much faster load balancers than have been built before. Our system, called SilkRoad, is defined in a 400 line P4 program and when compiled to a state-of-the-art switching ASIC, we show it can load-balance ten million connections simultaneously at line rate.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,15–28,14,"Programmable switches, Load balancing","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
212,inproceedings,"Handley, Mark and Raiciu, Costin and Agache, Alexandru and Voinescu, Andrei and Moore, Andrew W. and Antichi, Gianni and W\'{o",Re-Architecting Datacenter Networks and Stacks for Low Latency and High Performance,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098825,10.1145/3098822.3098825,"Modern datacenter networks provide very high capacity via redundant Clos topologies and low switch latency, but transport protocols rarely deliver matching performance. We present NDP, a novel data-center transport architecture that achieves near-optimal completion times for short transfers and high flow throughput in a wide range of scenarios, including incast. NDP switch buffers are very shallow and when they fill the switches trim packets to headers and priority forward the headers. This gives receivers a full view of instantaneous demand from all senders, and is the basis for our novel, high-performance, multipath-aware transport protocol that can deal gracefully with massive incast events and prioritize traffic from different senders on RTT timescales. We implemented NDP in Linux hosts with DPDK, in a software switch, in a NetFPGA-based hardware switch, and in P4. We evaluate NDP's performance in our implementations and in large-scale simulations, simultaneously demonstrating support for very low-latency and high throughput.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,29–42,14,"Datacenters, Transport Protocols, Network Stacks","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
213,inproceedings,"Sun, Chen and Bi, Jun and Zheng, Zhilong and Yu, Heng and Hu, Hongxin",NFP: Enabling Network Function Parallelism in NFV,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098826,10.1145/3098822.3098826,"Software-based sequential service chains in Network Function Virtualization (NFV) could introduce significant performance overhead. Current acceleration efforts for NFV mainly target on optimizing each component of the sequential service chain. However, based on the statistics from real world enterprise networks, we observe that 53.8% network function (NF) pairs can work in parallel. In particular, 41.5% NF pairs can be parallelized without causing extra resource overhead. In this paper, we present NFP, a high performance framework, that innovatively enables network function parallelism to improve NFV performance. NFP consists of three logical components. First, NFP provides a policy specification scheme for operators to intuitively describe sequential or parallel NF chaining intents. Second, NFP orchestrator intelligently identifies NF dependency and automatically compiles the policies into high performance service graphs. Third, NFP infrastructure performs light-weight packet copying, distributed parallel packet delivery, and load-balanced merging of packet copies to support NF parallelism. We implement an NFP prototype based on DPDK in Linux containers. Our evaluation results show that NFP achieves significant latency reduction for real world service chains.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,43–56,14,"network function parallelism, service chain, NFV","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
214,inproceedings,"Zave, Pamela and Ferreira, Ronaldo A. and Zou, Xuan Kelvin and Morimoto, Masaharu and Rexford, Jennifer",Dynamic Service Chaining with Dysco,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098827,10.1145/3098822.3098827,"Middleboxes are crucial for improving network security and performance, but only if the right traffic goes through the right middleboxes at the right time. Existing traffic-steering techniques rely on a central controller to install fine-grained forwarding rules in network elements---at the expense of a large number of rules, a central point of failure, challenges in ensuring all packets of a session traverse the same middleboxes, and difficulties with middleboxes that modify the ""five tuple."" We argue that a session-level protocol is a fundamentally better approach to traffic steering, while naturally supporting host mobility and multihoming in an integrated fashion. In addition, a session-level protocol can enable new capabilities like dynamic service chaining, where the sequence of middleboxes can change during the life of a session, e.g., to remove a load-balancer that is no longer needed, replace a middlebox undergoing maintenance, or add a packet scrubber when traffic looks suspicious. Our Dysco protocol steers the packets of a TCP session through a service chain, and can dynamically reconfigure the chain for an ongoing session. Dysco requires no changes to end-host and middlebox applications, host TCP stacks, or IP routing. Dysco's distributed reconfiguration protocol handles the removal of proxies that terminate TCP connections, middleboxes that change the size of a byte stream, and concurrent requests to reconfigure different parts of a chain. Through formal verification using Spin and experiments with our Linux-based prototype, we show that Dysco is provably correct, highly scalable, and able to reconfigure service chains across a range of middleboxes.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,57–70,14,"Session Protocol, Spin, NFV, Verification","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
215,inproceedings,"Kulkarni, Sameer G. and Zhang, Wei and Hwang, Jinho and Rajagopalan, Shriram and Ramakrishnan, K. K. and Wood, Timothy and Arumaithurai, Mayutan and Fu, Xiaoming",NFVnice: Dynamic Backpressure and Scheduling for NFV Service Chains,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098828,10.1145/3098822.3098828,"Managing Network Function (NF) service chains requires careful system resource management. We propose NFVnice, a user space NF scheduling and service chain management framework to provide fair, efficient and dynamic resource scheduling capabilities on Network Function Virtualization (NFV) platforms. The NFVnice framework monitors load on a service chain at high frequency (1000Hz) and employs backpressure to shed load early in the service chain, thereby preventing wasted work. Borrowing concepts such as rate proportional scheduling from hardware packet schedulers, CPU shares are computed by accounting for heterogeneous packet processing costs of NFs, I/O, and traffic arrival characteristics. By leveraging cgroups, a user space process scheduling abstraction exposed by the operating system, NFVnice is capable of controlling when network functions should be scheduled. NFVnice improves NF performance by complementing the capabilities of the OS scheduler but without requiring changes to the OS's scheduling mechanisms. Our controlled experiments show that NFVnice provides the appropriate rate-cost proportional fair share of CPU to NFs and significantly improves NF performance (throughput and loss) by reducing wasted work across an NF chain, compared to using the default OS scheduler. NFVnice achieves this even for heterogeneous NFs with vastly different computational costs and for heterogeneous workloads.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,71–84,14,"Backpressure, Network Functions (NF), Cgroups, NF-Scheduling","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
216,inproceedings,"Narayana, Srinivas and Sivaraman, Anirudh and Nathan, Vikram and Goyal, Prateesh and Arun, Venkat and Alizadeh, Mohammad and Jeyakumar, Vimalkumar and Kim, Changhoon",Language-Directed Hardware Design for Network Performance Monitoring,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098829,10.1145/3098822.3098829,"Network performance monitoring today is restricted by existing switch support for measurement, forcing operators to rely heavily on endpoints with poor visibility into the network core. Switch vendors have added progressively more monitoring features to switches, but the current trajectory of adding specific features is unsustainable given the ever-changing demands of network operators. Instead, we ask what switch hardware primitives are required to support an expressive language of network performance questions. We believe that the resulting switch hardware design could address a wide variety of current and future performance monitoring needs.We present a performance query language, Marple, modeled on familiar functional constructs like map, filter, groupby, and zip. Marple is backed by a new programmable key-value store primitive on switch hardware. The key-value store performs flexible aggregations at line rate (e.g., a moving average of queueing latencies per flow), and scales to millions of keys. We present a Marple compiler that targets a P4-programmable software switch and a simulator for high-speed programmable switches. Marple can express switch queries that could previously run only on end hosts, while Marple queries only occupy a modest fraction of a switch's hardware resources.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,85–98,14,"network hardware, network programming, Network measurement","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
217,inproceedings,"Yuan, Yifei and Lin, Dong and Mishra, Ankit and Marwaha, Sajal and Alur, Rajeev and Loo, Boon Thau",Quantitative Network Monitoring with NetQRE,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098830,10.1145/3098822.3098830,"In network management today, dynamic updates are required for traffic engineering and for timely response to security threats. Decisions for such updates are based on monitoring network traffic to compute numerical quantities based on a variety of network and application-level performance metrics. Today's state-of-the-art tools lack programming abstractions that capture application or session-layer semantics, and thus require network operators to specify and reason about complex state machines and interactions across layers. To address this limitation, we present the design and implementation of NetQRE, a high-level declarative toolkit that aims to simplify the specification and implementation of such quantitative network policies. NetQRE integrates regular-expression-like pattern matching at flow-level as well as application-level payloads with aggregation operations such as sum and average counts. We describe a compiler for NetQRE that automatically generates an efficient implementation with low memory footprint. Our evaluation results demonstrate that NetQRE allows natural specification of a wide range of quantitative network tasks ranging from detecting security attacks to enforcing application-layer network management policies. NetQRE results in high performance that is comparable with optimized manually-written low-level code and is significantly more efficient than alternative solutions, and can provide timely enforcement of network policies that require quantitative network monitoring.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,99–112,14,"network monitoring language, quantitative regular expression, NetQRE","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
218,inproceedings,"Huang, Qun and Jin, Xin and Lee, Patrick P. C. and Li, Runhui and Tang, Lu and Chen, Yi-Chao and Zhang, Gong",SketchVisor: Robust Network Measurement for Software Packet Processing,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098831,10.1145/3098822.3098831,"Network measurement remains a missing piece in today's software packet processing platforms. Sketches provide a promising building block for filling this void by monitoring every packet with fixed-size memory and bounded errors. However, our analysis shows that existing sketch-based measurement solutions suffer from severe performance drops under high traffic load. Although sketches are efficiently designed, applying them in network measurement inevitably incurs heavy computational overhead.We present SketchVisor, a robust network measurement framework for software packet processing. It augments sketch-based measurement in the data plane with a fast path, which is activated under high traffic load to provide high-performance local measurement with slight accuracy degradations. It further recovers accurate network-wide measurement results via compressive sensing. We have built a SketchVisor prototype on top of Open vSwitch. Extensive testbed experiments show that SketchVisor achieves high throughput and high accuracy for a wide range of network measurement tasks and microbenchmarks.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,113–126,14,"Software packet processing, Network measurement, Sketch","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
219,inproceedings,"Ben Basat, Ran and Einziger, Gil and Friedman, Roy and Luizelli, Marcelo C. and Waisbard, Erez",Constant Time Updates in Hierarchical Heavy Hitters,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098832,10.1145/3098822.3098832,"Monitoring tasks, such as anomaly and DDoS detection, require identifying frequent flow aggregates based on common IP prefixes. These are known as hierarchical heavy hitters (HHH), where the hierarchy is determined based on the type of prefixes of interest in a given application. The per packet complexity of existing HHH algorithms is proportional to the size of the hierarchy, imposing significant overheads.In this paper, we propose a randomized constant time algorithm for HHH. We prove probabilistic precision bounds backed by an empirical evaluation. Using four real Internet packet traces, we demonstrate that our algorithm indeed obtains comparable accuracy and recall as previous works, while running up to 62 times faster. Finally, we extended Open vSwitch (OVS) with our algorithm and showed it is able to handle 13.8 million packets per second. In contrast, incorporating previous works in OVS only obtained 2.5 times lower throughput.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,127–140,14,"Heavy Hitters, Measurement, Streaming, Monitoring","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
220,inproceedings,"Zaostrovnykh, Arseniy and Pirelli, Solal and Pedrosa, Luis and Argyraki, Katerina and Candea, George",A Formally Verified NAT,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098833,10.1145/3098822.3098833,"We present a Network Address Translator (NAT) written in C and proven to be semantically correct according to RFC 3022, as well as crash-free and memory-safe. There exists a lot of recent work on network verification, but it mostly assumes models of network functions and proves properties specific to network configuration, such as reachability and absence of loops. Our proof applies directly to the C code of a network function, and it demonstrates the absence of implementation bugs. Prior work argued that this is not feasible (i.e., that verifying a real, stateful network function written in C does not scale) but we demonstrate otherwise: NAT is one of the most popular network functions and maintains per-flow state that needs to be properly updated and expired, which is a typical source of verification challenges. We tackle the scalability challenge with a new combination of symbolic execution and proof checking using separation logic; this combination matches well the typical structure of a network function. We then demonstrate that formally proven correctness in this case does not come at the cost of performance. The NAT code, proof toolchain, and proofs are available at [58].",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,141–154,14,"Lazy Proofs, Network-Function Verification, Symbolic Execution","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
221,inproceedings,"Beckett, Ryan and Gupta, Aarti and Mahajan, Ratul and Walker, David",A General Approach to Network Configuration Verification,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098834,10.1145/3098822.3098834,"We present Minesweeper, a tool to verify that a network satisfies a wide range of intended properties such as reachability or isolation among nodes, waypointing, black holes, bounded path length, load-balancing, functional equivalence of two routers, and fault-tolerance. Minesweeper translates network configuration files into a logical formula that captures the stable states to which the network forwarding will converge as a result of interactions between routing protocols such as OSPF, BGP and static routes. It then combines the formula with constraints that describe the intended property. If the combined formula is satisfiable, there exists a stable state of the network in which the property does not hold. Otherwise, no stable state (if any) violates the property. We used Minesweeper to check four properties of 152 real networks from a large cloud provider. We found 120 violations, some of which are potentially serious security vulnerabilities. We also evaluated Minesweeper on synthetic benchmarks, and found that it can verify rich properties for networks with hundreds of routers in under five minutes. This performance is due to a suite of model-slicing and hoisting optimizations that we developed, which reduce runtime by over 460x for large networks.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,155–168,14,"Control plane analysis, Network verification","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
222,inproceedings,"Gupta, Trinabh and Fingler, Henrique and Alvisi, Lorenzo and Walfish, Michael",Pretzel: Email Encryption and Provider-Supplied Functions Are Compatible,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098835,10.1145/3098822.3098835,"Emails today are often encrypted, but only between mail servers---the vast majority of emails are exposed in plaintext to the mail servers that handle them. While better than no encryption, this arrangement leaves open the possibility of attacks, privacy violations, and other disclosures. Publicly, email providers have stated that default end-to-end encryption would conflict with essential functions (spam filtering, etc.), because the latter requires analyzing email text. The goal of this paper is to demonstrate that there is no conflict. We do so by designing, implementing, and evaluating Pretzel. Starting from a cryptographic protocol that enables two parties to jointly perform a classification task without revealing their inputs to each other, Pretzel refines and adapts this protocol to the email context. Our experimental evaluation of a prototype demonstrates that email can be encrypted end-to-end and providers can compute over it, at tolerable cost: clients must devote some storage and processing, and provider overhead is roughly 5x versus the status quo.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,169–182,14,"encrypted email, secure two-party computation, linear classifiers","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
223,inproceedings,"Langley, Adam and Riddoch, Alistair and Wilk, Alyssa and Vicente, Antonio and Krasic, Charles and Zhang, Dan and Yang, Fan and Kouranov, Fedor and Swett, Ian and Iyengar, Janardhan and Bailey, Jeff and Dorfman, Jeremy and Roskind, Jim and Kulik, Joanna and Westin, Patrik and Tenneti, Raman and Shade, Robbie and Hamilton, Ryan and Vasiliev, Victor and Chang, Wan-Teh and Shi, Zhongyi",The QUIC Transport Protocol: Design and Internet-Scale Deployment,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098842,10.1145/3098822.3098842,"We present our experience with QUIC, an encrypted, multiplexed, and low-latency transport protocol designed from the ground up to improve transport performance for HTTPS traffic and to enable rapid deployment and continued evolution of transport mechanisms. QUIC has been globally deployed at Google on thousands of servers and is used to serve traffic to a range of clients including a widely-used web browser (Chrome) and a popular mobile video streaming app (YouTube). We estimate that 7% of Internet traffic is now QUIC. We describe our motivations for developing a new transport, the principles that guided our design, the Internet-scale process that we used to perform iterative experiments on QUIC, performance improvements seen by our various services, and our experience deploying QUIC globally. We also share lessons about transport design and the Internet ecosystem that we learned from our deployment.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,183–196,14,,"Los Angeles, CA, USA",SIGCOMM '17,,,,,,
224,inproceedings,"Mao, Hongzi and Netravali, Ravi and Alizadeh, Mohammad",Neural Adaptive Video Streaming with Pensieve,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098843,10.1145/3098822.3098843,"Client-side video players employ adaptive bitrate (ABR) algorithms to optimize user quality of experience (QoE). Despite the abundance of recently proposed schemes, state-of-the-art ABR algorithms suffer from a key limitation: they use fixed control rules based on simplified or inaccurate models of the deployment environment. As a result, existing schemes inevitably fail to achieve optimal performance across a broad set of network conditions and QoE objectives.We propose Pensieve, a system that generates ABR algorithms using reinforcement learning (RL). Pensieve trains a neural network model that selects bitrates for future video chunks based on observations collected by client video players. Pensieve does not rely on pre-programmed models or assumptions about the environment. Instead, it learns to make ABR decisions solely through observations of the resulting performance of past decisions. As a result, Pensieve automatically learns ABR algorithms that adapt to a wide range of environments and QoE metrics. We compare Pensieve to state-of-the-art ABR algorithms using trace-driven and real world experiments spanning a wide variety of network conditions, QoE metrics, and video properties. In all considered scenarios, Pensieve outperforms the best state-of-the-art scheme, with improvements in average QoE of 12%--25%. Pensieve also generalizes well, outperforming existing schemes even on networks for which it was not explicitly trained.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,197–210,14,"bitrate adaptation, video streaming, reinforcement learning","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
225,inproceedings,"Marinos, Ilias and Watson, Robert N.M. and Handley, Mark and Stewart, Randall R.",Disk|Crypt|Net: Rethinking the Stack for High-Performance Video Streaming,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098844,10.1145/3098822.3098844,"Conventional operating systems used for video streaming employ an in-memory disk buffer cache to mask the high latency and low throughput of disks. However, data from Netflix servers show that this cache has a low hit rate, so does little to improve throughput. Latency is not the problem it once was either, due to PCIe-attached flash storage. With memory bandwidth increasingly becoming a bottleneck for video servers, especially when end-to-end encryption is considered, we revisit the interaction between storage and networking for video streaming servers in pursuit of higher performance.We show how to build high-performance userspace network services that saturate existing hardware while serving data directly from disks, with no need for a traditional disk buffer cache. Employing netmap, and developing a new diskmap service, which provides safe high-performance userspace direct I/O access to NVMe devices, we amortize system overheads by utilizing efficient batching of outstanding I/O requests, process-to-completion, and zerocopy operation. We demonstrate how a buffer-cache-free design is not only practical, but required in order to achieve efficient use of memory bandwidth on contemporary microarchitectures. Minimizing latency between DMA and CPU access by integrating storage and TCP control loops allows many operations to access only the last-level cache rather than bottle-necking on memory bandwidth. We illustrate the power of this design by building Atlas, a video streaming web server that outperforms state-of-the-art configurations, and achieves ~72Gbps of plaintext or encrypted network traffic using a fraction of the available CPU cores on commodity hardware.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,211–224,14,"Storage stacks, Network Performance, Network stacks","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
226,inproceedings,"Ghorbani, Soudeh and Yang, Zibin and Godfrey, P. Brighten and Ganjali, Yashar and Firoozshahian, Amin",DRILL: Micro Load Balancing for Low-Latency Data Center Networks,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098839,10.1145/3098822.3098839,"The trend towards simple datacenter network fabric strips most network functionality, including load balancing, out of the network core and pushes it to the edge. This slows reaction to microbursts, the main culprit of packet loss in datacenters. We investigate the opposite direction: could slightly smarter fabric significantly improve load balancing? This paper presents DRILL, a datacenter fabric for Clos networks which performs micro load balancing to distribute load as evenly as possible on microsecond timescales. DRILL employs per-packet decisions at each switch based on local queue occupancies and randomized algorithms to distribute load. Our design addresses the resulting key challenges of packet reordering and topological asymmetry. In simulations with a detailed switch hardware model and realistic workloads, DRILL outperforms recent edge-based load balancers, particularly under heavy load. Under 80% load, for example, it achieves 1.3-1.4x lower mean flow completion time than recent proposals, primarily due to shorter upstream queues. To test hardware feasibility, we implement DRILL in Verilog and estimate its area overhead to be less than 1%. Finally, we analyze DRILL's stability and throughput-efficiency.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,225–238,14,"Traffic engineering, Load balancing, Microbursts, Clos, Datacenters","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
227,inproceedings,"Cho, Inho and Jang, Keon and Han, Dongsu",Credit-Scheduled Delay-Bounded Congestion Control for Datacenters,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098840,10.1145/3098822.3098840,"Small RTTs (~tens of microseconds), bursty flow arrivals, and a large number of concurrent flows (thousands) in datacenters bring fundamental challenges to congestion control as they either force a flow to send at most one packet per RTT or induce a large queue build-up. The widespread use of shallow buffered switches also makes the problem more challenging with hosts generating many flows in bursts. In addition, as link speeds increase, algorithms that gradually probe for bandwidth take a long time to reach the fair-share. An ideal datacenter congestion control must provide 1) zero data loss, 2) fast convergence, 3) low buffer occupancy, and 4) high utilization. However, these requirements present conflicting goals.This paper presents a new radical approach, called ExpressPass, an end-to-end credit-scheduled, delay-bounded congestion control for datacenters. ExpressPass uses credit packets to control congestion even before sending data packets, which enables us to achieve bounded delay and fast convergence. It gracefully handles bursty flow arrivals. We implement ExpressPass using commodity switches and provide evaluations using testbed experiments and simulations. ExpressPass converges up to 80 times faster than DCTCP in 10 Gbps links, and the gap increases as link speeds become faster. It greatly improves performance under heavy incast workloads and significantly reduces the flow completion times, especially, for small and medium size flows compared to RCP, DCTCP, HULL, and DX under realistic workloads.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,239–252,14,"Datacenter Network, Credit-based, Congestion Control","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
228,inproceedings,"Zhang, Hong and Zhang, Junxue and Bai, Wei and Chen, Kai and Chowdhury, Mosharaf",Resilient Datacenter Load Balancing in the Wild,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098841,10.1145/3098822.3098841,"Production datacenters operate under various uncertainties such as traffic dynamics, topology asymmetry, and failures. Therefore, datacenter load balancing schemes must be resilient to these uncertainties; i.e., they should accurately sense path conditions and timely react to mitigate the fallouts. Despite significant efforts, prior solutions have important drawbacks. On the one hand, solutions such as Presto and DRB are oblivious to path conditions and blindly reroute at fixed granularity. On the other hand, solutions such as CONGA and CLOVE can sense congestion, but they can only reroute when flowlets emerge; thus, they cannot always react timely to uncertainties. To make things worse, these solutions fail to detect/handle failures such as blackholes and random packet drops, which greatly degrades their performance.In this paper, we introduce Hermes, a datacenter load balancer that is resilient to the aforementioned uncertainties. At its heart, Hermes leverages comprehensive sensing to detect path conditions including failures unattended before, and it reacts using timely yet cautious rerouting. Hermes is a practical edge-based solution with no switch modification. We have implemented Hermes with commodity switches and evaluated it through both testbed experiments and large-scale simulations. Our results show that Hermes achieves comparable performance to CONGA and Presto in normal cases, and well handles uncertainties: under asymmetries, Hermes achieves up to 10% and 20% better flow completion time (FCT) than CONGA and CLOVE; under switch failures, it outperforms all other schemes by over 32%.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,253–266,14,"Load balancing, Datacenter fabric, Distributed","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
229,inproceedings,"Mellette, William M. and McGuinness, Rob and Roy, Arjun and Forencich, Alex and Papen, George and Snoeren, Alex C. and Porter, George","RotorNet: A Scalable, Low-Complexity, Optical Datacenter Network",2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098838,10.1145/3098822.3098838,"The ever-increasing bandwidth requirements of modern datacenters have led researchers to propose networks based upon optical circuit switches, but these proposals face significant deployment challenges. In particular, previous proposals dynamically configure circuit switches in response to changes in workload, requiring network-wide demand estimation, centralized circuit assignment, and tight time synchronization between various network elements--- resulting in a complex and unwieldy control plane. Moreover, limitations in the technologies underlying the individual circuit switches restrict both the rate at which they can be reconfigured and the scale of the network that can be constructed.We propose RotorNet, a circuit-based network design that addresses these two challenges. While RotorNet dynamically reconfigures its constituent circuit switches, it decouples switch configuration from traffic patterns, obviating the need for demand collection and admitting a fully decentralized control plane. At the physical layer, RotorNet relaxes the requirements on the underlying circuit switches---in particular by not requiring individual switches to implement a full crossbar---enabling them to scale to 1000s of ports. We show that RotorNet outperforms comparably priced Fat Tree topologies under a variety of workload conditions, including traces taken from two commercial datacenters. We also demonstrate a small-scale RotorNet operating in practice on an eight-node testbed.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,267–280,14,"Datacenter, optical switching","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
230,inproceedings,"Kassing, Simon and Valadarsky, Asaf and Shahaf, Gal and Schapira, Michael and Singla, Ankit","Beyond Fat-Trees without Antennae, Mirrors, and Disco-Balls",2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098836,10.1145/3098822.3098836,"Recent studies have observed that large data center networks often have a few hotspots while most of the network is underutilized. Consequently, numerous data center network designs have explored the approach of identifying these communication hotspots in real-time and eliminating them by leveraging flexible optical or wireless connections to dynamically alter the network topology. These proposals are based on the premise that statically wired network topologies, which lack the opportunity for such online optimization, are fundamentally inefficient, and must be built at uniform full capacity to handle unpredictably skewed traffic.We show this assumption to be false. Our results establish that state-of-the-art static networks can also achieve the performance benefits claimed by dynamic, reconfigurable designs of the same cost: for the skewed traffic workloads used to make the case for dynamic networks, the evaluated static networks can achieve performance matching full-bandwidth fat-trees at two-thirds of the cost. Surprisingly, this can be accomplished even without relying on any form of online optimization, including the optimization of routing configuration in response to the traffic demands.Our results substantially lower the barriers for improving upon today's data centers by showing that a static, cabling-friendly topology built using commodity equipment yields superior performance when combined with well-understood routing methods.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,281–294,14,"Topology, Routing, Data center","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
231,inproceedings,"Xia, Yiting and Sun, Xiaoye Steven and Dzinamarira, Simbarashe and Wu, Dingming and Huang, Xin Sunny and Ng, T. S. Eugene",A Tale of Two Topologies: Exploring Convertible Data Center Network Architectures with Flat-Tree,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098837,10.1145/3098822.3098837,"This paper promotes convertible data center network architectures, which can dynamically change the network topology to combine the benefits of multiple architectures. We propose the flat-tree prototype architecture as the first step to realize this concept. Flat-tree can be implemented as a Clos network and later be converted to approximate random graphs of different sizes, thus achieving both Clos-like implementation simplicity and random-graph-like transmission performance. We present the detailed design for the network architecture and the control system. Simulations using real data center traffic traces show that flat-tree is able to optimize various workloads with different topology options. We implement an example flat-tree network on a 20-switch 24-server testbed. The traffic reaches the maximal throughput in 2.5s after a topology change, proving the feasibility of converting topology at run time. The network core bandwidth is increased by 27.6% just by converting the topology from Clos to approximate random graph. This improvement can be translated into acceleration of applications as we observe reduced communication time in Spark and Hadoop jobs.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,295–308,14,"Convertible data center networks, Clos networks, Random graph networks","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
232,inproceedings,"Eletreby, Rashad and Zhang, Diana and Kumar, Swarun and Ya\u{g",Empowering Low-Power Wide Area Networks in Urban Settings,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098845,10.1145/3098822.3098845,"Low-Power Wide Area Networks (LP-WANs) are an attractive emerging platform to connect the Internet-of-things. LP-WANs enable low-cost devices with a 10-year battery to communicate at few kbps to a base station, kilometers away. But deploying LP-WANs in large urban environments is challenging, given the sheer density of nodes that causes interference, coupled with attenuation from buildings that limits signal range. Yet, state-of-the-art techniques to address these limitations demand inordinate hardware complexity at the base stations or clients, increasing their size and cost.This paper presents Choir, a system that overcomes challenges pertaining to density and range of urban LP-WANs despite the limited capabilities of base station and client hardware. First, Choir proposes a novel technique that aims to disentangle and decode large numbers of interfering transmissions at a simple, single-antenna LP-WAN base station. It does so, perhaps counter-intuitively, by taking the hardware imperfections of low-cost LP-WAN clients to its advantage. Second, Choir exploits the correlation of sensed data collected by LP-WAN nodes to collaboratively reach a faraway base station, even if individual clients are beyond its range. We implement and evaluate Choir on USRP N210 base stations serving a 10 square kilometer area surrounding Carnegie Mellon University campus. Our results reveal that Choir improves network throughput of commodity LP-WAN clients by 6.84 x and expands communication range by 2.65 x.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,309–321,13,,"Los Angeles, CA, USA",SIGCOMM '17,,,,,,
233,inproceedings,"Song, Zhenyu and Shangguan, Longfei and Jamieson, Kyle",Wi-Fi Goes to Town: Rapid Picocell Switching for Wireless Transit Networks,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098846,10.1145/3098822.3098846,"This paper presents the design and implementation of Wi-Fi Goes to Town, the first Wi-Fi based roadside hotspot network designed to operate at vehicular speeds with meter-sized picocells. Wi-Fi Goes to Town APs make delivery decisions to the vehicular clients they serve at millisecond-level granularities, exploiting path diversity in roadside networks. In order to accomplish this, we introduce new buffer management algorithms that allow participating APs to manage each others' queues, rapidly quenching each others' transmissions and flushing each others' queues. We furthermore integrate our fine-grained AP selection and queue management into 802.11's frame aggregation and block acknowledgement functions, making the system effective at modern 802.11 bit rates that need frame aggregation to maintain high spectral efficiency. We have implemented our system in an eight-AP network alongside a nearby road, and evaluate its performance with mobile clients moving at up to 35 mph. Depending on the clients' speed, Wi-Fi Goes to Town achieves a 2.4-4.7x TCP throughput improvement over a baseline fast handover protocol that captures the state of the art in Wi-Fi roaming, including the recent IEEE 802.11k and 802.11r standards.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,322–334,13,"Transit Networks, Handover, Wi-Fi","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
234,inproceedings,"Ma, Yunfei and Selby, Nicholas and Adib, Fadel",Drone Relays for Battery-Free Networks,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098847,10.1145/3098822.3098847,"Battery-free sensors, such as RFIDs, are annually attached to billions of items including pharmaceutical drugs, clothes, and manufacturing parts. The fundamental challenge with battery-free sensors is that they are only reliable at short distances of tens of centimeters to few meters. As a result, today's systems for communicating with and localizing battery-free sensors are crippled by the limited range.To overcome this challenge, this paper presents RFly, a system that leverages drones as relays for battery-free networks. RFly delivers two key innovations. It introduces the first full-duplex relay for battery-free networks. The relay can seamlessly integrate with a deployed RFID infrastructure, and it preserves phase and timing characteristics of the forwarded packets. RFly also develops the first RF-localization algorithm that can operate through a mobile relay.We built a hardware prototype of RFly's relay into a custom PCB circuit and mounted it on a Parrot Bebop drone. Our experimental evaluation demonstrates that RFly enables communication with commercial RFIDs at over 50 m. Moreover, its through-relay localization algorithm has a median accuracy of 19 centimeters. These results demonstrate that RFly provides powerful primitives for communication and localization in battery-free networks.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,335–347,13,"Localization, Battery-free, SAR, Relay, Full-Duplex, Drones, RFID","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
235,inproceedings,"Qazi, Zafar Ayyub and Walls, Melvin and Panda, Aurojit and Sekar, Vyas and Ratnasamy, Sylvia and Shenker, Scott",A High Performance Packet Core for Next Generation Cellular Networks,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098848,10.1145/3098822.3098848,"Cellular traffic continues to grow rapidly making the scalability of the cellular infrastructure a critical issue. However, there is mounting evidence that the current Evolved Packet Core (EPC) is ill-suited to meet these scaling demands: EPC solutions based on specialized appliances are expensive to scale and recent software EPCs perform poorly, particularly with increasing numbers of devices or signaling traffic.In this paper, we design and evaluate a new system architecture for a software EPC that achieves high and scalable performance. We postulate that the poor scaling of existing EPC systems stems from the manner in which the system is decomposed which leads to device state being duplicated across multiple components which in turn results in frequent interactions between the different components. We propose an alternate approach in which state for a single device is consolidated in one location and EPC functions are (re)organized for efficient access to this consolidated state. In effect, our design ""slices"" the EPC by user.We prototype and evaluate PEPC, a software EPC that implements the key components of our design. We show that PEPC achieves 3-7x higher throughput than comparable software EPCs that have been implemented in industry and over 10x higher throughput than a popular open-source implementation (OpenAirInterface). Compared to the industrial EPC implementations, PEPC sustains high data throughput for 10-100x more users devices per core, and a 10x higher ratio of signaling-to-data traffic. In addition to high performance, PEPC's by-user organization enables efficient state migration and customization of processing pipelines. We implement user migration in PEPC and show that state can be migrated with little disruption, e.g., migration adds only up to 4μs of latency to median per packet latencies.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,348–361,14,"EPC, Network Function, Cellular Networks","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
236,inproceedings,"Zhuo, Danyang and Ghobadi, Monia and Mahajan, Ratul and F\""{o",Understanding and Mitigating Packet Corruption in Data Center Networks,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098849,10.1145/3098822.3098849,"We take a comprehensive look at packet corruption in data center networks, which leads to packet losses and application performance degradation. By studying 350K links across 15 production data centers, we find that the extent of corruption losses is significant and that its characteristics differ markedly from congestion losses. Corruption impacts fewer links than congestion, but imposes a heavier loss rate; and unlike congestion, corruption rate on a link is stable over time and is not correlated with its utilization.Based on these observations, we developed CorrOpt, a system to mitigate corruption. To minimize corruption losses, it intelligently selects which corrupting links can be safely disabled, while ensuring that each top-of-rack switch has a minimum number of paths to reach other switches. CorrOpt also recommends specific actions (e.g., replace cables, clean connectors) to repair disabled links, based on our analysis of common symptoms of different root causes of corruption. Our recommendation engine has been deployed in over seventy data centers of a large cloud provider. Our analysis shows that, compared to current state of the art, CorrOpt can reduce corruption losses by three to six orders of magnitude and improve repair accuracy by 60%.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,362–375,14,"CorrOpt, Packet Corruption, Optics, Fault Mitigation, Data Center Networks","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
237,inproceedings,"Iordanou, Costas and Soriente, Claudio and Sirivianos, Michael and Laoutaris, Nikolaos",Who is Fiddling with Prices? Building and Deploying a Watchdog Service for E-Commerce,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098850,10.1145/3098822.3098850,"We present the design, implementation, validation, and deployment of the Price Sheriff, a highly distributed system for detecting various types of online price discrimination in e-commerce. The Price Sheriff uses a peer-to-peer architecture, sandboxing, and secure multiparty computation to allow users to tunnel price check requests through the browsers of other peers without tainting their local or server-side browsing history and state. Having operated the Price Sheriff for several months with approximately one thousand real users, we identify several instances of cross-border price discrimination based on the country of origin. Even within national borders, we identify several retailers that return different prices for the same product to different users. We examine whether the observed differences are due to personal-data-induced discrimination or A/B testing, and conclude that it is the latter.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,376–389,14,Online Price Discrimination,"Los Angeles, CA, USA",SIGCOMM '17,,,,,,
238,inproceedings,"Ruamviboonsuk, Vaspol and Netravali, Ravi and Uluyol, Muhammed and Madhyastha, Harsha V.",Vroom: Accelerating the Mobile Web with Server-Aided Dependency Resolution,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098851,10.1145/3098822.3098851,"The existing slowness of the web on mobile devices frustrates users and hurts the revenue of website providers. Prior studies have attributed high page load times to dependencies within the page load process: network latency in fetching a resource delays its processing, which in turn delays when dependent resources can be discovered and fetched.To securely address the impact that these dependencies have on page load times, we present Vroom, a rethink of how clients and servers interact to facilitate web page loads. Unlike existing solutions, which require clients to either trust proxy servers or discover all the resources on any page themselves, Vroom's key characteristics are that clients fetch every resource directly from the domain that hosts it but web servers aid clients in discovering resources. Input from web servers decouples a client's processing of resources from its fetching of resources, thereby enabling independent use of both the CPU and the network. As a result, Vroom reduces the median page load time by more than 5 seconds across popular News and Sports sites. To enable these benefits, our contributions lie in making web servers capable of accurately aiding clients in resource discovery and judiciously scheduling a client's receipt of resources.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,390–403,14,"Page load times, Mobile web, Web performance","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
239,inproceedings,"Saeed, Ahmed and Dukkipati, Nandita and Valancius, Vytautas and The Lam, Vinh and Contavalli, Carlo and Vahdat, Amin",Carousel: Scalable Traffic Shaping at End Hosts,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098852,10.1145/3098822.3098852,"Traffic shaping, including pacing and rate limiting, is fundamental to the correct and efficient operation of both datacenter and wide area networks. Sample use cases include policy-based bandwidth allocation to flow aggregates, rate-based congestion control algorithms, and packet pacing to avoid bursty transmissions that can overwhelm router buffers. Driven by the need to scale to millions of flows and to apply complex policies, traffic shaping is moving from network switches into the end hosts, typically implemented in software in the kernel networking stack.In this paper, we show that the performance overhead of end-host traffic shaping is substantial limits overall system scalability as we move to thousands of individual traffic classes per server. Measurements from production servers show that shaping at hosts consumes considerable CPU and memory, unnecessarily drops packets, suffers from head of line blocking and inaccuracy, and does not provide backpressure up the stack. We present Carousel, a framework that scales to tens of thousands of policies and flows per server, built from the synthesis of three key ideas: i) a single queue shaper using time as the basis for releasing packets, ii) fine-grained, just-in-time freeing of resources in higher layers coupled to actual packet departures, and iii) one shaper per CPU core, with lock-free coordination. Our production experience in serving video traffic at a Cloud service provider shows that Carousel shapes traffic accurately while improving overall machine CPU utilization by 8% (an improvement of 20% in the CPU utilization attributed to networking) relative to state-of-art deployments. It also conforms 10 times more accurately to target rates, and consumes two orders of magnitude less memory than existing approaches.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,404–417,14,"Rate-limiters, Timing Wheel, Traffic shaping, Pacing, Backpressure","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
240,inproceedings,"Schlinker, Brandon and Kim, Hyojeong and Cui, Timothy and Katz-Bassett, Ethan and Madhyastha, Harsha V. and Cunha, Italo and Quinn, James and Hasan, Saif and Lapukhov, Petr and Zeng, Hongyi",Engineering Egress with Edge Fabric: Steering Oceans of Content to the World,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098853,10.1145/3098822.3098853,"Large content providers build points of presence around the world, each connected to tens or hundreds of networks. Ideally, this connectivity lets providers better serve users, but providers cannot obtain enough capacity on some preferred peering paths to handle peak traffic demands. These capacity constraints, coupled with volatile traffic and performance and the limitations of the 20 year old BGP protocol, make it difficult to best use this connectivity.We present Edge Fabric, an SDN-based system we built and deployed to tackle these challenges for Facebook, which serves over two billion users from dozens of points of presence on six continents. We provide the first public details on the connectivity of a provider of this scale, including opportunities and challenges. We describe how Edge Fabric operates in near real-time to avoid congesting links at the edge of Facebook's network. Our evaluation on production traffic worldwide demonstrates that Edge Fabric efficiently uses interconnections without congesting them and degrading performance. We also present real-time performance measurements of available routes and investigate incorporating them into routing decisions. We relate challenges, solutions, and lessons from four years of operating and evolving Edge Fabric.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,418–431,14,"Traffic Engineering, Internet Routing, Border Gateway Protocol, Software Defined Networking, Content Distribution Network","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
241,inproceedings,"Yap, Kok-Kiong and Motiwala, Murtaza and Rahe, Jeremy and Padgett, Steve and Holliman, Matthew and Baldus, Gary and Hines, Marcus and Kim, Taeeun and Narayanan, Ashok and Jain, Ankur and Lin, Victor and Rice, Colin and Rogan, Brian and Singh, Arjun and Tanaka, Bert and Verma, Manish and Sood, Puneet and Tariq, Mukarram and Tierney, Matt and Trumic, Dzevad and Valancius, Vytautas and Ying, Calvin and Kallahalla, Mahesh and Koley, Bikash and Vahdat, Amin","Taking the Edge off with Espresso: Scale, Reliability and Programmability for Global Internet Peering",2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098854,10.1145/3098822.3098854,"We present the design of Espresso, Google's SDN-based Internet peering edge routing infrastructure. This architecture grew out of a need to exponentially scale the Internet edge cost-effectively and to enable application-aware routing at Internet-peering scale. Espresso utilizes commodity switches and host-based routing/packet processing to implement a novel fine-grained traffic engineering capability. Overall, Espresso provides Google a scalable peering edge that is programmable, reliable, and integrated with global traffic systems. Espresso also greatly accelerated deployment of new networking features at our peering edge. Espresso has been in production for two years and serves over 22% of Google's total traffic to the Internet.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,432–445,14,"Networking, Peering Routers, Traffic Engineering","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
242,inproceedings,"Giotsas, Vasileios and Dietzel, Christoph and Smaragdakis, Georgios and Feldmann, Anja and Berger, Arthur and Aben, Emile",Detecting Peering Infrastructure Outages in the Wild,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098855,10.1145/3098822.3098855,"Peering infrastructures, namely, colocation facilities and Internet exchange points, are located in every major city, have hundreds of network members, and support hundreds of thousands of interconnections around the globe. These infrastructures are well provisioned and managed, but outages have to be expected, e.g., due to power failures, human errors, attacks, and natural disasters. However, little is known about the frequency and impact of outages at these critical infrastructures with high peering concentration.In this paper, we develop a novel and lightweight methodology for detecting peering infrastructure outages. Our methodology relies on the observation that BGP communities, announced with routing updates, are an excellent and yet unexplored source of information allowing us to pinpoint outage locations with high accuracy. We build and operate a system that can locate the epicenter of infrastructure outages at the level of a building and track the reaction of networks in near real-time. Our analysis unveils four times as many outages as compared to those publicly reported over the past five years. Moreover, we show that such outages have significant impact on remote networks and peering infrastructures. Our study provides a unique view of the Internet's behavior under stress that often goes unreported.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,446–459,14,"Peering, Interconnection Facility, Resilience, Outages, IXP, BGP Community, Colocation","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
243,inproceedings,"Holterbach, Thomas and Vissicchio, Stefano and Dainotti, Alberto and Vanbever, Laurent",SWIFT: Predictive Fast Reroute,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098856,10.1145/3098822.3098856,"Network operators often face the problem of remote outages in transit networks leading to significant (sometimes on the order of minutes) downtimes. The issue is that BGP, the Internet routing protocol, often converges slowly upon such outages, as large bursts of messages have to be processed and propagated router by router.In this paper, we present SWIFT, a fast-reroute framework which enables routers to restore connectivity in few seconds upon remote outages. SWIFT is based on two novel techniques. First, SWIFT deals with slow outage notification by predicting the overall extent of a remote failure out of few control-plane (BGP) messages. The key insight is that significant inference speed can be gained at the price of some accuracy. Second, SWIFT introduces a new data-plane encoding scheme, which enables quick and flexible update of the affected forwarding entries. SWIFT is deployable on existing devices, without modifying BGP.We present a complete implementation of SWIFT and demonstrate that it is both fast and accurate. In our experiments with real BGP traces, SWIFT predicts the extent of a remote outage in few seconds with an accuracy of ~90% and can restore connectivity for 99% of the affected destinations.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,460–473,14,"Convergence, Fast Reroute, Root Cause Analysis, BGP","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
244,inproceedings,"Sambasivan, Raja R. and Tran-Lam, David and Akella, Aditya and Steenkiste, Peter",Bootstrapping Evolvability for Inter-Domain Routing with D-BGP,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098857,10.1145/3098822.3098857,"The Internet's inter-domain routing infrastructure, provided today by BGP, is extremely rigid and does not facilitate the introduction of new inter-domain routing protocols. This rigidity has made it incredibly difficult to widely deploy critical fixes to BGP. It has also depressed ASes' ability to sell value-added services or replace BGP entirely with a more sophisticated protocol. Even if operators undertook the significant effort needed to fix or replace BGP, it is likely the next protocol will be just as difficult to change or evolve. To help, this paper identifies two features needed in the routing infrastructure (i.e., within any inter-domain routing protocol) to facilitate evolution to new protocols. To understand their utility, it presents D-BGP, a version of BGP that incorporates them.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,474–487,14,"Control plane, BGP, Routing, Evolvability, Extensibility","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
245,inproceedings,"Luckie, Matthew and Beverly, Robert",The Impact of Router Outages on the AS-Level Internet,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3098858,10.1145/3098822.3098858,"We propose and evaluate a new metric for understanding the dependence of the AS-level Internet on individual routers. Whereas prior work uses large volumes of reachability probes to infer outages, we design an efficient active probing technique that directly and unambiguously reveals router restarts. We use our technique to survey 149,560 routers across the Internet for 2.5 years. 59,175 of the surveyed routers (40%) experience at least one reboot, and we quantify the resulting impact of each router outage on global IPv4 and IPv6 BGP reachability.Our technique complements existing data and control plane outage analysis methods by providing a causal link from BGP reachability failures to the responsible router(s) and multi-homing configurations. While we found the Internet core to be largely robust, we identified specific routers that were single points of failure for the prefixes they advertised. In total, 2,385 routers -- 4.0% of the routers that restarted over the course of 2.5 years of probing -- were single points of failure for 3,396 IPv6 prefixes announced by 1,708 ASes. We inferred 59% of these routers were the customer-edge border router. 2,374 (70%) of the withdrawn prefixes were not covered by a less specific prefix, so 1,726 routers (2.9%) of those that restarted were single points of failure for at least one network. However, a covering route did not imply reachability during a router outage, as no previously-responsive address in a withdrawn more specific prefix responded during a one-week sample. We validate our reboot and single point of failure inference techniques with four networks, finding no false positive or false negative reboots, but find some false negatives in our single point of failure inferences.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,488–501,14,"BGP, single points of failure, routing, Internet reliability","Los Angeles, CA, USA",SIGCOMM '17,,,,,,
246,inproceedings,"Rexford, Jennifer",Hitting the Nail on the Head: Interdisciplinary Research in Computer Networking,2017,9781450346535,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3098822.3140284,10.1145/3098822.3140284,"This is an exciting time in computer networking. The Internet is one of the most influential inventions of all time--a research experiment that, within our own lifetimes, escaped from the lab to become a global communications infrastructure. We see seemingly non-stop innovation in compelling services delivered over the Internet, end-host devices connected to the Internet, and communication media underlying the Internet, constantly giving our networks new challenges to address. In turn, computer networks arise in increasingly diverse settings, including data-center networks, cellular networks, vehicular networks, ad hoc networks, overlay networks, and more. Designing and operating computer networks that offer good performance, reliability, security, and more lead to a wealth of fascinating and important research problems---""nails"" in search of a good hammer. Yet, to solve these big, hairy problems we often need to look beyond the field of computer networking to other established disciplines-sources of good ""hammers"". In this talk, I share my experiences conducting interdisciplinary research in computer networking, through example collaborative projects with great colleagues and a few hard-won lessons along the way.",Proceedings of the Conference of the ACM Special Interest Group on Data Communication,502,1,,"Los Angeles, CA, USA",SIGCOMM '17,,,,,,
247,inproceedings,"Li, Bojie and Tan, Kun and Luo, Layong (Larry) and Peng, Yanqing and Luo, Renqian and Xu, Ningyi and Xiong, Yongqiang and Cheng, Peng and Chen, Enhong",ClickNP: Highly Flexible and High Performance Network Processing with Reconfigurable Hardware,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934897,10.1145/2934872.2934897,"Highly flexible software network functions (NFs) are crucial components to enable multi-tenancy in the clouds. However, software packet processing on a commodity server has limited capacity and induces high latency. While software NFs could scale out using more servers, doing so adds significant cost. This paper focuses on accelerating NFs with programmable hardware, i.e., FPGA, which is now a mature technology and inexpensive for datacenters. However, FPGA is predominately programmed using low-level hardware description languages (HDLs), which are hard to code and difficult to debug. More importantly, HDLs are almost inaccessible for most software programmers. This paper presents ClickNP, a FPGA-accelerated platform for highly flexible and high-performance NFs with commodity servers. ClickNP is highly flexible as it is completely programmable using high-level C-like languages, and exposes a modular programming abstraction that resembles Click Modular Router. ClickNP is also high performance. Our prototype NFs show that they can process traffic at up to 200 million packets per second with ultra-low latency ($< 2mu$s). Compared to existing software counterparts, with FPGA, ClickNP improves throughput by 10x, while reducing latency by 10x. To the best of our knowledge, ClickNP is the first FPGA-accelerated platform for NFs, written completely in high-level language and achieving 40 Gbps line rate at any packet size.",Proceedings of the 2016 ACM SIGCOMM Conference,1–14,14,"Reconfigurable Hardware, Network Function Virtualization, FPGA, Compiler","Florianopolis, Brazil",SIGCOMM '16,,,,,,
248,inproceedings,"Sivaraman, Anirudh and Cheung, Alvin and Budiu, Mihai and Kim, Changhoon and Alizadeh, Mohammad and Balakrishnan, Hari and Varghese, George and McKeown, Nick and Licking, Steve",Packet Transactions: High-Level Programming for Line-Rate Switches,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934900,10.1145/2934872.2934900,"Many algorithms for congestion control, scheduling, network measurement, active queue management, and traffic engineering require custom processing of packets in the data plane of a network switch. To run at line rate, these data-plane algorithms must be implemented in hardware. With today's switch hardware, algorithms cannot be changed, nor new algorithms installed, after a switch has been built.This paper shows how to program data-plane algorithms in a high-level language and compile those programs into low-level microcode that can run on emerging programmable line-rate switching chips. The key challenge is that many data-plane algorithms create and modify algorithmic state. To achieve line-rate programmability for stateful algorithms, we introduce the notion of a packet transaction: a sequential packet-processing code block that is atomic and isolated from other such code blocks.We have developed this idea in Domino, a C-like imperative language to express data-plane algorithms. We show with many examples that Domino provides a convenient way to express sophisticated data-plane algorithms, and show that these algorithms can be run at line rate with modest estimated chip-area overhead.",Proceedings of the 2016 ACM SIGCOMM Conference,15–28,14,"Programmable switches, stateful data-plane algorithms","Florianopolis, Brazil",SIGCOMM '16,,,,,,
249,inproceedings,"Arashloo, Mina Tahmasbi and Koral, Yaron and Greenberg, Michael and Rexford, Jennifer and Walker, David",SNAP: Stateful Network-Wide Abstractions for Packet Processing,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934892,10.1145/2934872.2934892,"Early programming languages for software-defined networking (SDN) were built on top of the simple match-action paradigm offered by OpenFlow 1.0. However, emerging hardware and software switches offer much more sophisticated support for persistent state in the data plane, without involving a central controller. Nevertheless, managing stateful, distributed systems efficiently and correctly is known to be one of the most challenging programming problems. To simplify this new SDN problem, we introduce SNAP.SNAP offers a simpler ""centralized"" stateful programming model, by allowing programmers to develop programs on top of one big switch rather than many. These programs may contain reads and writes to global, persistent arrays, and as a result, programmers can implement a broad range of applications, from stateful firewalls to fine-grained traffic monitoring. The SNAP compiler relieves programmers of having to worry about how to distribute, place, and optimize access to these stateful arrays by doing it all for them. More specifically, the compiler discovers read/write dependencies between arrays and translates one-big-switch programs into an efficient internal representation based on a novel variant of binary decision diagrams. This internal representation is used to construct a mixed-integer linear program, which jointly optimizes the placement of state and the routing of traffic across the underlying physical topology. We have implemented a prototype compiler and applied it to about 20 SNAP programs over various topologies to demonstrate our techniques' scalability.",Proceedings of the 2016 ACM SIGCOMM Conference,29–43,15,"Software Defined Networks, Stateful Packet Processing, SNAP, Optimization, One Big Switch, Network Programming Language","Florianopolis, Brazil",SIGCOMM '16,,,,,,
250,inproceedings,"Sivaraman, Anirudh and Subramanian, Suvinay and Alizadeh, Mohammad and Chole, Sharad and Chuang, Shang-Tse and Agrawal, Anurag and Balakrishnan, Hari and Edsall, Tom and Katti, Sachin and McKeown, Nick",Programmable Packet Scheduling at Line Rate,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934899,10.1145/2934872.2934899,"Switches today provide a small menu of scheduling algorithms. While we can tweak scheduling parameters, we cannot modify algorithmic logic, or add a completely new algorithm, after the switch has been designed. This paper presents a design for a {em programmable",Proceedings of the 2016 ACM SIGCOMM Conference,44–57,14,"Programmable scheduling, switch hardware","Florianopolis, Brazil",SIGCOMM '16,,,,,,
251,inproceedings,"Govindan, Ramesh and Minei, Ina and Kallahalla, Mahesh and Koley, Bikash and Vahdat, Amin",Evolve or Die: High-Availability Design Principles Drawn from Googles Network Infrastructure,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934891,10.1145/2934872.2934891,"Maintaining the highest levels of availability for content providers is challenging in the face of scale, network evolution and complexity. Little, however, is known about failures large content providers are susceptible to, and what mechanisms they employ to ensure high availability. From a detailed analysis of over 100 high-impact failure events in a global-scale content provider encompassing several data centers and two WANs, we quantify several dimensions of availability failures. We find that failures are evenly distributed across different network types and planes, but that a large number of failures happen when a management operation is in progress within the network. We discuss some of these failures in detail, and also describe our design principles for high availability motivated by these failures, including using defense in depth, maintaining consistency across planes, failing open on large failures, carefully preventing and avoiding failures, and assessing root cause quickly. Our findings suggest that, as networks become more complicated, failures lurk everywhere, and, counter-intuitively, continuous incremental evolution of the network can, when applied together with our design principles, result in a more robust network.",Proceedings of the 2016 ACM SIGCOMM Conference,58–72,15,Availability; Control Plane; Management Plane,"Florianopolis, Brazil",SIGCOMM '16,,,,,,
252,inproceedings,"Jalaparti, Virajith and Bliznets, Ivan and Kandula, Srikanth and Lucier, Brendan and Menache, Ishai",Dynamic Pricing and Traffic Engineering for Timely Inter-Datacenter Transfers,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934893,10.1145/2934872.2934893,"Neither traffic engineering nor fixed prices (e.g., $/GB) alone fully address the challenges of highly utilized inter-datacenter WANs. The former offers more service to users who overstate their demands and poor service overall. The latter offers no service guarantees to customers, and providers have no lever to steer customer demand to lightly loaded paths/times. To address these issues, we design and evaluate Pretium -- a framework that combines dynamic pricing with traffic engineering for inter-datacenter bandwidth. In Pretium, users specify their required rates or transfer sizes with deadlines, and a price module generates a price quote for different guarantees (promises) on these requests. The price quote is generated using internal prices (which can vary over time and links) which are maintained and periodically updated by Pretium based on history. A supplementary schedule adjustment module gears the agreed-upon network transfers towards an efficient operating point by optimizing time-varying operation costs. Experiments using traces from a large production WAN show that Pretium improves total system efficiency (value of routed transfers minus operation costs) by more than 3.5X relative to current usage-based pricing schemes, while increasing the provider profits by 2X.",Proceedings of the 2016 ACM SIGCOMM Conference,73–86,14,"Inter-datacenter networks;, deadline scheduling, dynamic pricing;, percentile pricing;","Florianopolis, Brazil",SIGCOMM '16,,,,,,
253,inproceedings,"Jin, Xin and Li, Yiran and Wei, Da and Li, Siming and Gao, Jie and Xu, Lei and Li, Guangzhi and Xu, Wei and Rexford, Jennifer",Optimizing Bulk Transfers with Software-Defined Optical WAN,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934904,10.1145/2934872.2934904,"Bulk transfer on the wide-area network (WAN) is a fundamental service to many globally-distributed applications. It is challenging to efficiently utilize expensive WAN bandwidth to achieve short transfer completion time and meet mission-critical deadlines. Advancements in software-defined networking (SDN) and optical hardware make it feasible and beneficial to quickly reconfigure optical devices in the optical layer, which brings a new opportunity for traffic management on the WAN.We present Owan, a novel traffic management system that optimizes wide-area bulk transfers with centralized joint control of the optical and network layers. sysname can dynamically change the network-layer topology by reconfiguring the optical devices. We develop efficient algorithms to jointly optimize optical circuit setup, routing and rate allocation, and dynamically adapt them to traffic demand changes. We have built a prototype of Owan with commodity optical and electrical hardware. Testbed experiments and large-scale simulations on two ISP topologies and one inter-DC topology show that sysname completes transfers up to 4.45x faster on average, and up to 1.36x more transfers meet their deadlines, as compared to prior methods that only control the network layer.",Proceedings of the 2016 ACM SIGCOMM Conference,87–100,14,"optical networks, Software-defined networking, cross-layer network management, wide area networks, bulk transfers","Florianopolis, Brazil",SIGCOMM '16,,,,,,
254,inproceedings,"Liu, Zaoxing and Manousis, Antonis and Vorsanger, Gregory and Sekar, Vyas and Braverman, Vladimir",One Sketch to Rule Them All: Rethinking Network Flow Monitoring with UnivMon,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934906,10.1145/2934872.2934906,"Network management requires accurate estimates of metrics for traffic engineering (e.g., heavy hitters), anomaly detection (e.g., entropy of source addresses), and security (e.g., DDoS detection). Obtaining accurate estimates given router CPU and memory constraints is a challenging problem. Existing approaches fall in one of two undesirable extremes: (1) low fidelity general-purpose approaches such as sampling, or (2) high fidelity but complex algorithms customized to specific application-level metrics. Ideally, a solution should be both general (i.e., supports many applications) and provide accuracy comparable to custom algorithms. This paper presents UnivMon, a framework for flow monitoring which leverages recent theoretical advances and demonstrates that it is possible to achieve both generality and high accuracy. UnivMon uses an application-agnostic data plane monitoring primitive; different (and possibly unforeseen) estimation algorithms run in the control plane, and use the statistics from the data plane to compute application-level metrics. We present a proof-of-concept implementation of UnivMon using P4 and develop simple coordination techniques to provide a ``one-big-switch'' abstraction for network-wide monitoring. We evaluate the effectiveness of UnivMon using a range of trace-driven evaluations and show that it offers comparable (and sometimes better) accuracy relative to custom sketching solutions.",Proceedings of the 2016 ACM SIGCOMM Conference,101–114,14,"Flow Monitoring, Streaming Algorithm, Sketching","Florianopolis, Brazil",SIGCOMM '16,,,,,,
255,inproceedings,"Chen, Ang and Wu, Yang and Haeberlen, Andreas and Zhou, Wenchao and Loo, Boon Thau","The Good, the Bad, and the Differences: Better Network Diagnostics with Differential Provenance",2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934910,10.1145/2934872.2934910,"In this paper, we propose a new approach to diagnosing problems in complex distributed systems. Our approach is based on the insight that many of the trickiest problems are anomalies. For instance, in a network, problems often affect only a small fraction of the traffic (e.g., perhaps a certain subnet), or they only manifest infrequently. Thus, it is quite common for the operator to have “examples” of both working and non-working traffic readily available – perhaps a packet that was misrouted, and a similar packet that was routed correctly. In this case, the cause of the problem is likely to be wherever the two packets were treated differently by the network.We present the design of a debugger that can leverage this information using a novel concept that we call differential provenance. Differential provenance tracks the causal connections between network states and state changes, just like classical provenance, but it can additionally perform root-cause analysis by reasoning about the differences between two provenance trees. We have built a diagnostic tool that is based on differential provenance, and we have used our tool to debug a number of complex, realistic problems in two scenarios: software-defined networks and MapReduce jobs. Our results show that differential provenance can be maintained at relatively low cost, and that it can deliver very precise diagnostic information; in many cases, it can even identify the precise root cause of the problem.",Proceedings of the 2016 ACM SIGCOMM Conference,115–128,14,"Provenance, Debugging, Network diagnostics","Florianopolis, Brazil",SIGCOMM '16,,,,,,
256,inproceedings,"Moshref, Masoud and Yu, Minlan and Govindan, Ramesh and Vahdat, Amin",Trumpet: Timely and Precise Triggers in Data Centers,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934879,10.1145/2934872.2934879,"As data centers grow larger and strive to provide tight performance and availability SLAs, their monitoring infrastructure must move from passive systems that provide aggregated inputs to human operators, to active systems that enable programmed control. In this paper, we propose Trumpet, an event monitoring system that leverages CPU resources and end-host programmability, to monitor every packet and report events at millisecond timescales. Trumpet users can express many *network-wide events*, and the system efficiently detects these events using *triggers* at end-hosts. Using careful design, Trumpet can evaluate triggers by inspecting every packet at full line rate even on future generations of NICs, scale to thousands of triggers per end-host while bounding packet processing delay to a few microseconds, and report events to a controller within 10 milliseconds, even in the presence of attacks. We demonstrate these properties using an implementation of Trumpet, and also show that it allows operators to describe new network events such as detecting correlated bursts and loss, identifying the root cause of transient congestion, and detecting short-term anomalies at the scale of a data center tenant.",Proceedings of the 2016 ACM SIGCOMM Conference,129–143,15,"End-host Monitoring, Network Event Monitoring","Florianopolis, Brazil",SIGCOMM '16,,,,,,
257,inproceedings,"Mace, Jonathan and Bodik, Peter and Musuvathi, Madanlal and Fonseca, Rodrigo and Varadarajan, Krishnan",2DFQ: Two-Dimensional Fair Queuing for Multi-Tenant Cloud Services,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934878,10.1145/2934872.2934878,"In many important cloud services, different tenants execute their requests in the thread pool of the same process, requiring fair sharing of resources. However, using fair queue schedulers to provide fairness in this context is difficult because of high execution concurrency, and because request costs are unknown and have high variance. Using fair schedulers like WFQ and WF²Q in such settings leads to bursty schedules, where large requests block small ones for long periods of time. In this paper, we propose Two-Dimensional Fair Queueing (2DFQ), which spreads requests of different costs across di erent threads and minimizes the impact of tenants with unpredictable requests. In evaluation on production workloads from Azure Storage, a large-scale cloud system at Microsoft, we show that 2DFQ reduces the burstiness of service by 1-2 orders of magnitude. On workloads where many large requests compete with small ones, 2DFQ improves 99th percentile latencies by up to 2 orders of magnitude.",Proceedings of the 2016 ACM SIGCOMM Conference,144–159,16,"Fair Request Scheduling, Multi-Tenant Systems","Florianopolis, Brazil",SIGCOMM '16,,,,,,
258,inproceedings,"Zhang, Hong and Chen, Li and Yi, Bairen and Chen, Kai and Chowdhury, Mosharaf and Geng, Yanhui",CODA: Toward Automatically Identifying and Scheduling Coflows in the Dark,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934880,10.1145/2934872.2934880,"Leveraging application-level requirements using coflows has recently been shown to improve application-level communication performance in data-parallel clusters. However, existing coflow-based solutions rely on modifying applications to extract coflows, making them inapplicable to many practical scenarios.In this paper, we present CODA, a first attempt at automatically identifying and scheduling coflows without any application-level modifications. We employ an incremental clustering algorithm to perform fast, application-transparent coflow identification and complement it by proposing an error-tolerant coflow scheduler to mitigate occasional identification errors. Testbed experiments and large-scale simulations with production workloads show that CODA can identify coflows with over 90% accuracy, and its scheduler is robust to inaccuracies, enabling communication stages to complete 2.4x (5.1x) faster on average (95-th percentile) compared to per-flow mechanisms. Overall, CODA's performance is comparable to that of solutions requiring application modifications.",Proceedings of the 2016 ACM SIGCOMM Conference,160–173,14,"data-intensive applications;, datacenter networks, Coflow;","Florianopolis, Brazil",SIGCOMM '16,,,,,,
259,inproceedings,"Chen, Li and Chen, Kai and Bai, Wei and Alizadeh, Mohammad",Scheduling Mix-Flows in Commodity Datacenters with Karuna,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934888,10.1145/2934872.2934888,"Cloud applications generate a mix of flows with and without deadlines. Scheduling such mix-flows is a key challenge; our experiments show that trivially combining existing schemes for deadline/non-deadline flows is problematic. For example, prioritizing deadline flows hurts flow completion time (FCT) for non-deadline flows, with minor improvement for deadline miss rate.We present Karuna, a first systematic solution for scheduling mix-flows. Our key insight is that deadline flows should meet their deadlines while minimally impacting the FCT of non-deadline flows. To achieve this goal, we design a novel Minimal-impact Congestion control Protocol (MCP) that handles deadline flows with as little bandwidth as possible. For non-deadline flows, we extend an existing FCT minimization scheme to schedule flows with known and unknown sizes. Karuna requires no switch modifications and is back- ward compatible with legacy TCP/IP stacks. Our testbed experiments and simulations show that Karuna effectively schedules mix-flows, for example, reducing the 95th percentile FCT of non-deadline flows by up to 47.78% at high load compared to pFabric, while maintaining low (<5.8%) deadline miss rate.",Proceedings of the 2016 ACM SIGCOMM Conference,174–187,14,"Deadline, Datacenter networks, Flow scheduling","Florianopolis, Brazil",SIGCOMM '16,,,,,,
260,inproceedings,"Nagaraj, Kanthi and Bharadia, Dinesh and Mao, Hongzi and Chinchali, Sandeep and Alizadeh, Mohammad and Katti, Sachin",NUMFabric: Fast and Flexible Bandwidth Allocation in Datacenters,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934890,10.1145/2934872.2934890,"We present xFabric, a novel datacenter transport design that provides flexible and fast bandwidth allocation control. xFabric is flexible: it enables operators to specify how bandwidth is allocated amongst contending flows to optimize for different service-level objectives such as minimizing flow completion times, weighted allocations, different notions of fairness, etc. xFabric is also very fast, it converges to the specified allocation one-to-two order of magnitudes faster than prior schemes. Underlying xFabric, is a novel distributed algorithm that uses in-network packet scheduling to rapidly solve general network utility maximization problems for bandwidth allocation. We evaluate xFabric using realistic datacenter topologies and highly dynamic workloads and show that it is able to provide flexibility and fast convergence in such stressful environments.",Proceedings of the 2016 ACM SIGCOMM Conference,188–201,14,"Resource Allocation, Networking, NUM, Datacenters","Florianopolis, Brazil",SIGCOMM '16,,,,,,
261,inproceedings,"Guo, Chuanxiong and Wu, Haitao and Deng, Zhong and Soni, Gaurav and Ye, Jianxi and Padhye, Jitu and Lipshteyn, Marina",RDMA over Commodity Ethernet at Scale,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934908,10.1145/2934872.2934908,"Over the past one and half years, we have been using RDMA over commodity Ethernet (RoCEv2) to support some of Microsoft's highly-reliable, latency-sensitive services. This paper describes the challenges we encountered during the process and the solutions we devised to address them. In order to scale RoCEv2 beyond VLAN, we have designed a DSCP-based priority flow-control (PFC) mechanism to ensure large-scale deployment. We have addressed the safety challenges brought by PFC-induced deadlock (yes, it happened!), RDMA transport livelock, and the NIC PFC pause frame storm problem. We have also built the monitoring and management systems to make sure RDMA works as expected. Our experiences show that the safety and scalability issues of running RoCEv2 at scale can all be addressed, and RDMA can replace TCP for intra data center communications and achieve low latency, low CPU overhead, and high throughput.",Proceedings of the 2016 ACM SIGCOMM Conference,202–215,14,"PFC, PFC propagation, RDMA, RoCEv2, Deadlock","Florianopolis, Brazil",SIGCOMM '16,,,,,,
262,inproceedings,"Ghobadi, Monia and Mahajan, Ratul and Phanishayee, Amar and Devanur, Nikhil and Kulkarni, Janardhan and Ranade, Gireeja and Blanche, Pierre-Alexandre and Rastegarfar, Houman and Glick, Madeleine and Kilper, Daniel",ProjecToR: Agile Reconfigurable Data Center Interconnect,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934911,10.1145/2934872.2934911,"We explore a novel, free-space optics based approach for building data center interconnects. It uses a digital micromirror device (DMD) and mirror assembly combination as a transmitter and a photodetector on top of the rack as a receiver (Figure 1). Our approach enables all pairs of racks to establish direct links, and we can reconfigure such links (i.e., connect different rack pairs) within 12 us. To carry traffic from a source to a destination rack, transmitters and receivers in our interconnect can be dynamically linked in millions of ways. We develop topology construction and routing methods to exploit this flexibility, including a flow scheduling algorithm that is a constant factor approximation to the offline optimal solution. Experiments with a small prototype point to the feasibility of our approach. Simulations using realistic data center workloads show that, compared to the conventional folded-Clos interconnect, our approach can improve mean flow completion time by 30-95% and reduce cost by 25-40%.",Proceedings of the 2016 ACM SIGCOMM Conference,216–229,14,"Free-Space Optics, Data Centers, Reconfigurability","Florianopolis, Brazil",SIGCOMM '16,,,,,,
263,inproceedings,"Cronkite-Ratcliff, Bryce and Bergman, Aran and Vargaftik, Shay and Ravi, Madhusudhan and McKeown, Nick and Abraham, Ittai and Keslassy, Isaac",Virtualized Congestion Control,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934889,10.1145/2934872.2934889,"New congestion control algorithms are rapidly improving datacenters by reducing latency, overcoming incast, increasing throughput and improving fairness. Ideally, the operating system in every server and virtual machine is updated to support new congestion control algorithms. However, legacy applications often cannot be upgraded to a new operating system version, which means the advances are off-limits to them. Worse, as we show, legacy applications can be squeezed out, which in the worst case prevents the entire network from adopting new algorithms.Our goal is to make it easy to deploy new and improved congestion control algorithms into multitenant datacenters, without having to worry about TCP-friendliness with non-participating virtual machines. This paper presents a solution we call virtualized congestion control. The datacenter owner may introduce a new congestion control algorithm in the hypervisors. Internally, the hypervisors translate between the new congestion control algorithm and the old legacy congestion control, allowing legacy applications to enjoy the benefits of the new algorithm. We have implemented proof-of-concept systems for virtualized congestion control in the Linux kernel and in VMware’s ESXi hypervisor, achieving improved fairness, performance, and control over guest bandwidth allocations.",Proceedings of the 2016 ACM SIGCOMM Conference,230–243,14,"TCP., DCTCP;, datacenters;, Virtualized congestion control;, hypervisors;, ECN;, algorithmic virtualization;","Florianopolis, Brazil",SIGCOMM '16,,,,,,
264,inproceedings,"He, Keqiang and Rozner, Eric and Agarwal, Kanak and Gu, Yu (Jason) and Felter, Wes and Carter, John and Akella, Aditya",AC/DC TCP: Virtual Congestion Control Enforcement for Datacenter Networks,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934903,10.1145/2934872.2934903,"Multi-tenant datacenters are successful because tenants can seamlessly port their applications and services to the cloud. Virtual Machine (VM) technology plays an integral role in this success by enabling a diverse set of software to be run on a unified underlying framework. This flexibility, however, comes at the cost of dealing with out-dated, inefficient, or misconfigured TCP stacks implemented in the VMs. This paper investigates if administrators can take control of a VM's TCP congestion control algorithm without making changes to the VM or network hardware. We propose AC/DC TCP, a scheme that exerts fine-grained control over arbitrary tenant TCP stacks by enforcing per-flow congestion control in the virtual switch (vSwitch). Our scheme is light-weight, flexible, scalable and can police non-conforming flows. In our evaluation the computational overhead of AC/DC TCP is less than one percentage point and we show implementing an administrator-defined congestion control algorithm in the vSwitch (i.e., DCTCP) closely tracks its native performance, regardless of the VM's TCP stack.",Proceedings of the 2016 ACM SIGCOMM Conference,244–257,14,"Virtualization, Congestion Control, Datacenter Networks","Florianopolis, Brazil",SIGCOMM '16,,,,,,
265,inproceedings,"Jiang, Yurong and Sivalingam, Lenin Ravindranath and Nath, Suman and Govindan, Ramesh",WebPerf: Evaluating What-If Scenarios for Cloud-Hosted Web Applications,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934882,10.1145/2934872.2934882,"Developers deploying web applications in the cloud often need to determine how changes such as service tiers or runtime loads may affect user-perceived page load time. We devise and evaluate a systematic methodology for exploring such ""what-if"" questions at the time a web application is deployed. Given a website, a web request, and “whatif” scenario, with a hypothetical configuration and runtime condition, our methodology, embedded in a system called WebPerf, can estimate a distribution of end-to-end response times for the request under the “what-if” scenario. WebPerf makes three contributions: (1) automated instrumentation of web sites written with increasingly popular task parallel libraries, to capture causal call dependencies of various computation and asynchronous I/O calls; (2) an algorithm to use the call dependencies, together with online- and offlineprofiled models of various I/O calls to estimate a distribution of end-to-end latency of the request; and (3) an algorithm to optimize modeling errors by deciding how many measurements to take within a limited time. We have implemented WebPerf for Microsoft Azure. Our experiments with five real websites and seven scenarios show that the median error of WebPerf’s estimation is within 7% for all applications and scenarios.",Proceedings of the 2016 ACM SIGCOMM Conference,258–271,14,"Async-Await, What-if, Dependency, Instrumentation","Florianopolis, Brazil",SIGCOMM '16,,,,,,
266,inproceedings,"Sun, Yi and Yin, Xiaoqi and Jiang, Junchen and Sekar, Vyas and Lin, Fuyuan and Wang, Nanshu and Liu, Tao and Sinopoli, Bruno",CS2P: Improving Video Bitrate Selection and Adaptation with Data-Driven Throughput Prediction,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934898,10.1145/2934872.2934898,"Bitrate adaptation is critical in ensuring good users’ quality-of-experience (QoE) in Internet video delivery system. Several efforts have argued that accurate throughput prediction can dramatically improve (1) initial bitrate selection for low startup delay and high initial resolution; (2) midstream bitrate adaptation for high QoE. However, prior ef- forts did not systematically quantify real-world throughput predictability or develop good prediction algorithms. To bridge this gap, this paper makes three key technical contributions: First, we analyze the throughput characteristics in a dataset with 20M+ sessions. We find: (a) Sessions sharing similar key features (e.g., ISP, region) present similar initial values and dynamical patterns; (b) There is a natural “stateful” dynamical behavior within a given session. Second, building on these insights, we develop CS2P, a better throughput prediction system. CS2P leverages data-driven approach to learn (a) clusters of similar sessions, (b) an initial throughput predictor, and (c) a Hidden-Markov-Model based midstream predictor modeling the stateful evolution of throughput. Third, we develop a prototype system and show by trace-driven simulation and real-world experiments that CS2P outperforms state-of-art by 40% and 50% median pre- diction error respectively for initial and midstream through- put and improves QoE by 14% over buffer-based adaptation algorithm.",Proceedings of the 2016 ACM SIGCOMM Conference,272–285,14,"Bitrate Adaptation, Throughput Prediction, TCP, Internet Video, Dynamic Adaptive Streaming over HTTP (DASH)","Florianopolis, Brazil",SIGCOMM '16,,,,,,
267,inproceedings,"Jiang, Junchen and Das, Rajdeep and Ananthanarayanan, Ganesh and Chou, Philip A. and Padmanabhan, Venkata and Sekar, Vyas and Dominique, Esbjorn and Goliszewski, Marcin and Kukoleca, Dalibor and Vafin, Renat and Zhang, Hui",Via: Improving Internet Telephony Call Quality Using Predictive Relay Selection,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934907,10.1145/2934872.2934907,"Interactive real-time streaming applications such as audio-video conferencing, online gaming and app streaming, place stringent requirements on the network in terms of delay, jitter, and packet loss. Many of these applications inherently involve client-to-client communication, which is particularly challenging since the performance requirements need to be met while traversing the public wide-area network (WAN). This is different from the typical situation of cloud-to-client communication, where the WAN can often be bypassed by moving a communication end-point to a cloud “edge”, close to the client. Can we nevertheless take advantage of cloud resources to improve the performance of real-time client-to-client streaming over the WAN?In this paper, we start by analyzing data from a large VoIP provider whose clients are spread across over 21,000 AS’es and nearly all the countries, to understand the challenges faced by interactive audio streaming in the wild. We find that while inter-AS and international paths exhibit significantly worse performance than intra-AS and domestic paths, the pattern of poor performance is nevertheless quite scattered, both temporally and spatially. So any effort to improve performance would have to be fine-grained and dynamic.Then, we turn to the idea of overlay routing, but in the context of the well-provisioned, managed network of a cloud provider rather than peer-to-peer as has been considered in past work. Such a network typically has a global footprint and peers with a large number of network providers. When the performance of a call via the direct path is predicted to be poor, the call traffic could be directed to enter the managed network close to one end point and exit it close to the other end point, thereby avoiding wide-area communication over the public Internet. We present and evaluate data-driven techniques to deciding whether to relay a call through the managed network and if so how to pick the ingress and egress relays to maximize performance, all while operating within a budget for relaying calls via the managed overlay network. We show that call performance can potentially improve by 40%-80% on average, with our techniques closely matching it.",Proceedings of the 2016 ACM SIGCOMM Conference,286–299,14,"Predictive Relay Selection, Quality of Experience, Internet Telephony, Managed Overlay Networks","Florianopolis, Brazil",SIGCOMM '16,,,,,,
268,inproceedings,"Gember-Jacobson, Aaron and Viswanathan, Raajay and Akella, Aditya and Mahajan, Ratul",Fast Control Plane Analysis Using an Abstract Representation,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934876,10.1145/2934872.2934876,"Networks employ complex, and hence error-prone, routing control plane configurations. In many cases, the impact of errors manifests only under failures and leads to devastating effects. Thus, it is important to proactively verify control plane behavior under arbitrary link failures. State-of-the-art verifiers are either too slow or impractical to use for such verification tasks. In this paper we propose a new high level abstraction for control planes, ARC, that supports fast control plane analyses under arbitrary failures. ARC can check key invariants without generating the data plane--which is the main reason for current tools' ineffectiveness. This is possible because of the nature of verification tasks and the constrained nature of control plane designs in networks today. We develop algorithms to derive a network's ARC from its configuration files. Our evaluation over 314 networks shows that ARC computation is quick, and that ARC can verify key invariants in under 1s in most cases, which is orders-of-magnitude faster than the state-of-the-art.",Proceedings of the 2016 ACM SIGCOMM Conference,300–313,14,"control plane, Network verification, abstract representation","Florianopolis, Brazil",SIGCOMM '16,,,,,,
269,inproceedings,"Stoenescu, Radu and Popovici, Matei and Negreanu, Lorina and Raiciu, Costin",SymNet: Scalable Symbolic Execution for Modern Networks,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934881,10.1145/2934872.2934881,"We present SymNet, a network static analysis tool based on symbolic execution. SymNet injects symbolic packets and tracks their evolution through the network. Our key novelty is SEFL, a language we designed for expressing data plane processing in a symbolic-execution friendly manner. SymNet statically analyzes an abstract data plane model that consists of the SEFL code for every node and the links between nodes. SymNet can check networks containing routers with hundreds of thousands of prefixes and NATs in seconds, while verifying packet header memory-safety and covering network functionality such as dynamic tunneling, stateful processing and encryption. We used SymNet to debug mid- dlebox interactions from the literature, to check properties of our department’s network and the Stanford backbone. Modeling network functionality is not easy. To aid users we have developed parsers that automatically generate SEFL models from router and switch tables, firewall configura- tions and arbitrary Click modular router configurations. The parsers rely on prebuilt models that are exact and fast to an- alyze. Finally, we have built an automated testing tool that combines symbolic execution and testing to check whether the model is an accurate representation of the real code.",Proceedings of the 2016 ACM SIGCOMM Conference,314–327,14,"Symbolic execution friendly language, SymNet, Data plane verification","Florianopolis, Brazil",SIGCOMM '16,,,,,,
270,inproceedings,"Beckett, Ryan and Mahajan, Ratul and Millstein, Todd and Padhye, Jitendra and Walker, David",Don't Mind the Gap: Bridging Network-Wide Objectives and Device-Level Configurations,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934909,10.1145/2934872.2934909,"We develop Propane, a language and compiler to help network operators with a challenging, error-prone task—bridging the gap between network-wide routing objectives and low-level configurations of devices that run complex, distributed protocols. The language allows operators to specify their objectives naturally, using high-level constraints on both the shape and relative preference of traffic paths. The compiler automatically translates these specifications to router-level BGP configurations, using an effective intermediate representation that compactly encodes the flow of routing information along policy-compliant paths. It guarantees that the compiled configurations correctly implement the specified policy under all possible combinations of failures. We show that Propane can effectively express the policies of datacenter and backbone networks of a large cloud provider; and despite its strong guarantees, our compiler scales to networks with hundreds or thousands of routers.",Proceedings of the 2016 ACM SIGCOMM Conference,328–341,14,"Fault Tolerance, Distributed Systems, Compilation, BGP, Domain-specific Language, Synthesis, Propane","Florianopolis, Brazil",SIGCOMM '16,,,,,,
271,inproceedings,"Cohen, Avichai and Gilad, Yossi and Herzberg, Amir and Schapira, Michael",Jumpstarting BGP Security with Path-End Validation,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934883,10.1145/2934872.2934883,"Extensive standardization and R&D efforts are dedicated to establishing secure interdomain routing. These efforts focus on two mechanisms: origin authentication with RPKI, and path validation with BGPsec. However, while RPKI is finally gaining traction, the adoption of BGPsec seems not even on the horizon due to inherent, possibly insurmountable, obstacles, including the need to replace today's routing infrastructure, the overhead of online cryptography, and meagre benefits in partial deployment. Consequently, secure interdomain routing remains a distant dream. We propose an easily deployable, modest extension to RPKI, called ``path-end validation'', which does not entail replacing/upgrading today's BGP routers nor online cryptographic operations. We show, through rigorous security analyses and extensive simulations on empirically-derived datasets, that path-end validation yields significant security benefits even in very limited partial adoption. We present an open-source, readily deployable prototype implementation of path-end validation.",Proceedings of the 2016 ACM SIGCOMM Conference,342–355,14,"RPKI, BGP security, Routing security","Florianopolis, Brazil",SIGCOMM '16,,,,,,
272,inproceedings,"Iyer, Vikram and Talla, Vamsi and Kellogg, Bryce and Gollakota, Shyamnath and Smith, Joshua",Inter-Technology Backscatter: Towards Internet Connectivity for Implanted Devices,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934894,10.1145/2934872.2934894,"We introduce inter-technology backscatter, a novel approach that transforms wireless transmissions from one technology to another, on the air. Specifically, we show for the first time that Bluetooth transmissions can be used to create Wi-Fi and ZigBee-compatible signals using backscatter communication. Since Bluetooth, Wi-Fi and ZigBee radios are widely available, this approach enables a backscatter design that works using only commodity devices.We build prototype backscatter hardware using an FPGA and experiment with various Wi-Fi, Bluetooth and ZigBee devices. Our experiments show we can create 2--11~Mbps Wi-Fi standards-compliant signals by backscattering Bluetooth transmissions. To show the generality of our approach, we also demonstrate generation of standards-complaint ZigBee signals by backscattering Bluetooth transmissions. Finally, we build proof-of-concepts for previously infeasible applications including the first contact lens form-factor antenna prototype and an implantable neural recording interface that communicate directly with commodity devices such as smartphones and watches, thus enabling the vision of Internet connected implanted devices.",Proceedings of the 2016 ACM SIGCOMM Conference,356–369,14,"Implantable Devices, Backscatter, Internet of things","Florianopolis, Brazil",SIGCOMM '16,,,,,,
273,inproceedings,"ZHANG, PENGYU and Rostami, Mohammad and Hu, Pan and Ganesan, Deepak",Enabling Practical Backscatter Communication for On-Body Sensors,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934901,10.1145/2934872.2934901,"In this paper, we look at making backscatter practical for ultra-low power on-body sensors by leveraging radios on existing smartphones and wearables (e.g. WiFi and Bluetooth). The difficulty lies in the fact that in order to extract the weak backscattered signal, the system needs to deal with self-interference from the wireless carrier (WiFi or Bluetooth) without relying on built-in capability to cancel or reject the carrier interference.Frequency-shifted backscatter (or FS-Backscatter) is based on a novel idea --- the backscatter tag shifts the carrier signal to an adjacent non-overlapping frequency band (i.e. adjacent WiFi or Bluetooth band) and isolates the spectrum of the backscattered signal from the spectrum of the primary signal to enable more robust decoding. We show that this enables communication of up to 4.8 meters using commercial WiFi and Bluetooth radios as the carrier generator and receiver. We also show that we can support a range of bitrates using packet-level and bit-level decoding methods. We build on this idea and show that we can also leverage multiple radios typically present on mobile and wearable devices to construct multi-carrier or multi-receiver scenarios to improve robustness. Finally, we also address the problem of designing an ultra-low power tag that can frequency shift by 20MHz while consuming tens of micro-watts. Our results show that FS-Backscatter is practical in typical mobile and static on-body sensing scenarios while only using commodity radios and antennas.",Proceedings of the 2016 ACM SIGCOMM Conference,370–383,14,Backscatter; Sensor; Wireless,"Florianopolis, Brazil",SIGCOMM '16,,,,,,
274,inproceedings,"Hu, Pan and Zhang, Pengyu and Rostami, Mohammad and Ganesan, Deepak",Braidio: An Integrated Active-Passive Radio for Mobile Devices with Asymmetric Energy Budgets,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934902,10.1145/2934872.2934902,"While many radio technologies are available for mobile devices, none of them are designed to deal with asymmetric available energy. Battery capacities of mobile devices vary by up to three orders of magnitude between laptops and wearables, and our inability to deal with such asymmetry has limited the lifetime of constrained portable devices.This paper presents a radically new design for low-power radios --- one that is capable of dynamically splitting the power burden of communication between the transmitter and receiver in proportion to the available energy on the two devices. We achieve this with a novel carrier offload method that dynamically moves carrier generation across end points. While such a design might raise the specter of a high-power, large form-factor radio, we show that this integration can be achieved with no more than a BLE-style active radio augmented with a few additional components. Our design, Braidio is a low-power, tightly integrated, low-cost radio capable of operating as an active and passive transceiver. When these modes operate in an interleaved (braided) manner, the end result is a power-proportional low-power radio that is able to achieve 1:2546 to 3546:1 power consumption ratios between a transmitter and a receiver, all while operating at low power.",Proceedings of the 2016 ACM SIGCOMM Conference,384–397,14,Backscatter; Wireless; Architecture; Asymmetric; Energy,"Florianopolis, Brazil",SIGCOMM '16,,,,,,
275,inproceedings,"Vasisht, Deepak and Kumar, Swarun and Rahul, Hariharan and Katabi, Dina",Eliminating Channel Feedback in Next-Generation Cellular Networks,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934895,10.1145/2934872.2934895,"This paper focuses on a simple, yet fundamental question: ``Can a node infer the wireless channels on one frequency band by observing the channels on a different frequency band?'' This question arises in cellular networks, where the uplink and the downlink operate on different frequencies. Addressing this question is critical for the deployment of key 5G solutions such as massive MIMO, multi-user MIMO, and distributed MIMO, which require channel state information.We introduce R2-F2, a system that enables LTE base stations to infer the downlink channels to a client by observing the uplink channels from that client. By doing so, R2-F2 extends the concept of reciprocity to LTE cellular networks, where downlink and uplink transmissions occur on different frequency bands. It also removes a major hurdle for the deployment of 5G MIMO solutions. We have implemented R2-F2 in software radios and integrated it within the LTE OFDM physical layer. Our results show that the channels computed by R2-F2 deliver accurate MIMO beamforming (to within 0.7~dB of beamforming gains with ground truth channels) while eliminating channel feedback overhead.",Proceedings of the 2016 ACM SIGCOMM Conference,398–411,14,"LTE, FDD Systems","Florianopolis, Brazil",SIGCOMM '16,,,,,,
276,inproceedings,"Hamed, Ezzeldin and Rahul, Hariharan and Abdelghany, Mohammed A. and Katabi, Dina",Real-Time Distributed MIMO Systems,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934905,10.1145/2934872.2934905,"Recent years have seen a lot of work in moving distributed MIMO from theory to practice. While this prior work demonstrates the feasibility of synchronizing multiple transmitters in time, frequency, and phase, none of them deliver a full-fledged PHY capable of supporting distributed MIMO in real-time. Further, none of them can address dynamic environments or mobile clients. Addressing these challenges, requires new solutions for low-overhead and fast tracking of wireless channels, which are the key parameters of any distributed MIMO system. It also requires a software-hardware architecture that can deliver a distributed MIMO within a full-fledged 802.11 PHY, while still meeting the tight timing constraints of the 802.11 protocol. This architecture also needs to perform coordinated power control across distributed MIMO nodes, as opposed to simply letting each node perform power control as if it were operating alone. This paper describes the design and implementation of MegaMIMO 2.0, a system that achieves these goals and delivers the first real-time fully distributed 802.11 MIMO system.",Proceedings of the 2016 ACM SIGCOMM Conference,412–425,14,"Multi-user MIMO, Wireless Networks, Distributed MIMO","Florianopolis, Brazil",SIGCOMM '16,,,,,,
277,inproceedings,"Sung, Yu-Wei Eric and Tie, Xiaozheng and Wong, Starsky H.Y. and Zeng, Hongyi",Robotron: Top-down Network Management at Facebook Scale,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934874,10.1145/2934872.2934874,"Network management facilitates a healthy and sustainable network. However, its practice is not well understood outside the network engineering community. In this paper, we present Robotron, a system for managing a massive production network in a top-down fashion. The system's goal is to reduce effort and errors on management tasks by minimizing direct human interaction with network devices. Engineers use Robotron to express high-level design intent, which is translated into low-level device configurations and deployed safely. Robotron also monitors devices' operational state to ensure it does not deviate from the desired state. Since 2008, Robotron has been used to manage tens of thousands of network devices connecting hundreds of thousands of servers globally at Facebook.",Proceedings of the 2016 ACM SIGCOMM Conference,426–439,14,"Robotron, Network Management, Facebook","Florianopolis, Brazil",SIGCOMM '16,,,,,,
278,inproceedings,"Arzani, Behnaz and Ciraci, Selim and Loo, Boon Thau and Schuster, Assaf and Outhred, Geoff",Taking the Blame Game out of Data Centers Operations with NetPoirot,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934884,10.1145/2934872.2934884,"Today, root cause analysis of failures in data centers is mostly done through manual inspection. More often than not, cus- tomers blame the network as the culprit. However, other components of the system might have caused these failures. To troubleshoot, huge volumes of data are collected over the entire data center. Correlating such large volumes of diverse data collected from different vantage points is a daunting task even for the most skilled technicians. In this paper, we revisit the question: how much can you infer about a failure in the data center using TCP statistics collected at one of the endpoints? Using an agent that cap- tures TCP statistics we devised a classification algorithm that identifies the root cause of failure using this information at a single endpoint. Using insights derived from this classi- fication algorithm we identify dominant TCP metrics that indicate where/why problems occur in the network. We val- idate and test these methods using data that we collect over a period of six months in a production data center.",Proceedings of the 2016 ACM SIGCOMM Conference,440–453,14,"Network manageability, Network transport protocols, Network monitoring, Network performance analysis, Network reliability","Florianopolis, Brazil",SIGCOMM '16,,,,,,
279,inproceedings,"Lee, Ki Suh and Wang, Han and Shrivastav, Vishal and Weatherspoon, Hakim",Globally Synchronized Time via Datacenter Networks,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934885,10.1145/2934872.2934885,"In this paper, we present Datacenter Time Protocol (DTP), a clock synchronization protocol that does not use packets at all, but is able to achieve nanosecond precision. In essence, DTP uses the physical layer of network devices to implement a decentralized clock synchronization protocol. By doing so, DTP eliminates most non-deterministic elements in clock synchronization protocols. Further, DTP uses control messages in the physical layer for communicating hundreds of thousands of protocol messages without interfering with higher layer packets. Thus, DTP has virtually zero overhead since it does not add load at layers 2 or higher layers. It does require replacing network devices, which can be done incrementally. We demonstrate that the precision provided by DTP is bounded by 25.6 nanoseconds for directly connected nodes, and in general, is bounded by 4TD where D is the longest distance between any two servers in a network in terms of number of hops and T is the period of the fastest clock (≈ 6.4ns). Moreover, in software, a DTP daemon can access the DTP clock with usually better than 4T (≈ 25.6ns) precision. As a result, the end-to-end precision can be better than 4T D + 8T nanoseconds. By contrast, the precision of the state of the art protocol is not bounded: The precision is hundreds of nanoseconds when a network is idle and can decrease to hundreds of microseconds when a network is heavily congested.",Proceedings of the 2016 ACM SIGCOMM Conference,454–467,14,Datacenter Networks,"Florianopolis, Brazil",SIGCOMM '16,,,,,,
280,inproceedings,"Flach, Tobias and Papageorge, Pavlos and Terzis, Andreas and Pedrosa, Luis and Cheng, Yuchung and Karim, Tayeb and Katz-Bassett, Ethan and Govindan, Ramesh",An Internet-Wide Analysis of Traffic Policing,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934873,10.1145/2934872.2934873,"Large flows like videos consume significant bandwidth. Some ISPs actively manage these high volume flows with techniques like policing, which enforces a flow rate by dropping excess traffic. While the existence of policing is well known, our contribution is an Internet-wide study quantifying its prevalence and impact on video quality metrics. We developed a heuristic to identify policing from server-side traces and built a pipeline to deploy it at scale on traces from a large online content provider, collected from hundreds of servers worldwide. Using a dataset of 270 billion packets served to 28,400 client ASes, we find that, depending on region, up to 7% of lossy transfers are policed. Loss rates are on average six times higher when a trace is policed, and it impacts video playback quality. We show that alternatives to policing, like pacing and shaping, can achieve traffic management goals while avoiding the deleterious effects of policing.",Proceedings of the 2016 ACM SIGCOMM Conference,468–482,15,"Traffic shaping, Network measurement, TCP, Traffic policing","Florianopolis, Brazil",SIGCOMM '16,,,,,,
281,inproceedings,"Yiakoumis, Yiannis and Katti, Sachin and McKeown, Nick",Neutral Net Neutrality,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934896,10.1145/2934872.2934896,"Should applications receive special treatment from the network? And if so, who decides which applications are preferred? This discussion, known as net neutrality, goes beyond technology and is a hot political topic. In this paper we approach net neutrality from a user's perspective. Through user studies, we demonstrate that users do indeed want some services to receive preferential treatment; and their preferences have a heavy-tail: a one-size-fits-all approach is unlikely to work. This suggests that users should be able to decide how their traffic is treated. A crucial part to enable user preferences, is the mechanism to express them. To this end, we present network cookies, a general mechanism to express user preferences to the network. Using cookies, we prototype Boost, a user-defined fast-lane and deploy it in 161 homes.",Proceedings of the 2016 ACM SIGCOMM Conference,483–496,14,"Network Cookies, Zero-Rating, Fast-Lanes, Differentiated Services, Net Neutrality","Florianopolis, Brazil",SIGCOMM '16,,,,,,
282,inproceedings,"McCauley, James and Zhao, Mingjie and Jackson, Ethan J. and Raghavan, Barath and Ratnasamy, Sylvia and Shenker, Scott",The Deforestation of L2,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934877,10.1145/2934872.2934877,"A major staple of layer 2 has long been the combination of flood-and-learn Ethernet switches with some variant of the Spanning Tree Protocol. However, STP has significant shortcomings -- chiefly, that it throws away network capacity by removing links, and that it can be relatively slow to reconverge after topology changes. In recent years, attempts to rectify these shortcomings have been made by either making L2 look more like L3 (notably TRILL and SPB, which both incorporate L3-like routing) or by replacing L2 switches with ""L3 switching"" hardware and extending IP all the way to the host. In this paper, we examine an alternate point in the L2 design space, which is simple (in that it is a single data plane mechanism with no separate control plane), converges quickly, delivers packets during convergence, utilizes all available links, and can be extended to support both equal-cost multipath and efficient multicast.",Proceedings of the 2016 ACM SIGCOMM Conference,497–510,14,"L2 routing, spanning tree","Florianopolis, Brazil",SIGCOMM '16,,,,,,
283,inproceedings,"Bremler-Barr, Anat and Harchol, Yotam and Hay, David","OpenBox: A Software-Defined Framework for Developing, Deploying, and Managing Network Functions",2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934875,10.1145/2934872.2934875,"We present OpenBox — a software-defined framework for network-wide development, deployment, and management of network functions (NFs). OpenBox effectively decouples the control plane of NFs from their data plane, similarly to SDN solutions that only address the network’s forwarding plane. OpenBox consists of three logic components. First, user-defined OpenBox applications provide NF specifications through the OpenBox north-bound API. Second, a logically-centralized OpenBox controller is able to merge logic of multiple NFs, possibly from multiple tenants, and to use a network-wide view to efficiently deploy and scale NFs across the network data plane. Finally, OpenBox instances constitute OpenBox’s data plane and are implemented either purely in software or contain specific hardware accelerators (e.g., a TCAM). In practice, different NFs carry out similar processing steps on the same packet, and our experiments indeed show a significant improvement of the network performance when using OpenBox. Moreover, OpenBox readily supports smart NF placement, NF scaling, and multi-tenancy through its controller.",Proceedings of the 2016 ACM SIGCOMM Conference,511–524,14,"Network functions, Software-Defined Networks, Middleboxes","Florianopolis, Brazil",SIGCOMM '16,,,,,,
284,inproceedings,"Shahbaz, Muhammad and Choi, Sean and Pfaff, Ben and Kim, Changhoon and Feamster, Nick and McKeown, Nick and Rexford, Jennifer","PISCES: A Programmable, Protocol-Independent Software Switch",2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934886,10.1145/2934872.2934886,"Hypervisors use software switches to steer packets to and from virtual machines (VMs). These switches frequently need upgrading and customization—to support new protocol headers or encapsulations for tunneling and overlays, to improve measurement and debugging features, and even to add middlebox-like functions. Software switches are typically based on a large body of code, including kernel code, and changing the switch is a formidable undertaking requiring domain mastery of network protocol design and developing, testing, and maintaining a large, complex codebase. Changing how a software switch forwards packets should not require intimate knowledge of its implementation. Instead, it should be possible to specify how packets are processed and forwarded in a high-level domain-specific language (DSL) such as P4, and compiled to run on a software switch. We present PISCES, a software switch derived from Open vSwitch (OVS), a hard-wired hypervisor switch, whose behavior is customized using P4. PISCES is not hard-wired to specific protocols; this independence makes it easy to add new features. We also show how the compiler can analyze the high-level specification to optimize forwarding performance. Our evaluation shows that PISCES performs comparably to OVS and that PISCES programs are about 40 times shorter than equivalent changes to OVS source code.",Proceedings of the 2016 ACM SIGCOMM Conference,525–538,14,"Software Switch, Compiler Optimizations, P4, OVS, Programmable Data Planes, Domain-Specific Languages (DSL), Software-Defined Networks (SDN), PISCES","Florianopolis, Brazil",SIGCOMM '16,,,,,,
285,inproceedings,Moln\'{a,Dataplane Specialization for High-Performance OpenFlow Software Switching,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2934887,10.1145/2934872.2934887,"OpenFlow is an amazingly expressive dataplane programming language, but this expressiveness comes at a severe performance price as switches must do excessive packet classification in the fast path. The prevalent OpenFlow software switch architecture is therefore built on flow caching, but this imposes intricate limitations on the workloads that can be supported efficiently and may even open the door to malicious cache overflow attacks. In this paper we argue that instead of enforcing the same universal flow cache semantics to all OpenFlow applications and optimize for the common case, a switch should rather automatically specialize its dataplane piecemeal with respect to the configured workload. We introduce ESwitch, a novel switch architecture that uses on-the-fly template-based code generation to compile any OpenFlow pipeline into efficient machine code, which can then be readily used as fast path. We present a proof-of-concept prototype and we demonstrate on illustrative use cases that ESwitch yields a simpler architecture, superior packet processing speed, improved latency and CPU scalability, and predictable performance. Our prototype can easily scale beyond 100 Gbps on a single Intel blade even with complex OpenFlow pipelines.",Proceedings of the 2016 ACM SIGCOMM Conference,539–552,14,"packet classification, OpenFlow software switching, template-based code generation","Florianopolis, Brazil",SIGCOMM '16,,,,,,
286,inproceedings,"Zhang, Yuchao and Xu, Ke and Yao, Guang and Zhang, Miao and Nie, Xiaohui",PieBridge: A Cross-DR Scale Large Data Transmission Scheduling System,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959046,10.1145/2934872.2959046,"Cross-DR WAN (Datacenter Region Wide Area Network) with various services are deployed to provide timely data information and analytics for users in a wide range of geographical locations. For its reliability and performance, data duplication synchronization is essential among different IDCs (Internet datacenters). However, this problem poses a challenge. First, data duplication requires huge amount of bandwidth whereas the bandwidth of cross-DR links and the upload/download rates of server interfaces are limited. Second, data transmissions are time sensitive, but the current network cannot complete such tasks in a timely manner. In this work, we present PieBridge, a cross-RD data duplicate transmission platform that accommodates hundreds of TBs of data generated from user applications online data analytics. We deployed PieBridge on the IDCs of Baidu and obtained promising performance results in comparison with the prevalent approaches.",Proceedings of the 2016 ACM SIGCOMM Conference,553–554,2,"Cross-DR WAN, Large-scale Data Transmission","Florianopolis, Brazil",SIGCOMM '16,,,,,,
287,inproceedings,"Li, Ziyang and Zhang, Yiming and Zhao, Yunxiang and Peng, Yuxing and Li, Dongsheng",Best Effort Task Scheduling for Data Parallel Jobs,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959047,10.1145/2934872.2959047,"The tasks of data-parallel computation jobs come up with diverse and time-varying resource requirements. The dynamic nature of task requirements brings challenges on making good scheduling decisions, due to it is hard to keep work-conserving. In this paper, we present BETS to cope with the requirement dynamics that aims at utilizing cluster resources fully. BETS employs a task model that represents for runtime task requirements, a coarse-grained task pipeline to make use of resources in a time-division multiplexing fashion, and fine-grained resource management to guarantee performance.",Proceedings of the 2016 ACM SIGCOMM Conference,555–556,2,"Data-parallel jobs, Task model, Task scheduling","Florianopolis, Brazil",SIGCOMM '16,,,,,,
288,inproceedings,"Jeong, Seong Hoon and Kang, Ah Reum and Kim, Joongheon and Kim, Huy Kang and Mohaisen, Aziz",A Longitudinal Analysis of .I2p Leakage in the Public DNS Infrastructure,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2960423,10.1145/2934872.2960423,"The Invisible Internet Project (I2P) is an overlay network that provides secure and anonymous communication channels. EepSites are the anonymous websites hosted in the I2P network. To access the eepSites, DNS requests of a domain name suffixed with the {sf .i2p",Proceedings of the 2016 ACM SIGCOMM Conference,557–558,2,"Network Analysis, DNS, Security, I2P, Privacy","Florianopolis, Brazil",SIGCOMM '16,,,,,,
289,inproceedings,"Choi, Byungkwon and Kim, Jeongmin and Han, Dongsu",Application-Specific Acceleration Framework for Mobile Applications,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959049,10.1145/2934872.2959049,"Minimizing response times for mobile applications is critical for quality user experience that often impacts the revenue of mobile services. Generalized approaches to accelerated mobile applications (e.g., TCP acceleration, SPDY, compression) are less effective because they do not take account for application specific behaviors. In contrast, application specific approaches build application-specific proxies by leveraging the app-specific protocol behaviors to enable dynamic caching and/or prefetching. However, this is non-trivial because it requires manual analysis of application level protocols and their interactions. Therefore, only a small number of apps enjoyed the benefit.This paper addresses the problem of scaling the number of applications by automating the process. To this end, we present a framework for mobile application acceleration that leverages automatic protocol analysis to automatically discover opportunities for app acceleration. The framework automatically finds out when and where to prefetch or perform dynamic caching. We present the framework design and a preliminary result that demonstrates its viability.",Proceedings of the 2016 ACM SIGCOMM Conference,559–560,2,"static analysis, automatic proxying, protocol behavior, android","Florianopolis, Brazil",SIGCOMM '16,,,,,,
290,inproceedings,"Li, Yue and Iannone, Luigi",Performance Evaluation of Locator/Identifier Separation Protocol through RIPE Atlas,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959050,10.1145/2934872.2959050,The emph{Locator/Identifier Separation Protocol,Proceedings of the 2016 ACM SIGCOMM Conference,561–562,2,"RIPE Atlas, measurement, LISP, experimentation","Florianopolis, Brazil",SIGCOMM '16,,,,,,
291,inproceedings,"Chang, Liqiong and Xiong, Jie and Chen, Xiaojiang and Wang, Ju and Hu, Junhao and Fang, Dingyi and Wang, Wei",TafLoc: Time-Adaptive and Fine-Grained Device-Free Localization with Little Cost,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959051,10.1145/2934872.2959051,"Many emerging applications drive the needs of device-free localization (DfL), in which the target can be localized without any device attached. Because of the ubiquitousness of WiFi infrastructures nowadays, the widely available Received Signal Strength (RSS) information at the WiFi Access points are commonly employed for localization purposes. However, current RSS based DfL systems have one main drawback hindering their real-life applications. That is, the RSS measurements (fingerprints) vary slowly in time even without any change in the environment and frequent updates of RSS at each location lead to a high human labor cost. In this paper, we propose an RSS based low cost DfL system named TafLoc which is able to accurately localize the target over a long time scale. To reduce the amount of human labor cost in updating the RSS fingerprints, TafLoc represents the RSS fingerprints as a matrix which has several unique properties. Based on these properties, we propose a novel fingerprint matrix reconstruction scheme to update the whole fingerprint database with just a few RSS measurements, thus the labor cost is greatly reduced. Extensive experiments illustrate the effectiveness of TafLoc, outperforming the state-of-the-art RSS based DfL systems.",Proceedings of the 2016 ACM SIGCOMM Conference,563–564,2,"Device Free Localization, Received Signal Strength, Time Adaptive, Fine-grained","Florianopolis, Brazil",SIGCOMM '16,,,,,,
292,inproceedings,"Fazzion, Elverton and Cunha, \'{I",Efficient Remapping of Internet Routing Events,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959052,10.1145/2934872.2959052,"Routing events impact multiple paths in the Internet, but current active topology mapping techniques monitor paths independently. Detecting a routing event on one Internet path does not trigger any measurements on other possibly-impacted paths. This approach leads to outdated and inconsistent routing information. We characterize routing events in the Internet and investigate probing strategies to efficiently identify paths impacted by a routing event. Our results indicate that targeted probing can help us quickly remap routing events and maintain more up-to-date and consistent topology maps.",Proceedings of the 2016 ACM SIGCOMM Conference,565–566,2,"Traceroute, Topology mapping, Routing events","Florianopolis, Brazil",SIGCOMM '16,,,,,,
293,inproceedings,"Edmundson, Anne and Ensafi, Roya and Feamster, Nick and Rexford, Jennifer",A First Look into Transnational Routing Detours,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959081,10.1145/2934872.2959081,"An increasing number of countries are passing laws that facilitate the mass surveillance of their citizens. In response, governments and citizens are increasingly paying attention to the countries that their Internet traffic traverses. In some cases, countries are taking extreme steps, such as building new IXPs and encouraging local interconnection to keep local traffic local. We find that although many of these efforts are extensive, they are often futile, due to the inherent lack of hosting and route diversity for many popular sites. We investigate how the use of overlay network relays and the DNS open resolver infrastructure can prevent traffic from traversing certain jurisdictions.",Proceedings of the 2016 ACM SIGCOMM Conference,567–568,2,"routing, surveillance","Florianopolis, Brazil",SIGCOMM '16,,,,,,
294,inproceedings,"Singh, Rachee and Gill, Phillipa",PathCache: A Path Prediction Toolkit,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959053,10.1145/2934872.2959053,"Path prediction on the Internet has been a topic of research in the networking community for close to a decade. Applications of path prediction solutions have ranged from optimizing selection of peers in peer- to-peer networks to improving and debugging CDN predictions. Recently, revelations of traffic correlation and surveillance on the Internet have raised the topic of path prediction in the context of network security. Specifically, predicting network paths can allow us to identify and avoid given organizations on network paths (e.g., to avoid traffic correlation attacks in Tor) or to infer the impact of hijacks and interceptions when direct measurements are not available.In this poster we propose the design and implementation of PathCache which aims to reuse measurement data to estimate AS level paths on the Internet. Unlike similar systems, PathCache does not assume that routing on the Internet is destination based. Instead, we develop an algorithm to compute confidence in paths between ASes. These multiple paths ranked by their confidence values are returned to the user.",Proceedings of the 2016 ACM SIGCOMM Conference,569–570,2,Internet Measurement,"Florianopolis, Brazil",SIGCOMM '16,,,,,,
295,inproceedings,"Amar, Yousef and Haddadi, Hamed and Mortier, Richard",Privacy-Aware Infrastructure for Managing Personal Data,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959054,10.1145/2934872.2959054,"In recent times, we have seen a proliferation of personal data. This can be attributed not just to a larger proportion of our lives moving online, but also through the rise of ubiquitous sensing through mobile and IoT devices. Alongside this surge, concerns over privacy, trust, and security are expressed more and more as different parties attempt to take advantage of this rich assortment of data.The Databox seeks to enable all the advantages of personal data analytics while at the same time enforcing **accountability** and **control** in order to protect a user's privacy. In this work, we propose and delineate a personal networked device that allows users to **collate**, **curate**, and **mediate** their personal data.",Proceedings of the 2016 ACM SIGCOMM Conference,571–572,2,"Networks, Personal Data, Privacy","Florianopolis, Brazil",SIGCOMM '16,,,,,,
296,inproceedings,"De Silva, Upeka and Lertsinsrubtavee, Adisorn and Sathiaseelan, Arjuna and Kanchanasut, Kanchana",Named Data Networking Based Smart Home Lighting,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959055,10.1145/2934872.2959055,"In this paper, we provide an initial evaluation of a home smart lighting system - demonstrating the advantages of ICN paradigm through the primitive features of NDN architecture. A prototype of NDN based smart home lighting was developed and benchmarked against the IP cloud based approach.",Proceedings of the 2016 ACM SIGCOMM Conference,573–574,2,"Networks → Network design principles, Networks → Network protocol design, Networks → Home networks","Florianopolis, Brazil",SIGCOMM '16,,,,,,
297,inproceedings,"Yang, Haixiang and Wang, Xiaoliang and Nguyen, Cam-Tu and Lu, Sanglu",Conan: Content-Aware Access Network Flow Scheduling to Improve QoE of Home Users,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959056,10.1145/2934872.2959056,"There has always been a gap of perception between Internet Service Providers (ISPs) and their customers when considering the performance of network service. On one hand, ISPs invest to increase downstream speed of access network infrastructure. On the other hand, users cannot achieve perceived quality of experience (QoE). This paper addresses this problem by introducing a system, Conan, which enables content-aware flow scheduling to improve the QoE of users. Conan exploits to satisfy users' requirements in the access network (LAN), which is the performance bottleneck actually. By leveraging the technique of software defined networking (SDN), Conan are able to specify the expected network capacity for different applications. Automatic application identification is deployed at home gateway to improve the scalability, and flexible bandwidth allocation is realized at LAN for specified applications. Using video streaming service optimization as an example, we demonstrate that our system can automatically allocate bandwidth for video flows.",Proceedings of the 2016 ACM SIGCOMM Conference,575–576,2,"QoE, SDN, Dynamic Resource Allocation","Florianopolis, Brazil",SIGCOMM '16,,,,,,
298,inproceedings,"Fernandes, Eder Le\~{a",Horse: Towards an SDN Traffic Dynamics Simulator for Large Scale Networks,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959082,10.1145/2934872.2959082,"The Software Defined Networking (SDN) paradigm can be successfully applied to the inter-domain ecosystem to empower network fabrics with finer grained policies and traffic engineering capabilities. However, introducing SDN at the inter-domain level might also lead to misconfigurations with potential to negatively impact on the Internet. Simulators are a popular approach to verify network behaviour and test applications before going into production. In the case of SDN, the available options do not scale for large scale networks nor high traffic loads. In this paper we propose a new simulator to foster SDN research and improve our understanding on the impact of the new use cases over the traffic flow. A simulation tool capable of efficiently reproducing large scale networks, high traffic loads, and policies, by abstracting the interactions between switches and controllers of the SDN network.",Proceedings of the 2016 ACM SIGCOMM Conference,577–578,2,Software Defined Networking; simulation,"Florianopolis, Brazil",SIGCOMM '16,,,,,,
299,inproceedings,"Gao, Kai and Gu, Chen and Xiang, Qiao and Yang, Y. Richard and Bi, Jun",FAST: A Simple Programming Abstraction for Complex State-Dependent SDN Programming,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2960424,10.1145/2934872.2960424,"Handling state dependencies is a major challenge in modern SDN programming, but existing frameworks do not provide sufficient abstractions nor tools to address this challenge. In this paper, we propose a novel, high-level programming abstraction and implement the *Function Automation SysTem (FAST)*. With the two key features, i.e., *automated state dependency tracking* and *efficient re-execution scheduling*, we demonstrate that FAST substantially simplifies state-dependent SDN programming and boosts the performance.",Proceedings of the 2016 ACM SIGCOMM Conference,579–580,2,"SDN, Programming abstraction, State dependency","Florianopolis, Brazil",SIGCOMM '16,,,,,,
300,inproceedings,"Sun, Chen and Bi, Jun and Zheng, Zhilong and Hu, Hongxin",SLA-NFV: An SLA-Aware High Performance Framework for Network Function Virtualization,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959058,10.1145/2934872.2959058,"We propose SLA-NFV, a Service Level Agreement (SLA) aware framework, for building high-performance NFV, focusing on fulfilling SLAs of service subscribers (or tenants). SLA-NFV leverages a hybrid infrastructure with both software and programmable hardware to enhance NFV’s capability with respect to various SLAs. Evaluations show that a hybrid service chain could reduce latency by up to 60% compared with a pure soft- ware service chain.",Proceedings of the 2016 ACM SIGCOMM Conference,581–582,2,"NFV, High Performance, SLA, Hybrid Infrastructure","Florianopolis, Brazil",SIGCOMM '16,,,,,,
301,inproceedings,"Zhao, Shuai and Sydney, Ali and Medhi, Deep",Building Application-Aware Network Environments Using SDN for Optimizing Hadoop Applications,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959059,10.1145/2934872.2959059,"Hadoop has become the de facto standard for Big Data analytics, especially for workloads that use the MapReduce (M/R) framework. However, the lack of network awareness of the default MapReduce resource manager in Hadoop can cause unbalanced job scheduling, network bottleneck, and eventually increase the Hadoop run time if Hadoop nodes are clustered in several geographically distributed locations. In this paper, we present an application-aware network approach using software-defined networking (SDN) for distributed Hadoop clusters. We develop the SDN applications for this environment that consider network topology discovery, traffic monitoring, and flow rerouting in addition to loop avoidance mechanisms.",Proceedings of the 2016 ACM SIGCOMM Conference,583–584,2,"Hadoop, Software-Defined Networking, Application-Aware Networking","Florianopolis, Brazil",SIGCOMM '16,,,,,,
302,inproceedings,"Wang, Wen and Liu, Cong and Su, Jinshu and He, Wenbo",Achieving Consistent SDN Control With Declarative Applications,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959060,10.1145/2934872.2959060,"Software-defined networking enables applications act as blackboxes independently to control the network flexibly. However, these independent applications may generate conflicting control decisions. To reconcile applications automatically and dynamically, we implement control applications with Prolog, which enables applications to execute jointly to make consistent control decisions. When conflicts occur, we design a compromise algorithm by sacrificing a subset of applications to maximize the desired control objectives.",Proceedings of the 2016 ACM SIGCOMM Conference,585–586,2,"Software-Defined Networking, Network Control Consistency","Florianopolis, Brazil",SIGCOMM '16,,,,,,
303,inproceedings,"Li, Hao and Hu, Chengchen and Zhang, Peng and Xie, Lei",Modular SDN Compiler Design with Intermediate Representation,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959061,10.1145/2934872.2959061,"Software Defined Networking (SDN) is evolving to such a phase that multiple programming languages and rule specifications coexist. However, current SDN compilers are closely bound to both languages and rules, thus disable the interoperability and compatibility of SDN programs. To solve this problem, we propose to modularize the SDN compiler by leveraging intermediate representation (IR), a common technique for computer compiler design. Specifically, we introduce Semantic Rule (SR) as the first IR for SDN compilers, which is a simple, language-independent, and semantic-preserving representation. We develop two optimizations on the semantic rule to coordinate cross-language programs in a single network and compress the number of compiled rules. We implement a modular compiler prototype with the proposed SR, and demonstrate that RYU programs can run at both OpenFlow and POF network. With synthetic network configurations, we demonstrate that the optimizations on SRs are effective, efficient and scalable.",Proceedings of the 2016 ACM SIGCOMM Conference,587–588,2,"Software-Defined Networks, Intermediate representation","Florianopolis, Brazil",SIGCOMM '16,,,,,,
304,inproceedings,"Yang, Ji and Hu, Chengchen and Zheng, Peng and Wang, Ruilong and Zhang, Peng and Guan, Xiaohong",Rethinking the Design of OpenFlow Switch Counters,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959062,10.1145/2934872.2959062,"OpenFlow, as the Software Defined Networking (SDN) primitive, provides a simple forwarding plane abstraction, which heavily relies on the fast memory inside the OpenFlow Switch (OFS). OFS components, e.g. flow table, meter table, counters, have to compete for the limited fast memory resource. As a result, only a few counting functions are defined as mandatory in the OFS specification, although a lot of SDN proposals depend on a detailed states collected by the optional counters in the specification. This fact motivates us to rethink the way to maintain counters in the OFS. We propose a new architecture called CACTI, which only consumes several registers in the fast path and moves the completed counters into the on chip RAM like cache in the slow path processor. Theoretical analysis and experiments on the prototype system demonstrated the efficiency of our architecture: CACTI is capable to achieve the throughput of 29.4-39.7M pps packets per second (pps). No RAM resource is needed any more in the fast path, instead, CACTI consumes only 0.24-0.54% Look-Up Table and 0.35-0.43% flip-flops compared with the entire FPGA-based OFS design in the fast path, and the unused CPU cache in the slow path.",Proceedings of the 2016 ACM SIGCOMM Conference,589–590,2,"SDN counter, OpenFlow","Florianopolis, Brazil",SIGCOMM '16,,,,,,
305,inproceedings,"Qiao, Siyi and Hu, Chengchen and Guan, Xiaohong and Zou, Jianhua",Taming the Flow Table Overflow in OpenFlow Switch,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959063,10.1145/2934872.2959063,"SDN has become the wide area network technology, which the academic and industry most concerned about.The limited table sizes of today’s SDN switches has turned to the most prominent short planks in the network design implementation. TCAM based flow table can provide an excellent matching performance while it really costs much. Even the flow table overflow cannot be prevented by a fixed-capacity flow table. In this paper, we design FTS(Flow Table Sharing) mechanism that can improve the performance disaster caused by overflow. We demonstrate that FTS reduces both control messages quantity and RTT time by two orders of magnitude compared to current state-of-the-art OpenFlow table-miss handler.",Proceedings of the 2016 ACM SIGCOMM Conference,591–592,2,"SDN, flowtable, overflow, table-miss","Florianopolis, Brazil",SIGCOMM '16,,,,,,
306,inproceedings,"Voellmy, Andreas and Chen, Shenshen and Wang, Xin and Yang, Y. Richard",Magellan: Generating Multi-Table Datapath from Datapath Oblivious Algorithmic SDN Policies,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959064,10.1145/2934872.2959064,"Despite the emergence of multi-table pipelining as a key feature of next-generation SDN data-path models, there is no existing work that addresses the substantial programming challenge of utilizing multi-tables automatically. In this paper, we present Magellan, the first system that addresses the aforementioned challenge. Introducing two novel, substantial algorithms, map-explore and table-design, Magellan achieves automatic derivation and population of multi-table pipelines from a datapath-oblivious, high-level SDN program written in a general-purpose language. Comparing the flow tables generated by Magellan with those produced by standard SDN controllers including OpenDaylight and Floodlight, we show that Magellan uses between 46-68x fewer rules.",Proceedings of the 2016 ACM SIGCOMM Conference,593–594,2,"SDN, Multi-table pipeline, Programming model","Florianopolis, Brazil",SIGCOMM '16,,,,,,
307,inproceedings,"Liu, Bingyang and Bi, Jun and Zhou, Yu",Source Address Validation in Software Defined Networks,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2960425,10.1145/2934872.2960425,"In this paper, we present the preliminary design and implementation of SDN-SAVI, an SDN application that enables SAVI functionalities in SDN networks. In this proposal, all the functionalities are implemented on the controller without modifying SDN switches. To enforce SAVI on packets in the data plane, the controller installs binding tables in switches using existing SDN techniques, such as OpenFlow. With SDN-SAVI, a network administrator can now enforce SAVI in her network by merely integrating a module on the controller, rather than purchasing SAVI-capable switches and replacing legacy ones.",Proceedings of the 2016 ACM SIGCOMM Conference,595–596,2,"SDN, SAVI","Florianopolis, Brazil",SIGCOMM '16,,,,,,
308,inproceedings,"Shukla, Apoorv and Schmid, Stefan and Feldmann, Anja and Ludwig, Arne and Dudycz, Szymon and Schuetze, Andre",Towards Transiently Secure Updates in Asynchronous SDNs,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959083,10.1145/2934872.2959083,"Software-Defined Networks (SDNs) promise to overcome the often complex and error-prone operation of tradi- tional computer networks, by enabling programmabil- ity, automation and verifiability. Yet, SDNs also in- troduce new challenges, for example due to the asyn- chronous communication channel between the logically centralized control platform and the switches in the data plane. In particular, the asynchronous commu- nication of network update commands (e.g., OpenFlow FlowMod messages) may lead to transient inconsisten- cies, such as loops or bypassed waypoints (e.g., fire- walls). One approach to ensure transient consistency even in asynchronous environments is to employ smart scheduling algorithms: algorithms which update subsets of switches in each communication round only, where each subset in itself guarantees consistency. In this demo, we show how to change routing policies in a transiently consistent manner. We demonstrate two al- gorithms, namely, Wayup [5] and Peacock [4], which partition the network updates sent from SDN controller towards OpenFlow software switches into multiple rounds as per respective algorithms. Later, the barrier mes- sages are utilized to ensure reliable network updates.",Proceedings of the 2016 ACM SIGCOMM Conference,597–598,2,"Mininet, SDN","Florianopolis, Brazil",SIGCOMM '16,,,,,,
309,inproceedings,"Gkounis, Dimitrios and Klaedtke, Felix and Bifulco, Roberto and Karame, Ghassan O.",Cases for Including a Reference Monitor to SDN,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959066,10.1145/2934872.2959066,No abstract written,Proceedings of the 2016 ACM SIGCOMM Conference,599–600,2,,"Florianopolis, Brazil",SIGCOMM '16,,,,,,
310,inproceedings,"Cziva, Richard and Jouet, Simon and Pezaros, Dimitrios P",Roaming Edge VNFs Using Glasgow Network Functions,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959067,10.1145/2934872.2959067,"While the network edge is becoming more important for the provision of customized services in next generation mobile networks, current NFV architectures are unsuitable to meet the increasing future demand. They rely on commodity servers with resource-hungry Virtual Machines that are unable to provide the high network function density and mobility requirements necessary for upcoming wide-area and 5G networks.In this demo, we showcase Glasgow Network Functions (GNF), a virtualization framework suitable for next generation mobile networks that exploits lightweight network functions (NFs) deployed at the edge and transparently following  users' devices as they roam between cells.",Proceedings of the 2016 ACM SIGCOMM Conference,601–602,2,"network function virtualization, software-defined networks, container network functions, fifth-generation mobile networks, glasgow network functions","Florianopolis, Brazil",SIGCOMM '16,,,,,,
311,inproceedings,"Bernal, Mauricio V\'{a",A Transparent Highway for Inter-Virtual Network Function Communication with Open VSwitch,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959068,10.1145/2934872.2959068,"This paper presents a software architecture that can dynamically and transparently establish direct communication paths between DPDK-based virtual network functions executed in virtual machines, by recognizing new point-to-point connections in traffic steering rules. We demonstrate the huge advantages of this architecture in terms of performance and the possibility to implement it with localized modifications in Open vSwitch and DPDK, without touching the VNFs.",Proceedings of the 2016 ACM SIGCOMM Conference,603–604,2,"DPDK, Open vSwitch, performance, NFV","Florianopolis, Brazil",SIGCOMM '16,,,,,,
312,inproceedings,"Baldi, Mario and Bonafiglia, Roberto and Risso, Fulvio and Sapio, Amedeo",Modeling Native Software Components as Virtual Network Functions,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959069,10.1145/2934872.2959069,"Virtual Network Functions (VNFs) are often realized using virtual machines (VMs) because they provide an isolated environment compatible with classical cloud computing technologies. However, VMs are demanding in terms of required resources (CPU and memory) and therefore not suitable for low-cost devices like residential gateways. Such equipment often runs a Linux-based operating system that includes by default a (large) number of common network functions, which can provide some of the services otherwise offered by simple VNFs, but with reduced overhead. In this paper those native software components are made available through a Network Function Virtualization (NFV) platform, thus making their use transparent from the VNF developer point of view.",Proceedings of the 2016 ACM SIGCOMM Conference,605–606,2,"Service Orchestration, Virtual Network Functions, Software Defined Networks, Network Functions Virtualization","Florianopolis, Brazil",SIGCOMM '16,,,,,,
313,inproceedings,"Fontes, Ramon dos Reis and Rothenberg, Christian Esteve",Mininet-WiFi: A Platform for Hybrid Physical-Virtual Software-Defined Wireless Networking Research,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959070,10.1145/2934872.2959070,"Software-Defined Wireless Networking (SDWN) is being considered an appealing paradigm to design and operate wireless networks through higher-level abstractions and programmatic interfaces such as the OpenFlow protocol. Identified benefits include cost savings, service velocity and customization, resource optimization through novel approaches to user mobility, traffic offloading, multi-layer and multi-path routing, and so on. This demonstration features Mininet-WiFi as a SDWN emulator with the ability to run realistic experiments in hybrid physical-virtual environments, where users attending the conference are able to experience first hand by connecting their devices and interacting with virtual WiFi stations in a wireless mesh network or reach the Internet through the emulated SDWN infrastructure. OpenFlow 1.3 metering and IP header re-writing actions will showcase HTTP flow redirection and rate limitation of real users' wireless traffic.",Proceedings of the 2016 ACM SIGCOMM Conference,607–608,2,"SDWN, wireless, SDN, OpenFlow, emulator, mesh network","Florianopolis, Brazil",SIGCOMM '16,,,,,,
314,inproceedings,"H\""{a",Off-the-Shelf Software-Defined Wi-Fi Networks,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959071,10.1145/2934872.2959071,"Wi-Fi networks were one of the first use-cases for Software-defined networking (SDN). However, to deploy a software-defined Wi-Fi network today, one has to rely on research prototypes with availability, documentation, hardware requirements, and scalability issues. To alleviate this situation, we demonstrate two simple techniques to bring SDN functionality to existing Wi-Fi networks and discuss their benefits and short-comings. Researchers can use our techniques to convert their existing Wi-Fi testbeds into software defined Wi-Fi testbeds. Our two techniques thus significantly lower the barrier-to-entry for deploying software-defined Wi-Fi networks.",Proceedings of the 2016 ACM SIGCOMM Conference,609–610,2,"SDN, Testbeds, Wi-fi","Florianopolis, Brazil",SIGCOMM '16,,,,,,
315,inproceedings,"ZHANG, PENGYU and Bharadia, Dinesh and Joshi, Kiran and Katti, Sachin",Enabling Backscatter Communication among Commodity WiFi Radios,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959072,10.1145/2934872.2959072,"We present the first low power backscatter system that can be deployed completely using commodity WiFi infrastructure. With this system, a low power tag reflects existing 802.11b transmissions from a commodity WiFi transmitter, and the backscattered signals can be decoded as a standard WiFi packet by a commodity 802.11b receiver. The key invention is a novel technique called textbf{codeword translation",Proceedings of the 2016 ACM SIGCOMM Conference,611–612,2,Backscatter,"Florianopolis, Brazil",SIGCOMM '16,,,,,,
316,inproceedings,"Rostami, Ahmad and \""{O",Multi-Domain Orchestration across RAN and Transport for 5G,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959073,10.1145/2934872.2959073,"End-to-End programmability across radio, transport and compute resources is a key enabler for the fifth generation of mobile communication networks (5G). In our work we look into how SDN can realize the required cross-domain programmability, as well as slicing of resources towards multiple clients. We present design and implementation of a hierarchical, modular and programmable orchestration architecture across radio access networks and transport networks. We demonstrate how the developed multi-domain orchestration improves the service creation as well as resource utilization across the domains using real-time monitoring.",Proceedings of the 2016 ACM SIGCOMM Conference,613–614,2,"5G, SDN, multi-domain orchestration, network slicing","Florianopolis, Brazil",SIGCOMM '16,,,,,,
317,inproceedings,"Wang, Zhaoning and Cheng, Bo and Zhai, Zhongyi and Jin, Ying and Feng, Yimeng and Chen, Junliang",EasyApp: A Cross-Platform Mobile Applications Development Environment Based on OSGi,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959074,10.1145/2934872.2959074,"*** The rapid development of mobile internet abstracts many non-professional persons to creating mobile applications. Traditional development process cannot meet their needs. In this paper, we present a cross-platform mobile development environment based on OSGi framework, EasyApp. It provides a highly-integrated, UI-friendly and easily-operating environment. Applications are comprehensively developed with web techniques. Users could create mobile applications with draggable widgets. Native APIs of mobile phone can be invoked with abundant plugins. After designing, users could package and download applications of multiple platforms. ***",Proceedings of the 2016 ACM SIGCOMM Conference,615–616,2,"Widgets communication, End-user development, OSGi, Mobile application","Florianopolis, Brazil",SIGCOMM '16,,,,,,
318,inproceedings,"Wang, Yi and Lin, Dong and Li, Changtai and Zhang, Junping and Liu, Peng and Hu, Chengchen and Zhang, Gong",Application Driven Network: Providing On-Demand Services for Applications,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959075,10.1145/2934872.2959075,Application Driven Network(ADN) is a new paradigm that provides on-demand differentiated services for applications. A physical network in ADN is sliced into various logically isolated sub-networks. Each network slice can have its own network architecture and protocol to serve one application exclusively. ADN enhances the user experience while keeping the resource efficiency by further imposing multiplexing among these logically isolated sub-networks.,Proceedings of the 2016 ACM SIGCOMM Conference,617–618,2,"NFV, Application Driven Network, DiffServ, SDN","Florianopolis, Brazil",SIGCOMM '16,,,,,,
319,inproceedings,"Tilmans, Olivier and Vissicchio, Stefano and Vanbever, Laurent and Rexford, Jennifer",Fibbing in Action: On-Demand Load-Balancing for Better Video Delivery,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959084,10.1145/2934872.2959084,"Video streaming, in conjunction with social networks, have given birth to a new traffic pattern over the Internet: transient, localized traffic surges, known as flash crowds. Traditional traffic-engineering methods can hardly cope with these surges, as they are unpredictable by nature. Consequently, networks either have to be over-provisioned, which is expensive and wastes resources, or risk to periodically incur congestion, which infuriates customers. This demonstration shows how Fibbing can improve network performance and preserve users’ quality of experience when accessing video streams, by implementing a fine-grained load-balancing service. This service leverages two unique features of Fibbing: programming per destination load-balancing and implementing uneven splitting ratios.",Proceedings of the 2016 ACM SIGCOMM Conference,619–620,2,"Uneven load-balancing, Traffic engineering, Fibbing","Florianopolis, Brazil",SIGCOMM '16,,,,,,
320,inproceedings,"Fr\""{o",Capture and Replay: Reproducible Network Experiments in Mininet,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959076,10.1145/2934872.2959076,"Network emulations are widely used in the networking community. The network emulator Mininet recently gained popularity, as it allows running real Linux applications on top of an emulated network. The specification of the network includes the topology as well as static bandwidth, latency, and packet drops probability parameters. Even though evaluations with static parameters provide useful insights, real world measurements show dynamically changing bandwidths, posing special challenges that need to be addressed in network research.In this demo, we capture bandwidth traces in the wild and reproducibly replay these traces in Mininet. Our emph{capture and replay",Proceedings of the 2016 ACM SIGCOMM Conference,621–622,2,"Mininet, Bandwidth Replay, Reproducible Research","Florianopolis, Brazil",SIGCOMM '16,,,,,,
321,inproceedings,"Patra, P Gyanesh and Rothenberg, Christian Esteve and Pongr\'{a",MACSAD: Multi-Architecture Compiler System for Abstract Dataplanes (Aka Partnering P4 with ODP),2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959077,10.1145/2934872.2959077,"Software Defined Networking (SDN) strives for deep programmable hardware and software dataplanes without giving up on performance. Domain Specific Languages (DSL) such as P4 seek to provide top-down high-level capabilities to define the datapath pipeline agnostic to the network platform and independent from any network protocols. At the crossroads, bottom-up industry efforts at the OpenDataPlane (ODP) initiative are pursuing open-source multiarchitecture APIs for dataplane programmability across various networking platforms. Towards P4 code reuse for various targets (portability), we propose MACSAD as a compiler system that brings together the higher-level P4 language and the abstract, target-independent ODP APIs. The demo showcases two P4 applications compiled into heterogeneous datapath platforms supporting ODP.",Proceedings of the 2016 ACM SIGCOMM Conference,623–624,2,"OpenDataPlane, P4, Software Defined Networking","Florianopolis, Brazil",SIGCOMM '16,,,,,,
322,inproceedings,"Chaviaras, Gavriil and Gigis, Petros and Sermpezis, Pavlos and Dimitropoulos, Xenofontas",ARTEMIS: Real-Time Detection and Automatic Mitigation for BGP Prefix Hijacking,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959078,10.1145/2934872.2959078,"Prefix hijacking is a common phenomenon in the Internet that often causes routing problems and economic losses. In this demo, we propose ARTEMIS, a tool that enables network administrators to detect and mitigate prefix hijacking incidents, against their own prefixes. ARTEMIS is based on the real-time monitoring of BGP data in the Internet, and software-defined networking (SDN) principles, and can completely mitigate a prefix hijacking within a few minutes (e.g., 5-6mins in our experiments) after it has been~launched.",Proceedings of the 2016 ACM SIGCOMM Conference,625–626,2,"Network management, Network security, Network monitoring","Florianopolis, Brazil",SIGCOMM '16,,,,,,
323,inproceedings,"Su, Jinshu and Chen, Shuhui and Han, Biao and Xu, Chengcheng and Wang, Xin",A 60Gbps DPI Prototype Based on Memory-Centric FPGA,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959079,10.1145/2934872.2959079,"Deep packet inspection (DPI) is widely used in content-aware network applications to detect string features. It is of vital importance to improve the DPI performance due to the ever-increasing link speed. In this demo, we propose a novel DPI architecture with a hierarchy memory structure and parallel matching engines based on memory-centric FPGA. The implemented DPI prototype is able to provide up to 60Gbps full-text string matching throughput and fast rules update speed.",Proceedings of the 2016 ACM SIGCOMM Conference,627–628,2,"DPI, hierarchical memory, string matching","Florianopolis, Brazil",SIGCOMM '16,,,,,,
324,inproceedings,"Laki, S\'{a",High Speed Packet Forwarding Compiled from Protocol Independent Data Plane Specifications,2016,9781450341936,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2934872.2959080,10.1145/2934872.2959080,"P4 is a high level language for programming network switches that allows for great flexibility in the description of packet structure and processing, independent of the specifics of the underlying hardware. In this demo, we present our prototype P4 compiler in which the hardware independent and hardware specific functionalities are separated. We have identified the requisites of the latter, which form the interface of our target specific Hardware Abstraction Library (HAL); the compiler turns P4 code into a target independent core program that is linked to this library and invokes its operations. The two stage separation improves portability: to support a new architecture, only the hardware dependent library has to be implemented. In the demo, we demonstrate the flexibility of our compiler with a HAL for Intel DPDK, and show the packet processing and forwarding performance of compiled switches in different scenarios.",Proceedings of the 2016 ACM SIGCOMM Conference,629–630,2,"P4, Programmable data plane, SDN, Packet forwarding","Florianopolis, Brazil",SIGCOMM '16,,,,,,
325,inproceedings,"Kandula, Srikanth",Session Details: SDN,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3260998,10.1145/3260998,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,,,,"London, United Kingdom",SIGCOMM '15,,,,,,
326,article,"Kumar, Alok and Jain, Sushant and Naik, Uday and Raghuraman, Anand and Kasinadhuni, Nikhil and Zermeno, Enrique Cauich and Gunn, C. Stephen and Ai, Jing and Carlin, Bj\""{o","BwE: Flexible, Hierarchical Bandwidth Allocation for WAN Distributed Computing",2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787478,10.1145/2829988.2787478,"WAN bandwidth remains a constrained resource that is economically infeasible to substantially overprovision. Hence, it is important to allocate capacity according to service priority and based on the incremental value of additional allocation. For example, it may be the highest priority for one service to receive 10Gb/s of bandwidth but upon reaching such an allocation, incremental priority may drop sharply favoring allocation to other services. Motivated by the observation that individual flows with fixed priority may not be the ideal basis for bandwidth allocation, we present the design and implementation of Bandwidth Enforcer (BwE), a global, hierarchical bandwidth allocation infrastructure. BwE supports: i) service-level bandwidth allocation following prioritized bandwidth functions where a service can represent an arbitrary collection of flows, ii)independent allocation and delegation policies according to user-defined hierarchy, all accounting for a global view of bandwidth and failure conditions, iii) multi-path forwarding common in traffic-engineered networks, and iv) a central administrative point to override (perhaps faulty) policy during exceptional conditions. BwE has delivered more service efficient bandwidth utilization and simpler management in production for multiple years.",,1–14,14,"wide-area networks, software-defined network, max-min fair, bandwidth allocation",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
327,inproceedings,"Kumar, Alok and Jain, Sushant and Naik, Uday and Raghuraman, Anand and Kasinadhuni, Nikhil and Zermeno, Enrique Cauich and Gunn, C. Stephen and Ai, Jing and Carlin, Bj\""{o","BwE: Flexible, Hierarchical Bandwidth Allocation for WAN Distributed Computing",2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787478,10.1145/2785956.2787478,"WAN bandwidth remains a constrained resource that is economically infeasible to substantially overprovision. Hence, it is important to allocate capacity according to service priority and based on the incremental value of additional allocation. For example, it may be the highest priority for one service to receive 10Gb/s of bandwidth but upon reaching such an allocation, incremental priority may drop sharply favoring allocation to other services. Motivated by the observation that individual flows with fixed priority may not be the ideal basis for bandwidth allocation, we present the design and implementation of Bandwidth Enforcer (BwE), a global, hierarchical bandwidth allocation infrastructure. BwE supports: i) service-level bandwidth allocation following prioritized bandwidth functions where a service can represent an arbitrary collection of flows, ii)independent allocation and delegation policies according to user-defined hierarchy, all accounting for a global view of bandwidth and failure conditions, iii) multi-path forwarding common in traffic-engineered networks, and iv) a central administrative point to override (perhaps faulty) policy during exceptional conditions. BwE has delivered more service efficient bandwidth utilization and simpler management in production for multiple years.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,1–14,14,"bandwidth allocation, software-defined network, max-min fair, wide-area networks","London, United Kingdom",SIGCOMM '15,,,,,,
328,article,"Hartert, Renaud and Vissicchio, Stefano and Schaus, Pierre and Bonaventure, Olivier and Filsfils, Clarence and Telkamp, Thomas and Francois, Pierre",A Declarative and Expressive Approach to Control Forwarding Paths in Carrier-Grade Networks,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787495,10.1145/2829988.2787495,"SDN simplifies network management by relying on declarativity (high-level interface) and expressiveness (network flexibility). We propose a solution to support those features while preserving high robustness and scalability as needed in carrier-grade networks. Our solution is based on (i) a two-layer architecture separating connectivity and optimization tasks; and (ii) a centralized optimizer called framework, which translates high-level goals expressed almost in natural language into compliant network configurations. Our evaluation on real and synthetic topologies shows that framework improves the state of the art by (i) achieving better trade-offs for classic goals covered by previous works, (ii) supporting a larger set of goals (refined traffic engineering and service chaining), and (iii) optimizing large ISP networks in few seconds. We also quantify the gains of our implementation, running Segment Routing on top of IS-IS, over possible alternatives (RSVP-TE and OpenFlow).",,15–28,14,"optimization, traffic engineering, service chaining, mpls, software defined networking (sdn), isp, segment routing (sr)",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
329,inproceedings,"Hartert, Renaud and Vissicchio, Stefano and Schaus, Pierre and Bonaventure, Olivier and Filsfils, Clarence and Telkamp, Thomas and Francois, Pierre",A Declarative and Expressive Approach to Control Forwarding Paths in Carrier-Grade Networks,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787495,10.1145/2785956.2787495,"SDN simplifies network management by relying on declarativity (high-level interface) and expressiveness (network flexibility). We propose a solution to support those features while preserving high robustness and scalability as needed in carrier-grade networks. Our solution is based on (i) a two-layer architecture separating connectivity and optimization tasks; and (ii) a centralized optimizer called framework, which translates high-level goals expressed almost in natural language into compliant network configurations. Our evaluation on real and synthetic topologies shows that framework improves the state of the art by (i) achieving better trade-offs for classic goals covered by previous works, (ii) supporting a larger set of goals (refined traffic engineering and service chaining), and (iii) optimizing large ISP networks in few seconds. We also quantify the gains of our implementation, running Segment Routing on top of IS-IS, over possible alternatives (RSVP-TE and OpenFlow).",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,15–28,14,"software defined networking (sdn), optimization, mpls, service chaining, isp, segment routing (sr), traffic engineering","London, United Kingdom",SIGCOMM '15,,,,,,
330,article,"Prakash, Chaithan and Lee, Jeongkeun and Turner, Yoshio and Kang, Joon-Myung and Akella, Aditya and Banerjee, Sujata and Clark, Charles and Ma, Yadi and Sharma, Puneet and Zhang, Ying",PGA: Using Graphs to Express and Automatically Reconcile Network Policies,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787506,10.1145/2829988.2787506,"Software Defined Networking (SDN) and cloud automation enable a large number of diverse parties (network operators, application admins, tenants/end-users) and control programs (SDN Apps, network services) to generate network policies independently and dynamically. Yet existing policy abstractions and frameworks do not support natural expression and automatic composition of high-level policies from diverse sources. We tackle the open problem of automatic, correct and fast composition of multiple independently specified network policies. We first develop a high-level Policy Graph Abstraction (PGA) that allows network policies to be expressed simply and independently, and leverage the graph structure to detect and resolve policy conflicts efficiently. Besides supporting ACL policies, PGA also models and composes service chaining policies, i.e., the sequence of middleboxes to be traversed, by merging multiple service chain requirements into conflict-free composed chains. Our system validation using a large enterprise network policy dataset demonstrates practical composition times even for very large inputs, with only sub-millisecond runtime latencies.",,29–42,14,"policy graphs, network manageability, network programming interfaces, middleboxes, network appliances, data center networks, programmable networks, network domains, software-defined networks, network management",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
331,inproceedings,"Prakash, Chaithan and Lee, Jeongkeun and Turner, Yoshio and Kang, Joon-Myung and Akella, Aditya and Banerjee, Sujata and Clark, Charles and Ma, Yadi and Sharma, Puneet and Zhang, Ying",PGA: Using Graphs to Express and Automatically Reconcile Network Policies,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787506,10.1145/2785956.2787506,"Software Defined Networking (SDN) and cloud automation enable a large number of diverse parties (network operators, application admins, tenants/end-users) and control programs (SDN Apps, network services) to generate network policies independently and dynamically. Yet existing policy abstractions and frameworks do not support natural expression and automatic composition of high-level policies from diverse sources. We tackle the open problem of automatic, correct and fast composition of multiple independently specified network policies. We first develop a high-level Policy Graph Abstraction (PGA) that allows network policies to be expressed simply and independently, and leverage the graph structure to detect and resolve policy conflicts efficiently. Besides supporting ACL policies, PGA also models and composes service chaining policies, i.e., the sequence of middleboxes to be traversed, by merging multiple service chain requirements into conflict-free composed chains. Our system validation using a large enterprise network policy dataset demonstrates practical composition times even for very large inputs, with only sub-millisecond runtime latencies.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,29–42,14,"middleboxes, data center networks, programmable networks, network manageability, policy graphs, network domains, network appliances, network programming interfaces, software-defined networks, network management","London, United Kingdom",SIGCOMM '15,,,,,,
332,article,"Vissicchio, Stefano and Tilmans, Olivier and Vanbever, Laurent and Rexford, Jennifer",Central Control Over Distributed Routing,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787497,10.1145/2829988.2787497,"Centralizing routing decisions offers tremendous flexibility, but sacrifices the robustness of distributed protocols. In this paper, we present Fibbing, an architecture that achieves both flexibility and robustness through central control over distributed routing. Fibbing introduces fake nodes and links into an underlying link-state routing protocol, so that routers compute their own forwarding tables based on the augmented topology. Fibbing is expressive, and readily supports flexible load balancing, traffic engineering, and backup routes. Based on high-level forwarding requirements, the Fibbing controller computes a compact augmented topology and injects the fake components through standard routing-protocol messages. Fibbing works with any unmodified routers speaking OSPF. Our experiments also show that it can scale to large networks with many forwarding requirements, introduces minimal overhead, and quickly reacts to network and controller failures.",,43–56,14,"link-state routing, SDN, fibbing",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
333,inproceedings,"Vissicchio, Stefano and Tilmans, Olivier and Vanbever, Laurent and Rexford, Jennifer",Central Control Over Distributed Routing,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787497,10.1145/2785956.2787497,"Centralizing routing decisions offers tremendous flexibility, but sacrifices the robustness of distributed protocols. In this paper, we present Fibbing, an architecture that achieves both flexibility and robustness through central control over distributed routing. Fibbing introduces fake nodes and links into an underlying link-state routing protocol, so that routers compute their own forwarding tables based on the augmented topology. Fibbing is expressive, and readily supports flexible load balancing, traffic engineering, and backup routes. Based on high-level forwarding requirements, the Fibbing controller computes a compact augmented topology and injects the fake components through standard routing-protocol messages. Fibbing works with any unmodified routers speaking OSPF. Our experiments also show that it can scale to large networks with many forwarding requirements, introduces minimal overhead, and quickly reacts to network and controller failures.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,43–56,14,"fibbing, SDN, link-state routing","London, United Kingdom",SIGCOMM '15,,,,,,
334,inproceedings,"Rizzo, Luigi",Session Details: Network Algorithmics and Economics,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3260999,10.1145/3260999,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,,,,"London, United Kingdom",SIGCOMM '15,,,,,,
335,article,"Asai, Hirochika and Ohara, Yasuhiro",Poptrie: A Compressed Trie with Population Count for Fast and Scalable Software IP Routing Table Lookup,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787474,10.1145/2829988.2787474,"Internet of Things leads to routing table explosion. An inexpensive approach for IP routing table lookup is required against ever growing size of the Internet. We contribute by a fast and scalable software routing lookup algorithm based on a multiway trie, called Poptrie. Named after our approach to traversing the tree, it leverages the population count instruction on bit-vector indices for the descendant nodes to compress the data structure within the CPU cache. Poptrie outperforms the state-of-the-art technologies, Tree BitMap, DXR and SAIL, in all of the evaluations using random and real destination queries on 35 routing tables, including the real global tier-1 ISP's full-route routing table. Poptrie peaks between 174 and over 240 Million lookups per second (Mlps) with a single core and tables with 500--800k routes, consistently 4--578% faster than all competing algorithms in all the tests we ran. We provide the comprehensive performance evaluation, remarkably with the CPU cycle analysis. This paper shows the suitability of Poptrie in the future Internet including IPv6, where a larger route table is expected with longer prefixes.",,57–70,14,"longest prefix match, ip routing table lookup, trie",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
336,inproceedings,"Asai, Hirochika and Ohara, Yasuhiro",Poptrie: A Compressed Trie with Population Count for Fast and Scalable Software IP Routing Table Lookup,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787474,10.1145/2785956.2787474,"Internet of Things leads to routing table explosion. An inexpensive approach for IP routing table lookup is required against ever growing size of the Internet. We contribute by a fast and scalable software routing lookup algorithm based on a multiway trie, called Poptrie. Named after our approach to traversing the tree, it leverages the population count instruction on bit-vector indices for the descendant nodes to compress the data structure within the CPU cache. Poptrie outperforms the state-of-the-art technologies, Tree BitMap, DXR and SAIL, in all of the evaluations using random and real destination queries on 35 routing tables, including the real global tier-1 ISP's full-route routing table. Poptrie peaks between 174 and over 240 Million lookups per second (Mlps) with a single core and tables with 500--800k routes, consistently 4--578% faster than all competing algorithms in all the tests we ran. We provide the comprehensive performance evaluation, remarkably with the CPU cycle analysis. This paper shows the suitability of Poptrie in the future Internet including IPv6, where a larger route table is expected with longer prefixes.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,57–70,14,"trie, ip routing table lookup, longest prefix match","London, United Kingdom",SIGCOMM '15,,,,,,
337,article,"Zheng, Liang and Joe-Wong, Carlee and Tan, Chee Wei and Chiang, Mung and Wang, Xinyu",How to Bid the Cloud,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787473,10.1145/2829988.2787473,"Amazon's Elastic Compute Cloud (EC2) uses auction-based spot pricing to sell spare capacity, allowing users to bid for cloud resources at a highly reduced rate. Amazon sets the spot price dynamically and accepts user bids above this price. Jobs with lower bids (including those already running) are interrupted and must wait for a lower spot price before resuming. Spot pricing thus raises two basic questions: how might the provider set the price, and what prices should users bid? Computing users' bidding strategies is particularly challenging: higher bid prices reduce the probability of, and thus extra time to recover from, interruptions, but may increase users' cost. We address these questions in three steps: (1) modeling the cloud provider's setting of the spot price and matching the model to historically offered prices, (2) deriving optimal bidding strategies for different job requirements and interruption overheads, and (3) adapting these strategies to MapReduce jobs with master and slave nodes having different interruption overheads. We run our strategies on EC2 for a variety of job sizes and instance types, showing that spot pricing reduces user cost by 90% with a modest increase in completion time compared to on-demand pricing.",,71–84,14,"cloud pricing, optimization, spot instance",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
338,inproceedings,"Zheng, Liang and Joe-Wong, Carlee and Tan, Chee Wei and Chiang, Mung and Wang, Xinyu",How to Bid the Cloud,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787473,10.1145/2785956.2787473,"Amazon's Elastic Compute Cloud (EC2) uses auction-based spot pricing to sell spare capacity, allowing users to bid for cloud resources at a highly reduced rate. Amazon sets the spot price dynamically and accepts user bids above this price. Jobs with lower bids (including those already running) are interrupted and must wait for a lower spot price before resuming. Spot pricing thus raises two basic questions: how might the provider set the price, and what prices should users bid? Computing users' bidding strategies is particularly challenging: higher bid prices reduce the probability of, and thus extra time to recover from, interruptions, but may increase users' cost. We address these questions in three steps: (1) modeling the cloud provider's setting of the spot price and matching the model to historically offered prices, (2) deriving optimal bidding strategies for different job requirements and interruption overheads, and (3) adapting these strategies to MapReduce jobs with master and slave nodes having different interruption overheads. We run our strategies on EC2 for a variety of job sizes and instance types, showing that spot pricing reduces user cost by 90% with a modest increase in completion time compared to on-demand pricing.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,71–84,14,"spot instance, optimization, cloud pricing","London, United Kingdom",SIGCOMM '15,,,,,,
339,article,"Howard, Heidi and Crowcroft, Jon",Coracle: Evaluating Consensus at the Internet Edge,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790010,10.1145/2829988.2790010,"Distributed consensus is fundamental in distributed systems for achieving fault-tolerance. The Paxos algorithm has long dominated this domain, although it has been recently challenged by algorithms such as Raft and Viewstamped Replication Revisited. These algorithms rely on Paxos's original assumptions, unfortunately these assumptions are now at odds with the reality of the modern internet. Our insight is that current consensus algorithms have significant availability issues when deployed outside the well defined context of the datacenter. To illustrate this problem, we developed Coracle, a tool for evaluating distributed consensus algorithms in settings that more accurately represent realistic deployments. We have used Coracle to test two examples of network configurations that contradict the liveness claims of the Raft algorithm. Through the process of exercising these algorithms under more realistic assumptions, we demonstrate wider availability issues faced by consensus algorithms when deployed on real world networks.",,85–86,2,"dependable systems, distributed consensus, fault-tolerance",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
340,inproceedings,"Howard, Heidi and Crowcroft, Jon",Coracle: Evaluating Consensus at the Internet Edge,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790010,10.1145/2785956.2790010,"Distributed consensus is fundamental in distributed systems for achieving fault-tolerance. The Paxos algorithm has long dominated this domain, although it has been recently challenged by algorithms such as Raft and Viewstamped Replication Revisited. These algorithms rely on Paxos's original assumptions, unfortunately these assumptions are now at odds with the reality of the modern internet. Our insight is that current consensus algorithms have significant availability issues when deployed outside the well defined context of the datacenter. To illustrate this problem, we developed Coracle, a tool for evaluating distributed consensus algorithms in settings that more accurately represent realistic deployments. We have used Coracle to test two examples of network configurations that contradict the liveness claims of the Raft algorithm. Through the process of exercising these algorithms under more realistic assumptions, we demonstrate wider availability issues faced by consensus algorithms when deployed on real world networks.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,85–86,2,"distributed consensus, fault-tolerance, dependable systems","London, United Kingdom",SIGCOMM '15,,,,,,
341,article,"Fiadino, Pierdomenico and D'Alconzo, Alessandro and Schiavone, Mirko and Casas, Pedro",Challenging Entropy-Based Anomaly Detection and Diagnosis in Cellular Networks,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790011,10.1145/2829988.2790011,"In this paper we challenge the applicability of entropy-based approaches for detecting and diagnosis network traffic anomalies, and claim that full statistics (i.e., empirical probability distributions) should be applied to improve the change-detection capabilities. We support our claim by detecting and diagnosing large-scale traffic anomalies in a real cellular network, caused by specific OTT (Over The Top) services and smartphone devices. Our results clearly suggest that anomaly detection and diagnosis based on entropy analysis is prone to errors and misses typical characteristics of traffic anomalies, particularly in the studied scenario.",,87–88,2,,,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
342,inproceedings,"Fiadino, Pierdomenico and D'Alconzo, Alessandro and Schiavone, Mirko and Casas, Pedro",Challenging Entropy-Based Anomaly Detection and Diagnosis in Cellular Networks,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790011,10.1145/2785956.2790011,"In this paper we challenge the applicability of entropy-based approaches for detecting and diagnosis network traffic anomalies, and claim that full statistics (i.e., empirical probability distributions) should be applied to improve the change-detection capabilities. We support our claim by detecting and diagnosing large-scale traffic anomalies in a real cellular network, caused by specific OTT (Over The Top) services and smartphone devices. Our results clearly suggest that anomaly detection and diagnosis based on entropy analysis is prone to errors and misses typical characteristics of traffic anomalies, particularly in the studied scenario.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,87–88,2,,"London, United Kingdom",SIGCOMM '15,,,,,,
343,article,"Bogdanov, Kirill and Pe\'{o",Toward Automated Testing of Geo-Distributed Replica Selection Algorithms,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790013,10.1145/2829988.2790013,"Many geo-distributed systems rely on a replica selection algorithms to communicate with the closest set of replicas. Unfortunately, the bursty nature of the Internet traffic and ever changing network conditions present a problem in identifying the best choices of replicas. Suboptimal replica choices result in increased response latency and reduced system performance. In this work we present GeoPerf, a tool that tries to automate testing of geo-distributed replica selection algorithms. We used GeoPerf to test Cassandra and MongoDB, two popular data stores, and found bugs in each of these systems.",,89–90,2,"wide area networks, replica selection algorithms, software testing and debugging, symbolic execution",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
344,inproceedings,"Bogdanov, Kirill and Pe\'{o",Toward Automated Testing of Geo-Distributed Replica Selection Algorithms,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790013,10.1145/2785956.2790013,"Many geo-distributed systems rely on a replica selection algorithms to communicate with the closest set of replicas. Unfortunately, the bursty nature of the Internet traffic and ever changing network conditions present a problem in identifying the best choices of replicas. Suboptimal replica choices result in increased response latency and reduced system performance. In this work we present GeoPerf, a tool that tries to automate testing of geo-distributed replica selection algorithms. We used GeoPerf to test Cassandra and MongoDB, two popular data stores, and found bugs in each of these systems.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,89–90,2,"software testing and debugging, symbolic execution, replica selection algorithms, wide area networks","London, United Kingdom",SIGCOMM '15,,,,,,
345,article,"van Rijswijk-Deij, Roland and Jonker, Mattijs and Sperotto, Anna and Pras, Aiko",The Internet of Names: A DNS Big Dataset,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2789996,10.1145/2829988.2789996,"The Domain Name System (DNS) is part of the core infrastructure of the Internet. Tracking changes in the DNS over time provides valuable information about the evolution of the Internet's infrastructure. Until now, only one large-scale approach to perform these kinds of measurements existed, passive DNS (pDNS). While pDNS is useful for applications like tracing security incidents, it does not provide sufficient information to reliably track DNS changes over time. We use a complementary approach based on active measurements, which provides a unique, comprehensive dataset on the evolution of DNS over time. Our high-performance infrastructure performs Internet-scale active measurements, currently querying over 50% of the DNS name space on a daily basis. Our infrastructure is designed from the ground up to enable big data analysis approaches on, e.g., a Hadoop cluster. With this novel approach we aim for a quantum leap in DNS-based measurement and analysis of the Internet.",,91–92,2,"active measurements, DNS, big data, internet evolution",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
346,inproceedings,"van Rijswijk-Deij, Roland and Jonker, Mattijs and Sperotto, Anna and Pras, Aiko",The Internet of Names: A DNS Big Dataset,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2789996,10.1145/2785956.2789996,"The Domain Name System (DNS) is part of the core infrastructure of the Internet. Tracking changes in the DNS over time provides valuable information about the evolution of the Internet's infrastructure. Until now, only one large-scale approach to perform these kinds of measurements existed, passive DNS (pDNS). While pDNS is useful for applications like tracing security incidents, it does not provide sufficient information to reliably track DNS changes over time. We use a complementary approach based on active measurements, which provides a unique, comprehensive dataset on the evolution of DNS over time. Our high-performance infrastructure performs Internet-scale active measurements, currently querying over 50% of the DNS name space on a daily basis. Our infrastructure is designed from the ground up to enable big data analysis approaches on, e.g., a Hadoop cluster. With this novel approach we aim for a quantum leap in DNS-based measurement and analysis of the Internet.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,91–92,2,"big data, active measurements, internet evolution, DNS","London, United Kingdom",SIGCOMM '15,,,,,,
347,article,"Yuan, Zhenlong and Xue, Yibo and van der Schaar, Mihaela",BitMiner: Bits Mining in Internet Traffic Classification,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2789997,10.1145/2829988.2789997,"Traditionally, signatures used for traffic classification are constructed at the byte-level. However, as more and more data-transfer formats of network protocols and applications are encoded at the bit-level, byte-level signatures are losing their effectiveness in traffic classification. In this poster, we creatively construct bit-level signatures by associating the bit-values with their bit-positions in each traffic flow. Furthermore, we present BitMiner, an automated traffic mining tool that can mine application signatures at the most fine-grained bit-level granularity. Our preliminary test on popular peer-to-peer (P2P) applications, e.g. Skype, Google Hangouts, PPTV, eMule, Xunlei and QQDownload, reveals that although they all have no byte-level signatures, there are significant bit-level signatures hidden in their traffic.",,93–94,2,"traffic classification, bits mining, bit-level signatures",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
348,inproceedings,"Yuan, Zhenlong and Xue, Yibo and van der Schaar, Mihaela",BitMiner: Bits Mining in Internet Traffic Classification,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2789997,10.1145/2785956.2789997,"Traditionally, signatures used for traffic classification are constructed at the byte-level. However, as more and more data-transfer formats of network protocols and applications are encoded at the bit-level, byte-level signatures are losing their effectiveness in traffic classification. In this poster, we creatively construct bit-level signatures by associating the bit-values with their bit-positions in each traffic flow. Furthermore, we present BitMiner, an automated traffic mining tool that can mine application signatures at the most fine-grained bit-level granularity. Our preliminary test on popular peer-to-peer (P2P) applications, e.g. Skype, Google Hangouts, PPTV, eMule, Xunlei and QQDownload, reveals that although they all have no byte-level signatures, there are significant bit-level signatures hidden in their traffic.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,93–94,2,"traffic classification, bit-level signatures, bits mining","London, United Kingdom",SIGCOMM '15,,,,,,
349,article,"Rifai, Myriana and Lopez-Pacheco, Dino and Urvoy-Keller, Guillaume",Coarse-Grained Scheduling with Software-Defined Networking Switches,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790004,10.1145/2829988.2790004,"Software-Defined Networking (SDN) enables consolidation of the control plane of a set of network equipments with a fine-grained control of traffic flows inside the network. In this work, we demonstrate that some coarse-grained scheduling mechanisms can be easily offered by SDN switches without requiring any unsupported operation in OpenFlow. We leverage the feedback loop - flow statistics - exposed by SDN switches to the controller, combined with priority queuing mechanisms, usually available in typical switches on their output ports. We illustrate our approach through experimentations with an OpenvSwitch SDN switch controlled by a Beacon controller.",,95–96,2,,,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
350,inproceedings,"Rifai, Myriana and Lopez-Pacheco, Dino and Urvoy-Keller, Guillaume",Coarse-Grained Scheduling with Software-Defined Networking Switches,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790004,10.1145/2785956.2790004,"Software-Defined Networking (SDN) enables consolidation of the control plane of a set of network equipments with a fine-grained control of traffic flows inside the network. In this work, we demonstrate that some coarse-grained scheduling mechanisms can be easily offered by SDN switches without requiring any unsupported operation in OpenFlow. We leverage the feedback loop - flow statistics - exposed by SDN switches to the controller, combined with priority queuing mechanisms, usually available in typical switches on their output ports. We illustrate our approach through experimentations with an OpenvSwitch SDN switch controlled by a Beacon controller.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,95–96,2,,"London, United Kingdom",SIGCOMM '15,,,,,,
351,article,"Bao, Jinzhen and Dong, Dezun and Zhao, Baokang and Luo, Zhang and Wu, Chunqing and Gong, Zhenghu",FlyCast: Free-Space Optics Accelerating Multicast Communications in Physical Layer,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790002,10.1145/2829988.2790002,"In this paper, we propose FlyCast, an architecture using the physical layer of free-space optics (FSO) to accelerate multicast communication. FlyCast leverages off-the-shelf devices (e.g. switchable mirror, beam splitter) to physically split the FSO beam to multi receivers on demand, which enables to build dynamical multicast trees in physical layer and accelerates multicast communications. We demonstrate the feasibility of FlyCast through our theoretical analysis and the proof-of-concept prototype.",,97–98,2,"data center network, multicast, free space optics",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
352,inproceedings,"Bao, Jinzhen and Dong, Dezun and Zhao, Baokang and Luo, Zhang and Wu, Chunqing and Gong, Zhenghu",FlyCast: Free-Space Optics Accelerating Multicast Communications in Physical Layer,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790002,10.1145/2785956.2790002,"In this paper, we propose FlyCast, an architecture using the physical layer of free-space optics (FSO) to accelerate multicast communication. FlyCast leverages off-the-shelf devices (e.g. switchable mirror, beam splitter) to physically split the FSO beam to multi receivers on demand, which enables to build dynamical multicast trees in physical layer and accelerates multicast communications. We demonstrate the feasibility of FlyCast through our theoretical analysis and the proof-of-concept prototype.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,97–98,2,"multicast, data center network, free space optics","London, United Kingdom",SIGCOMM '15,,,,,,
353,article,"Jeong, Seong Hoon and Kang, Ah Reum and Kim, Huy Kang",Analysis of Game Bot's Behavioral Characteristics in Social Interaction Networks of MMORPG,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790005,10.1145/2829988.2790005,"MMORPG (Massively Multiplayer Online Role-Playing Game) is one of the best platforms to observe human's behaviors. In collaboration with a leading online game company, NCSoft, we can observe all behaviors in a large-scale of commercialized MMORPG. Especially, we analyzed the behavioral differences between game bots and human users. We categorized the five groups, Bot-Bot, Bot-All, Human-Human, Human-All and All-All, and we observe the characteristics of six social interaction networks for each group. As a result, we found that there are significant differences in social behaviors between game bots and human.",,99–100,2,"social network analysis, game bot, massively multiplayer online game",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
354,inproceedings,"Jeong, Seong Hoon and Kang, Ah Reum and Kim, Huy Kang",Analysis of Game Bot's Behavioral Characteristics in Social Interaction Networks of MMORPG,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790005,10.1145/2785956.2790005,"MMORPG (Massively Multiplayer Online Role-Playing Game) is one of the best platforms to observe human's behaviors. In collaboration with a leading online game company, NCSoft, we can observe all behaviors in a large-scale of commercialized MMORPG. Especially, we analyzed the behavioral differences between game bots and human users. We categorized the five groups, Bot-Bot, Bot-All, Human-Human, Human-All and All-All, and we observe the characteristics of six social interaction networks for each group. As a result, we found that there are significant differences in social behaviors between game bots and human.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,99–100,2,"massively multiplayer online game, game bot, social network analysis","London, United Kingdom",SIGCOMM '15,,,,,,
355,article,"Wu, Haibo and Li, Jun and Zhi, Jiang",Could End System Caching and Cooperation Replace In-Network Caching in CCN?,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790006,10.1145/2829988.2790006,"CCN has been witnessed as a promising future Internet architecture. In-network caching has been paid much attention, but there is still no consensus on its usage, due to its non-negligible costs. Meanwhile, massive storage and bandwidth resources of end systems still remain underutilized. To this end, we present an End System Caching and Cooperation scheme in CCN, called ESCC to realize content distribution of CCN, without using costly in-network caching. ESCC enables fast content distribution through clients caching and sharing contents with each other. Experiments show that ESCC can achieve better performance than the universal caching. It is also quite simple, efficient, robust and has low overhead. ESCC could be a candidate substitute for the costly and unnecessary universal caching.",,101–102,2,"end system, caching, cooperation, CCN",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
356,inproceedings,"Wu, Haibo and Li, Jun and Zhi, Jiang",Could End System Caching and Cooperation Replace In-Network Caching in CCN?,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790006,10.1145/2785956.2790006,"CCN has been witnessed as a promising future Internet architecture. In-network caching has been paid much attention, but there is still no consensus on its usage, due to its non-negligible costs. Meanwhile, massive storage and bandwidth resources of end systems still remain underutilized. To this end, we present an End System Caching and Cooperation scheme in CCN, called ESCC to realize content distribution of CCN, without using costly in-network caching. ESCC enables fast content distribution through clients caching and sharing contents with each other. Experiments show that ESCC can achieve better performance than the universal caching. It is also quite simple, efficient, robust and has low overhead. ESCC could be a candidate substitute for the costly and unnecessary universal caching.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,101–102,2,"CCN, cooperation, caching, end system","London, United Kingdom",SIGCOMM '15,,,,,,
357,article,"Sides, Mor and Bremler-Barr, Anat and Rosensweig, Elisha",Yo-Yo Attack: Vulnerability In Auto-Scaling Mechanism,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790017,10.1145/2829988.2790017,,,103–104,2,"auto-scaling, cloud attack",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
358,inproceedings,"Sides, Mor and Bremler-Barr, Anat and Rosensweig, Elisha",Yo-Yo Attack: Vulnerability In Auto-Scaling Mechanism,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790017,10.1145/2785956.2790017,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,103–104,2,"cloud attack, auto-scaling","London, United Kingdom",SIGCOMM '15,,,,,,
359,article,Szab\'{o,Towards the 5G Revolution: A Software Defined Network Architecture Exploiting Network Coding as a Service,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790025,10.1145/2829988.2790025,"Many networking visioners agree that 5G will be much more than the incremental improvement, in terms of data rate, of 4G. Besides the mobile networks, 5G will fundamentally influence the core infrastructure as well. In our vision the realization of the challenging promises of 5G (e.g. extremely fast, low-overhead, low-delay access of mostly cloudified services and content) will require the massive use of multipathing equipped with low overhead transport solutions tailored to fast, reliable and secure data retrieval from cloud architectures. In this demo we present a prototype architecture supporting such services by making use of automatically configured multipath service chains implementing network coding based transport solutions over off-the-shelf software defined networking (SDN) components.",,105–106,2,"NFV, SDN, mininet, network coding",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
360,inproceedings,Szab\'{o,Towards the 5G Revolution: A Software Defined Network Architecture Exploiting Network Coding as a Service,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790025,10.1145/2785956.2790025,"Many networking visioners agree that 5G will be much more than the incremental improvement, in terms of data rate, of 4G. Besides the mobile networks, 5G will fundamentally influence the core infrastructure as well. In our vision the realization of the challenging promises of 5G (e.g. extremely fast, low-overhead, low-delay access of mostly cloudified services and content) will require the massive use of multipathing equipped with low overhead transport solutions tailored to fast, reliable and secure data retrieval from cloud architectures. In this demo we present a prototype architecture supporting such services by making use of automatically configured multipath service chains implementing network coding based transport solutions over off-the-shelf software defined networking (SDN) components.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,105–106,2,"network coding, NFV, mininet, SDN","London, United Kingdom",SIGCOMM '15,,,,,,
361,article,"Reuter, Andreas and W\""{a",RPKI MIRO: Monitoring and Inspection of RPKI Objects,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790026,10.1145/2829988.2790026,"The Resource Public Key Infrastructure (RPKI) stores attestation objects for Internet resources. In this demo, we present RPKI MIRO, an open source software framework to monitor and inspect these RPKI objects. RPKI MIRO provides resource owners, RPKI operators, researchers, and lecturers with intuitive access to the content of the deployed RPKI repositories. It helps to optimize the repository structure and to identify failures.",,107–108,2,"secure inter-domain routing, PKI monitoring, RPKI measurement",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
362,inproceedings,"Reuter, Andreas and W\""{a",RPKI MIRO: Monitoring and Inspection of RPKI Objects,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790026,10.1145/2785956.2790026,"The Resource Public Key Infrastructure (RPKI) stores attestation objects for Internet resources. In this demo, we present RPKI MIRO, an open source software framework to monitor and inspect these RPKI objects. RPKI MIRO provides resource owners, RPKI operators, researchers, and lecturers with intuitive access to the content of the deployed RPKI repositories. It helps to optimize the repository structure and to identify failures.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,107–108,2,"PKI monitoring, secure inter-domain routing, RPKI measurement","London, United Kingdom",SIGCOMM '15,,,,,,
363,article,"Yau, Simon and Ge, Liang and Hsieh, Ping-Chun and Hou, I-Hong and Cui, Shuguang and Kumar, P.R. and Ekbal, Amal and Kundargi, Nikhil",WiMAC: Rapid Implementation Platform for User Definable MAC Protocols Through Separation,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790031,10.1145/2829988.2790031,"This demo presents WiMAC, a general-purpose wireless testbed for researchers to quickly prototype a wide variety of real-time MAC protocols for wireless networks. As the interface between the link layer and the physical layer, MAC protocols are often tightly coupled with the underlying physical layer, and need to have extremely small latencies. Implementing a new MAC requires a long time. In fact, very few MACs have ever been implemented, even though dozens of new MAC protocols have been proposed. To enable quick prototyping, we employ the mechanism vs. policy separation to decompose the functionality in the MAC layer and the PHY layer. Built on the separation framework, WiMAC achieves the independence of the software from the hardware, offering a high degree of function reuse and design flexibility. Hence, our platform not only supports easy cross-layer design but also allows protocol changes on the fly. Following the 802.11-like reference design, we demonstrate that deploying a new MAC protocol is quick and simple on the proposed platform through the implementation of the CSMA/CA and CHAIN protocols.",,109–110,2,"software-defined radio, MAC, wireless testbed",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
364,inproceedings,"Yau, Simon and Ge, Liang and Hsieh, Ping-Chun and Hou, I-Hong and Cui, Shuguang and Kumar, P.R. and Ekbal, Amal and Kundargi, Nikhil",WiMAC: Rapid Implementation Platform for User Definable MAC Protocols Through Separation,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790031,10.1145/2785956.2790031,"This demo presents WiMAC, a general-purpose wireless testbed for researchers to quickly prototype a wide variety of real-time MAC protocols for wireless networks. As the interface between the link layer and the physical layer, MAC protocols are often tightly coupled with the underlying physical layer, and need to have extremely small latencies. Implementing a new MAC requires a long time. In fact, very few MACs have ever been implemented, even though dozens of new MAC protocols have been proposed. To enable quick prototyping, we employ the mechanism vs. policy separation to decompose the functionality in the MAC layer and the PHY layer. Built on the separation framework, WiMAC achieves the independence of the software from the hardware, offering a high degree of function reuse and design flexibility. Hence, our platform not only supports easy cross-layer design but also allows protocol changes on the fly. Following the 802.11-like reference design, we demonstrate that deploying a new MAC protocol is quick and simple on the proposed platform through the implementation of the CSMA/CA and CHAIN protocols.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,109–110,2,"MAC, wireless testbed, software-defined radio","London, United Kingdom",SIGCOMM '15,,,,,,
365,article,"Raza, Ali and Zaki, Yasir and P\""{o",Extreme Web Caching for Faster Web Browsing,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790032,10.1145/2829988.2790032,"Modern web pages are very complex; each web page consists of hundreds of objects that are linked from various servers all over the world. While mechanisms such as caching reduce the overall number of end-to-end requests saving bandwidth and loading time, there is still a large portion of content that is re-fetched -- despite not having changed. In this demo, we present Extreme Cache, a web caching architecture that enhances the web browsing experience through a smart pre-fetching engine. Our extreme cache tries to predict the rate of change of web page objects to bring cacheable content closer to the user.",,111–112,2,"max-age, caching, HTTP",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
366,inproceedings,"Raza, Ali and Zaki, Yasir and P\""{o",Extreme Web Caching for Faster Web Browsing,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790032,10.1145/2785956.2790032,"Modern web pages are very complex; each web page consists of hundreds of objects that are linked from various servers all over the world. While mechanisms such as caching reduce the overall number of end-to-end requests saving bandwidth and loading time, there is still a large portion of content that is re-fetched -- despite not having changed. In this demo, we present Extreme Cache, a web caching architecture that enhances the web browsing experience through a smart pre-fetching engine. Our extreme cache tries to predict the rate of change of web page objects to bring cacheable content closer to the user.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,111–112,2,"caching, HTTP, max-age","London, United Kingdom",SIGCOMM '15,,,,,,
367,article,"Ernits, Margus and Tammek\""{a",I-Tee: A Fully Automated Cyber Defense Competition for Students,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790033,10.1145/2829988.2790033,"We present an Intelligent Training Exercise Environment (i-tee), a fully automated Cyber Defense Competition platform. The main features of i-tee are: automated attacks, automated scoring with immediate feedback using a scoreboard, and background traffic generation. The main advantage of this platform is easy integration into existing curricula and suitability for continuous education as well as on-site training at companies. This platform implements a modular approach called learning spaces for implementing different competitions and hands-on labs. The platform is highly automated to enable execution with up to 30 teams by one person using a single server. The platform is publicly available under MIT license.",,113–114,2,"cyber security exercises, virtual networks, auto-configuration",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
368,inproceedings,"Ernits, Margus and Tammek\""{a",I-Tee: A Fully Automated Cyber Defense Competition for Students,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790033,10.1145/2785956.2790033,"We present an Intelligent Training Exercise Environment (i-tee), a fully automated Cyber Defense Competition platform. The main features of i-tee are: automated attacks, automated scoring with immediate feedback using a scoreboard, and background traffic generation. The main advantage of this platform is easy integration into existing curricula and suitability for continuous education as well as on-site training at companies. This platform implements a modular approach called learning spaces for implementing different competitions and hands-on labs. The platform is highly automated to enable execution with up to 30 teams by one person using a single server. The platform is publicly available under MIT license.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,113–114,2,"cyber security exercises, auto-configuration, virtual networks","London, United Kingdom",SIGCOMM '15,,,,,,
369,article,"W\""{a",See How ISPs Care: An RPKI Validation Extension for Web Browsers,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790034,10.1145/2829988.2790034,"The Resource Public Key Infrastructure (RPKI) allows BGP routers to verify the origin AS of an IP prefix. In this demo, we present a software extension which performs prefix origin validation in the web browser of end users. The browser extension shows the RPKI validation outcome of the web server infrastructure for the requested web domain. It follows the common plug-in concepts and does not require special modifications of the browser software. It operates on live data and helps end users as well as operators to gain better insight into the Internet security landscape.",,115–116,2,"web, RPKI, secure inter-domain routing, BGP, deployment",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
370,inproceedings,"W\""{a",See How ISPs Care: An RPKI Validation Extension for Web Browsers,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790034,10.1145/2785956.2790034,"The Resource Public Key Infrastructure (RPKI) allows BGP routers to verify the origin AS of an IP prefix. In this demo, we present a software extension which performs prefix origin validation in the web browser of end users. The browser extension shows the RPKI validation outcome of the web server infrastructure for the requested web domain. It follows the common plug-in concepts and does not require special modifications of the browser software. It operates on live data and helps end users as well as operators to gain better insight into the Internet security landscape.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,115–116,2,"deployment, RPKI, web, secure inter-domain routing, BGP","London, United Kingdom",SIGCOMM '15,,,,,,
371,article,"Schulz-Zander, Julius and Mayer, Carlos and Ciobotaru, Bogdan and Schmid, Stefan and Feldmann, Anja and Riggio, Roberto",Programming the Home and Enterprise WiFi with OpenSDWN,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790037,10.1145/2829988.2790037,"The quickly growing demand for wireless networks and the numerous application-specific requirements stand in stark contrast to today's inflexible management and operation of WiFi networks. In this paper, we present and evaluate OpenSDWN, a novel WiFi architecture based on an SDN/NFV approach. OpenSDWN exploits datapath programmability to enable service differentiation and fine-grained transmission control, facilitating the prioritization of critical applications. OpenSDWN implements per-client virtual access points and per-client virtual middleboxes, to render network functions more flexible and support mobility and seamless migration. OpenSDWN can also be used to out-source the control over the home network to a participatory interface or to an Internet Service Provider.",,117–118,2,"programmable ran, wi-fi, network function virtualization, enterprise wlans, software-defined networking",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
372,inproceedings,"Schulz-Zander, Julius and Mayer, Carlos and Ciobotaru, Bogdan and Schmid, Stefan and Feldmann, Anja and Riggio, Roberto",Programming the Home and Enterprise WiFi with OpenSDWN,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790037,10.1145/2785956.2790037,"The quickly growing demand for wireless networks and the numerous application-specific requirements stand in stark contrast to today's inflexible management and operation of WiFi networks. In this paper, we present and evaluate OpenSDWN, a novel WiFi architecture based on an SDN/NFV approach. OpenSDWN exploits datapath programmability to enable service differentiation and fine-grained transmission control, facilitating the prioritization of critical applications. OpenSDWN implements per-client virtual access points and per-client virtual middleboxes, to render network functions more flexible and support mobility and seamless migration. OpenSDWN can also be used to out-source the control over the home network to a participatory interface or to an Internet Service Provider.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,117–118,2,"network function virtualization, enterprise wlans, software-defined networking, programmable ran, wi-fi","London, United Kingdom",SIGCOMM '15,,,,,,
373,article,"Hamed, Ezzeldin and Rahul, Hariharan and Abdelghany, Mohammed A. and Katabi, Dina",A Real-Time 802.11 Compatible Distributed MIMO System,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790042,10.1145/2829988.2790042,"We present a demonstration of a real-time distributed MIMO system, DMIMO. DMIMO synchronizes transmissions from 4 distributed MIMO transmitters in time, frequency and phase, and performs distributed multi-user beamforming to independent clients. DMIMO is built on top of a Zynq hardware platform integrated with an FMCOMMS2 RF front end. The platform implements a custom 802.11n compatible MIMO PHY layer which is augmented with a lightweight distributed synchronization engine. The demonstration shows the received constellation points, channels, and effective data throughput at each client. It also shows how these vary as a function of interference, the timeliness of channel feedback, and the transmission rates used by the different transmitters.",,119–120,2,"wireless networks, multi-user MIMO, MIMO, signal processing, 802.11, network protocols, wireless access points, distributed MIMO",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
374,inproceedings,"Hamed, Ezzeldin and Rahul, Hariharan and Abdelghany, Mohammed A. and Katabi, Dina",A Real-Time 802.11 Compatible Distributed MIMO System,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790042,10.1145/2785956.2790042,"We present a demonstration of a real-time distributed MIMO system, DMIMO. DMIMO synchronizes transmissions from 4 distributed MIMO transmitters in time, frequency and phase, and performs distributed multi-user beamforming to independent clients. DMIMO is built on top of a Zynq hardware platform integrated with an FMCOMMS2 RF front end. The platform implements a custom 802.11n compatible MIMO PHY layer which is augmented with a lightweight distributed synchronization engine. The demonstration shows the received constellation points, channels, and effective data throughput at each client. It also shows how these vary as a function of interference, the timeliness of channel feedback, and the transmission rates used by the different transmitters.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,119–120,2,"MIMO, distributed MIMO, 802.11, network protocols, wireless networks, wireless access points, multi-user MIMO, signal processing","London, United Kingdom",SIGCOMM '15,,,,,,
375,article,"Vasisht, Deepak and Kumar, Swarun and Katabi, Dina",Sub-Nanosecond Time of Flight on Commercial Wi-Fi Cards,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790043,10.1145/2829988.2790043,"The time-of-flight of a signal captures the time it takes to propagate from a transmitter to a receiver. Time-of-flight is perhaps the most intuitive method for localization using wireless signals. If one can accurately measure the time-of-flight from a transmitter, one can compute the transmitter's distance simply by multiplying the time-of-flight by the speed of light. Today, GPS, the most widely used outdoor localization system, localizes a device using the time-of-flight of radio signals from satellites. However, applying the same concept to indoor localization has proven difficult. Systems for localization in indoor spaces are expected to deliver high accuracy (e.g., a meter or less) using consumer-oriented technologies (e.g., Wi-Fi on one's cellphone). Unfortunately, past work could not measure time-of-flight at such an accuracy on Wi-Fi devices. As a result, over the years, research on accurate indoor positioning has moved towards more complex alternatives such as employing large multi-antenna arrays to compute the angle-of-arrival of the signal. These new techniques have delivered highly accurate indoor localization systems. Despite these advances, time-of-flight based localization has some of the basic desirable features that state-of-the-art indoor localization systems lack. In particular, measuring time-of-flight does not require more than a single antenna on the receiver. In fact, by measuring time-of-flight of a signal to just two antennas, a receiver can intersect the corresponding distances to locate its source. Thus, a receiver can locate a wireless transmitter with no support from the surrounding infrastructure. This is quite unlike current indoor localization systems, which require multiple access points at known locations, to find the distance between a pair of mobile devices. Furthermore, each of these access points need to have many antennas -- far beyond what is supported in commercial Wi-Fi devices.In this demo, we will present Chronos, a system that combines a set of novel algorithms to measure the time-of-flight to sub-nanosecond accuracy on commercial Wi-Fi cards. In particular, we will measure distance/time-of-flight between two devices equipped with commercial Wi-Fi cards, without any support from the infrastructure or environment fingerprinting.",,121–122,2,"localization, time-of-flight, wireless",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
376,inproceedings,"Vasisht, Deepak and Kumar, Swarun and Katabi, Dina",Sub-Nanosecond Time of Flight on Commercial Wi-Fi Cards,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790043,10.1145/2785956.2790043,"The time-of-flight of a signal captures the time it takes to propagate from a transmitter to a receiver. Time-of-flight is perhaps the most intuitive method for localization using wireless signals. If one can accurately measure the time-of-flight from a transmitter, one can compute the transmitter's distance simply by multiplying the time-of-flight by the speed of light. Today, GPS, the most widely used outdoor localization system, localizes a device using the time-of-flight of radio signals from satellites. However, applying the same concept to indoor localization has proven difficult. Systems for localization in indoor spaces are expected to deliver high accuracy (e.g., a meter or less) using consumer-oriented technologies (e.g., Wi-Fi on one's cellphone). Unfortunately, past work could not measure time-of-flight at such an accuracy on Wi-Fi devices. As a result, over the years, research on accurate indoor positioning has moved towards more complex alternatives such as employing large multi-antenna arrays to compute the angle-of-arrival of the signal. These new techniques have delivered highly accurate indoor localization systems. Despite these advances, time-of-flight based localization has some of the basic desirable features that state-of-the-art indoor localization systems lack. In particular, measuring time-of-flight does not require more than a single antenna on the receiver. In fact, by measuring time-of-flight of a signal to just two antennas, a receiver can intersect the corresponding distances to locate its source. Thus, a receiver can locate a wireless transmitter with no support from the surrounding infrastructure. This is quite unlike current indoor localization systems, which require multiple access points at known locations, to find the distance between a pair of mobile devices. Furthermore, each of these access points need to have many antennas -- far beyond what is supported in commercial Wi-Fi devices.In this demo, we will present Chronos, a system that combines a set of novel algorithms to measure the time-of-flight to sub-nanosecond accuracy on commercial Wi-Fi cards. In particular, we will measure distance/time-of-flight between two devices equipped with commercial Wi-Fi cards, without any support from the infrastructure or environment fingerprinting.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,121–122,2,"wireless, localization, time-of-flight","London, United Kingdom",SIGCOMM '15,,,,,,
377,inproceedings,"Banerjee, Sujata",Session Details: Experience Track 1,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3261000,10.1145/3261000,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,,,,"London, United Kingdom",SIGCOMM '15,,,,,,
378,article,"Roy, Arjun and Zeng, Hongyi and Bagga, Jasmeet and Porter, George and Snoeren, Alex C.",Inside the Social Network's (Datacenter) Network,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787472,10.1145/2829988.2787472,"Large cloud service providers have invested in increasingly larger datacenters to house the computing infrastructure required to support their services. Accordingly, researchers and industry practitioners alike have focused a great deal of effort designing network fabrics to efficiently interconnect and manage the traffic within these datacenters in performant yet efficient fashions. Unfortunately, datacenter operators are generally reticent to share the actual requirements of their applications, making it challenging to evaluate the practicality of any particular design.Moreover, the limited large-scale workload information available in the literature has, for better or worse, heretofore largely been provided by a single datacenter operator whose use cases may not be widespread. In this work, we report upon the network traffic observed in some of Facebook's datacenters. While Facebook operates a number of traditional datacenter services like Hadoop, its core Web service and supporting cache infrastructure exhibit a number of behaviors that contrast with those reported in the literature. We report on the contrasting locality, stability, and predictability of network traffic in Facebook's datacenters, and comment on their implications for network architecture, traffic engineering, and switch design.",,123–137,15,datacenter traffic patterns,,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
379,inproceedings,"Roy, Arjun and Zeng, Hongyi and Bagga, Jasmeet and Porter, George and Snoeren, Alex C.",Inside the Social Network's (Datacenter) Network,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787472,10.1145/2785956.2787472,"Large cloud service providers have invested in increasingly larger datacenters to house the computing infrastructure required to support their services. Accordingly, researchers and industry practitioners alike have focused a great deal of effort designing network fabrics to efficiently interconnect and manage the traffic within these datacenters in performant yet efficient fashions. Unfortunately, datacenter operators are generally reticent to share the actual requirements of their applications, making it challenging to evaluate the practicality of any particular design.Moreover, the limited large-scale workload information available in the literature has, for better or worse, heretofore largely been provided by a single datacenter operator whose use cases may not be widespread. In this work, we report upon the network traffic observed in some of Facebook's datacenters. While Facebook operates a number of traditional datacenter services like Hadoop, its core Web service and supporting cache infrastructure exhibit a number of behaviors that contrast with those reported in the literature. We report on the contrasting locality, stability, and predictability of network traffic in Facebook's datacenters, and comment on their implications for network architecture, traffic engineering, and switch design.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,123–137,15,datacenter traffic patterns,"London, United Kingdom",SIGCOMM '15,,,,,,
380,article,"Guo, Chuanxiong and Yuan, Lihua and Xiang, Dong and Dang, Yingnong and Huang, Ray and Maltz, Dave and Liu, Zhaoyi and Wang, Vin and Pang, Bin and Chen, Hua and Lin, Zhi-Wei and Kurien, Varugis",Pingmesh: A Large-Scale System for Data Center Network Latency Measurement and Analysis,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787496,10.1145/2829988.2787496,"Can we get network latency between any two servers at any time in large-scale data center networks? The collected latency data can then be used to address a series of challenges: telling if an application perceived latency issue is caused by the network or not, defining and tracking network service level agreement (SLA), and automatic network troubleshooting. We have developed the Pingmesh system for large-scale data center network latency measurement and analysis to answer the above question affirmatively. Pingmesh has been running in Microsoft data centers for more than four years, and it collects tens of terabytes of latency data per day. Pingmesh is widely used by not only network software developers and engineers, but also application and service developers and operators.",,139–152,14,"network troubleshooting, silent packet drops, data center networking",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
381,inproceedings,"Guo, Chuanxiong and Yuan, Lihua and Xiang, Dong and Dang, Yingnong and Huang, Ray and Maltz, Dave and Liu, Zhaoyi and Wang, Vin and Pang, Bin and Chen, Hua and Lin, Zhi-Wei and Kurien, Varugis",Pingmesh: A Large-Scale System for Data Center Network Latency Measurement and Analysis,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787496,10.1145/2785956.2787496,"Can we get network latency between any two servers at any time in large-scale data center networks? The collected latency data can then be used to address a series of challenges: telling if an application perceived latency issue is caused by the network or not, defining and tracking network service level agreement (SLA), and automatic network troubleshooting. We have developed the Pingmesh system for large-scale data center network latency measurement and analysis to answer the above question affirmatively. Pingmesh has been running in Microsoft data centers for more than four years, and it collects tens of terabytes of latency data per day. Pingmesh is widely used by not only network software developers and engineers, but also application and service developers and operators.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,139–152,14,"data center networking, network troubleshooting, silent packet drops","London, United Kingdom",SIGCOMM '15,,,,,,
382,inproceedings,"Banerjee, Sujata",Session Details: Experience Track 2,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3261001,10.1145/3261001,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,,,,"London, United Kingdom",SIGCOMM '15,,,,,,
383,article,"Biswas, Sanjit and Bicket, John and Wong, Edmund and Musaloiu-E, Raluca and Bhartia, Apurv and Aguayo, Dan",Large-Scale Measurements of Wireless Network Behavior,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787489,10.1145/2829988.2787489,"Meraki is a cloud-based network management system which provides centralized configuration, monitoring, and network troubleshooting tools across hundreds of thousands of sites worldwide. As part of its architecture, the Meraki system has built a database of time-series measurements of wireless link, client, and application behavior for monitoring and debugging purposes. This paper studies an anonymized subset of measurements, containing data from approximately ten thousand radio access points, tens of thousands of links, and 5.6 million clients from one-week periods in January 2014 and January 2015 to provide a deeper understanding of real-world network behavior. This paper observes the following phenomena: wireless network usage continues to grow quickly, driven most by growth in the number of devices connecting to each network. Intermediate link delivery rates are common indoors across a wide range of deployment environments. Typical access points share spectrum with dozens of nearby networks, but the presence of a network on a channel does not predict channel utilization. Most access points see 2.4 GHz channel utilization of 20% or more, with the top decile seeing greater than 50%, and the majority of the channel use contains decodable 802.11 headers.",,153–165,13,"network usage data, 802.11, large-scale measurements",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
384,inproceedings,"Biswas, Sanjit and Bicket, John and Wong, Edmund and Musaloiu-E, Raluca and Bhartia, Apurv and Aguayo, Dan",Large-Scale Measurements of Wireless Network Behavior,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787489,10.1145/2785956.2787489,"Meraki is a cloud-based network management system which provides centralized configuration, monitoring, and network troubleshooting tools across hundreds of thousands of sites worldwide. As part of its architecture, the Meraki system has built a database of time-series measurements of wireless link, client, and application behavior for monitoring and debugging purposes. This paper studies an anonymized subset of measurements, containing data from approximately ten thousand radio access points, tens of thousands of links, and 5.6 million clients from one-week periods in January 2014 and January 2015 to provide a deeper understanding of real-world network behavior. This paper observes the following phenomena: wireless network usage continues to grow quickly, driven most by growth in the number of devices connecting to each network. Intermediate link delivery rates are common indoors across a wide range of deployment environments. Typical access points share spectrum with dozens of nearby networks, but the presence of a network on a channel does not predict channel utilization. Most access points see 2.4 GHz channel utilization of 20% or more, with the top decile seeing greater than 50%, and the majority of the channel use contains decodable 802.11 headers.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,153–165,13,"large-scale measurements, 802.11, network usage data","London, United Kingdom",SIGCOMM '15,,,,,,
385,article,"Chen, Fangfei and Sitaraman, Ramesh K. and Torres, Marcelo",End-User Mapping: Next Generation Request Routing for Content Delivery,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787500,10.1145/2829988.2787500,"Content Delivery Networks (CDNs) deliver much of the world's web, video, and application content on the Internet today. A key component of a CDN is the mapping system that uses the DNS protocol to route each client's request to a ``proximal'' server that serves the requested content. While traditional mapping systems identify a client using the IP of its name server, we describe our experience in building and rolling-out a novel system called end-user mapping that identifies the client directly by using a prefix of the client's IP address. Using measurements from Akamai's production network during the roll-out, we show that end-user mapping provides significant performance benefits for clients who use public resolvers, including an eight-fold decrease in mapping distance, a two-fold decrease in RTT and content download time, and a 30% improvement in the time-to-first byte. We also quantify the scaling challenges in implementing end-user mapping such as the 8-fold increase in DNS queries. Finally, we show that a CDN with a larger number of deployment locations is likely to benefit more from end-user mapping than a CDN with a smaller number of deployments.",,167–181,15,"network measurement, content delivery networks, request routing, server assignment, web performance, DNS, name servers, load balancing, akamai",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
386,inproceedings,"Chen, Fangfei and Sitaraman, Ramesh K. and Torres, Marcelo",End-User Mapping: Next Generation Request Routing for Content Delivery,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787500,10.1145/2785956.2787500,"Content Delivery Networks (CDNs) deliver much of the world's web, video, and application content on the Internet today. A key component of a CDN is the mapping system that uses the DNS protocol to route each client's request to a ``proximal'' server that serves the requested content. While traditional mapping systems identify a client using the IP of its name server, we describe our experience in building and rolling-out a novel system called end-user mapping that identifies the client directly by using a prefix of the client's IP address. Using measurements from Akamai's production network during the roll-out, we show that end-user mapping provides significant performance benefits for clients who use public resolvers, including an eight-fold decrease in mapping distance, a two-fold decrease in RTT and content download time, and a 30% improvement in the time-to-first byte. We also quantify the scaling challenges in implementing end-user mapping such as the 8-fold increase in DNS queries. Finally, we show that a CDN with a larger number of deployment locations is likely to benefit more from end-user mapping than a CDN with a smaller number of deployments.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,167–181,15,"akamai, server assignment, DNS, request routing, load balancing, content delivery networks, network measurement, name servers, web performance","London, United Kingdom",SIGCOMM '15,,,,,,
387,article,"Singh, Arjun and Ong, Joon and Agarwal, Amit and Anderson, Glen and Armistead, Ashby and Bannon, Roy and Boving, Seb and Desai, Gaurav and Felderman, Bob and Germano, Paulie and Kanagala, Anand and Provost, Jeff and Simmons, Jason and Tanda, Eiichi and Wanderer, Jim and H\""{o",Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google's Datacenter Network,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787508,10.1145/2829988.2787508,"We present our approach for overcoming the cost, operational complexity, and limited scale endemic to datacenter networks a decade ago. Three themes unify the five generations of datacenter networks detailed in this paper. First, multi-stage Clos topologies built from commodity switch silicon can support cost-effective deployment of building-scale networks. Second, much of the general, but complex, decentralized network routing and management protocols supporting arbitrary deployment scenarios were overkill for single-operator, pre-planned datacenter networks. We built a centralized control mechanism based on a global configuration pushed to all datacenter switches. Third, modular hardware design coupled with simple, robust software allowed our design to also support inter-cluster and wide-area networks. Our datacenter networks run at dozens of sites across the planet, scaling in capacity by 100x over ten years to more than 1Pbps of bisection bandwidth.",,183–197,15,"merchant silicon, datacenter networks, clos topology, centralized control and management",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
388,inproceedings,"Singh, Arjun and Ong, Joon and Agarwal, Amit and Anderson, Glen and Armistead, Ashby and Bannon, Roy and Boving, Seb and Desai, Gaurav and Felderman, Bob and Germano, Paulie and Kanagala, Anand and Provost, Jeff and Simmons, Jason and Tanda, Eiichi and Wanderer, Jim and H\""{o",Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google's Datacenter Network,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787508,10.1145/2785956.2787508,"We present our approach for overcoming the cost, operational complexity, and limited scale endemic to datacenter networks a decade ago. Three themes unify the five generations of datacenter networks detailed in this paper. First, multi-stage Clos topologies built from commodity switch silicon can support cost-effective deployment of building-scale networks. Second, much of the general, but complex, decentralized network routing and management protocols supporting arbitrary deployment scenarios were overkill for single-operator, pre-planned datacenter networks. We built a centralized control mechanism based on a global configuration pushed to all datacenter switches. Third, modular hardware design coupled with simple, robust software allowed our design to also support inter-cluster and wide-area networks. Our datacenter networks run at dozens of sites across the planet, scaling in capacity by 100x over ten years to more than 1Pbps of bisection bandwidth.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,183–197,15,"clos topology, centralized control and management, datacenter networks, merchant silicon","London, United Kingdom",SIGCOMM '15,,,,,,
389,inproceedings,"Sekar, Vyas",Session Details: Middleboxes,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3261002,10.1145/3261002,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,,,,"London, United Kingdom",SIGCOMM '15,,,,,,
390,article,"Naylor, David and Schomp, Kyle and Varvello, Matteo and Leontiadis, Ilias and Blackburn, Jeremy and L\'{o",Multi-Context TLS (McTLS): Enabling Secure In-Network Functionality in TLS,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787482,10.1145/2829988.2787482,"A significant fraction of Internet traffic is now encrypted and HTTPS will likely be the default in HTTP/2. However, Transport Layer Security (TLS), the standard protocol for encryption in the Internet, assumes that all functionality resides at the endpoints, making it impossible to use in-network services that optimize network resource usage, improve user experience, and protect clients and servers from security threats. Re-introducing in-network functionality into TLS sessions today is done through hacks, often weakening overall security.In this paper we introduce multi-context TLS (mcTLS), which extends TLS to support middleboxes. mcTLS breaks the current ""all-or-nothing"" security model by allowing endpoints and content providers to explicitly introduce middleboxes in secure end-to-end sessions while controlling which parts of the data they can read or write.We evaluate a prototype mcTLS implementation in both controlled and ""live"" experiments, showing that its benefits come at the cost of minimal overhead. More importantly, we show that mcTLS can be incrementally deployed and requires only small changes to client, server, and middlebox software.",,199–212,14,"ssl, tls, https",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
391,inproceedings,"Naylor, David and Schomp, Kyle and Varvello, Matteo and Leontiadis, Ilias and Blackburn, Jeremy and L\'{o",Multi-Context TLS (McTLS): Enabling Secure In-Network Functionality in TLS,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787482,10.1145/2785956.2787482,"A significant fraction of Internet traffic is now encrypted and HTTPS will likely be the default in HTTP/2. However, Transport Layer Security (TLS), the standard protocol for encryption in the Internet, assumes that all functionality resides at the endpoints, making it impossible to use in-network services that optimize network resource usage, improve user experience, and protect clients and servers from security threats. Re-introducing in-network functionality into TLS sessions today is done through hacks, often weakening overall security.In this paper we introduce multi-context TLS (mcTLS), which extends TLS to support middleboxes. mcTLS breaks the current ""all-or-nothing"" security model by allowing endpoints and content providers to explicitly introduce middleboxes in secure end-to-end sessions while controlling which parts of the data they can read or write.We evaluate a prototype mcTLS implementation in both controlled and ""live"" experiments, showing that its benefits come at the cost of minimal overhead. More importantly, we show that mcTLS can be incrementally deployed and requires only small changes to client, server, and middlebox software.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,199–212,14,"https, ssl, tls","London, United Kingdom",SIGCOMM '15,,,,,,
392,article,"Sherry, Justine and Lan, Chang and Popa, Raluca Ada and Ratnasamy, Sylvia",BlindBox: Deep Packet Inspection over Encrypted Traffic,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787502,10.1145/2829988.2787502,"Many network middleboxes perform deep packet inspection (DPI), a set of useful tasks which examine packet payloads. These tasks include intrusion detection (IDS), exfiltration detection, and parental filtering. However, a long-standing issue is that once packets are sent over HTTPS, middleboxes can no longer accomplish their tasks because the payloads are encrypted. Hence, one is faced with the choice of only one of two desirable properties: the functionality of middleboxes and the privacy of encryption. We propose BlindBox, the first system that simultaneously provides {em both",,213–226,14,"middlebox privacy, network privacy, searchable encryption",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
393,inproceedings,"Sherry, Justine and Lan, Chang and Popa, Raluca Ada and Ratnasamy, Sylvia",BlindBox: Deep Packet Inspection over Encrypted Traffic,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787502,10.1145/2785956.2787502,"Many network middleboxes perform deep packet inspection (DPI), a set of useful tasks which examine packet payloads. These tasks include intrusion detection (IDS), exfiltration detection, and parental filtering. However, a long-standing issue is that once packets are sent over HTTPS, middleboxes can no longer accomplish their tasks because the payloads are encrypted. Hence, one is faced with the choice of only one of two desirable properties: the functionality of middleboxes and the privacy of encryption. We propose BlindBox, the first system that simultaneously provides {em both",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,213–226,14,"middlebox privacy, network privacy, searchable encryption","London, United Kingdom",SIGCOMM '15,,,,,,
394,article,"Sherry, Justine and Gao, Peter Xiang and Basu, Soumya and Panda, Aurojit and Krishnamurthy, Arvind and Maciocco, Christian and Manesh, Maziar and Martins, Jo\~{a",Rollback-Recovery for Middleboxes,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787501,10.1145/2829988.2787501,"Network middleboxes must offer high availability, with automatic failover when a device fails. Achieving high availability is challenging because failover must correctly restore lost state (e.g., activity logs, port mappings) but must do so quickly (e.g., in less than typical transport timeout values to minimize disruption to applications) and with little overhead to failure-free operation (e.g., additional per-packet latencies of 10-100s of us). No existing middlebox design provides failover that is correct, fast to recover, and imposes little increased latency on failure-free operations. We present a new design for fault-tolerance in middleboxes that achieves these three goals. Our system, FTMB (for Fault-Tolerant MiddleBox), adopts the classical approach of ""rollback recovery"" in which a system uses information logged during normal operation to correctly reconstruct state after a failure. However, traditional rollback recovery cannot maintain high throughput given the frequent output rate of middleboxes. Hence, we design a novel solution to record middlebox state which relies on two mechanisms: (1) 'ordered logging', which provides lightweight logging of the information needed after recovery, and (2) a `parallel release' algorithm which, when coupled with ordered logging, ensures that recovery is always correct. We implement ordered logging and parallel release in Click and show that for our test applications our design adds only 30$mu$s of latency to median per packet latencies. Our system introduces moderate throughput overheads (5-30%) and can reconstruct lost state in 40-275ms for practical systems.",,227–240,14,"parallel fault-tolerance, middlebox reliability",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
395,inproceedings,"Sherry, Justine and Gao, Peter Xiang and Basu, Soumya and Panda, Aurojit and Krishnamurthy, Arvind and Maciocco, Christian and Manesh, Maziar and Martins, Jo\~{a",Rollback-Recovery for Middleboxes,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787501,10.1145/2785956.2787501,"Network middleboxes must offer high availability, with automatic failover when a device fails. Achieving high availability is challenging because failover must correctly restore lost state (e.g., activity logs, port mappings) but must do so quickly (e.g., in less than typical transport timeout values to minimize disruption to applications) and with little overhead to failure-free operation (e.g., additional per-packet latencies of 10-100s of us). No existing middlebox design provides failover that is correct, fast to recover, and imposes little increased latency on failure-free operations. We present a new design for fault-tolerance in middleboxes that achieves these three goals. Our system, FTMB (for Fault-Tolerant MiddleBox), adopts the classical approach of ""rollback recovery"" in which a system uses information logged during normal operation to correctly reconstruct state after a failure. However, traditional rollback recovery cannot maintain high throughput given the frequent output rate of middleboxes. Hence, we design a novel solution to record middlebox state which relies on two mechanisms: (1) 'ordered logging', which provides lightweight logging of the information needed after recovery, and (2) a `parallel release' algorithm which, when coupled with ordered logging, ensures that recovery is always correct. We implement ordered logging and parallel release in Click and show that for our test applications our design adds only 30$mu$s of latency to median per packet latencies. Our system introduces moderate throughput overheads (5-30%) and can reconstruct lost state in 40-275ms for practical systems.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,227–240,14,"parallel fault-tolerance, middlebox reliability","London, United Kingdom",SIGCOMM '15,,,,,,
396,article,"Zhou, Dong and Fan, Bin and Lim, Hyeontaek and Andersen, David G. and Kaminsky, Michael and Mitzenmacher, Michael and Wang, Ren and Singh, Ajaypal",Scaling Up Clustered Network Appliances with ScaleBricks,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787503,10.1145/2829988.2787503,"This paper presents ScaleBricks, a new design for building scalable, clustered network appliances that must ""pin"" flow state to a specific handling node without being able to choose which node that should be. ScaleBricks applies a new, compact lookup structure to route packets directly to the appropriate handling node, without incurring the cost of multiple hops across the internal interconnect. Its lookup structure is many times smaller than the alternative approach of fully replicating a forwarding table onto all nodes. As a result, ScaleBricks is able to improve throughput and latency while simultaneously increasing the total number of flows that can be handled by such a cluster. This architecture is effective in practice: Used to optimize packet forwarding in an existing commercial LTE-to-Internet gateway, it increases the throughput of a four-node cluster by 23%, reduces latency by up to 10%, saves memory, and stores up to 5.7x more entries in the forwarding table.",,241–254,14,"scalability, network function virtualization, hashing algorithms",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
397,inproceedings,"Zhou, Dong and Fan, Bin and Lim, Hyeontaek and Andersen, David G. and Kaminsky, Michael and Mitzenmacher, Michael and Wang, Ren and Singh, Ajaypal",Scaling Up Clustered Network Appliances with ScaleBricks,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787503,10.1145/2785956.2787503,"This paper presents ScaleBricks, a new design for building scalable, clustered network appliances that must ""pin"" flow state to a specific handling node without being able to choose which node that should be. ScaleBricks applies a new, compact lookup structure to route packets directly to the appropriate handling node, without incurring the cost of multiple hops across the internal interconnect. Its lookup structure is many times smaller than the alternative approach of fully replicating a forwarding table onto all nodes. As a result, ScaleBricks is able to improve throughput and latency while simultaneously increasing the total number of flows that can be handled by such a cluster. This architecture is effective in practice: Used to optimize packet forwarding in an existing commercial LTE-to-Internet gateway, it increases the throughput of a four-node cluster by 23%, reduces latency by up to 10%, saves memory, and stores up to 5.7x more entries in the forwarding table.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,241–254,14,"network function virtualization, hashing algorithms, scalability","London, United Kingdom",SIGCOMM '15,,,,,,
398,inproceedings,"Jamieson, Kyle",Session Details: Wireless,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3261003,10.1145/3261003,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,,,,"London, United Kingdom",SIGCOMM '15,,,,,,
399,article,"Hu, Pan and Zhang, Pengyu and Ganesan, Deepak",Laissez-Faire: Fully Asymmetric Backscatter Communication,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787477,10.1145/2829988.2787477,"Backscatter provides dual-benefits of energy harvesting and low-power communication, making it attractive to a broad class of wireless sensors. But the design of a protocol that enables extremely power-efficient radios for harvesting-based sensors as well as high-rate data transfer for data-rich sensors presents a conundrum. In this paper, we present a new {em fully asymmetric",,255–267,13,"backscatter, wireless, architecture",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
400,inproceedings,"Hu, Pan and Zhang, Pengyu and Ganesan, Deepak",Laissez-Faire: Fully Asymmetric Backscatter Communication,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787477,10.1145/2785956.2787477,"Backscatter provides dual-benefits of energy harvesting and low-power communication, making it attractive to a broad class of wireless sensors. But the design of a protocol that enables extremely power-efficient radios for harvesting-based sensors as well as high-rate data transfer for data-rich sensors presents a conundrum. In this paper, we present a new {em fully asymmetric",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,255–267,13,"architecture, backscatter, wireless","London, United Kingdom",SIGCOMM '15,,,,,,
401,article,"Kotaru, Manikanta and Joshi, Kiran and Bharadia, Dinesh and Katti, Sachin",SpotFi: Decimeter Level Localization Using WiFi,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787487,10.1145/2829988.2787487,"This paper presents the design and implementation of SpotFi, an accurate indoor localization system that can be deployed on commodity WiFi infrastructure. SpotFi only uses information that is already exposed by WiFi chips and does not require any hardware or firmware changes, yet achieves the same accuracy as state-of-the-art localization systems. SpotFi makes two key technical contributions. First, SpotFi incorporates super-resolution algorithms that can accurately compute the angle of arrival (AoA) of multipath components even when the access point (AP) has only three antennas. Second, it incorporates novel filtering and estimation techniques to identify AoA of direct path between the localization target and AP by assigning values for each path depending on how likely the particular path is the direct path. Our experiments in a multipath rich indoor environment show that SpotFi achieves a median accuracy of 40 cm and is robust to indoor hindrances such as obstacles and multipath.",,269–282,14,"OFDM, internet of things (IOT), wireless, indoor localization, CSI, wifi",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
402,inproceedings,"Kotaru, Manikanta and Joshi, Kiran and Bharadia, Dinesh and Katti, Sachin",SpotFi: Decimeter Level Localization Using WiFi,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787487,10.1145/2785956.2787487,"This paper presents the design and implementation of SpotFi, an accurate indoor localization system that can be deployed on commodity WiFi infrastructure. SpotFi only uses information that is already exposed by WiFi chips and does not require any hardware or firmware changes, yet achieves the same accuracy as state-of-the-art localization systems. SpotFi makes two key technical contributions. First, SpotFi incorporates super-resolution algorithms that can accurately compute the angle of arrival (AoA) of multipath components even when the access point (AP) has only three antennas. Second, it incorporates novel filtering and estimation techniques to identify AoA of direct path between the localization target and AP by assigning values for each path depending on how likely the particular path is the direct path. Our experiments in a multipath rich indoor environment show that SpotFi achieves a median accuracy of 40 cm and is robust to indoor hindrances such as obstacles and multipath.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,269–282,14,"wireless, CSI, OFDM, indoor localization, internet of things (IOT), wifi","London, United Kingdom",SIGCOMM '15,,,,,,
403,article,"Bharadia, Dinesh and Joshi, Kiran Raj and Kotaru, Manikanta and Katti, Sachin",BackFi: High Throughput WiFi Backscatter,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787490,10.1145/2829988.2787490,"We present BackFi, a novel communication system that enables high throughput, long range communication between very low power backscatter devices and WiFi APs using ambient WiFi transmissions as the excitation signal. Specifically, we show that it is possible to design devices and WiFi APs such that the WiFi AP in the process of transmitting data to normal WiFi clients can decode backscatter signals which the devices generate by modulating information on to the ambient WiFi transmission. We show via prototypes and experiments that it is possible to achieve communication rates of up to 5 Mbps at a range of 1 m and 1 Mbps at a range of 5 meters. Such performance is an order to three orders of magnitude better than the best known prior WiFi backscatter system [27,25]. BackFi design is energy efficient, as it relies on backscattering alone and needs insignificant power, hence the energy consumed per bit is small.",,283–296,14,"wifi backscatter, backscatter decoder, full duplex backscatter, ambient backscatter, internet of things (iot), backscatter communication",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
404,inproceedings,"Bharadia, Dinesh and Joshi, Kiran Raj and Kotaru, Manikanta and Katti, Sachin",BackFi: High Throughput WiFi Backscatter,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787490,10.1145/2785956.2787490,"We present BackFi, a novel communication system that enables high throughput, long range communication between very low power backscatter devices and WiFi APs using ambient WiFi transmissions as the excitation signal. Specifically, we show that it is possible to design devices and WiFi APs such that the WiFi AP in the process of transmitting data to normal WiFi clients can decode backscatter signals which the devices generate by modulating information on to the ambient WiFi transmission. We show via prototypes and experiments that it is possible to achieve communication rates of up to 5 Mbps at a range of 1 m and 1 Mbps at a range of 5 meters. Such performance is an order to three orders of magnitude better than the best known prior WiFi backscatter system [27,25]. BackFi design is energy efficient, as it relies on backscattering alone and needs insignificant power, hence the energy consumed per bit is small.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,283–296,14,"full duplex backscatter, backscatter decoder, backscatter communication, wifi backscatter, ambient backscatter, internet of things (iot)","London, United Kingdom",SIGCOMM '15,,,,,,
405,article,"Abari, Omid and Vasisht, Deepak and Katabi, Dina and Chandrakasan, Anantha",Caraoke: An E-Toll Transponder Network for Smart Cities,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787504,10.1145/2829988.2787504,"Electronic toll collection transponders, e.g., E-ZPass, are a widely-used wireless technology. About 70% to 89% of the cars in US have these devices, and some states plan to make them mandatory. As wireless devices however, they lack a basic function: a MAC protocol that prevents collisions. Hence, today, they can be queried only with directional antennas in isolated spots. However, if one could interact with e-toll transponders anywhere in the city despite collisions, it would enable many smart applications. For example, the city can query the transponders to estimate the vehicle flow at every intersection. It can also localize the cars using their wireless signals, and detect those that run a red-light. The same infrastructure can also deliver smart street-parking, where a user parks anywhere on the street, the city localizes his car, and automatically charges his account. This paper presents Caraoke, a networked system for delivering smart services using e-toll transponders. Our design operates with existing unmodified transponders, allowing for applications that communicate with, localize, and count transponders, despite wireless collisions. To do so, Caraoke exploits the structure of the transponders' signal and its properties in the frequency domain. We built Caraoke reader into a small PCB that harvests solar energy and can be easily deployed on street lamps. We also evaluated Caraoke on four streets on our campus and demonstrated its capabilities.",,297–310,14,"wireless, RF localization, smart city, active RFID, electronic toll collection (ETC)",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
406,inproceedings,"Abari, Omid and Vasisht, Deepak and Katabi, Dina and Chandrakasan, Anantha",Caraoke: An E-Toll Transponder Network for Smart Cities,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787504,10.1145/2785956.2787504,"Electronic toll collection transponders, e.g., E-ZPass, are a widely-used wireless technology. About 70% to 89% of the cars in US have these devices, and some states plan to make them mandatory. As wireless devices however, they lack a basic function: a MAC protocol that prevents collisions. Hence, today, they can be queried only with directional antennas in isolated spots. However, if one could interact with e-toll transponders anywhere in the city despite collisions, it would enable many smart applications. For example, the city can query the transponders to estimate the vehicle flow at every intersection. It can also localize the cars using their wireless signals, and detect those that run a red-light. The same infrastructure can also deliver smart street-parking, where a user parks anywhere on the street, the city localizes his car, and automatically charges his account. This paper presents Caraoke, a networked system for delivering smart services using e-toll transponders. Our design operates with existing unmodified transponders, allowing for applications that communicate with, localize, and count transponders, despite wireless collisions. To do so, Caraoke exploits the structure of the transponders' signal and its properties in the frequency domain. We built Caraoke reader into a small PCB that harvests solar energy and can be easily deployed on street lamps. We also evaluated Caraoke on four streets on our campus and demonstrated its capabilities.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,297–310,14,"active RFID, RF localization, wireless, smart city, electronic toll collection (ETC)","London, United Kingdom",SIGCOMM '15,,,,,,
407,inproceedings,"Akella, Aditya",Session Details: CDN and Wide Area Infrastructure,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3261004,10.1145/3261004,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,,,,"London, United Kingdom",SIGCOMM '15,,,,,,
408,article,"Mukerjee, Matthew K. and Naylor, David and Jiang, Junchen and Han, Dongsu and Seshan, Srinivasan and Zhang, Hui","Practical, Real-Time Centralized Control for CDN-Based Live Video Delivery",2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787475,10.1145/2829988.2787475,"Live video delivery is expected to reach a peak of 50 Tbps this year. This surging popularity is fundamentally changing the Internet video delivery landscape. CDNs must meet users' demands for fast join times, high bitrates, and low buffering ratios, while minimizing their own cost of delivery and responding to issues in real-time. Wide-area latency, loss, and failures, as well as varied workloads (""mega-events"" to long-tail), make meeting these demands challenging.An analysis of video sessions concluded that a centralized controller could improve user experience, but CDN systems have shied away from such designs due to the difficulty of quickly handling failures, a requirement of both operators and users. We introduce VDN, a practical approach to a video delivery network that uses a centralized algorithm for live video optimization. VDN provides CDN operators with real-time, fine-grained control. It does this in spite of challenges resulting from the wide-area (e.g., state inconsistency, partitions, failures) by using a hybrid centralized+distributed control plane, increasing average bitrate by 1.7x and decreasing cost by 2x in different scenarios.",,311–324,14,"hybrid control, central optimization, live video, CDNs",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
409,inproceedings,"Mukerjee, Matthew K. and Naylor, David and Jiang, Junchen and Han, Dongsu and Seshan, Srinivasan and Zhang, Hui","Practical, Real-Time Centralized Control for CDN-Based Live Video Delivery",2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787475,10.1145/2785956.2787475,"Live video delivery is expected to reach a peak of 50 Tbps this year. This surging popularity is fundamentally changing the Internet video delivery landscape. CDNs must meet users' demands for fast join times, high bitrates, and low buffering ratios, while minimizing their own cost of delivery and responding to issues in real-time. Wide-area latency, loss, and failures, as well as varied workloads (""mega-events"" to long-tail), make meeting these demands challenging.An analysis of video sessions concluded that a centralized controller could improve user experience, but CDN systems have shied away from such designs due to the difficulty of quickly handling failures, a requirement of both operators and users. We introduce VDN, a practical approach to a video delivery network that uses a centralized algorithm for live video optimization. VDN provides CDN operators with real-time, fine-grained control. It does this in spite of challenges resulting from the wide-area (e.g., state inconsistency, partitions, failures) by using a hybrid centralized+distributed control plane, increasing average bitrate by 1.7x and decreasing cost by 2x in different scenarios.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,311–324,14,"live video, CDNs, hybrid control, central optimization","London, United Kingdom",SIGCOMM '15,,,,,,
410,article,"Yin, Xiaoqi and Jindal, Abhishek and Sekar, Vyas and Sinopoli, Bruno",A Control-Theoretic Approach for Dynamic Adaptive Video Streaming over HTTP,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787486,10.1145/2829988.2787486,"User-perceived quality-of-experience (QoE) is critical in Internet video applications as it impacts revenues for content providers and delivery systems. Given that there is little support in the network for optimizing such measures, bottlenecks could occur anywhere in the delivery system. Consequently, a robust bitrate adaptation algorithm in client-side players is critical to ensure good user experience. Previous studies have shown key limitations of state-of-art commercial solutions and proposed a range of heuristic fixes. Despite the emergence of several proposals, there is still a distinct lack of consensus on: (1) How best to design this client-side bitrate adaptation logic (e.g., use rate estimates vs. buffer occupancy); (2) How well specific classes of approaches will perform under diverse operating regimes (e.g., high throughput variability); or (3) How do they actually balance different QoE objectives (e.g., startup delay vs. rebuffering). To this end, this paper makes three key technical contributions. First, to bring some rigor to this space, we develop a principled control-theoretic model to reason about a broad spectrum of strategies. Second, we propose a novel model predictive control algorithm that can optimally combine throughput and buffer occupancy information to outperform traditional approaches. Third, we present a practical implementation in a reference video player to validate our approach using realistic trace-driven emulations.",,325–338,14,"bitrate adaptation, dash, internet video, model predictive control",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
411,inproceedings,"Yin, Xiaoqi and Jindal, Abhishek and Sekar, Vyas and Sinopoli, Bruno",A Control-Theoretic Approach for Dynamic Adaptive Video Streaming over HTTP,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787486,10.1145/2785956.2787486,"User-perceived quality-of-experience (QoE) is critical in Internet video applications as it impacts revenues for content providers and delivery systems. Given that there is little support in the network for optimizing such measures, bottlenecks could occur anywhere in the delivery system. Consequently, a robust bitrate adaptation algorithm in client-side players is critical to ensure good user experience. Previous studies have shown key limitations of state-of-art commercial solutions and proposed a range of heuristic fixes. Despite the emergence of several proposals, there is still a distinct lack of consensus on: (1) How best to design this client-side bitrate adaptation logic (e.g., use rate estimates vs. buffer occupancy); (2) How well specific classes of approaches will perform under diverse operating regimes (e.g., high throughput variability); or (3) How do they actually balance different QoE objectives (e.g., startup delay vs. rebuffering). To this end, this paper makes three key technical contributions. First, to bring some rigor to this space, we develop a principled control-theoretic model to reason about a broad spectrum of strategies. Second, we propose a novel model predictive control algorithm that can optimally combine throughput and buffer occupancy information to outperform traditional approaches. Third, we present a practical implementation in a reference video player to validate our approach using realistic trace-driven emulations.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,325–338,14,"dash, model predictive control, internet video, bitrate adaptation","London, United Kingdom",SIGCOMM '15,,,,,,
412,article,"Liu, Zhi and Wang, Xiang and Yang, Baohua and Li, Jun",BitCuts: Towards Fast Packet Classification for Order-Independent Rules,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2789998,10.1145/2829988.2789998,,,339–340,2,"order-independent rules, packet classification",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
413,inproceedings,"Liu, Zhi and Wang, Xiang and Yang, Baohua and Li, Jun",BitCuts: Towards Fast Packet Classification for Order-Independent Rules,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2789998,10.1145/2785956.2789998,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,339–340,2,"order-independent rules, packet classification","London, United Kingdom",SIGCOMM '15,,,,,,
414,article,"Alan Chang, Michael and Holterbach, Thomas and Happe, Markus and Vanbever, Laurent",Supercharge Me: Boost Router Convergence with SDN,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790007,10.1145/2829988.2790007,"By enabling logically-centralized and direct control of the forwarding behavior of a network, Software-Defined Networking (SDN) holds great promise in terms of improving network management, performance, and costs. Realizing this vision is challenging though as SDN proposals to date require substantial and expensive changes to the existing network architecture before the benefits can be realized. As a result, the number of SDN deployments has been rather limited in scope. To kickstart a wide-scale SDN deployment, there is a need for low-risk, high return solutions that solve a timely problem. As one possible solution, we show how we can significantly improve the performance of legacy IP routers, i.e. ""supercharge"" them, by combining them with SDN-enabled devices. In this abstract, we supercharge one particular aspect of the router performance: its convergence time after a link or a node failure.",,341–342,2,"routers, fast convergence, network performance analysis",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
415,inproceedings,"Alan Chang, Michael and Holterbach, Thomas and Happe, Markus and Vanbever, Laurent",Supercharge Me: Boost Router Convergence with SDN,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790007,10.1145/2785956.2790007,"By enabling logically-centralized and direct control of the forwarding behavior of a network, Software-Defined Networking (SDN) holds great promise in terms of improving network management, performance, and costs. Realizing this vision is challenging though as SDN proposals to date require substantial and expensive changes to the existing network architecture before the benefits can be realized. As a result, the number of SDN deployments has been rather limited in scope. To kickstart a wide-scale SDN deployment, there is a need for low-risk, high return solutions that solve a timely problem. As one possible solution, we show how we can significantly improve the performance of legacy IP routers, i.e. ""supercharge"" them, by combining them with SDN-enabled devices. In this abstract, we supercharge one particular aspect of the router performance: its convergence time after a link or a node failure.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,341–342,2,"fast convergence, network performance analysis, routers","London, United Kingdom",SIGCOMM '15,,,,,,
416,article,"Bifulco, Roberto and Matsiuk, Anton",Towards Scalable SDN Switches: Enabling Faster Flow Table Entries Installation,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790008,10.1145/2829988.2790008,,,343–344,2,"software-defined networking, reactive control plane, openflow",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
417,inproceedings,"Bifulco, Roberto and Matsiuk, Anton",Towards Scalable SDN Switches: Enabling Faster Flow Table Entries Installation,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790008,10.1145/2785956.2790008,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,343–344,2,"software-defined networking, reactive control plane, openflow","London, United Kingdom",SIGCOMM '15,,,,,,
418,article,"Afek, Yehuda and Bremler-Barr, Anat and Landau Feibish, Shir and Schiff, Liron",Sampling and Large Flow Detection in SDN,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790009,10.1145/2829988.2790009,,,345–346,2,"software defined networks, heavy hitters, network monitoring",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
419,inproceedings,"Afek, Yehuda and Bremler-Barr, Anat and Landau Feibish, Shir and Schiff, Liron",Sampling and Large Flow Detection in SDN,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790009,10.1145/2785956.2790009,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,345–346,2,"heavy hitters, network monitoring, software defined networks","London, United Kingdom",SIGCOMM '15,,,,,,
420,article,"Benchaita, Walid and Ghamri-Doudane, Samir and Tixeuil, S\'{e",On the Optimization of Request Routing for Content Delivery,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790016,10.1145/2829988.2790016,"We present a flexible scheme and an optimization algorithm for request routing in Content Delivery Networks (CDN). Our online approach, which is based on Lyapunov theory, provides a stable quality of service to clients, while improving content delivery delays. It also reduces data transport costs for operators.",,347–348,2,,,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
421,inproceedings,"Benchaita, Walid and Ghamri-Doudane, Samir and Tixeuil, S\'{e",On the Optimization of Request Routing for Content Delivery,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790016,10.1145/2785956.2790016,"We present a flexible scheme and an optimization algorithm for request routing in Content Delivery Networks (CDN). Our online approach, which is based on Lyapunov theory, provides a stable quality of service to clients, while improving content delivery delays. It also reduces data transport costs for operators.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,347–348,2,,"London, United Kingdom",SIGCOMM '15,,,,,,
422,article,"Kheirkhah, Morteza and Wakeman, Ian and Parisis, George",Short vs. Long Flows: A Battle That Both Can Win,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790018,10.1145/2829988.2790018,"In this paper, we introduce MMPTCP, a novel transport protocol which aims at unifying the way data is transported in data centres. MMPTCP runs in two phases; initially, it randomly scatters packets in the network under a single congestion window exploiting all available paths. This is beneficial to latency-sensitive flows. During the second phase, MMPTCP runs in Multi-Path TCP (MPTCP) mode, which has been shown to be very efficient for long flows. Initial evaluation shows that our approach significantly improves short flow completion times while providing high throughput for long flows and high overall network utilisation.",,349–350,2,"packet scatter, data center, NS-3, multi-path TCP",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
423,inproceedings,"Kheirkhah, Morteza and Wakeman, Ian and Parisis, George",Short vs. Long Flows: A Battle That Both Can Win,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790018,10.1145/2785956.2790018,"In this paper, we introduce MMPTCP, a novel transport protocol which aims at unifying the way data is transported in data centres. MMPTCP runs in two phases; initially, it randomly scatters packets in the network under a single congestion window exploiting all available paths. This is beneficial to latency-sensitive flows. During the second phase, MMPTCP runs in Multi-Path TCP (MPTCP) mode, which has been shown to be very efficient for long flows. Initial evaluation shows that our approach significantly improves short flow completion times while providing high throughput for long flows and high overall network utilisation.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,349–350,2,"multi-path TCP, packet scatter, NS-3, data center","London, United Kingdom",SIGCOMM '15,,,,,,
424,article,"Manihatty Bojan, Neelakandan and Zilberman, Noa and Antichi, Gianni and Moore, Andrew W.",Extreme Data-Rate Scheduling for the Data Center,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790019,10.1145/2829988.2790019,"Designing scalable and cost-effective data center interconnect architectures based on electrical packet switches is challenging. To overcome this challenge, researchers have tried to harness the advantages of optics in data center environment. This has resulted in exploration of hybrid switching architectures that contains an optical circuit switch to serve long bursts of traffic along with an electrical packet switch serving short bursts of traffic. The performance of such hybrid switching architectures in data center is dependent on the schedulers. Building hybrid schedulers is challenging because of varying properties of data center traffic, increasing network demands, requirements imposed by hybrid network architecture etc. Slow schedulers can negatively impact the performance of the data center network because of poor resource utilization. With future demands, this problem is going to escalate motivating the need for faster schedulers. One approach to do this would be to use a hardware based scheduler. In this paper we propose a framework that can be used to explore and evaluate hardware based hybrid schedulers.",,351–352,2,"switching, data center networks, scheduling, optical networks",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
425,inproceedings,"Manihatty Bojan, Neelakandan and Zilberman, Noa and Antichi, Gianni and Moore, Andrew W.",Extreme Data-Rate Scheduling for the Data Center,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790019,10.1145/2785956.2790019,"Designing scalable and cost-effective data center interconnect architectures based on electrical packet switches is challenging. To overcome this challenge, researchers have tried to harness the advantages of optics in data center environment. This has resulted in exploration of hybrid switching architectures that contains an optical circuit switch to serve long bursts of traffic along with an electrical packet switch serving short bursts of traffic. The performance of such hybrid switching architectures in data center is dependent on the schedulers. Building hybrid schedulers is challenging because of varying properties of data center traffic, increasing network demands, requirements imposed by hybrid network architecture etc. Slow schedulers can negatively impact the performance of the data center network because of poor resource utilization. With future demands, this problem is going to escalate motivating the need for faster schedulers. One approach to do this would be to use a hardware based scheduler. In this paper we propose a framework that can be used to explore and evaluate hardware based hybrid schedulers.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,351–352,2,"optical networks, data center networks, scheduling, switching","London, United Kingdom",SIGCOMM '15,,,,,,
426,article,"Donovan, Sean and Feamster, Nick",Alternative Trust Sources: Reducing DNSSEC Signature Verification Operations with TLS,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790001,10.1145/2829988.2790001,"DNSSEC has been in development for 20 years. It provides for provable security when retrieving domain names through the use of a public key infrastructure (PKI). Unfortunately, there is also significant overhead involved with DNSSEC: verifying certificate chains of signed DNS messages involves extra computation, queries to remote resolvers, additional transfers, and introduces added latency into the DNS query path. We pose the question: is it possible to achieve practical security without always verifying this certificate chain if we use a different, outside source of trust between resolvers? We believe we can. Namely, by using a long-lived, mutually authenticated TLS connection between pairs of DNS resolvers, we suggest that we can maintain near-equivalent levels of security with very little extra overhead compared to a non-DNSSEC enabled resolver. By using a reputation system or probabilistically verifying a portion of DNSSEC responses would allow for near-equivalent levels of security to be reached, even in the face of compromised resolvers.",,353–354,2,"DNSsec, networksecurity",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
427,inproceedings,"Donovan, Sean and Feamster, Nick",Alternative Trust Sources: Reducing DNSSEC Signature Verification Operations with TLS,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790001,10.1145/2785956.2790001,"DNSSEC has been in development for 20 years. It provides for provable security when retrieving domain names through the use of a public key infrastructure (PKI). Unfortunately, there is also significant overhead involved with DNSSEC: verifying certificate chains of signed DNS messages involves extra computation, queries to remote resolvers, additional transfers, and introduces added latency into the DNS query path. We pose the question: is it possible to achieve practical security without always verifying this certificate chain if we use a different, outside source of trust between resolvers? We believe we can. Namely, by using a long-lived, mutually authenticated TLS connection between pairs of DNS resolvers, we suggest that we can maintain near-equivalent levels of security with very little extra overhead compared to a non-DNSSEC enabled resolver. By using a reputation system or probabilistically verifying a portion of DNSSEC responses would allow for near-equivalent levels of security to be reached, even in the face of compromised resolvers.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,353–354,2,"networksecurity, DNSsec","London, United Kingdom",SIGCOMM '15,,,,,,
428,article,"Jamshed, Muhammad A. and Kim, Donghwi and Moon, YoungGyoun and Han, Dongsu and Park, KyoungSoo",A Case for a Stateful Middlebox Networking Stack,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2789999,10.1145/2829988.2789999,,,355–356,2,"middlebox, networked systems",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
429,inproceedings,"Jamshed, Muhammad A. and Kim, Donghwi and Moon, YoungGyoun and Han, Dongsu and Park, KyoungSoo",A Case for a Stateful Middlebox Networking Stack,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2789999,10.1145/2785956.2789999,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,355–356,2,"middlebox, networked systems","London, United Kingdom",SIGCOMM '15,,,,,,
430,article,"Cao, Zhen and Fitschen, J\""{u",FreeSurf: Application-Centric Wireless Access with SDN,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790000,10.1145/2829988.2790000,,,357–358,2,"wireless networks, authentication, access control, SDN",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
431,inproceedings,"Cao, Zhen and Fitschen, J\""{u",FreeSurf: Application-Centric Wireless Access with SDN,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790000,10.1145/2785956.2790000,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,357–358,2,"access control, wireless networks, SDN, authentication","London, United Kingdom",SIGCOMM '15,,,,,,
432,article,L\'{e,EPOXIDE: A Modular Prototype for SDN Troubleshooting,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790027,10.1145/2829988.2790027,"SDN opens a new chapter in network troubleshooting as besides misconfigurations and firmware/hardware errors, software bugs can occur all over the SDN stack. As an answer to this challenge the networking community developed a wealth of piecemeal SDN troubleshooting tools aiming to track down misconfigurations or bugs of a specific nature (e.g. in a given SDN layer). In this demonstration we present EPOXIDE, an Emacs based modular framework, which can effectively combine existing network and software troubleshooting tools in a single platform and defines a possible way of integrated SDN troubleshooting.",,359–360,2,"network troubleshooting, EMACS, debugging, SDN",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
433,inproceedings,L\'{e,EPOXIDE: A Modular Prototype for SDN Troubleshooting,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790027,10.1145/2785956.2790027,"SDN opens a new chapter in network troubleshooting as besides misconfigurations and firmware/hardware errors, software bugs can occur all over the SDN stack. As an answer to this challenge the networking community developed a wealth of piecemeal SDN troubleshooting tools aiming to track down misconfigurations or bugs of a specific nature (e.g. in a given SDN layer). In this demonstration we present EPOXIDE, an Emacs based modular framework, which can effectively combine existing network and software troubleshooting tools in a single platform and defines a possible way of integrated SDN troubleshooting.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,359–360,2,"SDN, network troubleshooting, EMACS, debugging","London, United Kingdom",SIGCOMM '15,,,,,,
434,article,"Bari, Md. Faizul and Chowdhury, Shihabur Rahman and Ahmed, Reaz and Boutaba, Raouf",Nf.Io: A File System Abstraction for NFV Orchestration,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790028,10.1145/2829988.2790028,,,361–362,2,"file system abstraction, service chain orchestration, network function virtualization",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
435,inproceedings,"Bari, Md. Faizul and Chowdhury, Shihabur Rahman and Ahmed, Reaz and Boutaba, Raouf",Nf.Io: A File System Abstraction for NFV Orchestration,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790028,10.1145/2785956.2790028,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,361–362,2,"service chain orchestration, file system abstraction, network function virtualization","London, United Kingdom",SIGCOMM '15,,,,,,
436,article,"Zilberman, Noa and Audzevich, Yury and Kalogeridou, Georgina and Manihatty-Bojan, Neelakandan and Zhang, Jingyun and Moore, Andrew",NetFPGA: Rapid Prototyping of Networking Devices in Open Source,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790029,10.1145/2829988.2790029,"The demand-led growth of datacenter networks has meant that many constituent technologies are beyond the budget of the wider community. In order to make and validate timely and relevant new contributions, the wider community requires accessible evaluation, experimentation and demonstration environments with specification comparable to the subsystems of the most massive datacenter networks. We demonstrate NetFPGA, an open-source platform for rapid prototyping of networking devices with I/O capabilities up to 100Gbps. NetFPGA offers an integrated environment that enables networking research by users from a wide range of disciplines: from hardware-centric research to formal methods.",,363–364,2,"high-speed, programmable hardware, networking, netFPGA",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
437,inproceedings,"Zilberman, Noa and Audzevich, Yury and Kalogeridou, Georgina and Manihatty-Bojan, Neelakandan and Zhang, Jingyun and Moore, Andrew",NetFPGA: Rapid Prototyping of Networking Devices in Open Source,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790029,10.1145/2785956.2790029,"The demand-led growth of datacenter networks has meant that many constituent technologies are beyond the budget of the wider community. In order to make and validate timely and relevant new contributions, the wider community requires accessible evaluation, experimentation and demonstration environments with specification comparable to the subsystems of the most massive datacenter networks. We demonstrate NetFPGA, an open-source platform for rapid prototyping of networking devices with I/O capabilities up to 100Gbps. NetFPGA offers an integrated environment that enables networking research by users from a wide range of disciplines: from hardware-centric research to formal methods.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,363–364,2,"networking, programmable hardware, high-speed, netFPGA","London, United Kingdom",SIGCOMM '15,,,,,,
438,article,"Lantz, Bob and O'Connor, Brian",A Mininet-Based Virtual Testbed for Distributed SDN Development,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790030,10.1145/2829988.2790030,"The need for fault tolerance and scalability is leading to the development of distributed SDN operating systems and applications. But how can you develop such systems and applications reliably without access to an expensive testbed? We continue to observe SDN development practices using full system virtualization or heavyweight containers, increasing complexity and overhead while decreasing usability. We demonstrate a simpler and more efficient approach: using Mininet's cluster mode to easily deploy a virtual testbed of lightweight containers on a single machine, an ad hoc cluster, or a dedicated hardware testbed. By adding an open source, distributed network operating system such as ONOS, we can create a flexible and scalable open source development platform for distributed SDN system and application software development.",,365–366,2,"network applications, software defined networking, container based emulation, demonstration, network OS, distributed applications, mininet, distributed systems, SDN",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
439,inproceedings,"Lantz, Bob and O'Connor, Brian",A Mininet-Based Virtual Testbed for Distributed SDN Development,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790030,10.1145/2785956.2790030,"The need for fault tolerance and scalability is leading to the development of distributed SDN operating systems and applications. But how can you develop such systems and applications reliably without access to an expensive testbed? We continue to observe SDN development practices using full system virtualization or heavyweight containers, increasing complexity and overhead while decreasing usability. We demonstrate a simpler and more efficient approach: using Mininet's cluster mode to easily deploy a virtual testbed of lightweight containers on a single machine, an ad hoc cluster, or a dedicated hardware testbed. By adding an open source, distributed network operating system such as ONOS, we can create a flexible and scalable open source development platform for distributed SDN system and application software development.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,365–366,2,"network OS, network applications, distributed systems, mininet, software defined networking, container based emulation, demonstration, distributed applications, SDN","London, United Kingdom",SIGCOMM '15,,,,,,
440,article,"Alistarh, Dan and Ballani, Hitesh and Costa, Paolo and Funnell, Adam and Benjamin, Joshua and Watts, Philip and Thomsen, Benn","A High-Radix, Low-Latency Optical Switch for Data Centers",2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790035,10.1145/2829988.2790035,"We demonstrate an optical switch design that can scale up to a thousand ports with high per-port bandwidth (25 Gbps+) and low switching latency (40 ns). Our design uses a broadcast and select architecture, based on a passive star coupler and fast tunable transceivers. In addition we employ time division multiplexing to achieve very low switching latency. Our demo shows the feasibility of the switch data plane using a small testbed, comprising two transmitters and a receiver, connected through a star coupler.",,367–368,2,"TDMA, WDM, optical switching",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
441,inproceedings,"Alistarh, Dan and Ballani, Hitesh and Costa, Paolo and Funnell, Adam and Benjamin, Joshua and Watts, Philip and Thomsen, Benn","A High-Radix, Low-Latency Optical Switch for Data Centers",2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790035,10.1145/2785956.2790035,"We demonstrate an optical switch design that can scale up to a thousand ports with high per-port bandwidth (25 Gbps+) and low switching latency (40 ns). Our design uses a broadcast and select architecture, based on a passive star coupler and fast tunable transceivers. In addition we employ time division multiplexing to achieve very low switching latency. Our demo shows the feasibility of the switch data plane using a small testbed, comprising two transmitters and a receiver, connected through a star coupler.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,367–368,2,"WDM, optical switching, TDMA","London, United Kingdom",SIGCOMM '15,,,,,,
442,article,"Antichi, Gianni and Rotsos, Charalampos and Moore, Andrew W.",Enabling Performance Evaluation Beyond 10 Gbps,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790036,10.1145/2829988.2790036,"Despite network monitoring and testing being critical for computer networks, current solutions are both extremely expensive and inflexible. This demo presents OSNT (www.osnt.org), a community-driven, high-performance, open-source traffic generator and capture system built on top of the NetFPGA-10G board which enables flexible network testing. The platform supports full line-rate traffic generation regardless of packet size across the four card ports, packet capture filtering and packet thinning in hardware and sub-msec time precision in traffic generation and capture, corrected using an external GPS device. Furthermore, it provides a software APIs to test the dataplane performance of multi-10G switches, providing a starting point for a number of different test cases. OSNT flexibility is further demonstrated through the OFLOPS-turbo platform: an integration of OSNT with the OFLOPS OpenFlow switch performance evaluation platform, enabling control and data plane evaluation of 10G switches. This demo showcases the applicability of the OSNT platform to evaluate the performance of legacy and OpenFlow-enabled networking devices, and demonstrates it using commercial switches.",,369–370,2,"OSNT, openflow, netFPGA, network testing, SDN, high-performance",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
443,inproceedings,"Antichi, Gianni and Rotsos, Charalampos and Moore, Andrew W.",Enabling Performance Evaluation Beyond 10 Gbps,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790036,10.1145/2785956.2790036,"Despite network monitoring and testing being critical for computer networks, current solutions are both extremely expensive and inflexible. This demo presents OSNT (www.osnt.org), a community-driven, high-performance, open-source traffic generator and capture system built on top of the NetFPGA-10G board which enables flexible network testing. The platform supports full line-rate traffic generation regardless of packet size across the four card ports, packet capture filtering and packet thinning in hardware and sub-msec time precision in traffic generation and capture, corrected using an external GPS device. Furthermore, it provides a software APIs to test the dataplane performance of multi-10G switches, providing a starting point for a number of different test cases. OSNT flexibility is further demonstrated through the OFLOPS-turbo platform: an integration of OSNT with the OFLOPS OpenFlow switch performance evaluation platform, enabling control and data plane evaluation of 10G switches. This demo showcases the applicability of the OSNT platform to evaluate the performance of legacy and OpenFlow-enabled networking devices, and demonstrates it using commercial switches.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,369–370,2,"openflow, high-performance, netFPGA, OSNT, SDN, network testing","London, United Kingdom",SIGCOMM '15,,,,,,
444,article,"Chang, Michael Alan and Tschaen, Bredan and Benson, Theophilus and Vanbever, Laurent",Chaos Monkey: Increasing SDN Reliability through Systematic Network Destruction,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790038,10.1145/2829988.2790038,,,371–372,2,,,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
445,inproceedings,"Chang, Michael Alan and Tschaen, Bredan and Benson, Theophilus and Vanbever, Laurent",Chaos Monkey: Increasing SDN Reliability through Systematic Network Destruction,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790038,10.1145/2785956.2790038,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,371–372,2,,"London, United Kingdom",SIGCOMM '15,,,,,,
446,article,"Lee, Jeongkeun and Kang, Joon-Myung and Prakash, Chaithan and Banerjee, Sujata and Turner, Yoshio and Akella, Aditya and Clark, Charles and Ma, Yadi and Sharma, Puneet and Zhang, Ying",Network Policy Whiteboarding and Composition,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790039,10.1145/2829988.2790039,"We present Policy Graph Abstraction (PGA) that graphically expresses network policies and service chain requirements, just as simple as drawing whiteboard diagrams. Different users independently draw policy graphs that can constrain each other. PGA graph clearly captures user intents and invariants and thus facilitates automatic composition of overlapping policies into a coherent policy.",,373–374,2,,,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
447,inproceedings,"Lee, Jeongkeun and Kang, Joon-Myung and Prakash, Chaithan and Banerjee, Sujata and Turner, Yoshio and Akella, Aditya and Clark, Charles and Ma, Yadi and Sharma, Puneet and Zhang, Ying",Network Policy Whiteboarding and Composition,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790039,10.1145/2785956.2790039,"We present Policy Graph Abstraction (PGA) that graphically expresses network policies and service chain requirements, just as simple as drawing whiteboard diagrams. Different users independently draw policy graphs that can constrain each other. PGA graph clearly captures user intents and invariants and thus facilitates automatic composition of overlapping policies into a coherent policy.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,373–374,2,,"London, United Kingdom",SIGCOMM '15,,,,,,
448,article,"Riggio, Roberto and Schulz-Zander, Julius and Bradai, Abbas",Virtual Network Function Orchestration with Scylla,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790040,10.1145/2829988.2790040,,,375–376,2,"enterprise WLANs, network function virtualization",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
449,inproceedings,"Riggio, Roberto and Schulz-Zander, Julius and Bradai, Abbas",Virtual Network Function Orchestration with Scylla,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790040,10.1145/2785956.2790040,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,375–376,2,"network function virtualization, enterprise WLANs","London, United Kingdom",SIGCOMM '15,,,,,,
450,article,"Sonkoly, Bal\'{a",Multi-Domain Service Orchestration Over Networks and Clouds: A Unified Approach,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790041,10.1145/2829988.2790041,"End-to-end service delivery often includes transparently inserted Network Functions (NFs) in the path. Flexible service chaining will require dynamic instantiation of both NFs and traffic forwarding overlays. Virtualization techniques in compute and networking, like cloud and Software Defined Networking (SDN), promise such flexibility for service providers. However, patching together existing cloud and network control mechanisms necessarily puts one over the above, e.g., OpenDaylight under an OpenStack controller. We designed and implemented a joint cloud and network resource virtualization and programming API. In this demonstration, we show that our abstraction is capable for flexible service chaining control over any technology domains.",,377–378,2,"multi-domain orchestration, SFC control plane, SDN, NFV",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
451,inproceedings,"Sonkoly, Bal\'{a",Multi-Domain Service Orchestration Over Networks and Clouds: A Unified Approach,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790041,10.1145/2785956.2790041,"End-to-end service delivery often includes transparently inserted Network Functions (NFs) in the path. Flexible service chaining will require dynamic instantiation of both NFs and traffic forwarding overlays. Virtualization techniques in compute and networking, like cloud and Software Defined Networking (SDN), promise such flexibility for service providers. However, patching together existing cloud and network control mechanisms necessarily puts one over the above, e.g., OpenDaylight under an OpenStack controller. We designed and implemented a joint cloud and network resource virtualization and programming API. In this demonstration, we show that our abstraction is capable for flexible service chaining control over any technology domains.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,377–378,2,"multi-domain orchestration, NFV, SFC control plane, SDN","London, United Kingdom",SIGCOMM '15,,,,,,
452,inproceedings,"Oran, Dave",Session Details: Scheduling and Resource Management 1,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3261005,10.1145/3261005,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,,,,"London, United Kingdom",SIGCOMM '15,,,,,,
453,article,"Ren, Xiaoqi and Ananthanarayanan, Ganesh and Wierman, Adam and Yu, Minlan",Hopper: Decentralized Speculation-Aware Cluster Scheduling at Scale,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787481,10.1145/2829988.2787481,"As clusters continue to grow in size and complexity, providing scalable and predictable performance is an increasingly important challenge. A crucial roadblock to achieving predictable performance is stragglers, i.e., tasks that take significantly longer than expected to run. At this point, speculative execution has been widely adopted to mitigate the impact of stragglers. However, speculation mechanisms are designed and operated independently of job scheduling when, in fact, scheduling a speculative copy of a task has a direct impact on the resources available for other jobs. In this work, we present Hopper, a job scheduler that is speculation-aware, i.e., that integrates the tradeoffs associated with speculation into job scheduling decisions. We implement both centralized and decentralized prototypes of the Hopper scheduler and show that 50% (66%) improvements over state-of-the-art centralized (decentralized) schedulers and speculation strategies can be achieved through the coordination of scheduling and speculation.",,379–392,14,"straggler, speculation, fairness, decentralized scheduling",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
454,inproceedings,"Ren, Xiaoqi and Ananthanarayanan, Ganesh and Wierman, Adam and Yu, Minlan",Hopper: Decentralized Speculation-Aware Cluster Scheduling at Scale,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787481,10.1145/2785956.2787481,"As clusters continue to grow in size and complexity, providing scalable and predictable performance is an increasingly important challenge. A crucial roadblock to achieving predictable performance is stragglers, i.e., tasks that take significantly longer than expected to run. At this point, speculative execution has been widely adopted to mitigate the impact of stragglers. However, speculation mechanisms are designed and operated independently of job scheduling when, in fact, scheduling a speculative copy of a task has a direct impact on the resources available for other jobs. In this work, we present Hopper, a job scheduler that is speculation-aware, i.e., that integrates the tradeoffs associated with speculation into job scheduling decisions. We implement both centralized and decentralized prototypes of the Hopper scheduler and show that 50% (66%) improvements over state-of-the-art centralized (decentralized) schedulers and speculation strategies can be achieved through the coordination of scheduling and speculation.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,379–392,14,"speculation, straggler, decentralized scheduling, fairness","London, United Kingdom",SIGCOMM '15,,,,,,
455,article,"Chowdhury, Mosharaf and Stoica, Ion",Efficient Coflow Scheduling Without Prior Knowledge,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787480,10.1145/2829988.2787480,"Inter-coflow scheduling improves application-level communication performance in data-parallel clusters. However, existing efficient schedulers require a priori coflow information and ignore cluster dynamics like pipelining, task failures, and speculative executions, which limit their applicability. Schedulers without prior knowledge compromise on performance to avoid head-of-line blocking. In this paper, we present Aalo that strikes a balance and efficiently schedules coflows without prior knowledge.Aalo employs Discretized Coflow-Aware Least-Attained Service (D-CLAS) to separate coflows into a small number of priority queues based on how much they have already sent across the cluster. By performing prioritization across queues and by scheduling coflows in the FIFO order within each queue, Aalo's non-clairvoyant scheduler reduces coflow completion times while guaranteeing starvation freedom. EC2 deployments and trace-driven simulations show that communication stages complete 1.93X faster on average and 3.59X faster at the 95th percentile using Aalo in comparison to per-flow mechanisms. Aalo's performance is comparable to that of solutions using prior knowledge, and Aalo outperforms them in presence of cluster dynamics.",,393–406,14,"coflow, datacenter networks, data-intensive applications",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
456,inproceedings,"Chowdhury, Mosharaf and Stoica, Ion",Efficient Coflow Scheduling Without Prior Knowledge,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787480,10.1145/2785956.2787480,"Inter-coflow scheduling improves application-level communication performance in data-parallel clusters. However, existing efficient schedulers require a priori coflow information and ignore cluster dynamics like pipelining, task failures, and speculative executions, which limit their applicability. Schedulers without prior knowledge compromise on performance to avoid head-of-line blocking. In this paper, we present Aalo that strikes a balance and efficiently schedules coflows without prior knowledge.Aalo employs Discretized Coflow-Aware Least-Attained Service (D-CLAS) to separate coflows into a small number of priority queues based on how much they have already sent across the cluster. By performing prioritization across queues and by scheduling coflows in the FIFO order within each queue, Aalo's non-clairvoyant scheduler reduces coflow completion times while guaranteeing starvation freedom. EC2 deployments and trace-driven simulations show that communication stages complete 1.93X faster on average and 3.59X faster at the 95th percentile using Aalo in comparison to per-flow mechanisms. Aalo's performance is comparable to that of solutions using prior knowledge, and Aalo outperforms them in presence of cluster dynamics.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,393–406,14,"datacenter networks, coflow, data-intensive applications","London, United Kingdom",SIGCOMM '15,,,,,,
457,inproceedings,"Oran, Dave",Session Details: Scheduling and Resource Management 2,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3261006,10.1145/3261006,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,,,,"London, United Kingdom",SIGCOMM '15,,,,,,
458,article,"Jalaparti, Virajith and Bodik, Peter and Menache, Ishai and Rao, Sriram and Makarychev, Konstantin and Caesar, Matthew",Network-Aware Scheduling for Data-Parallel Jobs: Plan When You Can,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787488,10.1145/2829988.2787488,"To reduce the impact of network congestion on big data jobs, cluster management frameworks use various heuristics to schedule compute tasks and/or network flows. Most of these schedulers consider the job input data fixed and greedily schedule the tasks and flows that are ready to run. However, a large fraction of production jobs are recurring with predictable characteristics, which allows us to plan ahead for them. Coordinating the placement of data and tasks of these jobs allows for significantly improving their network locality and freeing up bandwidth, which can be used by other jobs running on the cluster. With this intuition, we develop Corral, a scheduling framework that uses characteristics of future workloads to determine an offline schedule which (i) jointly places data and compute to achieve better data locality, and (ii) isolates jobs both spatially (by scheduling them in different parts of the cluster) and temporally, improving their performance. We implement Corral on Apache Yarn, and evaluate it on a 210 machine cluster using production workloads. Compared to Yarn's capacity scheduler, Corral reduces the makespan of these workloads up to 33% and the median completion time up to 56%, with 20-90% reduction in data transferred across racks.",,407–420,14,"data-intensive applications, cross-layer optimization, joint data and compute placement, cluster schedulers",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
459,inproceedings,"Jalaparti, Virajith and Bodik, Peter and Menache, Ishai and Rao, Sriram and Makarychev, Konstantin and Caesar, Matthew",Network-Aware Scheduling for Data-Parallel Jobs: Plan When You Can,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787488,10.1145/2785956.2787488,"To reduce the impact of network congestion on big data jobs, cluster management frameworks use various heuristics to schedule compute tasks and/or network flows. Most of these schedulers consider the job input data fixed and greedily schedule the tasks and flows that are ready to run. However, a large fraction of production jobs are recurring with predictable characteristics, which allows us to plan ahead for them. Coordinating the placement of data and tasks of these jobs allows for significantly improving their network locality and freeing up bandwidth, which can be used by other jobs running on the cluster. With this intuition, we develop Corral, a scheduling framework that uses characteristics of future workloads to determine an offline schedule which (i) jointly places data and compute to achieve better data locality, and (ii) isolates jobs both spatially (by scheduling them in different parts of the cluster) and temporally, improving their performance. We implement Corral on Apache Yarn, and evaluate it on a 210 machine cluster using production workloads. Compared to Yarn's capacity scheduler, Corral reduces the makespan of these workloads up to 33% and the median completion time up to 56%, with 20-90% reduction in data transferred across racks.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,407–420,14,"cluster schedulers, joint data and compute placement, data-intensive applications, cross-layer optimization","London, United Kingdom",SIGCOMM '15,,,,,,
460,article,"Pu, Qifan and Ananthanarayanan, Ganesh and Bodik, Peter and Kandula, Srikanth and Akella, Aditya and Bahl, Paramvir and Stoica, Ion",Low Latency Geo-Distributed Data Analytics,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787505,10.1145/2829988.2787505,"Low latency analytics on geographically distributed datasets (across datacenters, edge clusters) is an upcoming and increasingly important challenge. The dominant approach of aggregating all the data to a single datacenter significantly inflates the timeliness of analytics. At the same time, running queries over geo-distributed inputs using the current intra-DC analytics frameworks also leads to high query response times because these frameworks cannot cope with the relatively low and variable capacity of WAN links. We present Iridium, a system for low latency geo-distributed analytics. Iridium achieves low query response times by optimizing placement of both data and tasks of the queries. The joint data and task placement optimization, however, is intractable. Therefore, Iridium uses an online heuristic to redistribute datasets among the sites prior to queries' arrivals, and places the tasks to reduce network bottlenecks during the query's execution. Finally, it also contains a knob to budget WAN usage. Evaluation across eight worldwide EC2 regions using production queries show that Iridium speeds up queries by 3\texttimes{",,421–434,14,"geo-distributed, low latency, network aware, data analytics, wan analytics",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
461,inproceedings,"Pu, Qifan and Ananthanarayanan, Ganesh and Bodik, Peter and Kandula, Srikanth and Akella, Aditya and Bahl, Paramvir and Stoica, Ion",Low Latency Geo-Distributed Data Analytics,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787505,10.1145/2785956.2787505,"Low latency analytics on geographically distributed datasets (across datacenters, edge clusters) is an upcoming and increasingly important challenge. The dominant approach of aggregating all the data to a single datacenter significantly inflates the timeliness of analytics. At the same time, running queries over geo-distributed inputs using the current intra-DC analytics frameworks also leads to high query response times because these frameworks cannot cope with the relatively low and variable capacity of WAN links. We present Iridium, a system for low latency geo-distributed analytics. Iridium achieves low query response times by optimizing placement of both data and tasks of the queries. The joint data and task placement optimization, however, is intractable. Therefore, Iridium uses an online heuristic to redistribute datasets among the sites prior to queries' arrivals, and places the tasks to reduce network bottlenecks during the query's execution. Finally, it also contains a knob to budget WAN usage. Evaluation across eight worldwide EC2 regions using production queries show that Iridium speeds up queries by 3\texttimes{",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,421–434,14,"network aware, data analytics, wan analytics, geo-distributed, low latency","London, United Kingdom",SIGCOMM '15,,,,,,
462,article,"Jang, Keon and Sherry, Justine and Ballani, Hitesh and Moncaster, Toby",Silo: Predictable Message Latency in the Cloud,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787479,10.1145/2829988.2787479,"Many cloud applications can benefit from guaranteed latency for their network messages, however providing such predictability is hard, especially in multi-tenant datacenters. We identify three key requirements for such predictability: guaranteed network bandwidth, guaranteed packet delay and guaranteed burst allowance. We present Silo, a system that offers these guarantees in multi-tenant datacenters. Silo leverages the tight coupling between bandwidth and delay: controlling tenant bandwidth leads to deterministic bounds on network queuing delay. Silo builds upon network calculus to place tenant VMs with competing requirements such that they can coexist. A novel hypervisor-based policing mechanism achieves packet pacing at sub-microsecond granularity, ensuring tenants do not exceed their allowances. We have implemented a Silo prototype comprising a VM placement manager and a Windows filter driver. Silo does not require any changes to applications, guest OSes or network switches. We show that Silo can ensure predictable message latency for cloud applications while imposing low overhead.",,435–448,14,"traffic pacing, network QoS, network calculus, latency SLA, guaranteed latency",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
463,inproceedings,"Jang, Keon and Sherry, Justine and Ballani, Hitesh and Moncaster, Toby",Silo: Predictable Message Latency in the Cloud,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787479,10.1145/2785956.2787479,"Many cloud applications can benefit from guaranteed latency for their network messages, however providing such predictability is hard, especially in multi-tenant datacenters. We identify three key requirements for such predictability: guaranteed network bandwidth, guaranteed packet delay and guaranteed burst allowance. We present Silo, a system that offers these guarantees in multi-tenant datacenters. Silo leverages the tight coupling between bandwidth and delay: controlling tenant bandwidth leads to deterministic bounds on network queuing delay. Silo builds upon network calculus to place tenant VMs with competing requirements such that they can coexist. A novel hypervisor-based policing mechanism achieves packet pacing at sub-microsecond granularity, ensuring tenants do not exceed their allowances. We have implemented a Silo prototype comprising a VM placement manager and a Windows filter driver. Silo does not require any changes to applications, guest OSes or network switches. We show that Silo can ensure predictable message latency for cloud applications while imposing low overhead.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,435–448,14,"latency SLA, network calculus, guaranteed latency, network QoS, traffic pacing","London, United Kingdom",SIGCOMM '15,,,,,,
464,inproceedings,"Porter, George",Session Details: Datacenter Networking,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3261007,10.1145/3261007,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,,,,"London, United Kingdom",SIGCOMM '15,,,,,,
465,article,"Schlinker, Brandon and Mysore, Radhika Niranjan and Smith, Sean and Mogul, Jeffrey C. and Vahdat, Amin and Yu, Minlan and Katz-Bassett, Ethan and Rubin, Michael",Condor: Better Topologies Through Declarative Design,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787476,10.1145/2829988.2787476,"The design space for large, multipath datacenter networks is large and complex, and no one design fits all purposes. Network architects must trade off many criteria to design cost-effective, reliable, and maintainable networks, and typically cannot explore much of the design space. We present Condor, our approach to enabling a rapid, efficient design cycle. Condor allows architects to express their requirements as constraints via a Topology Description Language (TDL), rather than having to directly specify network structures. Condor then uses constraint-based synthesis to rapidly generate candidate topologies, which can be analyzed against multiple criteria. We show that TDL supports concise descriptions of topologies such as fat-trees, BCube, and DCell; that we can generate known and novel variants of fat-trees with simple changes to a TDL file; and that we can synthesize large topologies in tens of seconds. We also show that Condor supports the daunting task of designing multi-phase network expansions that can be carried out on live networks.",,449–463,15,"slo compliance, expandable topologies, topology design",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
466,inproceedings,"Schlinker, Brandon and Mysore, Radhika Niranjan and Smith, Sean and Mogul, Jeffrey C. and Vahdat, Amin and Yu, Minlan and Katz-Bassett, Ethan and Rubin, Michael",Condor: Better Topologies Through Declarative Design,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787476,10.1145/2785956.2787476,"The design space for large, multipath datacenter networks is large and complex, and no one design fits all purposes. Network architects must trade off many criteria to design cost-effective, reliable, and maintainable networks, and typically cannot explore much of the design space. We present Condor, our approach to enabling a rapid, efficient design cycle. Condor allows architects to express their requirements as constraints via a Topology Description Language (TDL), rather than having to directly specify network structures. Condor then uses constraint-based synthesis to rapidly generate candidate topologies, which can be analyzed against multiple criteria. We show that TDL supports concise descriptions of topologies such as fat-trees, BCube, and DCell; that we can generate known and novel variants of fat-trees with simple changes to a TDL file; and that we can synthesize large topologies in tens of seconds. We also show that Condor supports the daunting task of designing multi-phase network expansions that can be carried out on live networks.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,449–463,15,"expandable topologies, slo compliance, topology design","London, United Kingdom",SIGCOMM '15,,,,,,
467,article,"He, Keqiang and Rozner, Eric and Agarwal, Kanak and Felter, Wes and Carter, John and Akella, Aditya",Presto: Edge-Based Load Balancing for Fast Datacenter Networks,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787507,10.1145/2829988.2787507,"Datacenter networks deal with a variety of workloads, ranging from latency-sensitive small flows to bandwidth-hungry large flows. Load balancing schemes based on flow hashing, e.g., ECMP, cause congestion when hash collisions occur and can perform poorly in asymmetric topologies. Recent proposals to load balance the network require centralized traffic engineering, multipath-aware transport, or expensive specialized hardware. We propose a mechanism that avoids these limitations by (i) pushing load-balancing functionality into the soft network edge (e.g., virtual switches) such that no changes are required in the transport layer, customer VMs, or networking hardware, and (ii) load balancing on fine-grained, near-uniform units of data (flowcells) that fit within end-host segment offload optimizations used to support fast networking speeds. We design and implement such a soft-edge load balancing scheme, called Presto, and evaluate it on a 10 Gbps physical testbed. We demonstrate the computational impact of packet reordering on receivers and propose a mechanism to handle reordering in the TCP receive offload functionality. Presto's performance closely tracks that of a single, non-blocking switch over many workloads and is adaptive to failures and topology asymmetry.",,465–478,14,"software-defined networking, load balancing",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
468,inproceedings,"He, Keqiang and Rozner, Eric and Agarwal, Kanak and Felter, Wes and Carter, John and Akella, Aditya",Presto: Edge-Based Load Balancing for Fast Datacenter Networks,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787507,10.1145/2785956.2787507,"Datacenter networks deal with a variety of workloads, ranging from latency-sensitive small flows to bandwidth-hungry large flows. Load balancing schemes based on flow hashing, e.g., ECMP, cause congestion when hash collisions occur and can perform poorly in asymmetric topologies. Recent proposals to load balance the network require centralized traffic engineering, multipath-aware transport, or expensive specialized hardware. We propose a mechanism that avoids these limitations by (i) pushing load-balancing functionality into the soft network edge (e.g., virtual switches) such that no changes are required in the transport layer, customer VMs, or networking hardware, and (ii) load balancing on fine-grained, near-uniform units of data (flowcells) that fit within end-host segment offload optimizations used to support fast networking speeds. We design and implement such a soft-edge load balancing scheme, called Presto, and evaluate it on a 10 Gbps physical testbed. We demonstrate the computational impact of packet reordering on receivers and propose a mechanism to handle reordering in the TCP receive offload functionality. Presto's performance closely tracks that of a single, non-blocking switch over many workloads and is adaptive to failures and topology asymmetry.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,465–478,14,"load balancing, software-defined networking","London, United Kingdom",SIGCOMM '15,,,,,,
469,article,"Zhu, Yibo and Kang, Nanxi and Cao, Jiaxin and Greenberg, Albert and Lu, Guohan and Mahajan, Ratul and Maltz, Dave and Yuan, Lihua and Zhang, Ming and Zhao, Ben Y. and Zheng, Haitao",Packet-Level Telemetry in Large Datacenter Networks,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787483,10.1145/2829988.2787483,"Debugging faults in complex networks often requires capturing and analyzing traffic at the packet level. In this task, datacenter networks (DCNs) present unique challenges with their scale, traffic volume, and diversity of faults. To troubleshoot faults in a timely manner, DCN administrators must a) identify affected packets inside large volume of traffic; b) track them across multiple network components; c) analyze traffic traces for fault patterns; and d) test or confirm potential causes. To our knowledge, no tool today can achieve both the specificity and scale required for this task.We present Everflow, a packet-level network telemetry system for large DCNs. Everflow traces specific packets by implementing a powerful packet filter on top of ""match and mirror"" functionality of commodity switches. It shuffles captured packets to multiple analysis servers using load balancers built on switch ASICs, and it sends ""guided probes"" to test or confirm potential faults. We present experiments that demonstrate Everflow's scalability, and share experiences of troubleshooting network faults gathered from running it for over 6 months in Microsoft's DCNs.",,479–491,13,"probe, datacenter network, failure detection",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
470,inproceedings,"Zhu, Yibo and Kang, Nanxi and Cao, Jiaxin and Greenberg, Albert and Lu, Guohan and Mahajan, Ratul and Maltz, Dave and Yuan, Lihua and Zhang, Ming and Zhao, Ben Y. and Zheng, Haitao",Packet-Level Telemetry in Large Datacenter Networks,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787483,10.1145/2785956.2787483,"Debugging faults in complex networks often requires capturing and analyzing traffic at the packet level. In this task, datacenter networks (DCNs) present unique challenges with their scale, traffic volume, and diversity of faults. To troubleshoot faults in a timely manner, DCN administrators must a) identify affected packets inside large volume of traffic; b) track them across multiple network components; c) analyze traffic traces for fault patterns; and d) test or confirm potential causes. To our knowledge, no tool today can achieve both the specificity and scale required for this task.We present Everflow, a packet-level network telemetry system for large DCNs. Everflow traces specific packets by implementing a powerful packet filter on top of ""match and mirror"" functionality of commodity switches. It shuffles captured packets to multiple analysis servers using load balancers built on switch ASICs, and it sends ""guided probes"" to test or confirm potential faults. We present experiments that demonstrate Everflow's scalability, and share experiences of troubleshooting network faults gathered from running it for over 6 months in Microsoft's DCNs.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,479–491,13,"failure detection, datacenter network, probe","London, United Kingdom",SIGCOMM '15,,,,,,
471,article,"Ballani, Hitesh and Costa, Paolo and Gkantsidis, Christos and Grosvenor, Matthew P. and Karagiannis, Thomas and Koromilas, Lazaros and O'Shea, Greg",Enabling End-Host Network Functions,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787493,10.1145/2829988.2787493,"Many network functions executed in modern datacenters, e.g., load balancing, application-level QoS, and congestion control, exhibit three common properties at the data-plane: they need to access and modify state, to perform computations, and to access application semantics -- this is critical since many network functions are best expressed in terms of application-level messages. In this paper, we argue that the end hosts are a natural enforcement point for these functions and we present Eden, an architecture for implementing network functions at datacenter end hosts with minimal network support. Eden comprises three components, a centralized controller, an enclave at each end host, and Eden-compliant applications called stages. To implement network functions, the controller configures stages to classify their data into messages and the enclaves to apply action functions based on a packet's class. Our Eden prototype includes enclaves implemented both in the OS kernel and on programmable NICs. Through case studies, we show how application-level classification and the ability to run actual programs on the data-path allows Eden to efficiently support a broad range of network functions at the network's edge.",,493–507,15,"network management, network functions, data-plane programming, SDN, software defined networking",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
472,inproceedings,"Ballani, Hitesh and Costa, Paolo and Gkantsidis, Christos and Grosvenor, Matthew P. and Karagiannis, Thomas and Koromilas, Lazaros and O'Shea, Greg",Enabling End-Host Network Functions,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787493,10.1145/2785956.2787493,"Many network functions executed in modern datacenters, e.g., load balancing, application-level QoS, and congestion control, exhibit three common properties at the data-plane: they need to access and modify state, to perform computations, and to access application semantics -- this is critical since many network functions are best expressed in terms of application-level messages. In this paper, we argue that the end hosts are a natural enforcement point for these functions and we present Eden, an architecture for implementing network functions at datacenter end hosts with minimal network support. Eden comprises three components, a centralized controller, an enclave at each end host, and Eden-compliant applications called stages. To implement network functions, the controller configures stages to classify their data into messages and the enclaves to apply action functions based on a packet's class. Our Eden prototype includes enclaves implemented both in the OS kernel and on programmable NICs. Through case studies, we show how application-level classification and the ability to run actual programs on the data-path allows Eden to efficiently support a broad range of network functions at the network's edge.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,493–507,15,"network functions, network management, data-plane programming, software defined networking, SDN","London, United Kingdom",SIGCOMM '15,,,,,,
473,inproceedings,"Winstein, Keith",Session Details: Congestion Control and Transport Protocols,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3261008,10.1145/3261008,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,,,,"London, United Kingdom",SIGCOMM '15,,,,,,
474,article,"Zaki, Yasir and P\""{o",Adaptive Congestion Control for Unpredictable Cellular Networks,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787498,10.1145/2829988.2787498,"Legacy congestion controls including TCP and its variants are known to perform poorly over cellular networks due to highly variable capacities over short time scales, self-inflicted packet delays, and packet losses unrelated to congestion. To cope with these challenges, we present Verus, an end-to-end congestion control protocol that uses delay measurements to react quickly to the capacity changes in cellular networks without explicitly attempting to predict the cellular channel dynamics. The key idea of Verus is to continuously learn a delay profile that captures the relationship between end-to-end packet delay and outstanding window size over short epochs and uses this relationship to increment or decrement the window size based on the observed short-term packet delay variations. While the delay-based control is primarily for congestion avoidance, Verus uses standard TCP features including multiplicative decrease upon packet loss and slow start. Through a combination of simulations, empirical evaluations using cellular network traces, and real-world evaluations against standard TCP flavors and state of the art protocols like Sprout, we show that Verus outperforms these protocols in cellular channels. In comparison to TCP Cubic, Verus achieves an order of magnitude (> 10x) reduction in delay over 3G and LTE networks while achieving comparable throughput (sometimes marginally higher). In comparison to Sprout, Verus achieves up to 30% higher throughput in rapidly changing cellular networks.",,509–522,14,"delay-based, transport protocol, cellular network, congestion control",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
475,inproceedings,"Zaki, Yasir and P\""{o",Adaptive Congestion Control for Unpredictable Cellular Networks,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787498,10.1145/2785956.2787498,"Legacy congestion controls including TCP and its variants are known to perform poorly over cellular networks due to highly variable capacities over short time scales, self-inflicted packet delays, and packet losses unrelated to congestion. To cope with these challenges, we present Verus, an end-to-end congestion control protocol that uses delay measurements to react quickly to the capacity changes in cellular networks without explicitly attempting to predict the cellular channel dynamics. The key idea of Verus is to continuously learn a delay profile that captures the relationship between end-to-end packet delay and outstanding window size over short epochs and uses this relationship to increment or decrement the window size based on the observed short-term packet delay variations. While the delay-based control is primarily for congestion avoidance, Verus uses standard TCP features including multiplicative decrease upon packet loss and slow start. Through a combination of simulations, empirical evaluations using cellular network traces, and real-world evaluations against standard TCP flavors and state of the art protocols like Sprout, we show that Verus outperforms these protocols in cellular channels. In comparison to TCP Cubic, Verus achieves an order of magnitude (> 10x) reduction in delay over 3G and LTE networks while achieving comparable throughput (sometimes marginally higher). In comparison to Sprout, Verus achieves up to 30% higher throughput in rapidly changing cellular networks.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,509–522,14,"congestion control, delay-based, cellular network, transport protocol","London, United Kingdom",SIGCOMM '15,,,,,,
476,article,"Zhu, Yibo and Eran, Haggai and Firestone, Daniel and Guo, Chuanxiong and Lipshteyn, Marina and Liron, Yehonatan and Padhye, Jitendra and Raindel, Shachar and Yahia, Mohamad Haj and Zhang, Ming",Congestion Control for Large-Scale RDMA Deployments,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787484,10.1145/2829988.2787484,"Modern datacenter applications demand high throughput (40Gbps) and ultra-low latency (< 10 μs per hop) from the network, with low CPU overhead. Standard TCP/IP stacks cannot meet these requirements, but Remote Direct Memory Access (RDMA) can. On IP-routed datacenter networks, RDMA is deployed using RoCEv2 protocol, which relies on Priority-based Flow Control (PFC) to enable a drop-free network. However, PFC can lead to poor application performance due to problems like head-of-line blocking and unfairness. To alleviates these problems, we introduce DCQCN, an end-to-end congestion control scheme for RoCEv2. To optimize DCQCN performance, we build a fluid model, and provide guidelines for tuning switch buffer thresholds, and other protocol parameters. Using a 3-tier Clos network testbed, we show that DCQCN dramatically improves throughput and fairness of RoCEv2 RDMA traffic. DCQCN is implemented in Mellanox NICs, and is being deployed in Microsoft's datacenters.",,523–536,14,"datacenter transport, PFC, RDMA, congestion control, ECN",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
477,inproceedings,"Zhu, Yibo and Eran, Haggai and Firestone, Daniel and Guo, Chuanxiong and Lipshteyn, Marina and Liron, Yehonatan and Padhye, Jitendra and Raindel, Shachar and Yahia, Mohamad Haj and Zhang, Ming",Congestion Control for Large-Scale RDMA Deployments,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787484,10.1145/2785956.2787484,"Modern datacenter applications demand high throughput (40Gbps) and ultra-low latency (< 10 μs per hop) from the network, with low CPU overhead. Standard TCP/IP stacks cannot meet these requirements, but Remote Direct Memory Access (RDMA) can. On IP-routed datacenter networks, RDMA is deployed using RoCEv2 protocol, which relies on Priority-based Flow Control (PFC) to enable a drop-free network. However, PFC can lead to poor application performance due to problems like head-of-line blocking and unfairness. To alleviates these problems, we introduce DCQCN, an end-to-end congestion control scheme for RoCEv2. To optimize DCQCN performance, we build a fluid model, and provide guidelines for tuning switch buffer thresholds, and other protocol parameters. Using a 3-tier Clos network testbed, we show that DCQCN dramatically improves throughput and fairness of RoCEv2 RDMA traffic. DCQCN is implemented in Mellanox NICs, and is being deployed in Microsoft's datacenters.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,523–536,14,"datacenter transport, ECN, congestion control, RDMA, PFC","London, United Kingdom",SIGCOMM '15,,,,,,
478,article,"Mittal, Radhika and Lam, Vinh The and Dukkipati, Nandita and Blem, Emily and Wassel, Hassan and Ghobadi, Monia and Vahdat, Amin and Wang, Yaogong and Wetherall, David and Zats, David",TIMELY: RTT-Based Congestion Control for the Datacenter,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787510,10.1145/2829988.2787510,"Datacenter transports aim to deliver low latency messaging together with high throughput. We show that simple packet delay, measured as round-trip times at hosts, is an effective congestion signal without the need for switch feedback. First, we show that advances in NIC hardware have made RTT measurement possible with microsecond accuracy, and that these RTTs are sufficient to estimate switch queueing. Then we describe how TIMELY can adjust transmission rates using RTT gradients to keep packet latency low while delivering high bandwidth. We implement our design in host software running over NICs with OS-bypass capabilities. We show using experiments with up to hundreds of machines on a Clos network topology that it provides excellent performance: turning on TIMELY for OS-bypass messaging over a fabric with PFC lowers 99 percentile tail latency by 9X while maintaining near line-rate throughput. Our system also outperforms DCTCP running in an optimized kernel, reducing tail latency by $13$X. To the best of our knowledge, TIMELY is the first delay-based congestion control protocol for use in the datacenter, and it achieves its results despite having an order of magnitude fewer RTT signals (due to NIC offload) than earlier delay-based schemes such as Vegas.",,537–550,14,"delay-based congestion control, os-bypass, rdma, datacenter transport",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
479,inproceedings,"Mittal, Radhika and Lam, Vinh The and Dukkipati, Nandita and Blem, Emily and Wassel, Hassan and Ghobadi, Monia and Vahdat, Amin and Wang, Yaogong and Wetherall, David and Zats, David",TIMELY: RTT-Based Congestion Control for the Datacenter,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787510,10.1145/2785956.2787510,"Datacenter transports aim to deliver low latency messaging together with high throughput. We show that simple packet delay, measured as round-trip times at hosts, is an effective congestion signal without the need for switch feedback. First, we show that advances in NIC hardware have made RTT measurement possible with microsecond accuracy, and that these RTTs are sufficient to estimate switch queueing. Then we describe how TIMELY can adjust transmission rates using RTT gradients to keep packet latency low while delivering high bandwidth. We implement our design in host software running over NICs with OS-bypass capabilities. We show using experiments with up to hundreds of machines on a Clos network topology that it provides excellent performance: turning on TIMELY for OS-bypass messaging over a fabric with PFC lowers 99 percentile tail latency by 9X while maintaining near line-rate throughput. Our system also outperforms DCTCP running in an optimized kernel, reducing tail latency by $13$X. To the best of our knowledge, TIMELY is the first delay-based congestion control protocol for use in the datacenter, and it achieves its results despite having an order of magnitude fewer RTT signals (due to NIC offload) than earlier delay-based schemes such as Vegas.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,537–550,14,"rdma, delay-based congestion control, os-bypass, datacenter transport","London, United Kingdom",SIGCOMM '15,,,,,,
480,article,"Costa, Paolo and Ballani, Hitesh and Razavi, Kaveh and Kash, Ian",R2C2: A Network Stack for Rack-Scale Computers,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787492,10.1145/2829988.2787492,"Rack-scale computers, comprising a large number of micro-servers connected by a direct-connect topology, are expected to replace servers as the building block in data centers. We focus on the problem of routing and congestion control across the rack's network, and find that high path diversity in rack topologies, in combination with workload diversity across it, means that traditional solutions are inadequate. We introduce R2C2, a network stack for rack-scale computers that provides flexible and efficient routing and congestion control. R2C2 leverages the fact that the scale of rack topologies allows for low-overhead broadcasting to ensure that all nodes in the rack are aware of all network flows. We thus achieve rate-based congestion control without any probing; each node independently determines the sending rate for its flows while respecting the provider's allocation policies. For routing, nodes dynamically choose the routing protocol for each flow in order to maximize overall utility. Through a prototype deployed across a rack emulation platform and a packet-level simulator, we show that R2C2 achieves very low queuing and high throughput for diverse and bursty workloads, and that routing flexibility can provide significant throughput gains.",,551–564,14,"congestion control, transport protocols, cloud computing, networks, data center networks, rack-scale network stack, rack-scale computers, route selection",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
481,inproceedings,"Costa, Paolo and Ballani, Hitesh and Razavi, Kaveh and Kash, Ian",R2C2: A Network Stack for Rack-Scale Computers,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787492,10.1145/2785956.2787492,"Rack-scale computers, comprising a large number of micro-servers connected by a direct-connect topology, are expected to replace servers as the building block in data centers. We focus on the problem of routing and congestion control across the rack's network, and find that high path diversity in rack topologies, in combination with workload diversity across it, means that traditional solutions are inadequate. We introduce R2C2, a network stack for rack-scale computers that provides flexible and efficient routing and congestion control. R2C2 leverages the fact that the scale of rack topologies allows for low-overhead broadcasting to ensure that all nodes in the rack are aware of all network flows. We thus achieve rate-based congestion control without any probing; each node independently determines the sending rate for its flows while respecting the provider's allocation policies. For routing, nodes dynamically choose the routing protocol for each flow in order to maximize overall utility. Through a prototype deployed across a rack emulation platform and a packet-level simulator, we show that R2C2 achieves very low queuing and high throughput for diverse and bursty workloads, and that routing flexibility can provide significant throughput gains.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,551–564,14,"cloud computing, data center networks, congestion control, rack-scale computers, transport protocols, networks, route selection, rack-scale network stack","London, United Kingdom",SIGCOMM '15,,,,,,
482,inproceedings,"Feldmann, Anja",Session Details: Wide Area Networks and Traffic,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3261009,10.1145/3261009,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,,,,"London, United Kingdom",SIGCOMM '15,,,,,,
483,article,"Durairajan, Ramakrishnan and Barford, Paul and Sommers, Joel and Willinger, Walter",InterTubes: A Study of the US Long-Haul Fiber-Optic Infrastructure,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787499,10.1145/2829988.2787499,"The complexity and enormous costs of installing new long-haul fiber-optic infrastructure has led to a significant amount of infrastructure sharing in previously installed conduits. In this paper, we study the characteristics and implications of infrastructure sharing by analyzing the long-haul fiber-optic network in the US. We start by using fiber maps provided by tier-1 ISPs and major cable providers to construct a map of the long-haul US fiber-optic infrastructure. We also rely on previously under-utilized data sources in the form of public records from federal, state, and municipal agencies to improve the fidelity of our map. We quantify the resulting map's connectivity characteristics and confirm a clear correspondence between long-haul fiber-optic, roadway, and railway infrastructures. Next, we examine the prevalence of high-risk links by mapping end-to-end paths resulting from large-scale traceroute campaigns onto our fiber-optic infrastructure map. We show how both risk and latency (i.e., propagation delay) can be reduced by deploying new links along previously unused transportation corridors and rights-of-way. In particular, focusing on a subset of high-risk links is sufficient to improve the overall robustness of the network to failures. Finally, we discuss the implications of our findings on issues related to performance, net neutrality, and policy decision-making.",,565–578,14,"risk mitigation, long-haul fiber map, shared risk",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
484,inproceedings,"Durairajan, Ramakrishnan and Barford, Paul and Sommers, Joel and Willinger, Walter",InterTubes: A Study of the US Long-Haul Fiber-Optic Infrastructure,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787499,10.1145/2785956.2787499,"The complexity and enormous costs of installing new long-haul fiber-optic infrastructure has led to a significant amount of infrastructure sharing in previously installed conduits. In this paper, we study the characteristics and implications of infrastructure sharing by analyzing the long-haul fiber-optic network in the US. We start by using fiber maps provided by tier-1 ISPs and major cable providers to construct a map of the long-haul US fiber-optic infrastructure. We also rely on previously under-utilized data sources in the form of public records from federal, state, and municipal agencies to improve the fidelity of our map. We quantify the resulting map's connectivity characteristics and confirm a clear correspondence between long-haul fiber-optic, roadway, and railway infrastructures. Next, we examine the prevalence of high-risk links by mapping end-to-end paths resulting from large-scale traceroute campaigns onto our fiber-optic infrastructure map. We show how both risk and latency (i.e., propagation delay) can be reduced by deploying new links along previously unused transportation corridors and rights-of-way. In particular, focusing on a subset of high-risk links is sufficient to improve the overall robustness of the network to failures. Finally, we discuss the implications of our findings on issues related to performance, net neutrality, and policy decision-making.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,565–578,14,"risk mitigation, shared risk, long-haul fiber map","London, United Kingdom",SIGCOMM '15,,,,,,
485,article,"Tune, Paul and Roughan, Matthew",Spatiotemporal Traffic Matrix Synthesis,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787471,10.1145/2829988.2787471,"Traffic matrices describe the volume of traffic between a set of sources and destinations within a network. These matrices are used in a variety of tasks in network planning and traffic engineering, such as the design of network topologies. Traffic matrices naturally possess complex spatiotemporal characteristics, but their proprietary nature means that little data about them is available publicly, and this situation is unlikely to change.Our goal is to develop techniques to synthesize traffic matrices for researchers who wish to test new network applications or protocols. The paucity of available data, and the desire to build a general framework for synthesis that could work in various settings requires a new look at this problem. We show how the principle of maximum entropy can be used to generate a wide variety of traffic matrices constrained by the needs of a particular task, and the available information, but otherwise avoiding hidden assumptions about the data. We demonstrate how the framework encompasses existing models and measurements, and we apply it in a simple case study to illustrate the value.",,579–592,14,"maximum entropy, traffic engineering, traffic matrix synthesis, spatiotemporal modeling, network design",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
486,inproceedings,"Tune, Paul and Roughan, Matthew",Spatiotemporal Traffic Matrix Synthesis,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787471,10.1145/2785956.2787471,"Traffic matrices describe the volume of traffic between a set of sources and destinations within a network. These matrices are used in a variety of tasks in network planning and traffic engineering, such as the design of network topologies. Traffic matrices naturally possess complex spatiotemporal characteristics, but their proprietary nature means that little data about them is available publicly, and this situation is unlikely to change.Our goal is to develop techniques to synthesize traffic matrices for researchers who wish to test new network applications or protocols. The paucity of available data, and the desire to build a general framework for synthesis that could work in various settings requires a new look at this problem. We show how the principle of maximum entropy can be used to generate a wide variety of traffic matrices constrained by the needs of a particular task, and the available information, but otherwise avoiding hidden assumptions about the data. We demonstrate how the framework encompasses existing models and measurements, and we apply it in a simple case study to illustrate the value.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,579–592,14,"maximum entropy, spatiotemporal modeling, traffic matrix synthesis, network design, traffic engineering","London, United Kingdom",SIGCOMM '15,,,,,,
487,article,"Choi, Hyunwoo and Kim, Jeongmin and Hong, Hyunwook and Kim, Yongdae and Lee, Jonghyup and Han, Dongsu",Extractocol: Automatic Extraction of Application-Level Protocol Behaviors for Android Applications,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790003,10.1145/2829988.2790003,,,593–594,2,"protocol behaviors, static analysis, android",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
488,inproceedings,"Choi, Hyunwoo and Kim, Jeongmin and Hong, Hyunwook and Kim, Yongdae and Lee, Jonghyup and Han, Dongsu",Extractocol: Automatic Extraction of Application-Level Protocol Behaviors for Android Applications,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790003,10.1145/2785956.2790003,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,593–594,2,"protocol behaviors, android, static analysis","London, United Kingdom",SIGCOMM '15,,,,,,
489,article,Pere\v{s,Rule-Level Data Plane Monitoring With Monocle,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790012,10.1145/2829988.2790012,"We present Monocle, a system that systematically monitors the network data plane, and verifies that it corresponds to the view that the SDN controller builds and tries to enforce in the switches. Our evaluation shows that Monocle is capable of fine-grained per-rule monitoring for the majority of rules. In addition, it can help controllers to cope with switches that exhibit transient inconsistencies between their control plane and data plane states.",,595–596,2,"rule updates, reliability, monitoring, software-defined networks",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
490,inproceedings,Pere\v{s,Rule-Level Data Plane Monitoring With Monocle,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790012,10.1145/2785956.2790012,"We present Monocle, a system that systematically monitors the network data plane, and verifies that it corresponds to the view that the SDN controller builds and tries to enforce in the switches. Our evaluation shows that Monocle is capable of fine-grained per-rule monitoring for the majority of rules. In addition, it can help controllers to cope with switches that exhibit transient inconsistencies between their control plane and data plane states.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,595–596,2,"monitoring, software-defined networks, rule updates, reliability","London, United Kingdom",SIGCOMM '15,,,,,,
491,article,"Schmidt, Florian and Hohlfeld, Oliver and Glebke, Ren\'{e",Santa: Faster Packet Delivery for Commonly Wished Replies,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790014,10.1145/2829988.2790014,"Increasing network speeds challenge the packet processing performance of networked systems. This can mainly be attributed to processing overhead caused by the split between the kernel-space network stack and user-space applications. To mitigate this overhead, we propose Santa, an application agnostic kernel-level cache of frequent requests. By allowing user-space applications to offload frequent requests to the kernel-space, Santa offers drastic performance improvements and unlocks the speed of kernel-space networking for legacy server software without requiring extensive changes.",,597–598,2,,,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
492,inproceedings,"Schmidt, Florian and Hohlfeld, Oliver and Glebke, Ren\'{e",Santa: Faster Packet Delivery for Commonly Wished Replies,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790014,10.1145/2785956.2790014,"Increasing network speeds challenge the packet processing performance of networked systems. This can mainly be attributed to processing overhead caused by the split between the kernel-space network stack and user-space applications. To mitigate this overhead, we propose Santa, an application agnostic kernel-level cache of frequent requests. By allowing user-space applications to offload frequent requests to the kernel-space, Santa offers drastic performance improvements and unlocks the speed of kernel-space networking for legacy server software without requiring extensive changes.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,597–598,2,,"London, United Kingdom",SIGCOMM '15,,,,,,
493,article,"Juluri, Parikshit and Medhi, Deep",Cache'n DASH: Efficient Caching for DASH,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790015,10.1145/2829988.2790015,"HTTP-based video streaming services have been dominating the global IP traffic over the last few years. Caching of video content reduces the load on the content servers. In the case of Dynamic Adaptive Streaming over HTTP (DASH), for every video the server needs to host multiple representations of the same video file. These individual representations are further broken down into smaller segments. Hence, for each video the server needs to host thousands of segments out of which, the client downloads a subset of the segments. Also, depending on the network conditions, the adaptation scheme used at the client-end might request a different set of video segments (varying in bitrate) for the same video. The caching of DASH videos presents unique challenges. In order to optimize the cache hits and minimize the misses for DASH video streaming services we propose an Adaptation Aware Cache (AAC) framework to determine the segments that are to be prefetched and retained in the cache. In the current scheme, we use bandwidth estimates at the cache server and the knowledge of the rate adaptation scheme used by the client to estimate the next segment requests, thus improving the prefetching at the cache.",,599–600,2,"cache, http, bandwidth estimation, dash, video",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
494,inproceedings,"Juluri, Parikshit and Medhi, Deep",Cache'n DASH: Efficient Caching for DASH,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790015,10.1145/2785956.2790015,"HTTP-based video streaming services have been dominating the global IP traffic over the last few years. Caching of video content reduces the load on the content servers. In the case of Dynamic Adaptive Streaming over HTTP (DASH), for every video the server needs to host multiple representations of the same video file. These individual representations are further broken down into smaller segments. Hence, for each video the server needs to host thousands of segments out of which, the client downloads a subset of the segments. Also, depending on the network conditions, the adaptation scheme used at the client-end might request a different set of video segments (varying in bitrate) for the same video. The caching of DASH videos presents unique challenges. In order to optimize the cache hits and minimize the misses for DASH video streaming services we propose an Adaptation Aware Cache (AAC) framework to determine the segments that are to be prefetched and retained in the cache. In the current scheme, we use bandwidth estimates at the cache server and the knowledge of the rate adaptation scheme used by the client to estimate the next segment requests, thus improving the prefetching at the cache.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,599–600,2,"dash, http, bandwidth estimation, video, cache","London, United Kingdom",SIGCOMM '15,,,,,,
495,article,"Chang, Liqiong and Chen, Xiaojiang and Fang, Dingyi and Wang, Ju and Xing, Tianzhang and Liu, Chen and Tang, Zhanyong",FALE: Fine-Grained Device Free Localization That Can Adaptively Work in Different Areas with Little Effort,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790020,10.1145/2829988.2790020,"Many emerging applications and the ubiquitous wireless signals have accelerated the development of Device Free localization (DFL) techniques, which can localize objects without the need to carry any wireless devices. Most traditional DFL methods have a main drawback that as the pre-obtained Received Signal Strength (RSS) measurements (i.e., fingerprint) in one area cannot be directly applied to the new area for localization, and the calibration process of each area will result in the human effort exhausting problem. In this paper, we propose FALE, a fine-grained transferring DFL method that can adaptively work in different areas with little human effort and low energy consumption. FALE employs a rigorously designed transferring function to transfer the fingerprint into a projected space, and reuse it across different areas, thus greatly reduce the human effort. On the other hand, FALE can reduce the data volume and energy consumption by taking advantage of the compressive sensing (CS) theory. Extensive real-word experimental results also illustrate the effectiveness of FALE.",,601–602,2,"transferring, received signal strength, area diversity, device free localization",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
496,inproceedings,"Chang, Liqiong and Chen, Xiaojiang and Fang, Dingyi and Wang, Ju and Xing, Tianzhang and Liu, Chen and Tang, Zhanyong",FALE: Fine-Grained Device Free Localization That Can Adaptively Work in Different Areas with Little Effort,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790020,10.1145/2785956.2790020,"Many emerging applications and the ubiquitous wireless signals have accelerated the development of Device Free localization (DFL) techniques, which can localize objects without the need to carry any wireless devices. Most traditional DFL methods have a main drawback that as the pre-obtained Received Signal Strength (RSS) measurements (i.e., fingerprint) in one area cannot be directly applied to the new area for localization, and the calibration process of each area will result in the human effort exhausting problem. In this paper, we propose FALE, a fine-grained transferring DFL method that can adaptively work in different areas with little human effort and low energy consumption. FALE employs a rigorously designed transferring function to transfer the fingerprint into a projected space, and reuse it across different areas, thus greatly reduce the human effort. On the other hand, FALE can reduce the data volume and energy consumption by taking advantage of the compressive sensing (CS) theory. Extensive real-word experimental results also illustrate the effectiveness of FALE.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,601–602,2,"device free localization, received signal strength, transferring, area diversity","London, United Kingdom",SIGCOMM '15,,,,,,
497,article,"Markmann, Tobias and Schmidt, Thomas C. and W\""{a",Federated End-to-End Authentication for the Constrained Internet of Things Using IBC and ECC,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790021,10.1145/2829988.2790021,"Authentication of smart objects is a major challenge for the Internet of Things (IoT), and has been left open in DTLS. Leveraging locally managed IPv6 addresses with identity-based cryptography (IBC), we propose an efficient end-to-end authentication that (a) assigns a robust and deployment-friendly federation scheme to gateways of IoT subnetworks, and (b) has been evaluated with a modern twisted Edwards elliptic curve cryptography (ECC). Our early results demonstrate feasibility and promise efficiency after ongoing optimizations.",,603–604,2,"smart objects, end-to-end security, ID-based cryptography, federation, authentication",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
498,inproceedings,"Markmann, Tobias and Schmidt, Thomas C. and W\""{a",Federated End-to-End Authentication for the Constrained Internet of Things Using IBC and ECC,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790021,10.1145/2785956.2790021,"Authentication of smart objects is a major challenge for the Internet of Things (IoT), and has been left open in DTLS. Leveraging locally managed IPv6 addresses with identity-based cryptography (IBC), we propose an efficient end-to-end authentication that (a) assigns a robust and deployment-friendly federation scheme to gateways of IoT subnetworks, and (b) has been evaluated with a modern twisted Edwards elliptic curve cryptography (ECC). Our early results demonstrate feasibility and promise efficiency after ongoing optimizations.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,603–604,2,"federation, ID-based cryptography, end-to-end security, smart objects, authentication","London, United Kingdom",SIGCOMM '15,,,,,,
499,article,"Pirzada, Hasnain Ali and Mahboob, Muhammad Raza and Qazi, Ihsan Ayyub",ESDN: Rethinking Datacenter Transports Using End-Host SDN Controllers,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790022,10.1145/2829988.2790022,We propose eSDN; a practical approach for deploying new datacenter transports without requiring any changes to the switches. eSDN uses light-weight SDN controllers at the end-hosts for querying network state. It obviates the need for statistics collection by a centralized controller especially on short timescales. We show that eSDN can scale well and allow a range of datacenter transports to be realized.,,605–606,2,"SDN, datacenters, transport protocols",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
500,inproceedings,"Pirzada, Hasnain Ali and Mahboob, Muhammad Raza and Qazi, Ihsan Ayyub",ESDN: Rethinking Datacenter Transports Using End-Host SDN Controllers,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790022,10.1145/2785956.2790022,We propose eSDN; a practical approach for deploying new datacenter transports without requiring any changes to the switches. eSDN uses light-weight SDN controllers at the end-hosts for querying network state. It obviates the need for statistics collection by a centralized controller especially on short timescales. We show that eSDN can scale well and allow a range of datacenter transports to be realized.,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,605–606,2,"datacenters, SDN, transport protocols","London, United Kingdom",SIGCOMM '15,,,,,,
501,article,"Reda, Waleed and Suresh, Lalith and Canini, Marco and Braithwaite, Sean",BRB: BetteR Batch Scheduling to Reduce Tail Latencies in Cloud Data Stores,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790023,10.1145/2829988.2790023,"A common pattern in the architectures of modern interactive web-services is that of large request fan-outs, where even a single end-user request (task) arriving at an application server triggers tens to thousands of data accesses (sub-tasks) to different stateful backend servers. The overall response time of each task is bottlenecked by the completion time of the slowest sub-task, making such workloads highly sensitive to the tail of latency distribution of the backend tier. The large number of decentralized application servers and skewed workload patterns exacerbate the challenge in addressing this problem. We address these challenges through BetteR Batch (BRB). By carefully scheduling requests in a decentralized and task-aware manner, BRB enables low-latency distributed storage systems to deliver predictable performance in the presence of large request fan-outs. Our preliminary simulation results based on production workloads show that our proposed design is at the 99th percentile latency within 38% of an ideal system model while offering latency improvements over the state-of-the-art by a factor of 2.",,607–608,2,"data storesle, data centers, load balancing, tail latency, batches",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
502,inproceedings,"Reda, Waleed and Suresh, Lalith and Canini, Marco and Braithwaite, Sean",BRB: BetteR Batch Scheduling to Reduce Tail Latencies in Cloud Data Stores,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790023,10.1145/2785956.2790023,"A common pattern in the architectures of modern interactive web-services is that of large request fan-outs, where even a single end-user request (task) arriving at an application server triggers tens to thousands of data accesses (sub-tasks) to different stateful backend servers. The overall response time of each task is bottlenecked by the completion time of the slowest sub-task, making such workloads highly sensitive to the tail of latency distribution of the backend tier. The large number of decentralized application servers and skewed workload patterns exacerbate the challenge in addressing this problem. We address these challenges through BetteR Batch (BRB). By carefully scheduling requests in a decentralized and task-aware manner, BRB enables low-latency distributed storage systems to deliver predictable performance in the presence of large request fan-outs. Our preliminary simulation results based on production workloads show that our proposed design is at the 99th percentile latency within 38% of an ideal system model while offering latency improvements over the state-of-the-art by a factor of 2.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,607–608,2,"data centers, batches, data storesle, load balancing, tail latency","London, United Kingdom",SIGCOMM '15,,,,,,
503,article,"Nan, Guoshun and Qiao, Xiuquan and Tu, Yukai and Tan, Wei and Guo, Lei and Chen, Junliang",Design and Implementation: The Native Web Browser and Server for Content-Centric Networking,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2790024,10.1145/2829988.2790024,"Content-Centric Networking (CCN) has recently emerged as a clean-slate Future Internet architecture which has a completely different communication pattern compared with exiting IP network. Since the World Wide Web has become one of the most popular and important applications on the Internet, how to effectively support the dominant browser and server based web applications is a key to the success of CCN. However, the existing web browsers and servers are mainly designed for the HTTP protocol over TCP/IP networks and cannot directly support CCN-based web applications. Existing research mainly focuses on plug-in or proxy/gateway approaches at client and server sides, and these schemes seriously impact the service performance due to multiple protocol conversions. To address above problems, we designed and implemented a CCN web browser and a CCN web server to natively support CCN protocol. To facilitate the smooth evolution from IP networks to CCN, CCNBrowser and CCNxTomcat also support the HTTP protocol besides the CCN. Experimental results show that CCNBrowser and CCNxTomcat outperform existing implementations. Finally, a real CCN-based web application is deployed on a CCN experimental testbed, which validates the applicability of CCNBrowser and CCNxTomcat.",,609–610,2,"web browser, content-centric networking, web server",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
504,inproceedings,"Nan, Guoshun and Qiao, Xiuquan and Tu, Yukai and Tan, Wei and Guo, Lei and Chen, Junliang",Design and Implementation: The Native Web Browser and Server for Content-Centric Networking,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2790024,10.1145/2785956.2790024,"Content-Centric Networking (CCN) has recently emerged as a clean-slate Future Internet architecture which has a completely different communication pattern compared with exiting IP network. Since the World Wide Web has become one of the most popular and important applications on the Internet, how to effectively support the dominant browser and server based web applications is a key to the success of CCN. However, the existing web browsers and servers are mainly designed for the HTTP protocol over TCP/IP networks and cannot directly support CCN-based web applications. Existing research mainly focuses on plug-in or proxy/gateway approaches at client and server sides, and these schemes seriously impact the service performance due to multiple protocol conversions. To address above problems, we designed and implemented a CCN web browser and a CCN web server to natively support CCN protocol. To facilitate the smooth evolution from IP networks to CCN, CCNBrowser and CCNxTomcat also support the HTTP protocol besides the CCN. Experimental results show that CCNBrowser and CCNxTomcat outperform existing implementations. Finally, a real CCN-based web application is deployed on a CCN experimental testbed, which validates the applicability of CCNBrowser and CCNxTomcat.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,609–610,2,"web server, web browser, content-centric networking","London, United Kingdom",SIGCOMM '15,,,,,,
505,inproceedings,"Byers, John","Session Details: Security, Privacy, and Censorship",2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3261010,10.1145/3261010,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,,,,"London, United Kingdom",SIGCOMM '15,,,,,,
506,article,"Levin, Dave and Lee, Youndo and Valenta, Luke and Li, Zhihao and Lai, Victoria and Lumezanu, Cristian and Spring, Neil and Bhattacharjee, Bobby",Alibi Routing,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787509,10.1145/2829988.2787509,"There are several mechanisms by which users can gain insight into where their packets have gone, but no mechanisms allow users undeniable proof that their packets did not traverse certain parts of the world while on their way to or from another host. This paper introduces the problem of finding ""proofs of avoidance"": evidence that the paths taken by a packet and its response avoided a user-specified set of ""forbidden"" geographic regions. Proving that something did not happen is often intractable, but we demonstrate a low-overhead proof structure built around the idea of what we call ""alibis"": relays with particular timing constraints that, when upheld, would make it impossible to traverse both the relay and the forbidden regions.We present Alibi Routing, a peer-to-peer overlay routing system for finding alibis securely and efficiently. One of the primary distinguishing characteristics of Alibi Routing is that it does not require knowledge of--or modifications to--the Internet's routing hardware or policies. Rather, Alibi Routing is able to derive its proofs of avoidance from user-provided GPS coordinates and speed of light propagation delays. Using a PlanetLab deployment and larger-scale simulations, we evaluate Alibi Routing to demonstrate that many source-destination pairs can avoid countries of their choosing with little latency inflation. We also identify when Alibi Routing does not work: it has difficulty avoiding regions that users are very close to (or, of course, inside of).",,611–624,14,"overlay routing, alibi routing, provable route avoidance, peer-to-peer, censorship avoidance",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
507,inproceedings,"Levin, Dave and Lee, Youndo and Valenta, Luke and Li, Zhihao and Lai, Victoria and Lumezanu, Cristian and Spring, Neil and Bhattacharjee, Bobby",Alibi Routing,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787509,10.1145/2785956.2787509,"There are several mechanisms by which users can gain insight into where their packets have gone, but no mechanisms allow users undeniable proof that their packets did not traverse certain parts of the world while on their way to or from another host. This paper introduces the problem of finding ""proofs of avoidance"": evidence that the paths taken by a packet and its response avoided a user-specified set of ""forbidden"" geographic regions. Proving that something did not happen is often intractable, but we demonstrate a low-overhead proof structure built around the idea of what we call ""alibis"": relays with particular timing constraints that, when upheld, would make it impossible to traverse both the relay and the forbidden regions.We present Alibi Routing, a peer-to-peer overlay routing system for finding alibis securely and efficiently. One of the primary distinguishing characteristics of Alibi Routing is that it does not require knowledge of--or modifications to--the Internet's routing hardware or policies. Rather, Alibi Routing is able to derive its proofs of avoidance from user-provided GPS coordinates and speed of light propagation delays. Using a PlanetLab deployment and larger-scale simulations, we evaluate Alibi Routing to demonstrate that many source-destination pairs can avoid countries of their choosing with little latency inflation. We also identify when Alibi Routing does not work: it has difficulty avoiding regions that users are very close to (or, of course, inside of).",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,611–624,14,"peer-to-peer, overlay routing, censorship avoidance, provable route avoidance, alibi routing","London, United Kingdom",SIGCOMM '15,,,,,,
508,article,"Konte, Maria and Perdisci, Roberto and Feamster, Nick",ASwatch: An AS Reputation System to Expose Bulletproof Hosting ASes,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787494,10.1145/2829988.2787494,,,625–638,14,"AS reputation, malicious networks, bulletproof hosting",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
509,inproceedings,"Konte, Maria and Perdisci, Roberto and Feamster, Nick",ASwatch: An AS Reputation System to Expose Bulletproof Hosting ASes,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787494,10.1145/2785956.2787494,,Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,625–638,14,"malicious networks, AS reputation, bulletproof hosting","London, United Kingdom",SIGCOMM '15,,,,,,
510,article,"Le Blond, Stevens and Choffnes, David and Caldwell, William and Druschel, Peter and Merritt, Nicholas","Herd: A Scalable, Traffic Analysis Resistant Anonymity Network for VoIP Systems",2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787491,10.1145/2829988.2787491,"Effectively anonymizing Voice-over-IP (VoIP) calls requires a scalable anonymity network that is resilient to traffic analysis and has sufficiently low delay for high-quality voice calls. The popular Tor anonymity network, for instance, is not designed for the former and cannot typically achieve the latter. In this paper, we present the design, implementation, and experimental evaluation of Herd, an anonymity network where a set of dedicated, fully interconnected cloud-based proxies yield suitably low-delay circuits, while untrusted superpeers add scalability. Herd provides caller/callee anonymity among the clients within a trust zone (e.g., jurisdiction) and under a strong adversarial model. Simulations based on a trace of 370 million mobile phone calls among 10.8 million users indicate that Herd achieves anonymity among millions of clients with low bandwidth requirements, and that superpeers decrease the bandwidth and CPU requirements of the trusted infrastructure by an order of magnitude. Finally, experiments using a prototype deployment on Amazon EC2 show that Herd has a delay low enough for high-quality calls in most cases.",,639–652,14,"anonymity networks, strong anonymity, voice-over-IP, intersection attacks",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
511,inproceedings,"Le Blond, Stevens and Choffnes, David and Caldwell, William and Druschel, Peter and Merritt, Nicholas","Herd: A Scalable, Traffic Analysis Resistant Anonymity Network for VoIP Systems",2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787491,10.1145/2785956.2787491,"Effectively anonymizing Voice-over-IP (VoIP) calls requires a scalable anonymity network that is resilient to traffic analysis and has sufficiently low delay for high-quality voice calls. The popular Tor anonymity network, for instance, is not designed for the former and cannot typically achieve the latter. In this paper, we present the design, implementation, and experimental evaluation of Herd, an anonymity network where a set of dedicated, fully interconnected cloud-based proxies yield suitably low-delay circuits, while untrusted superpeers add scalability. Herd provides caller/callee anonymity among the clients within a trust zone (e.g., jurisdiction) and under a strong adversarial model. Simulations based on a trace of 370 million mobile phone calls among 10.8 million users indicate that Herd achieves anonymity among millions of clients with low bandwidth requirements, and that superpeers decrease the bandwidth and CPU requirements of the trusted infrastructure by an order of magnitude. Finally, experiments using a prototype deployment on Amazon EC2 show that Herd has a delay low enough for high-quality calls in most cases.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,639–652,14,"anonymity networks, intersection attacks, strong anonymity, voice-over-IP","London, United Kingdom",SIGCOMM '15,,,,,,
512,article,"Burnett, Sam and Feamster, Nick",Encore: Lightweight Measurement of Web Censorship with Cross-Origin Requests,2015,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2829988.2787485,10.1145/2829988.2787485,"Despite the pervasiveness of Internet censorship, we have scant data on its extent, mechanisms, and evolution. Measuring censorship is challenging: it requires continual measurement of reachability to many target sites from diverse vantage points. Amassing suitable vantage points for longitudinal measurement is difficult; existing systems have achieved only small, short-lived deployments. We observe, however, that most Internet users access content via Web browsers, and the very nature of Web site design allows browsers to make requests to domains with different origins than the main Web page. We present Encore, a system that harnesses cross-origin requests to measure Web filtering from a diverse set of vantage points without requiring users to install custom software, enabling longitudinal measurements from many vantage points. We explain how Encore induces Web clients to perform cross-origin requests that measure Web filtering, design a distributed platform for scheduling and collecting these measurements, show the feasibility of a global-scale deployment with a pilot study and an analysis of potentially censored Web content, identify several cases of filtering in six months of measurements, and discuss ethical concerns that would arise with widespread deployment.",,653–667,15,"web security, network measurement, web censorship",,,October 2015,45,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
513,inproceedings,"Burnett, Sam and Feamster, Nick",Encore: Lightweight Measurement of Web Censorship with Cross-Origin Requests,2015,9781450335423,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2785956.2787485,10.1145/2785956.2787485,"Despite the pervasiveness of Internet censorship, we have scant data on its extent, mechanisms, and evolution. Measuring censorship is challenging: it requires continual measurement of reachability to many target sites from diverse vantage points. Amassing suitable vantage points for longitudinal measurement is difficult; existing systems have achieved only small, short-lived deployments. We observe, however, that most Internet users access content via Web browsers, and the very nature of Web site design allows browsers to make requests to domains with different origins than the main Web page. We present Encore, a system that harnesses cross-origin requests to measure Web filtering from a diverse set of vantage points without requiring users to install custom software, enabling longitudinal measurements from many vantage points. We explain how Encore induces Web clients to perform cross-origin requests that measure Web filtering, design a distributed platform for scheduling and collecting these measurements, show the feasibility of a global-scale deployment with a pilot study and an analysis of potentially censored Web content, identify several cases of filtering in six months of measurements, and discuss ethical concerns that would arise with widespread deployment.",Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication,653–667,15,"web censorship, network measurement, web security","London, United Kingdom",SIGCOMM '15,,,,,,
514,article,"Varghese, George",Keynote: Life in the Fast Lane,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626337,10.1145/2740070.2626337,"The most compelling ideas in systems are abstractions such as virtual memory, sockets, or packet scheduling. Algorithmics is the servant of abstraction, allowing system performance to approach that of the underlying hardware, sometimes by using efficient algorithms but often by simply leveraging other aspects of the system. I will survey the trajectory of network algorithmics starting with a focus on speed and scale in the 1990s to measurement and security in the 2000s. While doing so, I will reflect on my experiences in choosing problems and conducting research. I will conclude by describing my passion for the emerging field of network verification and its confluence with programming language research.George Varghese worked at DEC designing DECNET protocols before obtaining his Ph.D. in 1992 from MIT. He worked from 1993-1999 at Washington University and from 1999 to 2012 at UCSD, both as professor of computer science. He joined Microsoft Research as a Principal Researcher in 2012. He won the ONR Young Investigator Award in 1996, and was elected to be a Fellow of the Association for Computing Machinery (ACM) in 2002. He helped design the 40 Gbps forwarding engine for Procket Networks, subsequently acquired by Cisco Systems. His book ""Network Algorithmics"" was published in December 2004 by Morgan-Kaufman. He co-founded NetSift Inc. in May 2004. NetSift was acquired by Cisco in 2005. He was the 2014 winner of the IEEE Koji Kobayashi Computers and Communications Award",,1–2,2,"algorithmics, networks",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
515,inproceedings,"Varghese, George",Keynote: Life in the Fast Lane,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626337,10.1145/2619239.2626337,"The most compelling ideas in systems are abstractions such as virtual memory, sockets, or packet scheduling. Algorithmics is the servant of abstraction, allowing system performance to approach that of the underlying hardware, sometimes by using efficient algorithms but often by simply leveraging other aspects of the system. I will survey the trajectory of network algorithmics starting with a focus on speed and scale in the 1990s to measurement and security in the 2000s. While doing so, I will reflect on my experiences in choosing problems and conducting research. I will conclude by describing my passion for the emerging field of network verification and its confluence with programming language research.George Varghese worked at DEC designing DECNET protocols before obtaining his Ph.D. in 1992 from MIT. He worked from 1993-1999 at Washington University and from 1999 to 2012 at UCSD, both as professor of computer science. He joined Microsoft Research as a Principal Researcher in 2012. He won the ONR Young Investigator Award in 1996, and was elected to be a Fellow of the Association for Computing Machinery (ACM) in 2002. He helped design the 40 Gbps forwarding engine for Procket Networks, subsequently acquired by Cisco Systems. His book ""Network Algorithmics"" was published in December 2004 by Morgan-Kaufman. He co-founded NetSift Inc. in May 2004. NetSift was acquired by Cisco in 2005. He was the 2014 winner of the IEEE Koji Kobayashi Computers and Communications Award",Proceedings of the 2014 ACM Conference on SIGCOMM,1–2,2,"algorithmics, networks","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
516,inproceedings,"Godfrey, Brighten",Session Details: Data Plane,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246772,10.1145/3246772,,Proceedings of the 2014 ACM Conference on SIGCOMM,,,,"Chicago, Illinois, USA",SIGCOMM '14,,,,,,
517,article,"Jeyakumar, Vimalkumar and Alizadeh, Mohammad and Geng, Yilong and Kim, Changhoon and Mazi\`{e",Millions of Little Minions: Using Packets for Low Latency Network Programming and Visibility,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626292,10.1145/2740070.2626292,"This paper presents a practical approach to rapidly introducing new dataplane functionality into networks: End-hosts embed tiny programs into packets to actively query and manipulate a network's internal state. We show how this ""tiny packet program"" (TPP) interface gives end-hosts unprecedented visibility into network behavior, enabling them to work with the network to achieve a desired functionality. Our design leverages what each component does best: (a) switches forward and execute tiny packet programs (at most 5~instructions) in-band at line rate, and (b) end-hosts perform arbitrary (and easily updated) computation on network state. By implementing three different research proposals, we show that TPPs are useful. Using a hardware prototype on a NetFPGA, we show our design is feasible at a reasonable cost.",,3–14,12,"measurement, design, active networks, performance",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
518,inproceedings,"Jeyakumar, Vimalkumar and Alizadeh, Mohammad and Geng, Yilong and Kim, Changhoon and Mazi\`{e",Millions of Little Minions: Using Packets for Low Latency Network Programming and Visibility,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626292,10.1145/2619239.2626292,"This paper presents a practical approach to rapidly introducing new dataplane functionality into networks: End-hosts embed tiny programs into packets to actively query and manipulate a network's internal state. We show how this ""tiny packet program"" (TPP) interface gives end-hosts unprecedented visibility into network behavior, enabling them to work with the network to achieve a desired functionality. Our design leverages what each component does best: (a) switches forward and execute tiny packet programs (at most 5~instructions) in-band at line rate, and (b) end-hosts perform arbitrary (and easily updated) computation on network state. By implementing three different research proposals, we show that TPPs are useful. Using a hardware prototype on a NetFPGA, we show our design is feasible at a reasonable cost.",Proceedings of the 2014 ACM Conference on SIGCOMM,3–14,12,"design, performance, active networks, measurement","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
519,article,"Kogan, Kirill and Nikolenko, Sergey and Rottenstreich, Ori and Culhane, William and Eugster, Patrick",SAX-PAC (Scalable And EXpressive PAcket Classification),2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626294,10.1145/2740070.2626294,"Efficient packet classification is a core concern for network services. Traditional multi-field classification approaches, in both software and ternary content-addressable memory (TCAMs), entail tradeoffs between (memory) space and (lookup) time. TCAMs cannot efficiently represent range rules, a common class of classification rules confining values of packet fields to given ranges. The exponential space growth of TCAM entries relative to the number of fields is exacerbated when multiple fields contain ranges. In this work, we present a novel approach which identifies properties of many classifiers which can be implemented in linear space and with worst-case guaranteed logarithmic time emph{and",,15–26,12,"TCAM, packet classification",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
520,inproceedings,"Kogan, Kirill and Nikolenko, Sergey and Rottenstreich, Ori and Culhane, William and Eugster, Patrick",SAX-PAC (Scalable And EXpressive PAcket Classification),2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626294,10.1145/2619239.2626294,"Efficient packet classification is a core concern for network services. Traditional multi-field classification approaches, in both software and ternary content-addressable memory (TCAMs), entail tradeoffs between (memory) space and (lookup) time. TCAMs cannot efficiently represent range rules, a common class of classification rules confining values of packet fields to given ranges. The exponential space growth of TCAM entries relative to the number of fields is exacerbated when multiple fields contain ranges. In this work, we present a novel approach which identifies properties of many classifiers which can be implemented in linear space and with worst-case guaranteed logarithmic time emph{and",Proceedings of the 2014 ACM Conference on SIGCOMM,15–26,12,"TCAM, packet classification","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
521,article,"Gandhi, Rohan and Liu, Hongqiang Harry and Hu, Y. Charlie and Lu, Guohan and Padhye, Jitendra and Yuan, Lihua and Zhang, Ming",Duet: Cloud Scale Load Balancing with Hardware and Software,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626317,10.1145/2740070.2626317,"Load balancing is a foundational function of datacenter infrastructures and is critical to the performance of online services hosted in datacenters. As the demand for cloud services grows, expensive and hard-to-scale dedicated hardware load balancers are being replaced with software load balancers that scale using a distributed data plane that runs on commodity servers. Software load balancers offer low cost, high availability and high flexibility, but suffer high latency and low capacity per load balancer, making them less than ideal for applications that demand either high throughput, or low latency or both. In this paper, we present Duet, which offers all the benefits of software load balancer, along with low latency and high availability -- at next to no cost. We do this by exploiting a hitherto overlooked resource in the data center networks -- the switches themselves. We show how to embed the load balancing functionality into existing hardware switches, thereby achieving organic scalability at no extra cost. For flexibility and high availability, Duet seamlessly integrates the switch-based load balancer with a small deployment of software load balancer. We enumerate and solve several architectural and algorithmic challenges involved in building such a hybrid load balancer. We evaluate Duet using a prototype implementation, as well as extensive simulations driven by traces from our production data centers. Our evaluation shows that Duet provides 10x more capacity than a software load balancer, at a fraction of a cost, while reducing latency by a factor of 10 or more, and is able to quickly adapt to network dynamics including failures.",,27–38,12,"load balancing, SDN, datacenter",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
522,inproceedings,"Gandhi, Rohan and Liu, Hongqiang Harry and Hu, Y. Charlie and Lu, Guohan and Padhye, Jitendra and Yuan, Lihua and Zhang, Ming",Duet: Cloud Scale Load Balancing with Hardware and Software,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626317,10.1145/2619239.2626317,"Load balancing is a foundational function of datacenter infrastructures and is critical to the performance of online services hosted in datacenters. As the demand for cloud services grows, expensive and hard-to-scale dedicated hardware load balancers are being replaced with software load balancers that scale using a distributed data plane that runs on commodity servers. Software load balancers offer low cost, high availability and high flexibility, but suffer high latency and low capacity per load balancer, making them less than ideal for applications that demand either high throughput, or low latency or both. In this paper, we present Duet, which offers all the benefits of software load balancer, along with low latency and high availability -- at next to no cost. We do this by exploiting a hitherto overlooked resource in the data center networks -- the switches themselves. We show how to embed the load balancing functionality into existing hardware switches, thereby achieving organic scalability at no extra cost. For flexibility and high availability, Duet seamlessly integrates the switch-based load balancer with a small deployment of software load balancer. We enumerate and solve several architectural and algorithmic challenges involved in building such a hybrid load balancer. We evaluate Duet using a prototype implementation, as well as extensive simulations driven by traces from our production data centers. Our evaluation shows that Duet provides 10x more capacity than a software load balancer, at a fraction of a cost, while reducing latency by a factor of 10 or more, and is able to quickly adapt to network dynamics including failures.",Proceedings of the 2014 ACM Conference on SIGCOMM,27–38,12,"load balancing, SDN, datacenter","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
523,article,"Yang, Tong and Xie, Gaogang and Li, YanBiao and Fu, Qiaobin and Liu, Alex X. and Li, Qi and Mathy, Laurent",Guarantee IP Lookup Performance with FIB Explosion,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626297,10.1145/2740070.2626297,"The Forwarding Information Base (FIB) of backbone routers has been rapidly growing in size. An ideal IP lookup algorithm should achieve constant, yet small, IP lookup time and on-chip memory usage. However, no prior IP lookup algorithm achieves both requirements at the same time. In this paper, we first propose SAIL, a Splitting Approach to IP Lookup. One splitting is along the dimension of the lookup process, namely finding the prefix length and finding the next hop, and another splitting is along the dimension of prefix length, namely IP lookup on prefixes of length less than or equal to 24 and IP lookup on prefixes of length longer than 24. Second, we propose a suite of algorithms for IP lookup based on our SAIL framework. Third, we implemented our algorithms on four platforms: CPU, FPGA, GPU, and many-core. We conducted extensive experiments to evaluate our algorithms using real FIBs and real traffic from a major ISP in China. Experimental results show that our SAIL algorithms are several times or even two orders of magnitude faster than well known IP lookup algorithms.",,39–50,12,"sail, LPM, IP lookup, virtual router multi-FIB lookup",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
524,inproceedings,"Yang, Tong and Xie, Gaogang and Li, YanBiao and Fu, Qiaobin and Liu, Alex X. and Li, Qi and Mathy, Laurent",Guarantee IP Lookup Performance with FIB Explosion,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626297,10.1145/2619239.2626297,"The Forwarding Information Base (FIB) of backbone routers has been rapidly growing in size. An ideal IP lookup algorithm should achieve constant, yet small, IP lookup time and on-chip memory usage. However, no prior IP lookup algorithm achieves both requirements at the same time. In this paper, we first propose SAIL, a Splitting Approach to IP Lookup. One splitting is along the dimension of the lookup process, namely finding the prefix length and finding the next hop, and another splitting is along the dimension of prefix length, namely IP lookup on prefixes of length less than or equal to 24 and IP lookup on prefixes of length longer than 24. Second, we propose a suite of algorithms for IP lookup based on our SAIL framework. Third, we implemented our algorithms on four platforms: CPU, FPGA, GPU, and many-core. We conducted extensive experiments to evaluate our algorithms using real FIBs and real traffic from a major ISP in China. Experimental results show that our SAIL algorithms are several times or even two orders of magnitude faster than well known IP lookup algorithms.",Proceedings of the 2014 ACM Conference on SIGCOMM,39–50,12,"LPM, sail, IP lookup, virtual router multi-FIB lookup","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
525,inproceedings,"Perrig, Adrian",Session Details: Network Architecture 1,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246773,10.1145/3246773,,Proceedings of the 2014 ACM Conference on SIGCOMM,,,,"Chicago, Illinois, USA",SIGCOMM '14,,,,,,
526,article,"Heilman, Ethan and Cooper, Danny and Reyzin, Leonid and Goldberg, Sharon",From the Consent of the Routed: Improving the Transparency of the RPKI,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626293,10.1145/2740070.2626293,"The Resource Public Key Infrastructure (RPKI) is a new infrastructure that prevents some of the most devastating attacks on interdomain routing. However, the security benefits provided by the RPKI are accomplished via an architecture that empowers centralized authorities to unilaterally revoke any IP prefixes under their control. We propose mechanisms to improve the transparency of the RPKI, in order to mitigate the risk that it will be used for IP address takedowns. First, we present tools that detect and visualize changes to the RPKI that can potentially take down an IP prefix. We use our tools to identify errors and revocations in the production RPKI. Next, we propose modifications to the RPKI's architecture to (1) require any revocation of IP address space to receive consent from all impacted parties, and (2) detect when misbehaving authorities fail to obtain consent. We present a security analysis of our architecture, and estimate its overhead using data-driven analysis.",,51–62,12,"public key infrastructures, RPKI, transparency, security",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
527,inproceedings,"Heilman, Ethan and Cooper, Danny and Reyzin, Leonid and Goldberg, Sharon",From the Consent of the Routed: Improving the Transparency of the RPKI,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626293,10.1145/2619239.2626293,"The Resource Public Key Infrastructure (RPKI) is a new infrastructure that prevents some of the most devastating attacks on interdomain routing. However, the security benefits provided by the RPKI are accomplished via an architecture that empowers centralized authorities to unilaterally revoke any IP prefixes under their control. We propose mechanisms to improve the transparency of the RPKI, in order to mitigate the risk that it will be used for IP address takedowns. First, we present tools that detect and visualize changes to the RPKI that can potentially take down an IP prefix. We use our tools to identify errors and revocations in the production RPKI. Next, we propose modifications to the RPKI's architecture to (1) require any revocation of IP address space to receive consent from all impacted parties, and (2) detect when misbehaving authorities fail to obtain consent. We present a security analysis of our architecture, and estimate its overhead using data-driven analysis.",Proceedings of the 2014 ACM Conference on SIGCOMM,51–62,12,"public key infrastructures, security, transparency, RPKI","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
528,article,"Zhang, Zhiyong and Mara, Ovidiu and Argyraki, Katerina",Network Neutrality Inference,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626308,10.1145/2740070.2626308,"When can we reason about the neutrality of a network based on external observations? We prove conditions under which it is possible to (a) detect neutrality violations and (b) localize them to specific links, based on external observations. Our insight is that, when we make external observations from different vantage points, these will most likely be inconsistent with each other if the network is not neutral. Where existing tomographic techniques try to form solvable systems of equations to infer network properties, we try to form emph{un",,63–74,12,"network neutrality, network tomography",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
529,inproceedings,"Zhang, Zhiyong and Mara, Ovidiu and Argyraki, Katerina",Network Neutrality Inference,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626308,10.1145/2619239.2626308,"When can we reason about the neutrality of a network based on external observations? We prove conditions under which it is possible to (a) detect neutrality violations and (b) localize them to specific links, based on external observations. Our insight is that, when we make external observations from different vantage points, these will most likely be inconsistent with each other if the network is not neutral. Where existing tomographic techniques try to form solvable systems of equations to infer network properties, we try to form emph{un",Proceedings of the 2014 ACM Conference on SIGCOMM,63–74,12,"network neutrality, network tomography","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
530,article,"Naylor, David and Mukerjee, Matthew K. and Steenkiste, Peter",Balancing Accountability and Privacy in the Network,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626306,10.1145/2740070.2626306,"Though most would agree that accountability and privacy are both valuable, today's Internet provides little support for either. Previous efforts have explored ways to offer stronger guarantees for one of the two, typically at the expense of the other; indeed, at first glance accountability and privacy appear mutually exclusive. At the center of the tussle is the source address: in an accountable Internet, source addresses undeniably link packets and senders so hosts can be punished for bad behavior. In a privacy-preserving Internet, source addresses are hidden as much as possible.In this paper, we argue that a balance is possible. We introduce the Accountable and Private Internet Protocol (APIP), which splits source addresses into two separate fields --- an accountability address and a return address --- and introduces independent mechanisms for managing each. Accountability addresses, rather than pointing to hosts, point to accountability delegates, which agree to vouch for packets on their clients' behalves, taking appropriate action when misbehavior is reported. With accountability handled by delegates, senders are now free to mask their return addresses; we discuss a few techniques for doing so.",,75–86,12,"source address, accountability, privacy",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
531,inproceedings,"Naylor, David and Mukerjee, Matthew K. and Steenkiste, Peter",Balancing Accountability and Privacy in the Network,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626306,10.1145/2619239.2626306,"Though most would agree that accountability and privacy are both valuable, today's Internet provides little support for either. Previous efforts have explored ways to offer stronger guarantees for one of the two, typically at the expense of the other; indeed, at first glance accountability and privacy appear mutually exclusive. At the center of the tussle is the source address: in an accountable Internet, source addresses undeniably link packets and senders so hosts can be punished for bad behavior. In a privacy-preserving Internet, source addresses are hidden as much as possible.In this paper, we argue that a balance is possible. We introduce the Accountable and Private Internet Protocol (APIP), which splits source addresses into two separate fields --- an accountability address and a return address --- and introduces independent mechanisms for managing each. Accountability addresses, rather than pointing to hosts, point to accountability delegates, which agree to vouch for packets on their clients' behalves, taking appropriate action when misbehavior is reported. With accountability handled by delegates, senders are now free to mask their return addresses; we discuss a few techniques for doing so.",Proceedings of the 2014 ACM Conference on SIGCOMM,75–86,12,"privacy, accountability, source address","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
532,article,"Czyz, Jakub and Allman, Mark and Zhang, Jing and Iekel-Johnson, Scott and Osterweil, Eric and Bailey, Michael",Measuring IPv6 Adoption,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626295,10.1145/2740070.2626295,"After several IPv4 address exhaustion milestones in the last three years, it is becoming apparent that the world is running out of IPv4 addresses, and the adoption of the next generation Internet protocol, IPv6, though nascent, is accelerating. In order to better understand this unique and disruptive transition, we explore twelve metrics using ten global-scale datasets to create the longest and broadest measurement of IPv6 adoption to date. Using this perspective, we find that adoption, relative to IPv4, varies by two orders of magnitude depending on the measure examined and that care must be taken when evaluating adoption metrics in isolation. Further, we find that regional adoption is not uniform. Finally, and perhaps most surprisingly, we find that over the last three years, the nature of IPv6 utilization-in terms of traffic, content, reliance on transition technology, and performance-has shifted dramatically from prior findings, indicating a maturing of the protocol into production mode. We believe IPv6's recent growth and this changing utilization signal a true quantum leap.",,87–98,12,"internet, dns, measurement, IP, IPv4, IPv6",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
533,inproceedings,"Czyz, Jakub and Allman, Mark and Zhang, Jing and Iekel-Johnson, Scott and Osterweil, Eric and Bailey, Michael",Measuring IPv6 Adoption,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626295,10.1145/2619239.2626295,"After several IPv4 address exhaustion milestones in the last three years, it is becoming apparent that the world is running out of IPv4 addresses, and the adoption of the next generation Internet protocol, IPv6, though nascent, is accelerating. In order to better understand this unique and disruptive transition, we explore twelve metrics using ten global-scale datasets to create the longest and broadest measurement of IPv6 adoption to date. Using this perspective, we find that adoption, relative to IPv4, varies by two orders of magnitude depending on the measure examined and that care must be taken when evaluating adoption metrics in isolation. Further, we find that regional adoption is not uniform. Finally, and perhaps most surprisingly, we find that over the last three years, the nature of IPv6 utilization-in terms of traffic, content, reliance on transition technology, and performance-has shifted dramatically from prior findings, indicating a maturing of the protocol into production mode. We believe IPv6's recent growth and this changing utilization signal a true quantum leap.",Proceedings of the 2014 ACM Conference on SIGCOMM,87–98,12,"measurement, IPv6, internet, IPv4, dns, IP","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
534,article,"Peter, Simon and Javed, Umar and Zhang, Qiao and Woos, Doug and Anderson, Thomas and Krishnamurthy, Arvind",One Tunnel is (Often) Enough,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626318,10.1145/2740070.2626318,"A longstanding problem with the Internet is that it is vulnerable to outages, black holes, hijacking and denial of service. Although architectural solutions have been proposed to address many of these issues, they have had difficulty being adopted due to the need for widespread adoption before most users would see any benefit. This is especially relevant as the Internet is increasingly used for applications where correct and continuous operation is essential.In this paper, we study whether a simple, easy to implement model is sufficient for addressing the aforementioned Internet vulnerabilities. Our model, called ARROW (Advertised Reliable Routing Over Waypoints), is designed to allow users to configure reliable and secure end to end paths through participating providers. With ARROW, a highly reliable ISP offers tunneled transit through its network, along with packet transformation at the ingress, as a service to remote paying customers. Those customers can stitch together reliable end to end paths through a combination of participating and non-participating ISPs in order to improve the fault-tolerance, robustness, and security of mission critical transmissions. Unlike efforts to redesign the Internet from scratch, we show that ARROW can address a set of well-known Internet vulnerabilities, for most users, with the adoption of only a single transit ISP. To demonstrate ARROW, we have added it to a small-scale wide-area ISP we control. We evaluate its performance and failure recovery properties in both simulation and live settings.",,99–110,12,"BGP, source routing, overlay networks, internet, reliability",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
535,inproceedings,"Peter, Simon and Javed, Umar and Zhang, Qiao and Woos, Doug and Anderson, Thomas and Krishnamurthy, Arvind",One Tunnel is (Often) Enough,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626318,10.1145/2619239.2626318,"A longstanding problem with the Internet is that it is vulnerable to outages, black holes, hijacking and denial of service. Although architectural solutions have been proposed to address many of these issues, they have had difficulty being adopted due to the need for widespread adoption before most users would see any benefit. This is especially relevant as the Internet is increasingly used for applications where correct and continuous operation is essential.In this paper, we study whether a simple, easy to implement model is sufficient for addressing the aforementioned Internet vulnerabilities. Our model, called ARROW (Advertised Reliable Routing Over Waypoints), is designed to allow users to configure reliable and secure end to end paths through participating providers. With ARROW, a highly reliable ISP offers tunneled transit through its network, along with packet transformation at the ingress, as a service to remote paying customers. Those customers can stitch together reliable end to end paths through a combination of participating and non-participating ISPs in order to improve the fault-tolerance, robustness, and security of mission critical transmissions. Unlike efforts to redesign the Internet from scratch, we show that ARROW can address a set of well-known Internet vulnerabilities, for most users, with the adoption of only a single transit ISP. To demonstrate ARROW, we have added it to a small-scale wide-area ISP we control. We evaluate its performance and failure recovery properties in both simulation and live settings.",Proceedings of the 2014 ACM Conference on SIGCOMM,99–110,12,"reliability, overlay networks, internet, BGP, source routing","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
536,article,"Nam, Hyunwoo and Kim, Kyung-Hwa and Calin, Doru and Schulzrinne, Henning",YouSlow: A Performance Analysis Tool for Adaptive Bitrate Video Streaming,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631433,10.1145/2740070.2631433,"Adaptive bitrate (ABR) technologies are being widely used in today's popular HTTP-based video streaming such as YouTube and Netflix. Such a rate-switching algorithm embedded in a video player is designed to improve video quality-of-experience (QoE) by selecting an appropriate resolution based on the analysis of network conditions while the video is playing. However, a bad viewing experience is often caused by the video player having difficulty estimating transit or client-side network conditions accurately. In order to analyze the ABR streaming performance, we developed YouSlow, a web browser plug-in that can detect and report live buffer stalling events to our analysis tool. Currently, YouSlow has collected more than 20,000 of YouTube video stalling events over 40 countries.",,111–112,2,"HTTP video streaming, adaptive bitrate streaming (ABR), video quality of experience",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
537,inproceedings,"Nam, Hyunwoo and Kim, Kyung-Hwa and Calin, Doru and Schulzrinne, Henning",YouSlow: A Performance Analysis Tool for Adaptive Bitrate Video Streaming,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631433,10.1145/2619239.2631433,"Adaptive bitrate (ABR) technologies are being widely used in today's popular HTTP-based video streaming such as YouTube and Netflix. Such a rate-switching algorithm embedded in a video player is designed to improve video quality-of-experience (QoE) by selecting an appropriate resolution based on the analysis of network conditions while the video is playing. However, a bad viewing experience is often caused by the video player having difficulty estimating transit or client-side network conditions accurately. In order to analyze the ABR streaming performance, we developed YouSlow, a web browser plug-in that can detect and report live buffer stalling events to our analysis tool. Currently, YouSlow has collected more than 20,000 of YouTube video stalling events over 40 countries.",Proceedings of the 2014 ACM Conference on SIGCOMM,111–112,2,"HTTP video streaming, video quality of experience, adaptive bitrate streaming (ABR)","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
538,article,"Malhotra, Aanchal and Goldberg, Sharon",RPKI vs ROVER: Comparing the Risks of BGP Security Solutions,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631435,10.1145/2740070.2631435,"BGP, the Internet's interdomain routing protocol, is highly vulnerable to routing failures that result from unintentional misconfigurations or deliberate attacks. To defend against these failures, recent years have seen the adoption of the Resource Public Key Infrastructure (RPKI), which currently authorizes 4% of the Internet's routes. The RPKI is a completely new security infrastructure (requiring new servers, caches, and the design of new protocols), a fact that has given rise to some controversy. Thus, an alternative proposal has emerged: Route Origin Verification (ROVER",,113–114,2,"DNS, RPKI, public key infrastructure, routing security",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
539,inproceedings,"Malhotra, Aanchal and Goldberg, Sharon",RPKI vs ROVER: Comparing the Risks of BGP Security Solutions,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631435,10.1145/2619239.2631435,"BGP, the Internet's interdomain routing protocol, is highly vulnerable to routing failures that result from unintentional misconfigurations or deliberate attacks. To defend against these failures, recent years have seen the adoption of the Resource Public Key Infrastructure (RPKI), which currently authorizes 4% of the Internet's routes. The RPKI is a completely new security infrastructure (requiring new servers, caches, and the design of new protocols), a fact that has given rise to some controversy. Thus, an alternative proposal has emerged: Route Origin Verification (ROVER",Proceedings of the 2014 ACM Conference on SIGCOMM,113–114,2,"routing security, RPKI, public key infrastructure, DNS","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
540,article,"Zhang, Baobao and Bi, Jun and Wu, Jianping and Baker, Fred",CTE: Cost-Effective Intra-Domain Traffic Engineering,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631438,10.1145/2740070.2631438,,,115–116,2,"traffic engineering, access control list, static routes",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
541,inproceedings,"Zhang, Baobao and Bi, Jun and Wu, Jianping and Baker, Fred",CTE: Cost-Effective Intra-Domain Traffic Engineering,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631438,10.1145/2619239.2631438,,Proceedings of the 2014 ACM Conference on SIGCOMM,115–116,2,"traffic engineering, static routes, access control list","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
542,article,"Shirali-Shahreza, Sajad and Ganjali, Yashar",Traffic Statistics Collection with FleXam,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631441,10.1145/2740070.2631441,"One of the limitations of wildcard rules in Software Defined Networks, such as OpenFlow, is losing visibility. FleXam is a flexible sampling extension for OpenFlow that allows the controller to define which packets should be sampled, what parts of each packet should be selected, and where they should be sent. Here, we present an interactive demo showing how FleXam enables the controller to dynamically adjust sampling rates and change the sampling scheme to optimally keep up with a sampling budget in the context of a traffic statistics collection application.",,117–118,2,"openflow, SDN, traffic statistics, sampling",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
543,inproceedings,"Shirali-Shahreza, Sajad and Ganjali, Yashar",Traffic Statistics Collection with FleXam,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631441,10.1145/2619239.2631441,"One of the limitations of wildcard rules in Software Defined Networks, such as OpenFlow, is losing visibility. FleXam is a flexible sampling extension for OpenFlow that allows the controller to define which packets should be sampled, what parts of each packet should be selected, and where they should be sent. Here, we present an interactive demo showing how FleXam enables the controller to dynamically adjust sampling rates and change the sampling scheme to optimally keep up with a sampling budget in the context of a traffic statistics collection application.",Proceedings of the 2014 ACM Conference on SIGCOMM,117–118,2,"openflow, sampling, SDN, traffic statistics","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
544,article,"Molavi Kakhki, Arash and Razaghpanah, Abbas and Golani, Rajesh and Choffnes, David and Gill, Phillipa and Mislove, Alan",Identifying Traffic Differentiation on Cellular Data Networks,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631445,10.1145/2740070.2631445,"The goal of this research is to detect traffic differentiation in cellular data networks. We define service differentiation as any attempt to change the performance of network traffic traversing an ISP's boundaries. ISPs may implement differentiation policies for a number of reasons, including load balancing, bandwidth management, or business reasons. Specifically, we focus on detecting whether certain types of network traffic receive better (or worse) performance. As an example, a wireless provider might limit the performance of third-party VoIP or video calling services (or any other competing services) by introducing delays or reducing transfer rates to encourage users to use services provided by the wireless provider. Previous work explored this problem in limited environments. Glasnost focused on BitTorrent in the desktop/laptop environment, and lacked the ability to conduct controlled experiments to provide strong evidence of differentiation. NetDiff covered a wide range of passively gathered traffic from a large ISP but likewise did not support targeted, controlled experiments. We address these limitations with Mobile Replay.",,119–120,2,"traffic differentiation, net neutrality",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
545,inproceedings,"Molavi Kakhki, Arash and Razaghpanah, Abbas and Golani, Rajesh and Choffnes, David and Gill, Phillipa and Mislove, Alan",Identifying Traffic Differentiation on Cellular Data Networks,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631445,10.1145/2619239.2631445,"The goal of this research is to detect traffic differentiation in cellular data networks. We define service differentiation as any attempt to change the performance of network traffic traversing an ISP's boundaries. ISPs may implement differentiation policies for a number of reasons, including load balancing, bandwidth management, or business reasons. Specifically, we focus on detecting whether certain types of network traffic receive better (or worse) performance. As an example, a wireless provider might limit the performance of third-party VoIP or video calling services (or any other competing services) by introducing delays or reducing transfer rates to encourage users to use services provided by the wireless provider. Previous work explored this problem in limited environments. Glasnost focused on BitTorrent in the desktop/laptop environment, and lacked the ability to conduct controlled experiments to provide strong evidence of differentiation. NetDiff covered a wide range of passively gathered traffic from a large ISP but likewise did not support targeted, controlled experiments. We address these limitations with Mobile Replay.",Proceedings of the 2014 ACM Conference on SIGCOMM,119–120,2,"traffic differentiation, net neutrality","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
546,article,"Miao, Rui and Yu, Minlan and Jain, Navendu",NIMBUS: Cloud-Scale Attack Detection and Mitigation,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631446,10.1145/2740070.2631446,,,121–122,2,"software-defined networking, cloud attack",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
547,inproceedings,"Miao, Rui and Yu, Minlan and Jain, Navendu",NIMBUS: Cloud-Scale Attack Detection and Mitigation,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631446,10.1145/2619239.2631446,,Proceedings of the 2014 ACM Conference on SIGCOMM,121–122,2,"cloud attack, software-defined networking","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
548,article,"Mok, Ricky K.P. and Li, Weichao and Chang, Rocky K.C.",A User Behavior Based Cheat Detection Mechanism for Crowdtesting,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631447,10.1145/2740070.2631447,"Crowdtesting is increasingly popular among researchers to carry out subjective assessments of different services. Experimenters can easily assess to a huge pool of human subjects through crowdsourcing platforms. The workers are usually anonymous, and they participate in the experiments independently. Therefore, a fundamental problem threatening the integrity of these platforms is to detect various types of cheating from the workers. In this poster, we propose cheat-detection mechanism based on an analysis of the workers' mouse cursor trajectories. It provides a jQuery-based library to record browser events. We compute a set of metrics from the cursor traces to identify cheaters. We deploy our mechanism to the survey pages for our video quality assessment tasks published on Amazon Mechanical Turk. Our results show that cheaters' cursor movement is usually more direct and contains less pauses.",,123–124,2,"cheat-detection, cursor submovement, crowdsourcing",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
549,inproceedings,"Mok, Ricky K.P. and Li, Weichao and Chang, Rocky K.C.",A User Behavior Based Cheat Detection Mechanism for Crowdtesting,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631447,10.1145/2619239.2631447,"Crowdtesting is increasingly popular among researchers to carry out subjective assessments of different services. Experimenters can easily assess to a huge pool of human subjects through crowdsourcing platforms. The workers are usually anonymous, and they participate in the experiments independently. Therefore, a fundamental problem threatening the integrity of these platforms is to detect various types of cheating from the workers. In this poster, we propose cheat-detection mechanism based on an analysis of the workers' mouse cursor trajectories. It provides a jQuery-based library to record browser events. We compute a set of metrics from the cursor traces to identify cheaters. We deploy our mechanism to the survey pages for our video quality assessment tasks published on Amazon Mechanical Turk. Our results show that cheaters' cursor movement is usually more direct and contains less pauses.",Proceedings of the 2014 ACM Conference on SIGCOMM,123–124,2,"cheat-detection, cursor submovement, crowdsourcing","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
550,article,"Csoma, Attila and Sonkoly, Bal\'{a","ESCAPE: Extensible Service Chain Prototyping Environment Using Mininet, Click, NETCONF and POX",2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631448,10.1145/2740070.2631448,"Mininet is a great prototyping tool which combines existing SDN-related software components (e.g., Open vSwitch, OpenFlow controllers, network namespaces, cgroups) into a framework, which can automatically set up and configure customized OpenFlow testbeds scaling up to hundreds of nodes. Standing on the shoulders of Mininet, we implement a similar prototyping system called ESCAPE, which can be used to develop and test various components of the service chaining architecture. Our framework incorporates Click for implementing Virtual Network Functions (VNF), NETCONF for managing Click-based VNFs and POX for taking care of traffic steering. We also add our extensible Orchestrator module, which can accommodate mapping algorithms from abstract service descriptions to deployed and running service chains.",,125–126,2,"prototyping, NETCONF, mininet, click, SDN, service chain",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
551,inproceedings,"Csoma, Attila and Sonkoly, Bal\'{a","ESCAPE: Extensible Service Chain Prototyping Environment Using Mininet, Click, NETCONF and POX",2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631448,10.1145/2619239.2631448,"Mininet is a great prototyping tool which combines existing SDN-related software components (e.g., Open vSwitch, OpenFlow controllers, network namespaces, cgroups) into a framework, which can automatically set up and configure customized OpenFlow testbeds scaling up to hundreds of nodes. Standing on the shoulders of Mininet, we implement a similar prototyping system called ESCAPE, which can be used to develop and test various components of the service chaining architecture. Our framework incorporates Click for implementing Virtual Network Functions (VNF), NETCONF for managing Click-based VNFs and POX for taking care of traffic steering. We also add our extensible Orchestrator module, which can accommodate mapping algorithms from abstract service descriptions to deployed and running service chains.",Proceedings of the 2014 ACM Conference on SIGCOMM,125–126,2,"NETCONF, prototyping, SDN, service chain, click, mininet","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
552,article,"Gabielkov, Maksym and Rao, Ashwin and Legout, Arnaud",Sampling Online Social Networks: An Experimental Study of Twitter,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631452,10.1145/2740070.2631452,"Online social networks (OSNs) are an important source of information for scientists in different fields such as computer science, sociology, economics, etc. However, it is hard to study OSNs as they are very large. For instance, Facebook has 1.28 billion active users in March 2014 and Twitter claims 255 million active users in April 2014. Also, companies take measures to prevent crawls of their OSNs and refrain from sharing their data with the research community. For these reasons, we argue that sampling techniques will be the best technique to study OSNs in the future. In this work, we take an experimental approach to study the characteristics of well-known sampling techniques on a full social graph of Twitter crawled in 2012 [2]. Our contribution is to evaluate the behavior of these techniques on a real directed graph by considering two sampling scenarios: (a) obtaining most popular users (b) obtaining an unbiased sample of users, and to find the most suitable sampling techniques for each scenario.",,127–128,2,"social graph, twitter, social networks, sampling",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
553,inproceedings,"Gabielkov, Maksym and Rao, Ashwin and Legout, Arnaud",Sampling Online Social Networks: An Experimental Study of Twitter,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631452,10.1145/2619239.2631452,"Online social networks (OSNs) are an important source of information for scientists in different fields such as computer science, sociology, economics, etc. However, it is hard to study OSNs as they are very large. For instance, Facebook has 1.28 billion active users in March 2014 and Twitter claims 255 million active users in April 2014. Also, companies take measures to prevent crawls of their OSNs and refrain from sharing their data with the research community. For these reasons, we argue that sampling techniques will be the best technique to study OSNs in the future. In this work, we take an experimental approach to study the characteristics of well-known sampling techniques on a full social graph of Twitter crawled in 2012 [2]. Our contribution is to evaluate the behavior of these techniques on a real directed graph by considering two sampling scenarios: (a) obtaining most popular users (b) obtaining an unbiased sample of users, and to find the most suitable sampling techniques for each scenario.",Proceedings of the 2014 ACM Conference on SIGCOMM,127–128,2,"social graph, social networks, twitter, sampling","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
554,article,"Netravali, Ravi and Sivaraman, Anirudh and Winstein, Keith and Das, Somak and Goyal, Ameesh and Balakrishnan, Hari",Mahimahi: A Lightweight Toolkit for Reproducible Web Measurement,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631455,10.1145/2740070.2631455,"This demo presents a measurement toolkit, Mahimahi, that records websites and replays them under emulated network conditions. Mahimahi is structured as a set of arbitrarily composable UNIX shells. It includes two shells to record and replay Web pages, RecordShell and ReplayShell, as well as two shells for network emulation, DelayShell and LinkShell. In addition, Mahimahi includes a corpus of recorded websites along with benchmark results and link traces (https://github.com/ravinet/sites).Mahimahi improves on prior record-and-replay frameworks in three ways. First, it preserves the multi-origin nature of Web pages, present in approximately 98% of the Alexa U.S. Top 500, when replaying. Second, Mahimahi isolates its own network traffic, allowing multiple instances to run concurrently with no impact on the host machine and collected measurements. Finally, Mahimahi is not inherently tied to browsers and can be used to evaluate many different applications.A demo of Mahimahi recording and replaying a Web page over an emulated link can be found at http://youtu.be/vytwDKBA-8s. The source code and instructions to use Mahimahi are available at http://mahimahi.mit.edu/.",,129–130,2,"page load time, record-and-replay, web measurements",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
555,inproceedings,"Netravali, Ravi and Sivaraman, Anirudh and Winstein, Keith and Das, Somak and Goyal, Ameesh and Balakrishnan, Hari",Mahimahi: A Lightweight Toolkit for Reproducible Web Measurement,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631455,10.1145/2619239.2631455,"This demo presents a measurement toolkit, Mahimahi, that records websites and replays them under emulated network conditions. Mahimahi is structured as a set of arbitrarily composable UNIX shells. It includes two shells to record and replay Web pages, RecordShell and ReplayShell, as well as two shells for network emulation, DelayShell and LinkShell. In addition, Mahimahi includes a corpus of recorded websites along with benchmark results and link traces (https://github.com/ravinet/sites).Mahimahi improves on prior record-and-replay frameworks in three ways. First, it preserves the multi-origin nature of Web pages, present in approximately 98% of the Alexa U.S. Top 500, when replaying. Second, Mahimahi isolates its own network traffic, allowing multiple instances to run concurrently with no impact on the host machine and collected measurements. Finally, Mahimahi is not inherently tied to browsers and can be used to evaluate many different applications.A demo of Mahimahi recording and replaying a Web page over an emulated link can be found at http://youtu.be/vytwDKBA-8s. The source code and instructions to use Mahimahi are available at http://mahimahi.mit.edu/.",Proceedings of the 2014 ACM Conference on SIGCOMM,129–130,2,"page load time, web measurements, record-and-replay","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
556,article,"Bischof, Zachary S. and Bustamante, Fabi\'{a",A Time for Reliability: The Growing Importance of Being Always On,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631460,10.1145/2740070.2631460,"When a new technology reaches the market, we typically focus on the want or need that it can fulfill. As the technology becomes a commodity and its market matures, reliability often become a key differentiating factor between competing products. We posit that as broadband capacities continue to improve and users migrate to over-the-Internet services, such as on-demand video and voice-over-IP services, we will see this common pattern emerge for broadband services. In this poster, we present the first study of reliability in broadband networks. Using data collected from residential gateways (via FCC/SamKnows), we study the availability and reliability of fixed-line broadband services across the US. Using natural experiments, we look at the impact of increased network downtime on user network demand. We use traditional metrics (e.g. failure rate, MTBF, MTTR) to quantify broadband services, as well as each ISP's configured DNS. Since the impact of a network outage will depend on when it occurred (e.g. time of day), we compare ISP services by the annual average number of bytes lost, based on typical user demand during periods of network downtime.",,131–132,2,"broadband access networks, access link reliability",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
557,inproceedings,"Bischof, Zachary S. and Bustamante, Fabi\'{a",A Time for Reliability: The Growing Importance of Being Always On,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631460,10.1145/2619239.2631460,"When a new technology reaches the market, we typically focus on the want or need that it can fulfill. As the technology becomes a commodity and its market matures, reliability often become a key differentiating factor between competing products. We posit that as broadband capacities continue to improve and users migrate to over-the-Internet services, such as on-demand video and voice-over-IP services, we will see this common pattern emerge for broadband services. In this poster, we present the first study of reliability in broadband networks. Using data collected from residential gateways (via FCC/SamKnows), we study the availability and reliability of fixed-line broadband services across the US. Using natural experiments, we look at the impact of increased network downtime on user network demand. We use traditional metrics (e.g. failure rate, MTBF, MTTR) to quantify broadband services, as well as each ISP's configured DNS. Since the impact of a network outage will depend on when it occurred (e.g. time of day), we compare ISP services by the annual average number of bytes lost, based on typical user demand during periods of network downtime.",Proceedings of the 2014 ACM Conference on SIGCOMM,131–132,2,"broadband access networks, access link reliability","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
558,article,"Fiadino, Pierdomenico and Schiavone, Mirko and Casas, Pedro",Vivisecting Whatsapp through Large-Scale Measurements in Mobile Networks,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631461,10.1145/2740070.2631461,"WhatsApp, the new giant in instant multimedia messaging in mobile networks is rapidly increasing its popularity, taking over the traditional SMS/MMS messaging. In this paper we present the first large-scale characterization of WhatsApp, useful among others to ISPs willing to understand the impacts of this and similar applications on their networks. Through the combined analysis of passive measurements at the core of a national mobile network, worldwide geo-distributed active measurements, and traffic analysis at end devices, we show that: (i) the WhatsApp hosting architecture is highly centralized and exclusively located in the US; (ii) video sharing covers almost 40% of the total WhatsApp traffic volume; (iii) flow characteristics depend on the OS of the end device; (iv) despite the big latencies to US servers, download throughputs are as high as 1.5 Mbps; (v) users react immediately and negatively to service outages through social networks feedbacks.",,133–134,2,"mobile networks, large-scale measurements, instant multimedia messaging, whatsapp, service outages",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
559,inproceedings,"Fiadino, Pierdomenico and Schiavone, Mirko and Casas, Pedro",Vivisecting Whatsapp through Large-Scale Measurements in Mobile Networks,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631461,10.1145/2619239.2631461,"WhatsApp, the new giant in instant multimedia messaging in mobile networks is rapidly increasing its popularity, taking over the traditional SMS/MMS messaging. In this paper we present the first large-scale characterization of WhatsApp, useful among others to ISPs willing to understand the impacts of this and similar applications on their networks. Through the combined analysis of passive measurements at the core of a national mobile network, worldwide geo-distributed active measurements, and traffic analysis at end devices, we show that: (i) the WhatsApp hosting architecture is highly centralized and exclusively located in the US; (ii) video sharing covers almost 40% of the total WhatsApp traffic volume; (iii) flow characteristics depend on the OS of the end device; (iv) despite the big latencies to US servers, download throughputs are as high as 1.5 Mbps; (v) users react immediately and negatively to service outages through social networks feedbacks.",Proceedings of the 2014 ACM Conference on SIGCOMM,133–134,2,"instant multimedia messaging, mobile networks, service outages, large-scale measurements, whatsapp","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
560,article,"Rula, John P. and Bustamante, Fabian E.",Behind the Curtain: The Importance of Replica Selection in next Generation Cellular Networks,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631465,10.1145/2740070.2631465,,,135–136,2,"domain name system, cellular dns, content delivery networks",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
561,inproceedings,"Rula, John P. and Bustamante, Fabian E.",Behind the Curtain: The Importance of Replica Selection in next Generation Cellular Networks,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631465,10.1145/2619239.2631465,,Proceedings of the 2014 ACM Conference on SIGCOMM,135–136,2,"content delivery networks, cellular dns, domain name system","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
562,article,"Li, Yuliang and Yao, Guang and Bi, Jun",Flowinsight: Decoupling Visibility from Operability in SDN Data Plane,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631468,10.1145/2740070.2631468,,,137–138,2,"data plane, visibility, software-defined network",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
563,inproceedings,"Li, Yuliang and Yao, Guang and Bi, Jun",Flowinsight: Decoupling Visibility from Operability in SDN Data Plane,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631468,10.1145/2619239.2631468,,Proceedings of the 2014 ACM Conference on SIGCOMM,137–138,2,"software-defined network, visibility, data plane","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
564,article,"Jiang, Angela H. and Bischof, Zachary S. and Bustamante, Fabian E.",A Cliq of Content Curators,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631470,10.1145/2740070.2631470,"A social news site presents user-curated content, ranked by popularity. Popular curators like Reddit, or Facebook have become effective way of crowdsourcing news or sharing for personal opinions. Traditionally, these services require a centralized authority to aggregate data and determine what to display. However, the trust issues that arise from a centralized system are particularly damaging to the ""Web democracy"" that social news sites are meant to provide. In this poster, we present cliq, a decentralized social news curator. cliq is a P2P based social news curator that provides private and unbiased reporting. All users in cliq share responsibility for tracking and providing popular content. Any user data that cliq needs to store is also managed across the network. We first inform our design of cliq through an analysis of Reddit. We design a way to provide content curation without a persistent moderator, or usernames.",,139–140,2,"social news curation, P2P systems",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
565,inproceedings,"Jiang, Angela H. and Bischof, Zachary S. and Bustamante, Fabian E.",A Cliq of Content Curators,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631470,10.1145/2619239.2631470,"A social news site presents user-curated content, ranked by popularity. Popular curators like Reddit, or Facebook have become effective way of crowdsourcing news or sharing for personal opinions. Traditionally, these services require a centralized authority to aggregate data and determine what to display. However, the trust issues that arise from a centralized system are particularly damaging to the ""Web democracy"" that social news sites are meant to provide. In this poster, we present cliq, a decentralized social news curator. cliq is a P2P based social news curator that provides private and unbiased reporting. All users in cliq share responsibility for tracking and providing popular content. Any user data that cliq needs to store is also managed across the network. We first inform our design of cliq through an analysis of Reddit. We design a way to provide content curation without a persistent moderator, or usernames.",Proceedings of the 2014 ACM Conference on SIGCOMM,139–140,2,"P2P systems, social news curation","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
566,article,"Vallentin, Matthias and Charousset, Dominik and Schmidt, Thomas C. and Paxson, Vern and W\""{a",Native Actors: How to Scale Network Forensics,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631471,10.1145/2740070.2631471,"When an organization detects a security breach, it undertakes a forensic analysis to figure out what happened. This investigation involves inspecting a wide range of heterogeneous data sources spanning over a long period of time. The iterative nature of the analysis procedure requires an interactive experience with the data. However, the distributed processing paradigms we find in practice today fail to provide this requirement: the batch-oriented nature of MapReduce cannot deliver sub-second round-trip times, and distributed in-memory processing cannot store the terabytes of activity logs needed to inspect during an incident.We present the design and implementation of Visibility Across Space and Time (VAST), a distributed database to support interactive network forensics, and libcppa, its exceptionally scalable messaging core. The extended actor framework libcppa enables VAST to distribute lightweight tasks at negligible overhead. In our live demo, we showcase how VAST enables security analysts to grapple with the huge amounts of data often associated with incident investigations.",,141–142,2,"message-oriented middleware, network forensics, security",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
567,inproceedings,"Vallentin, Matthias and Charousset, Dominik and Schmidt, Thomas C. and Paxson, Vern and W\""{a",Native Actors: How to Scale Network Forensics,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631471,10.1145/2619239.2631471,"When an organization detects a security breach, it undertakes a forensic analysis to figure out what happened. This investigation involves inspecting a wide range of heterogeneous data sources spanning over a long period of time. The iterative nature of the analysis procedure requires an interactive experience with the data. However, the distributed processing paradigms we find in practice today fail to provide this requirement: the batch-oriented nature of MapReduce cannot deliver sub-second round-trip times, and distributed in-memory processing cannot store the terabytes of activity logs needed to inspect during an incident.We present the design and implementation of Visibility Across Space and Time (VAST), a distributed database to support interactive network forensics, and libcppa, its exceptionally scalable messaging core. The extended actor framework libcppa enables VAST to distribute lightweight tasks at negligible overhead. In our live demo, we showcase how VAST enables security analysts to grapple with the huge amounts of data often associated with incident investigations.",Proceedings of the 2014 ACM Conference on SIGCOMM,141–142,2,"message-oriented middleware, network forensics, security","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
568,article,"Koll, David and Li, Jun and Fu, Xiaoming","SOUP: An Online Social Network by the People, for the People",2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631432,10.1145/2740070.2631432,"With increasing frequency, users raise concerns about data privacy and protection in centralized Online Social Networks (OSNs), in which providers have the unprecedented privilege to access and exploit every user's private data at will. To mitigate these concerns, researchers have suggested to decentralize OSNs and thereby enable users to control and manage access to their data themselves. However, previously proposed decentralization approaches suffer from several drawbacks. To tackle their deficiencies, we introduce the Self-Organized Universe of People (SOUP). In this demonstration, we present a prototype of SOUP and share our experiences from a real-world deployment.",,143–144,2,"online social networks, DOSN, decentralization",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
569,inproceedings,"Koll, David and Li, Jun and Fu, Xiaoming","SOUP: An Online Social Network by the People, for the People",2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631432,10.1145/2619239.2631432,"With increasing frequency, users raise concerns about data privacy and protection in centralized Online Social Networks (OSNs), in which providers have the unprecedented privilege to access and exploit every user's private data at will. To mitigate these concerns, researchers have suggested to decentralize OSNs and thereby enable users to control and manage access to their data themselves. However, previously proposed decentralization approaches suffer from several drawbacks. To tackle their deficiencies, we introduce the Self-Organized Universe of People (SOUP). In this demonstration, we present a prototype of SOUP and share our experiences from a real-world deployment.",Proceedings of the 2014 ACM Conference on SIGCOMM,143–144,2,"online social networks, decentralization, DOSN","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
570,article,"Zhang, Bo and Wang, Jinfan and Wang, Xinyu and Cheng, Yingying and Jia, Xiaohua and He, Jianfei",AI3: Application-Independent Information Infrastructure,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631424,10.1145/2740070.2631424,"In the current Internet architecture, application service providers (ASPs) own users' data and social groups information, which made a handful of ASP companies growing bigger and bigger and denied small and medium companies from entering this business. We propose a new architecture, called Application Independent Information Infrastructure (AI3). The design goals of AI3 are: 1) Decoupling users' data from ASPs and users' social relations from ASPs, such that ASPs become independent from users' data and social relations. 2) Open architecture, such that different ASPs can interoperate with each other. This demo is to show a prototype of AI3. The demo has four parts: 1) ASPindependent data management in AI3; 2) ASP-independent management of users' social relations in AI3; 3) inter-domain data transport and user roaming; 4) real-time communications by using AI3. The demo video can be watched at: http://www.cs.cityu.edu.hk/~jia/AI3_DemoVideo.mp4",,145–146,2,"internet architecture, network infrastructure, storage system",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
571,inproceedings,"Zhang, Bo and Wang, Jinfan and Wang, Xinyu and Cheng, Yingying and Jia, Xiaohua and He, Jianfei",AI3: Application-Independent Information Infrastructure,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631424,10.1145/2619239.2631424,"In the current Internet architecture, application service providers (ASPs) own users' data and social groups information, which made a handful of ASP companies growing bigger and bigger and denied small and medium companies from entering this business. We propose a new architecture, called Application Independent Information Infrastructure (AI3). The design goals of AI3 are: 1) Decoupling users' data from ASPs and users' social relations from ASPs, such that ASPs become independent from users' data and social relations. 2) Open architecture, such that different ASPs can interoperate with each other. This demo is to show a prototype of AI3. The demo has four parts: 1) ASPindependent data management in AI3; 2) ASP-independent management of users' social relations in AI3; 3) inter-domain data transport and user roaming; 4) real-time communications by using AI3. The demo video can be watched at: http://www.cs.cityu.edu.hk/~jia/AI3_DemoVideo.mp4",Proceedings of the 2014 ACM Conference on SIGCOMM,145–146,2,"storage system, internet architecture, network infrastructure","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
572,article,"Bharadia, Dinesh and Joshi, Kiran and Katti, Sachin",Robust Full Duplex Radio Link,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631425,10.1145/2740070.2631425,"This paper presents demonstration of a real-time full duplex point-to-point link, where transmission and reception occurs in the same spectrum band simultaneously between a pair of full-duplex radios. This demo first builds a full duplex radio by implementing self-interference cancellation technique on top of a traditional half duplex radio architecture. We then establish a point-to-point link using a pair of these radios that can transmit and receive OFDM packets. By changing the environmental conditions around the full-duplex radios we then demonstrate the robustness of the self-interference cancellation to adapt to the changing environment.",,147–148,2,"wireless radio, interference cancellation, full duplex",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
573,inproceedings,"Bharadia, Dinesh and Joshi, Kiran and Katti, Sachin",Robust Full Duplex Radio Link,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631425,10.1145/2619239.2631425,"This paper presents demonstration of a real-time full duplex point-to-point link, where transmission and reception occurs in the same spectrum band simultaneously between a pair of full-duplex radios. This demo first builds a full duplex radio by implementing self-interference cancellation technique on top of a traditional half duplex radio architecture. We then establish a point-to-point link using a pair of these radios that can transmit and receive OFDM packets. By changing the environmental conditions around the full-duplex radios we then demonstrate the robustness of the self-interference cancellation to adapt to the changing environment.",Proceedings of the 2014 ACM Conference on SIGCOMM,147–148,2,"full duplex, interference cancellation, wireless radio","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
574,article,"Wamser, Florian and Zinner, Thomas and Iffl\""{a",Demonstrating the Prospects of Dynamic Application-Aware Networking in a Home Environment,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631450,10.1145/2740070.2631450,,,149–150,2,"application-aware networking, dynamic resource allocation, home networks, YouTube",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
575,inproceedings,"Wamser, Florian and Zinner, Thomas and Iffl\""{a",Demonstrating the Prospects of Dynamic Application-Aware Networking in a Home Environment,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631450,10.1145/2619239.2631450,,Proceedings of the 2014 ACM Conference on SIGCOMM,149–150,2,"YouTube, application-aware networking, dynamic resource allocation, home networks","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
576,inproceedings,"Sekar, Vyas",Session Details: Middleboxes and Network Services,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246774,10.1145/3246774,,Proceedings of the 2014 ACM Conference on SIGCOMM,,,,"Chicago, Illinois, USA",SIGCOMM '14,,,,,,
577,article,"Craven, Ryan and Beverly, Robert and Allman, Mark",A Middlebox-Cooperative TCP for a Non End-to-End Internet,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626321,10.1145/2740070.2626321,"Understanding, measuring, and debugging IP networks, particularly across administrative domains, is challenging. One particularly daunting aspect of the challenge is the presence of transparent middleboxes---which are now common in today's Internet. In-path middleboxes that modify packet headers are typically transparent to a TCP, yet can impact end-to-end performance or cause blackholes. We develop TCP HICCUPS to reveal packet header manipulation to both endpoints of a TCP connection. HICCUPS permits endpoints to cooperate with currently opaque middleboxes without prior knowledge of their behavior. For example, with visibility into end-to-end behavior, a TCP can selectively enable or disable performance enhancing options. This cooperation enables protocol innovation by allowing new IP or TCP functionality (e.g., ECN, SACK, Multipath TCP, Tcpcrypt) to be deployed without fear of such functionality being misconstrued, modified, or blocked along a path. HICCUPS is incrementally deployable and introduces no new options. We implement and deploy TCP HICCUPS across thousands of disparate Internet paths, highlighting the breadth and scope of subtle and hard to detect middlebox behaviors encountered. We then show how path diagnostic capabilities provided by HICCUPS can benefit applications and the network.",,151–162,12,"header integrity, header modifications, TCP, middlebox",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
578,inproceedings,"Craven, Ryan and Beverly, Robert and Allman, Mark",A Middlebox-Cooperative TCP for a Non End-to-End Internet,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626321,10.1145/2619239.2626321,"Understanding, measuring, and debugging IP networks, particularly across administrative domains, is challenging. One particularly daunting aspect of the challenge is the presence of transparent middleboxes---which are now common in today's Internet. In-path middleboxes that modify packet headers are typically transparent to a TCP, yet can impact end-to-end performance or cause blackholes. We develop TCP HICCUPS to reveal packet header manipulation to both endpoints of a TCP connection. HICCUPS permits endpoints to cooperate with currently opaque middleboxes without prior knowledge of their behavior. For example, with visibility into end-to-end behavior, a TCP can selectively enable or disable performance enhancing options. This cooperation enables protocol innovation by allowing new IP or TCP functionality (e.g., ECN, SACK, Multipath TCP, Tcpcrypt) to be deployed without fear of such functionality being misconstrued, modified, or blocked along a path. HICCUPS is incrementally deployable and introduces no new options. We implement and deploy TCP HICCUPS across thousands of disparate Internet paths, highlighting the breadth and scope of subtle and hard to detect middlebox behaviors encountered. We then show how path diagnostic capabilities provided by HICCUPS can benefit applications and the network.",Proceedings of the 2014 ACM Conference on SIGCOMM,151–162,12,"TCP, header modifications, header integrity, middlebox","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
579,article,"Gember-Jacobson, Aaron and Viswanathan, Raajay and Prakash, Chaithan and Grandl, Robert and Khalid, Junaid and Das, Sourav and Akella, Aditya",OpenNF: Enabling Innovation in Network Function Control,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626313,10.1145/2740070.2626313,"Network functions virtualization (NFV) together with software-defined networking (SDN) has the potential to help operators satisfy tight service level agreements, accurately monitor and manipulate network traffic, and minimize operating expenses. However, in scenarios that require packet processing to be redistributed across a collection of network function (NF) instances, simultaneously achieving all three goals requires a framework that provides efficient, coordinated control of both internal NF state and network forwarding state. To this end, we design a control plane called OpenNF. We use carefully designed APIs and a clever combination of events and forwarding updates to address race conditions, bound overhead, and accommodate a variety of NFs. Our evaluation shows that OpenNF offers efficient state control without compromising flexibility, and requires modest additions to NFs.",,163–174,12,"software-defined networking, middleboxes, network functions",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
580,inproceedings,"Gember-Jacobson, Aaron and Viswanathan, Raajay and Prakash, Chaithan and Grandl, Robert and Khalid, Junaid and Das, Sourav and Akella, Aditya",OpenNF: Enabling Innovation in Network Function Control,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626313,10.1145/2619239.2626313,"Network functions virtualization (NFV) together with software-defined networking (SDN) has the potential to help operators satisfy tight service level agreements, accurately monitor and manipulate network traffic, and minimize operating expenses. However, in scenarios that require packet processing to be redistributed across a collection of network function (NF) instances, simultaneously achieving all three goals requires a framework that provides efficient, coordinated control of both internal NF state and network forwarding state. To this end, we design a control plane called OpenNF. We use carefully designed APIs and a clever combination of events and forwarding updates to address race conditions, bound overhead, and accommodate a variety of NFs. Our evaluation shows that OpenNF offers efficient state control without compromising flexibility, and requires modest additions to NFs.",Proceedings of the 2014 ACM Conference on SIGCOMM,163–174,12,"software-defined networking, network functions, middleboxes","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
581,article,"Marinos, Ilias and Watson, Robert N.M. and Handley, Mark",Network Stack Specialization for Performance,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626311,10.1145/2740070.2626311,"Contemporary network stacks are masterpieces of generality, supporting many edge-node and middle-node functions. Generality comes at a high performance cost: current APIs, memory models, and implementations drastically limit the effectiveness of increasingly powerful hardware. Generality has historically been required so that individual systems could perform many functions. However, as providers have scaled services to support millions of users, they have transitioned toward thousands (or millions) of dedicated servers, each performing a few functions. We argue that the overhead of generality is now a key obstacle to effective scaling, making specialization not only viable, but necessary.We present Sandstorm and Namestorm, web and DNS servers that utilize a clean-slate userspace network stack that exploits knowledge of application-specific workloads. Based on the netmap framework, our novel approach merges application and network-stack memory models, aggressively amortizes protocol-layer costs based on application-layer knowledge, couples tightly with the NIC event model, and exploits microarchitectural features. Simultaneously, the servers retain use of conventional programming frameworks. We compare our approach with the FreeBSD and Linux stacks using the nginx web server and NSD name server, demonstrating 2--10x and 9x improvements in web-server and DNS throughput, lower CPU usage, linear multicore scaling, and saturated NIC hardware.",,175–186,12,"network- stack specialization, network performance, network stacks, clean-slate design",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
582,inproceedings,"Marinos, Ilias and Watson, Robert N.M. and Handley, Mark",Network Stack Specialization for Performance,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626311,10.1145/2619239.2626311,"Contemporary network stacks are masterpieces of generality, supporting many edge-node and middle-node functions. Generality comes at a high performance cost: current APIs, memory models, and implementations drastically limit the effectiveness of increasingly powerful hardware. Generality has historically been required so that individual systems could perform many functions. However, as providers have scaled services to support millions of users, they have transitioned toward thousands (or millions) of dedicated servers, each performing a few functions. We argue that the overhead of generality is now a key obstacle to effective scaling, making specialization not only viable, but necessary.We present Sandstorm and Namestorm, web and DNS servers that utilize a clean-slate userspace network stack that exploits knowledge of application-specific workloads. Based on the netmap framework, our novel approach merges application and network-stack memory models, aggressively amortizes protocol-layer costs based on application-layer knowledge, couples tightly with the NIC event model, and exploits microarchitectural features. Simultaneously, the servers retain use of conventional programming frameworks. We compare our approach with the FreeBSD and Linux stacks using the nginx web server and NSD name server, demonstrating 2--10x and 9x improvements in web-server and DNS throughput, lower CPU usage, linear multicore scaling, and saturated NIC hardware.",Proceedings of the 2014 ACM Conference on SIGCOMM,175–186,12,"network- stack specialization, clean-slate design, network performance, network stacks","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
583,article,"Huang, Te-Yuan and Johari, Ramesh and McKeown, Nick and Trunnell, Matthew and Watson, Mark",A Buffer-Based Approach to Rate Adaptation: Evidence from a Large Video Streaming Service,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626296,10.1145/2740070.2626296,"Existing ABR algorithms face a significant challenge in estimating future capacity: capacity can vary widely over time, a phenomenon commonly observed in commercial services. In this work, we suggest an alternative approach: rather than presuming that capacity estimation is required, it is perhaps better to begin by using only the buffer, and then ask when capacity estimation is needed. We test the viability of this approach through a series of experiments spanning millions of real users in a commercial service. We start with a simple design which directly chooses the video rate based on the current buffer occupancy. Our own investigation reveals that capacity estimation is unnecessary in steady state; however using simple capacity estimation (based on immediate past throughput) is important during the startup phase, when the buffer itself is growing from empty. This approach allows us to reduce the rebuffer rate by 10-20% compared to Netflix's then-default ABR algorithm, while delivering a similar average video rate, and a higher video rate in steady state.",,187–198,12,"video rate adaptation algorithm, http-based video streaming",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
584,inproceedings,"Huang, Te-Yuan and Johari, Ramesh and McKeown, Nick and Trunnell, Matthew and Watson, Mark",A Buffer-Based Approach to Rate Adaptation: Evidence from a Large Video Streaming Service,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626296,10.1145/2619239.2626296,"Existing ABR algorithms face a significant challenge in estimating future capacity: capacity can vary widely over time, a phenomenon commonly observed in commercial services. In this work, we suggest an alternative approach: rather than presuming that capacity estimation is required, it is perhaps better to begin by using only the buffer, and then ask when capacity estimation is needed. We test the viability of this approach through a series of experiments spanning millions of real users in a commercial service. We start with a simple design which directly chooses the video rate based on the current buffer occupancy. Our own investigation reveals that capacity estimation is unnecessary in steady state; however using simple capacity estimation (based on immediate past throughput) is important during the startup phase, when the buffer itself is growing from empty. This approach allows us to reduce the rebuffer rate by 10-20% compared to Netflix's then-default ABR algorithm, while delivering a similar average video rate, and a higher video rate in steady state.",Proceedings of the 2014 ACM Conference on SIGCOMM,187–198,12,"video rate adaptation algorithm, http-based video streaming","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
585,inproceedings,"Gollakota, Shyamnath",Session Details: Wireless (1),2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246775,10.1145/3246775,,Proceedings of the 2014 ACM Conference on SIGCOMM,,,,"Chicago, Illinois, USA",SIGCOMM '14,,,,,,
586,article,"Bharadia, Dinesh and Katti, Sachin",FastForward: Fast and Constructive Full Duplex Relays,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626327,10.1145/2740070.2626327,"This paper presents, FastForward (FF), a novel full duplex relay that constructively forwards signals such that wireless network throughput and coverage is significantly enhanced. FF is a Layer 1 in-band full duplex device, it receives and transmits signals directly and simultaneously on the same frequency. It cleanly integrates into existing networks (both WiFi and LTE) as a separate device and does not require changes to the clients. FF's key invention is a constructive filtering algorithm that transforms the signal at the relay such that when it reaches the destination, it constructively combines with the direct signals from the source and provides a significant throughput gain. We prototype FF using off-the-shelf software radios running a stock WiFi PHY and show experimentally that it provides a 3\texttimes{",,199–210,12,"full duplex relay, low latency cancellation, interference cancellation, full duplex",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
587,inproceedings,"Bharadia, Dinesh and Katti, Sachin",FastForward: Fast and Constructive Full Duplex Relays,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626327,10.1145/2619239.2626327,"This paper presents, FastForward (FF), a novel full duplex relay that constructively forwards signals such that wireless network throughput and coverage is significantly enhanced. FF is a Layer 1 in-band full duplex device, it receives and transmits signals directly and simultaneously on the same frequency. It cleanly integrates into existing networks (both WiFi and LTE) as a separate device and does not require changes to the clients. FF's key invention is a constructive filtering algorithm that transforms the signal at the relay such that when it reaches the destination, it constructively combines with the direct signals from the source and provides a significant throughput gain. We prototype FF using off-the-shelf software radios running a stock WiFi PHY and show experimentally that it provides a 3\texttimes{",Proceedings of the 2014 ACM Conference on SIGCOMM,199–210,12,"full duplex relay, interference cancellation, low latency cancellation, full duplex","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
588,article,"Kumar, Swarun and Hamed, Ezzeldin and Katabi, Dina and Erran Li, Li",LTE Radio Analytics Made Easy and Accessible,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626320,10.1145/2740070.2626320,"Despite the rapid growth of next-generation cellular networks, researchers and end-users today have limited visibility into the performance and problems of these networks. As LTE deployments move towards femto and pico cells, even operators struggle to fully understand the propagation and interference patterns affecting their service, particularly indoors. This paper introduces LTEye, the first open platform to monitor and analyze LTE radio performance at a fine temporal and spatial granularity. LTEye accesses the LTE PHY layer without requiring private user information or provider support. It provides deep insights into the PHY-layer protocols deployed in these networks. LTEye's analytics enable researchers and policy makers to uncover serious deficiencies in these networks due to inefficient spectrum utilization and inter-cell interference. In addition, LTEye extends synthetic aperture radar (SAR), widely used for radar and backscatter signals, to operate over cellular signals. This enables businesses and end-users to localize mobile users and capture the distribution of LTE performance across spatial locations in their facility. As a result, they can diagnose problems and better plan deployment of repeaters or femto cells. We implement LTEye on USRP software radios, and present empirical insights and analytics from multiple AT&T and Verizon base stations in our locality.",,211–222,12,"analytics, PHY, wireless, LTE, cellular",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
589,inproceedings,"Kumar, Swarun and Hamed, Ezzeldin and Katabi, Dina and Erran Li, Li",LTE Radio Analytics Made Easy and Accessible,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626320,10.1145/2619239.2626320,"Despite the rapid growth of next-generation cellular networks, researchers and end-users today have limited visibility into the performance and problems of these networks. As LTE deployments move towards femto and pico cells, even operators struggle to fully understand the propagation and interference patterns affecting their service, particularly indoors. This paper introduces LTEye, the first open platform to monitor and analyze LTE radio performance at a fine temporal and spatial granularity. LTEye accesses the LTE PHY layer without requiring private user information or provider support. It provides deep insights into the PHY-layer protocols deployed in these networks. LTEye's analytics enable researchers and policy makers to uncover serious deficiencies in these networks due to inefficient spectrum utilization and inter-cell interference. In addition, LTEye extends synthetic aperture radar (SAR), widely used for radar and backscatter signals, to operate over cellular signals. This enables businesses and end-users to localize mobile users and capture the distribution of LTE performance across spatial locations in their facility. As a result, they can diagnose problems and better plan deployment of repeaters or femto cells. We implement LTEye on USRP software radios, and present empirical insights and analytics from multiple AT&T and Verizon base stations in our locality.",Proceedings of the 2014 ACM Conference on SIGCOMM,211–222,12,"analytics, cellular, LTE, PHY, wireless","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
590,article,"Tu, Guan-Hua and Li, Yuanjie and Peng, Chunyi and Li, Chi-Yu and Wang, Hongyi and Lu, Songwu",Control-Plane Protocol Interactions in Cellular Networks,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626302,10.1145/2740070.2626302,"Control-plane protocols are complex in cellular networks. They communicate with one another along three dimensions of cross layers, cross (circuit-switched and packet-switched) domains, and cross (3G and 4G) systems. In this work, we propose signaling diagnosis tools and uncover six instances of problematic interactions. Such control-plane issues span both design defects in the 3GPP standards and operational slips by carriers. They are more damaging than data-plane failures. In the worst-case scenario, users may be out of service in 4G, or get stuck in 3G. We deduce root causes, propose solutions, and summarize learned lessons.",,223–234,12,"cellular networks, control-plane, protocol verification",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
591,inproceedings,"Tu, Guan-Hua and Li, Yuanjie and Peng, Chunyi and Li, Chi-Yu and Wang, Hongyi and Lu, Songwu",Control-Plane Protocol Interactions in Cellular Networks,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626302,10.1145/2619239.2626302,"Control-plane protocols are complex in cellular networks. They communicate with one another along three dimensions of cross layers, cross (circuit-switched and packet-switched) domains, and cross (3G and 4G) systems. In this work, we propose signaling diagnosis tools and uncover six instances of problematic interactions. Such control-plane issues span both design defects in the 3GPP standards and operational slips by carriers. They are more damaging than data-plane failures. In the worst-case scenario, users may be out of service in 4G, or get stuck in 3G. We deduce root causes, propose solutions, and summarize learned lessons.",Proceedings of the 2014 ACM Conference on SIGCOMM,223–234,12,"control-plane, protocol verification, cellular networks","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
592,article,"Wang, Jue and Vasisht, Deepak and Katabi, Dina",RF-IDraw: Virtual Touch Screen in the Air Using RF Signals,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626330,10.1145/2740070.2626330,"Prior work in RF-based positioning has mainly focused on discovering the absolute location of an RF source, where state-of-the-art systems can achieve an accuracy on the order of tens of centimeters using a large number of antennas. However, many applications in gaming and gesture based interface see more benefits in knowing the detailed shape of a motion. Such trajectory tracing requires a resolution several fold higher than what existing RF-based positioning systems can offer.This paper shows that one can provide a dramatic increase in trajectory tracing accuracy, even with a small number of antennas. The key enabler for our design is a multi-resolution positioning technique that exploits an intrinsic tradeoff between improving the resolution and resolving ambiguity in the location of the RF source. The unique property of this design is its ability to precisely reconstruct the minute details in the trajectory shape, even when the absolute position might have an offset. We built a prototype of our design with commercial off-the-shelf RFID readers and tags and used it to enable a virtual touch screen, which allows a user to interact with a desired computing device by gesturing or writing her commands in the air, where each letter is only a few centimeters wide.",,235–246,12,"RFID, virtual touch screen, grating lobes, trajectory tracing",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
593,inproceedings,"Wang, Jue and Vasisht, Deepak and Katabi, Dina",RF-IDraw: Virtual Touch Screen in the Air Using RF Signals,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626330,10.1145/2619239.2626330,"Prior work in RF-based positioning has mainly focused on discovering the absolute location of an RF source, where state-of-the-art systems can achieve an accuracy on the order of tens of centimeters using a large number of antennas. However, many applications in gaming and gesture based interface see more benefits in knowing the detailed shape of a motion. Such trajectory tracing requires a resolution several fold higher than what existing RF-based positioning systems can offer.This paper shows that one can provide a dramatic increase in trajectory tracing accuracy, even with a small number of antennas. The key enabler for our design is a multi-resolution positioning technique that exploits an intrinsic tradeoff between improving the resolution and resolving ambiguity in the location of the RF source. The unique property of this design is its ability to precisely reconstruct the minute details in the trajectory shape, even when the absolute position might have an offset. We built a prototype of our design with commercial off-the-shelf RFID readers and tags and used it to enable a virtual touch screen, which allows a user to interact with a desired computing device by gesturing or writing her commands in the air, where each letter is only a few centimeters wide.",Proceedings of the 2014 ACM Conference on SIGCOMM,235–246,12,"grating lobes, virtual touch screen, RFID, trajectory tracing","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
594,inproceedings,"Akella, Aditya",Session Details: Monitoring and Diagnostics,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246776,10.1145/3246776,,Proceedings of the 2014 ACM Conference on SIGCOMM,,,,"Chicago, Illinois, USA",SIGCOMM '14,,,,,,
595,article,"Wu, Yang and Zhao, Mingchen and Haeberlen, Andreas and Zhou, Wenchao and Loo, Boon Thau",Diagnosing Missing Events in Distributed Systems with Negative Provenance,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626335,10.1145/2740070.2626335,"When debugging a distributed system, it is sometimes necessary to explain the absence of an event - for instance, why a certain route is not available, or why a certain packet did not arrive. Existing debuggers offer some support for explaining the presence of events, usually by providing the equivalent of a backtrace in conventional debuggers, but they are not very good at answering 'Why not?' questions: there is simply no starting point for a possible backtrace.In this paper, we show that the concept of negative provenance can be used to explain the absence of events in distributed systems. Negative provenance relies on counterfactual reasoning to identify the conditions under which the missing event could have occurred. We define a formal model of negative provenance for distributed systems, and we present the design of a system called Y! that tracks both positive and negative provenance and can use them to answer diagnostic queries. We describe how we have used Y! to debug several realistic problems in two application domains: software-defined networks and BGP interdomain routing. Results from our experimental evaluation show that the overhead of Y! is moderate.",,383–394,12,"diagnostics, provenance, debugging",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
596,inproceedings,"Wu, Yang and Zhao, Mingchen and Haeberlen, Andreas and Zhou, Wenchao and Loo, Boon Thau",Diagnosing Missing Events in Distributed Systems with Negative Provenance,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626335,10.1145/2619239.2626335,"When debugging a distributed system, it is sometimes necessary to explain the absence of an event - for instance, why a certain route is not available, or why a certain packet did not arrive. Existing debuggers offer some support for explaining the presence of events, usually by providing the equivalent of a backtrace in conventional debuggers, but they are not very good at answering 'Why not?' questions: there is simply no starting point for a possible backtrace.In this paper, we show that the concept of negative provenance can be used to explain the absence of events in distributed systems. Negative provenance relies on counterfactual reasoning to identify the conditions under which the missing event could have occurred. We define a formal model of negative provenance for distributed systems, and we present the design of a system called Y! that tracks both positive and negative provenance and can use them to answer diagnostic queries. We describe how we have used Y! to debug several realistic problems in two application domains: software-defined networks and BGP interdomain routing. Results from our experimental evaluation show that the overhead of Y! is moderate.",Proceedings of the 2014 ACM Conference on SIGCOMM,383–394,12,"provenance, debugging, diagnostics","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
597,article,"Scott, Colin and Wundsam, Andreas and Raghavan, Barath and Panda, Aurojit and Or, Andrew and Lai, Jefferson and Huang, Eugene and Liu, Zhi and El-Hassany, Ahmed and Whitlock, Sam and Acharya, H.B. and Zarifis, Kyriakos and Shenker, Scott",Troubleshooting Blackbox SDN Control Software with Minimal Causal Sequences,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626304,10.1145/2740070.2626304,"Software bugs are inevitable in software-defined networking control software, and troubleshooting is a tedious, time-consuming task. In this paper we discuss how to improve control software troubleshooting by presenting a technique for automatically identifying a minimal sequence of inputs responsible for triggering a given bug, without making assumptions about the language or instrumentation of the software under test. We apply our technique to five open source SDN control platforms---Floodlight, NOX, POX, Pyretic, ONOS---and illustrate how the minimal causal sequences our system found aided the troubleshooting process.",,395–406,12,"SDN control software, test case minimization, troubleshooting",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
598,inproceedings,"Scott, Colin and Wundsam, Andreas and Raghavan, Barath and Panda, Aurojit and Or, Andrew and Lai, Jefferson and Huang, Eugene and Liu, Zhi and El-Hassany, Ahmed and Whitlock, Sam and Acharya, H.B. and Zarifis, Kyriakos and Shenker, Scott",Troubleshooting Blackbox SDN Control Software with Minimal Causal Sequences,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626304,10.1145/2619239.2626304,"Software bugs are inevitable in software-defined networking control software, and troubleshooting is a tedious, time-consuming task. In this paper we discuss how to improve control software troubleshooting by presenting a technique for automatically identifying a minimal sequence of inputs responsible for triggering a given bug, without making assumptions about the language or instrumentation of the software under test. We apply our technique to five open source SDN control platforms---Floodlight, NOX, POX, Pyretic, ONOS---and illustrate how the minimal causal sequences our system found aided the troubleshooting process.",Proceedings of the 2014 ACM Conference on SIGCOMM,395–406,12,"test case minimization, SDN control software, troubleshooting","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
599,article,"Rasley, Jeff and Stephens, Brent and Dixon, Colin and Rozner, Eric and Felter, Wes and Agarwal, Kanak and Carter, John and Fonseca, Rodrigo",Planck: Millisecond-Scale Monitoring and Control for Commodity Networks,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626310,10.1145/2740070.2626310,"Software-defined networking introduces the possibility of building self-tuning networks that constantly monitor network conditions and react rapidly to important events such as congestion. Unfortunately, state-of-the-art monitoring mechanisms for conventional networks require hundreds of milliseconds to seconds to extract global network state, like link utilization or the identity of ""elephant"" flows. Such latencies are adequate for responding to persistent issues, e.g., link failures or long-lasting congestion, but are inadequate for responding to transient problems, e.g., congestion induced by bursty workloads sharing a link. In this paper, we present Planck, a novel network measurement architecture that employs oversubscribed port mirroring to extract network information at 280 µs--7 ms timescales on a 1 Gbps commodity switch and 275 µs--4 ms timescales on a 10 Gbps commodity switch,over 11x and 18x faster than recent approaches, respectively (and up to 291x if switch firmware allowed buffering to be disabled on some ports). To demonstrate the value of Planck's speed and accuracy, we use it to drive a traffic engineering application that can reroute congested flows in milliseconds. On a 10 Gbps commodity switch, Planck-driven traffic engineering achieves aggregate throughput within 1--4% of optimal for most workloads we evaluated, even with flows as small as 50 MiB, an improvement of up to 53% over previous schemes.",,407–418,12,"traffic engineering, networking measurement, software-defined networking",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
600,inproceedings,"Rasley, Jeff and Stephens, Brent and Dixon, Colin and Rozner, Eric and Felter, Wes and Agarwal, Kanak and Carter, John and Fonseca, Rodrigo",Planck: Millisecond-Scale Monitoring and Control for Commodity Networks,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626310,10.1145/2619239.2626310,"Software-defined networking introduces the possibility of building self-tuning networks that constantly monitor network conditions and react rapidly to important events such as congestion. Unfortunately, state-of-the-art monitoring mechanisms for conventional networks require hundreds of milliseconds to seconds to extract global network state, like link utilization or the identity of ""elephant"" flows. Such latencies are adequate for responding to persistent issues, e.g., link failures or long-lasting congestion, but are inadequate for responding to transient problems, e.g., congestion induced by bursty workloads sharing a link. In this paper, we present Planck, a novel network measurement architecture that employs oversubscribed port mirroring to extract network information at 280 µs--7 ms timescales on a 1 Gbps commodity switch and 275 µs--4 ms timescales on a 10 Gbps commodity switch,over 11x and 18x faster than recent approaches, respectively (and up to 291x if switch firmware allowed buffering to be disabled on some ports). To demonstrate the value of Planck's speed and accuracy, we use it to drive a traffic engineering application that can reroute congested flows in milliseconds. On a 10 Gbps commodity switch, Planck-driven traffic engineering achieves aggregate throughput within 1--4% of optimal for most workloads we evaluated, even with flows as small as 50 MiB, an improvement of up to 53% over previous schemes.",Proceedings of the 2014 ACM Conference on SIGCOMM,407–418,12,"traffic engineering, networking measurement, software-defined networking","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
601,article,"Moshref, Masoud and Yu, Minlan and Govindan, Ramesh and Vahdat, Amin",DREAM: Dynamic Resource Allocation for Software-Defined Measurement,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626291,10.1145/2740070.2626291,"Software-defined networks can enable a variety of concurrent, dynamically instantiated, measurement tasks, that provide fine-grain visibility into network traffic. Recently, there have been many proposals to configure TCAM counters in hardware switches to monitor traffic. However, the TCAM memory at switches is fundamentally limited and the accuracy of the measurement tasks is a function of the resources devoted to them on each switch. This paper describes an adaptive measurement framework, called DREAM, that dynamically adjusts the resources devoted to each measurement task, while ensuring a user-specified level of accuracy. Since the trade-off between resource usage and accuracy can depend upon the type of tasks, their parameters, and traffic characteristics, DREAM does not assume an a priori characterization of this trade-off, but instead dynamically searches for a resource allocation that is sufficient to achieve a desired level of accuracy. A prototype implementation and simulations with three network-wide measurement tasks (heavy hitter, hierarchical heavy hitter and change detection) and diverse traffic show that DREAM can support more concurrent tasks with higher accuracy than several other alternatives.",,419–430,12,"software-defined measurement, resource allocation",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
602,inproceedings,"Moshref, Masoud and Yu, Minlan and Govindan, Ramesh and Vahdat, Amin",DREAM: Dynamic Resource Allocation for Software-Defined Measurement,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626291,10.1145/2619239.2626291,"Software-defined networks can enable a variety of concurrent, dynamically instantiated, measurement tasks, that provide fine-grain visibility into network traffic. Recently, there have been many proposals to configure TCAM counters in hardware switches to monitor traffic. However, the TCAM memory at switches is fundamentally limited and the accuracy of the measurement tasks is a function of the resources devoted to them on each switch. This paper describes an adaptive measurement framework, called DREAM, that dynamically adjusts the resources devoted to each measurement task, while ensuring a user-specified level of accuracy. Since the trade-off between resource usage and accuracy can depend upon the type of tasks, their parameters, and traffic characteristics, DREAM does not assume an a priori characterization of this trade-off, but instead dynamically searches for a resource allocation that is sufficient to achieve a desired level of accuracy. A prototype implementation and simulations with three network-wide measurement tasks (heavy hitter, hierarchical heavy hitter and change detection) and diverse traffic show that DREAM can support more concurrent tasks with higher accuracy than several other alternatives.",Proceedings of the 2014 ACM Conference on SIGCOMM,419–430,12,"resource allocation, software-defined measurement","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
603,inproceedings,"Mahajan, Ratul",Session Details: Novel Datacenter Network Designs,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246777,10.1145/3246777,,Proceedings of the 2014 ACM Conference on SIGCOMM,,,,"Chicago, Illinois, USA",SIGCOMM '14,,,,,,
604,article,"Liu, Yunpeng James and Gao, Peter Xiang and Wong, Bernard and Keshav, Srinivasan",Quartz: A New Design Element for Low-Latency DCNs,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626332,10.1145/2740070.2626332,"Most datacenter network (DCN) designs focus on maximizing bisection bandwidth rather than minimizing server-to-server latency. We explore architectural approaches to building low-latency DCNs and introduce Quartz, a design element consisting of a full mesh of switches. Quartz can be used to replace portions of either a hierarchical network or a random network. Our analysis shows that replacing high port-count core switches with Quartz can significantly reduce switching delays, and replacing groups of top-of-rack and aggregation switches with Quartz can significantly reduce congestion-related delays from cross-traffic. We overcome the complexity of wiring a complete mesh using low-cost optical multiplexers that enable us to efficiently implement a logical mesh as a physical ring. We evaluate our performance using both simulations and a small working prototype. Our evaluation results confirm our analysis, and demonstrate that it is possible to build low-latency DCNs using inexpensive commodity elements without significant concessions to cost, scalability, or wiring complexity.",,283–294,12,"WDM, datacenter, latency, optical technologies",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
605,inproceedings,"Liu, Yunpeng James and Gao, Peter Xiang and Wong, Bernard and Keshav, Srinivasan",Quartz: A New Design Element for Low-Latency DCNs,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626332,10.1145/2619239.2626332,"Most datacenter network (DCN) designs focus on maximizing bisection bandwidth rather than minimizing server-to-server latency. We explore architectural approaches to building low-latency DCNs and introduce Quartz, a design element consisting of a full mesh of switches. Quartz can be used to replace portions of either a hierarchical network or a random network. Our analysis shows that replacing high port-count core switches with Quartz can significantly reduce switching delays, and replacing groups of top-of-rack and aggregation switches with Quartz can significantly reduce congestion-related delays from cross-traffic. We overcome the complexity of wiring a complete mesh using low-cost optical multiplexers that enable us to efficiently implement a logical mesh as a physical ring. We evaluate our performance using both simulations and a small working prototype. Our evaluation results confirm our analysis, and demonstrate that it is possible to build low-latency DCNs using inexpensive commodity elements without significant concessions to cost, scalability, or wiring complexity.",Proceedings of the 2014 ACM Conference on SIGCOMM,283–294,12,"optical technologies, WDM, datacenter, latency","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
606,article,"Kalia, Anuj and Kaminsky, Michael and Andersen, David G.",Using RDMA Efficiently for Key-Value Services,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626299,10.1145/2740070.2626299,"This paper describes the design and implementation of HERD, a key-value system designed to make the best use of an RDMA network. Unlike prior RDMA-based key-value systems, HERD focuses its design on reducing network round trips while using efficient RDMA primitives; the result is substantially lower latency, and throughput that saturates modern, commodity RDMA hardware.HERD has two unconventional decisions: First, it does not use RDMA reads, despite the allure of operations that bypass the remote CPU entirely. Second, it uses a mix of RDMA and messaging verbs, despite the conventional wisdom that the messaging primitives are slow. A HERD client writes its request into the server's memory; the server computes the reply. This design uses a single round trip for all requests and supports up to 26 million key-value operations per second with 5μs average latency. Notably, for small key-value items, our full system throughput is similar to native RDMA read throughput and is over 2X higher than recent RDMA-based key-value systems. We believe that HERD further serves as an effective template for the construction of RDMA-based datacenter services.",,295–306,12,"infiniband, RDMA, ROCE, key-value stores",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
607,inproceedings,"Kalia, Anuj and Kaminsky, Michael and Andersen, David G.",Using RDMA Efficiently for Key-Value Services,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626299,10.1145/2619239.2626299,"This paper describes the design and implementation of HERD, a key-value system designed to make the best use of an RDMA network. Unlike prior RDMA-based key-value systems, HERD focuses its design on reducing network round trips while using efficient RDMA primitives; the result is substantially lower latency, and throughput that saturates modern, commodity RDMA hardware.HERD has two unconventional decisions: First, it does not use RDMA reads, despite the allure of operations that bypass the remote CPU entirely. Second, it uses a mix of RDMA and messaging verbs, despite the conventional wisdom that the messaging primitives are slow. A HERD client writes its request into the server's memory; the server computes the reply. This design uses a single round trip for all requests and supports up to 26 million key-value operations per second with 5μs average latency. Notably, for small key-value items, our full system throughput is similar to native RDMA read throughput and is over 2X higher than recent RDMA-based key-value systems. We believe that HERD further serves as an effective template for the construction of RDMA-based datacenter services.",Proceedings of the 2014 ACM Conference on SIGCOMM,295–306,12,"infiniband, ROCE, RDMA, key-value stores","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
608,article,"Perry, Jonathan and Ousterhout, Amy and Balakrishnan, Hari and Shah, Devavrat and Fugal, Hans","Fastpass: A Centralized ""Zero-Queue"" Datacenter Network",2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626309,10.1145/2740070.2626309,"An ideal datacenter network should provide several properties, including low median and tail latency, high utilization (throughput), fair allocation of network resources between users or applications, deadline-aware scheduling, and congestion (loss) avoidance. Current datacenter networks inherit the principles that went into the design of the Internet, where packet transmission and path selection decisions are distributed among the endpoints and routers. Instead, we propose that each sender should delegate control---to a centralized arbiter---of when each packet should be transmitted and what path it should follow.This paper describes Fastpass, a datacenter network architecture built using this principle. Fastpass incorporates two fast algorithms: the first determines the time at which each packet should be transmitted, while the second determines the path to use for that packet. In addition, Fastpass uses an efficient protocol between the endpoints and the arbiter and an arbiter replication strategy for fault-tolerant failover. We deployed and evaluated Fastpass in a portion of Facebook's datacenter network. Our results show that Fastpass achieves high throughput comparable to current networks at a 240x reduction is queue lengths (4.35 Mbytes reducing to 18 Kbytes), achieves much fairer and consistent flow throughputs than the baseline TCP (5200x reduction in the standard deviation of per-flow throughput with five concurrent connections), scalability from 1 to 8 cores in the arbiter implementation with the ability to schedule 2.21 Terabits/s of traffic in software on eight cores, and a 2.5x reduction in the number of TCP retransmissions in a latency-sensitive service at Facebook.",,307–318,12,"zero-queue, arbiter, centralized, data plane, datacenter, low latency, high throughput, scheduling",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
609,inproceedings,"Perry, Jonathan and Ousterhout, Amy and Balakrishnan, Hari and Shah, Devavrat and Fugal, Hans","Fastpass: A Centralized ""Zero-Queue"" Datacenter Network",2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626309,10.1145/2619239.2626309,"An ideal datacenter network should provide several properties, including low median and tail latency, high utilization (throughput), fair allocation of network resources between users or applications, deadline-aware scheduling, and congestion (loss) avoidance. Current datacenter networks inherit the principles that went into the design of the Internet, where packet transmission and path selection decisions are distributed among the endpoints and routers. Instead, we propose that each sender should delegate control---to a centralized arbiter---of when each packet should be transmitted and what path it should follow.This paper describes Fastpass, a datacenter network architecture built using this principle. Fastpass incorporates two fast algorithms: the first determines the time at which each packet should be transmitted, while the second determines the path to use for that packet. In addition, Fastpass uses an efficient protocol between the endpoints and the arbiter and an arbiter replication strategy for fault-tolerant failover. We deployed and evaluated Fastpass in a portion of Facebook's datacenter network. Our results show that Fastpass achieves high throughput comparable to current networks at a 240x reduction is queue lengths (4.35 Mbytes reducing to 18 Kbytes), achieves much fairer and consistent flow throughputs than the baseline TCP (5200x reduction in the standard deviation of per-flow throughput with five concurrent connections), scalability from 1 to 8 cores in the arbiter implementation with the ability to schedule 2.21 Terabits/s of traffic in software on eight cores, and a 2.5x reduction in the number of TCP retransmissions in a latency-sensitive service at Facebook.",Proceedings of the 2014 ACM Conference on SIGCOMM,307–318,12,"datacenter, scheduling, arbiter, low latency, data plane, centralized, high throughput, zero-queue","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
610,article,"Hamedazimi, Navid and Qazi, Zafar and Gupta, Himanshu and Sekar, Vyas and Das, Samir R. and Longtin, Jon P. and Shah, Himanshu and Tanwer, Ashish",FireFly: A Reconfigurable Wireless Data Center Fabric Using Free-Space Optics,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626328,10.1145/2740070.2626328,"Conventional static datacenter (DC) network designs offer extreme cost vs. performance tradeoffs---simple leaf-spine networks are cost-effective but oversubscribed, while ""fat tree""-like solutions offer good worst-case performance but are expensive. Recent results make a promising case for augmenting an oversubscribed network with reconfigurable inter-rack wireless or optical links. Inspired by the promise of reconfigurability, this paper presents FireFly, an inter-rack network solution that pushes DC network design to the extreme on three key fronts: (1) all links are reconfigurable; (2) all links are wireless; and (3) non top-of-rack switches are eliminated altogether. This vision, if realized, can offer significant benefits in terms of increased flexibility, reduced equipment cost, and minimal cabling complexity. In order to achieve this vision, we need to look beyond traditional RF wireless solutions due to their interference footprint which limits range and data rates. Thus, we make the case for using free-space optics (FSO). We demonstrate the viability of this architecture by (a) building a proof-of-concept prototype of a steerable small form factor FSO device using commodity components and (b) developing practical heuristics to address algorithmic and system-level challenges in network design and management.",,319–330,12,"free-space optics, reconfigurablility, data centers",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
611,inproceedings,"Hamedazimi, Navid and Qazi, Zafar and Gupta, Himanshu and Sekar, Vyas and Das, Samir R. and Longtin, Jon P. and Shah, Himanshu and Tanwer, Ashish",FireFly: A Reconfigurable Wireless Data Center Fabric Using Free-Space Optics,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626328,10.1145/2619239.2626328,"Conventional static datacenter (DC) network designs offer extreme cost vs. performance tradeoffs---simple leaf-spine networks are cost-effective but oversubscribed, while ""fat tree""-like solutions offer good worst-case performance but are expensive. Recent results make a promising case for augmenting an oversubscribed network with reconfigurable inter-rack wireless or optical links. Inspired by the promise of reconfigurability, this paper presents FireFly, an inter-rack network solution that pushes DC network design to the extreme on three key fronts: (1) all links are reconfigurable; (2) all links are wireless; and (3) non top-of-rack switches are eliminated altogether. This vision, if realized, can offer significant benefits in terms of increased flexibility, reduced equipment cost, and minimal cabling complexity. In order to achieve this vision, we need to look beyond traditional RF wireless solutions due to their interference footprint which limits range and data rates. Thus, we make the case for using free-space optics (FSO). We demonstrate the viability of this architecture by (a) building a proof-of-concept prototype of a steerable small form factor FSO device using commodity components and (b) developing practical heuristics to address algorithmic and system-level challenges in network design and management.",Proceedings of the 2014 ACM Conference on SIGCOMM,319–330,12,"reconfigurablility, free-space optics, data centers","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
612,article,"Rashmi, K.V. and Shah, Nihar B. and Gu, Dikang and Kuang, Hairong and Borthakur, Dhruba and Ramchandran, Kannan","A ""Hitchhiker's"" Guide to Fast and Efficient Data Reconstruction in Erasure-Coded Data Centers",2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626325,10.1145/2740070.2626325,"Erasure codes such as Reed-Solomon (RS) codes are being extensively deployed in data centers since they offer significantly higher reliability than data replication methods at much lower storage overheads. These codes however mandate much higher resources with respect to network bandwidth and disk IO during reconstruction of data that is missing or otherwise unavailable. Existing solutions to this problem either demand additional storage space or severely limit the choice of the system parameters. In this paper, we present ""Hitchhiker"", a new erasure-coded storage system that reduces both network traffic and disk IO by around 25% to 45% during reconstruction of missing or otherwise unavailable data, with no additional storage, the same fault tolerance, and arbitrary flexibility in the choice of parameters, as compared to RS-based systems. Hitchhiker 'rides' on top of RS codes, and is based on novel encoding and decoding techniques that will be presented in this paper. We have implemented Hitchhiker in the Hadoop Distributed File System (HDFS). When evaluating various metrics on the data-warehouse cluster in production at Facebook with real-time traffic and workloads, during reconstruction, we observe a 36% reduction in the computation time and a 32% reduction in the data read time, in addition to the 35% reduction in network traffic and disk IO. Hitchhiker can thus reduce the latency of degraded reads and perform faster recovery from failed or decommissioned machines.",,331–342,12,"recovery, degraded reads, distributed storage, network traffic, fault tolerance, disk IO, erasure codes",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
613,inproceedings,"Rashmi, K.V. and Shah, Nihar B. and Gu, Dikang and Kuang, Hairong and Borthakur, Dhruba and Ramchandran, Kannan","A ""Hitchhiker's"" Guide to Fast and Efficient Data Reconstruction in Erasure-Coded Data Centers",2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626325,10.1145/2619239.2626325,"Erasure codes such as Reed-Solomon (RS) codes are being extensively deployed in data centers since they offer significantly higher reliability than data replication methods at much lower storage overheads. These codes however mandate much higher resources with respect to network bandwidth and disk IO during reconstruction of data that is missing or otherwise unavailable. Existing solutions to this problem either demand additional storage space or severely limit the choice of the system parameters. In this paper, we present ""Hitchhiker"", a new erasure-coded storage system that reduces both network traffic and disk IO by around 25% to 45% during reconstruction of missing or otherwise unavailable data, with no additional storage, the same fault tolerance, and arbitrary flexibility in the choice of parameters, as compared to RS-based systems. Hitchhiker 'rides' on top of RS codes, and is based on novel encoding and decoding techniques that will be presented in this paper. We have implemented Hitchhiker in the Hadoop Distributed File System (HDFS). When evaluating various metrics on the data-warehouse cluster in production at Facebook with real-time traffic and workloads, during reconstruction, we observe a 36% reduction in the computation time and a 32% reduction in the data read time, in addition to the 35% reduction in network traffic and disk IO. Hitchhiker can thus reduce the latency of degraded reads and perform faster recovery from failed or decommissioned machines.",Proceedings of the 2014 ACM Conference on SIGCOMM,331–342,12,"distributed storage, erasure codes, fault tolerance, network traffic, degraded reads, recovery, disk IO","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
614,article,"Mukerjee, Matthew K. and Hong, JungAh and Jiang, Junchen and Naylor, David and Han, Dongsu and Seshan, Srinivasan and Zhang, Hui",Enabling near Real-Time Central Control for Live Video Delivery in CDNs,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631444,10.1145/2740070.2631444,"User-created live video streaming is marking a fundamental shift in the workload of live video delivery. However, live-video-specific challenges and the viral nature of user-created content makes it difficult for current CDNs to deliver 1) high-quality, 2) highly-scalable, and 3) highly-responsive service. We present the design and implementation of VDN, a new control plane for CDNs designed to optimize the delivery of live streams within the CDN. VDN satisfies these requirements by using two approaches: 1) optimizing directly for video quality (not just throughput) and 2) combining centralized control with local control, allowing VDN to adapt to traffic dynamics and network failures at fine timescales.",,343–344,2,"CDNs, central optimization, hybrid control, live video",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
615,inproceedings,"Mukerjee, Matthew K. and Hong, JungAh and Jiang, Junchen and Naylor, David and Han, Dongsu and Seshan, Srinivasan and Zhang, Hui",Enabling near Real-Time Central Control for Live Video Delivery in CDNs,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631444,10.1145/2619239.2631444,"User-created live video streaming is marking a fundamental shift in the workload of live video delivery. However, live-video-specific challenges and the viral nature of user-created content makes it difficult for current CDNs to deliver 1) high-quality, 2) highly-scalable, and 3) highly-responsive service. We present the design and implementation of VDN, a new control plane for CDNs designed to optimize the delivery of live streams within the CDN. VDN satisfies these requirements by using two approaches: 1) optimizing directly for video quality (not just throughput) and 2) combining centralized control with local control, allowing VDN to adapt to traffic dynamics and network failures at fine timescales.",Proceedings of the 2014 ACM Conference on SIGCOMM,343–344,2,"central optimization, CDNs, live video, hybrid control","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
616,article,"Donovan, Sean and Feamster, Nick",NetAssay: Providing New Monitoring Primitives for Network Operators,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631451,10.1145/2740070.2631451,"Home and business network operators have limited network statistics available over which management decisions can be made. Similarly, there are few triggered behaviors, such as usage or bandwidths cap for individual users, that are available. By looking at sources of traffic, based on Domain Name System (DNS) cues for content of particular web addresses or source Autonomous System (AS) of the traffic, network operators could create new and interesting rules for their network. NetAssay is a Software-Defined Networking (SDN)-based, network-wide monitoring and reaction framework. By integrating information from Border Gateway Protocol (BGP) and the Domain Name System, NetAssay is able to integrate formerly disparate sources of control information, and use it to provide better monitoring, more useful triggered events, and security benefits for network operators.",,345–346,2,"network management, network monitoring, software-defined networking",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
617,inproceedings,"Donovan, Sean and Feamster, Nick",NetAssay: Providing New Monitoring Primitives for Network Operators,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631451,10.1145/2619239.2631451,"Home and business network operators have limited network statistics available over which management decisions can be made. Similarly, there are few triggered behaviors, such as usage or bandwidths cap for individual users, that are available. By looking at sources of traffic, based on Domain Name System (DNS) cues for content of particular web addresses or source Autonomous System (AS) of the traffic, network operators could create new and interesting rules for their network. NetAssay is a Software-Defined Networking (SDN)-based, network-wide monitoring and reaction framework. By integrating information from Border Gateway Protocol (BGP) and the Domain Name System, NetAssay is able to integrate formerly disparate sources of control information, and use it to provide better monitoring, more useful triggered events, and security benefits for network operators.",Proceedings of the 2014 ACM Conference on SIGCOMM,345–346,2,"software-defined networking, network monitoring, network management","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
618,article,"Bao, Jinzhen and Zhao, Baokang and Yu, Wanrong and Feng, Zhenqian and Wu, Chunqing and Gong, Zhenghu",OpenSAN: A Software-Defined Satellite Network Architecture,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631454,10.1145/2740070.2631454,"In recent years, with the rapid development of satellite technology including On Board Processing (OBP) and Inter Satellite Link (ISL), satellite network devices such as space IP routers have been experimentally carried in space. However, there are many difficulties to build a future satellite network with current terrestrial Internet technologies due to the distinguished space features, such as the severely limited resources, remote hardware/software upgrade in space. In this paper, we propose OpenSAN, a novel architecture of software-defined satellite network. By decoupling the data plane and control plane, OpenSAN provides satellite network with high efficiency, fine-grained control, as well as flexibility to support future advanced network technology. Moreover, we also discuss some practical challenges in the deployment of OpenSAN.",,347–348,2,"software-defined network, satellite network",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
619,inproceedings,"Bao, Jinzhen and Zhao, Baokang and Yu, Wanrong and Feng, Zhenqian and Wu, Chunqing and Gong, Zhenghu",OpenSAN: A Software-Defined Satellite Network Architecture,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631454,10.1145/2619239.2631454,"In recent years, with the rapid development of satellite technology including On Board Processing (OBP) and Inter Satellite Link (ISL), satellite network devices such as space IP routers have been experimentally carried in space. However, there are many difficulties to build a future satellite network with current terrestrial Internet technologies due to the distinguished space features, such as the severely limited resources, remote hardware/software upgrade in space. In this paper, we propose OpenSAN, a novel architecture of software-defined satellite network. By decoupling the data plane and control plane, OpenSAN provides satellite network with high efficiency, fine-grained control, as well as flexibility to support future advanced network technology. Moreover, we also discuss some practical challenges in the deployment of OpenSAN.",Proceedings of the 2014 ACM Conference on SIGCOMM,347–348,2,"software-defined network, satellite network","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
620,article,"Alwabel, Abdulla and Yu, Minlan and Zhang, Ying and Mirkovic, Jelena",SENSS: Observe and Control Your Own Traffic in the Internet,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631459,10.1145/2740070.2631459,"We propose a new software-defined security service -- SENSS -- that enables a victim network to request services from remote ISPs for traffic that carries source IPs or destination IPs from this network's address space. These services range from statistics gathering, to filtering or quality of service guarantees, to route reports or modifications. The SENSS service has very simple, yet powerful, interfaces. This enables it to handle a variety of data plane and control plane attacks, while being easily implementable in today's ISP. Through extensive evaluations on realistic traffic traces and Internet topology, we show how SENSS can be used to quickly, safely and effectively mitigate a variety of large-scale attacks that are largely unhandled today.",,349–350,2,"design, privacy, management, SDN, security",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
621,inproceedings,"Alwabel, Abdulla and Yu, Minlan and Zhang, Ying and Mirkovic, Jelena",SENSS: Observe and Control Your Own Traffic in the Internet,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631459,10.1145/2619239.2631459,"We propose a new software-defined security service -- SENSS -- that enables a victim network to request services from remote ISPs for traffic that carries source IPs or destination IPs from this network's address space. These services range from statistics gathering, to filtering or quality of service guarantees, to route reports or modifications. The SENSS service has very simple, yet powerful, interfaces. This enables it to handle a variety of data plane and control plane attacks, while being easily implementable in today's ISP. Through extensive evaluations on realistic traffic traces and Internet topology, we show how SENSS can be used to quickly, safely and effectively mitigate a variety of large-scale attacks that are largely unhandled today.",Proceedings of the 2014 ACM Conference on SIGCOMM,349–350,2,"SDN, management, privacy, design, security","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
622,article,"Sundaresan, Srikanth and Feamster, Nick and Teixeira, Renata",Locating Throughput Bottlenecks in Home Networks,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631440,10.1145/2740070.2631440,"We present a demonstration of WTF (Where's The Fault?), a system that localizes performance problems in home and access networks. We implement WTF as custom firmware that runs in an off-the-shelf home router. WTF uses timing and buffering information from passively monitored traffic at home routers to detect both access link and wireless network bottlenecks.",,351–352,2,"troubleshooting, home networks, bottleneck location, performance diagnosis",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
623,inproceedings,"Sundaresan, Srikanth and Feamster, Nick and Teixeira, Renata",Locating Throughput Bottlenecks in Home Networks,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631440,10.1145/2619239.2631440,"We present a demonstration of WTF (Where's The Fault?), a system that localizes performance problems in home and access networks. We implement WTF as custom firmware that runs in an off-the-shelf home router. WTF uses timing and buffering information from passively monitored traffic at home routers to detect both access link and wireless network bottlenecks.",Proceedings of the 2014 ACM Conference on SIGCOMM,351–352,2,"performance diagnosis, bottleneck location, troubleshooting, home networks","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
624,article,"Ge, Xiongzi and Liu, Yi and Du, David H.C. and Zhang, Liang and Guan, Hongguang and Chen, Jian and Zhao, Yuping and Hu, Xinyu",OpenANFV: Accelerating Network Function Virtualization with a Consolidated Framework in Openstack,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631426,10.1145/2740070.2631426,"The resources of dedicated accelerators (e.g. FPGA) are still required to bridge the gap between software-based Middleboxs(MBs) and the commodity hardware. To consolidate various hardware resources in an elastic, programmable and reconfigurable manner, we design and build a flexible and consolidated framework, OpenANFV, to support virtualized accelerators for MBs in the cloud environment. OpenANFV is seamlessly and efficiently put into Openstack to provide high performance on top of commodity hardware to cope with various virtual function requirements. OpenANFV works as an independent component to manage and virtualize the acceleration resources (e.g. cinder manages block storage resources and nova manages computing resources). Specially, OpenANFV mainly has the following three features. (1)Automated Management. Provisioning for multiple Virtualized Network Functions (VNFs) is automated to meet the dynamic requirements of NFV environment. Such automation alleviates the time pressure of the complicated provisioning and configuration as well as reduces the probability of manually induced configuration errors. (2) Elasticity. VNFs are created, migrated, and destroyed on demand in real time. The reconfigurable hardware resources in pool can rapidly and flexibly offload the corresponding services to the accelerator platform in the dynamic NFV environment. (3) Coordinating with Openstack. The design and implementation of the OpenANFV APIs coordinate with the mechanisms in Openstack to support required virtualized MBs for multiple tenants.",,353–354,2,"FPGA, openstack, network function virtualization, middlebox",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
625,inproceedings,"Ge, Xiongzi and Liu, Yi and Du, David H.C. and Zhang, Liang and Guan, Hongguang and Chen, Jian and Zhao, Yuping and Hu, Xinyu",OpenANFV: Accelerating Network Function Virtualization with a Consolidated Framework in Openstack,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631426,10.1145/2619239.2631426,"The resources of dedicated accelerators (e.g. FPGA) are still required to bridge the gap between software-based Middleboxs(MBs) and the commodity hardware. To consolidate various hardware resources in an elastic, programmable and reconfigurable manner, we design and build a flexible and consolidated framework, OpenANFV, to support virtualized accelerators for MBs in the cloud environment. OpenANFV is seamlessly and efficiently put into Openstack to provide high performance on top of commodity hardware to cope with various virtual function requirements. OpenANFV works as an independent component to manage and virtualize the acceleration resources (e.g. cinder manages block storage resources and nova manages computing resources). Specially, OpenANFV mainly has the following three features. (1)Automated Management. Provisioning for multiple Virtualized Network Functions (VNFs) is automated to meet the dynamic requirements of NFV environment. Such automation alleviates the time pressure of the complicated provisioning and configuration as well as reduces the probability of manually induced configuration errors. (2) Elasticity. VNFs are created, migrated, and destroyed on demand in real time. The reconfigurable hardware resources in pool can rapidly and flexibly offload the corresponding services to the accelerator platform in the dynamic NFV environment. (3) Coordinating with Openstack. The design and implementation of the OpenANFV APIs coordinate with the mechanisms in Openstack to support required virtualized MBs for multiple tenants.",Proceedings of the 2014 ACM Conference on SIGCOMM,353–354,2,"FPGA, network function virtualization, middlebox, openstack","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
626,article,"Manco, Filipe and Martins, Joao and Huici, Felipe",Towards the Super Fluid Cloud,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631449,10.1145/2740070.2631449,"Traditionally, the number of VMs running on a server and how quickly these can be migrated has been less than optimal mostly because of the memory and CPU requirements imposed on the system by the full-fledged OSes that the VMs run. More recently, work towards VMs based on minimalistic or specialized OSes has started pushing the envelope of how reactive or fluid the cloud can be. In this demo we will demonstrate how to concurrently execute thousands of Xen-based VMs on a single inexpensive server. We will also show instantiation and migraion of such VMs in tens of milliseconds, and transparent, wide area migration of virtualized middleboxes by combining such VMs with the multi-path TCP (MPTCP) protocol.",,355–356,2,"cloud, server consolidation, performance, virtualization, xen",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
627,inproceedings,"Manco, Filipe and Martins, Joao and Huici, Felipe",Towards the Super Fluid Cloud,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631449,10.1145/2619239.2631449,"Traditionally, the number of VMs running on a server and how quickly these can be migrated has been less than optimal mostly because of the memory and CPU requirements imposed on the system by the full-fledged OSes that the VMs run. More recently, work towards VMs based on minimalistic or specialized OSes has started pushing the envelope of how reactive or fluid the cloud can be. In this demo we will demonstrate how to concurrently execute thousands of Xen-based VMs on a single inexpensive server. We will also show instantiation and migraion of such VMs in tens of milliseconds, and transparent, wide area migration of virtualized middleboxes by combining such VMs with the multi-path TCP (MPTCP) protocol.",Proceedings of the 2014 ACM Conference on SIGCOMM,355–356,2,"performance, server consolidation, virtualization, xen, cloud","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
628,article,"Stewart, Gordon and Gowda, Mahanth and Mainland, Geoffrey and Radunovic, Bozidar and Vytiniotis, Dimitrios and Patterson, Doug",Ziria: Language for Rapid Prototyping of Wireless PHY,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631427,10.1145/2740070.2631427,"Software-defined radios (SDR) have the potential to bring major innovation in wireless networking design. However, their impact so far has been limited due to complex programming tools. Most of the existing tools are either too slow to achieve the full line speeds of contemporary wireless PHYs or are too complex to master. In this demo we present our novel SDR programming environment called Ziria. Ziria consists of a novel programming language and an optimizing compiler. The compiler is able to synthesize very efficient SDR code from high-level PHY descriptions written in Ziria language. To illustrate its potential, we present the design of an LTE-like PHY layer in Ziria. We run it on the Sora SDR platform and demonstrate on a test-bed that it is able to operate in real-time.",,357–358,2,"SDR, domain specific language, wireless, programming, software-defined radio, DSL",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
629,inproceedings,"Stewart, Gordon and Gowda, Mahanth and Mainland, Geoffrey and Radunovic, Bozidar and Vytiniotis, Dimitrios and Patterson, Doug",Ziria: Language for Rapid Prototyping of Wireless PHY,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631427,10.1145/2619239.2631427,"Software-defined radios (SDR) have the potential to bring major innovation in wireless networking design. However, their impact so far has been limited due to complex programming tools. Most of the existing tools are either too slow to achieve the full line speeds of contemporary wireless PHYs or are too complex to master. In this demo we present our novel SDR programming environment called Ziria. Ziria consists of a novel programming language and an optimizing compiler. The compiler is able to synthesize very efficient SDR code from high-level PHY descriptions written in Ziria language. To illustrate its potential, we present the design of an LTE-like PHY layer in Ziria. We run it on the Sora SDR platform and demonstrate on a test-bed that it is able to operate in real-time.",Proceedings of the 2014 ACM Conference on SIGCOMM,357–358,2,"wireless, domain specific language, DSL, SDR, software-defined radio, programming","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
630,article,"Gebert, Steffen and Hock, David and Zinner, Thomas and Tran-Gia, Phuoc and Hoffmann, Marco and Jarschel, Michael and Schmidt, Ernst-Dieter and Braun, Ralf-Peter and Banse, Christian and K\""{o",Demonstrating the Optimal Placement of Virtualized Cellular Network Functions in Case of Large Crowd Events,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631428,10.1145/2740070.2631428,,,359–360,2,"function placement, network functions virtualisation",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
631,inproceedings,"Gebert, Steffen and Hock, David and Zinner, Thomas and Tran-Gia, Phuoc and Hoffmann, Marco and Jarschel, Michael and Schmidt, Ernst-Dieter and Braun, Ralf-Peter and Banse, Christian and K\""{o",Demonstrating the Optimal Placement of Virtualized Cellular Network Functions in Case of Large Crowd Events,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631428,10.1145/2619239.2631428,,Proceedings of the 2014 ACM Conference on SIGCOMM,359–360,2,"network functions virtualisation, function placement","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
632,article,"Hesmans, Benjamin and Bonaventure, Olivier",Tracing Multipath TCP Connections,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631453,10.1145/2740070.2631453,"Multipath TCP is a new extension to TCP that enables a host to transmit the packets from a given connection by using several interfaces. We propose mptcptrace, a software that enables a detailed analysis of Multipath TCP packet traces.",,361–362,2,Multipath TCP,,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
633,inproceedings,"Hesmans, Benjamin and Bonaventure, Olivier",Tracing Multipath TCP Connections,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631453,10.1145/2619239.2631453,"Multipath TCP is a new extension to TCP that enables a host to transmit the packets from a given connection by using several interfaces. We propose mptcptrace, a software that enables a detailed analysis of Multipath TCP packet traces.",Proceedings of the 2014 ACM Conference on SIGCOMM,361–362,2,Multipath TCP,"Chicago, Illinois, USA",SIGCOMM '14,,,,,,
634,article,"Schmidt, Mark and Heimgaertner, Florian and Menth, Michael",Demo: A Virtualized Lab Testbed with Physical Network Outlets for Hands-on Computer Networking Education,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631429,10.1145/2740070.2631429,"This demo presents a testbed for computer networking education. It leverages hardware virtualization to accommodate 6 PCs and 2 routers on a single testbed host to reduce costs, energy consumption, space requirements, and heat emission. The testbed excels by providing dedicated physical Ethernet and USB interfaces for virtual machines so that students can interconnect them with cables and switches like in a non-virtualized testbed",,363–364,2,"virtual machines, VLAN, computer networking education",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
635,inproceedings,"Schmidt, Mark and Heimgaertner, Florian and Menth, Michael",Demo: A Virtualized Lab Testbed with Physical Network Outlets for Hands-on Computer Networking Education,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631429,10.1145/2619239.2631429,"This demo presents a testbed for computer networking education. It leverages hardware virtualization to accommodate 6 PCs and 2 routers on a single testbed host to reduce costs, energy consumption, space requirements, and heat emission. The testbed excels by providing dedicated physical Ethernet and USB interfaces for virtual machines so that students can interconnect them with cables and switches like in a non-virtualized testbed",Proceedings of the 2014 ACM Conference on SIGCOMM,363–364,2,"computer networking education, virtual machines, VLAN","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
636,article,"Dong, Mo and Li, Qingxi and Zarchy, Doron and Godfrey, Brighten and Schapira, Michael",Rethinking Congestion Control Architecture: Performance-Oriented Congestion Control,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631456,10.1145/2740070.2631456,"After more than two decades of evolution, TCP and its end host based modifications can still suffer from severely degraded performance under real-world challenging network conditions. The reason, as we observe, is due to TCP family's fundamental architectural deficiency, which hardwires packet-level events to control responses and ignores emprical performance. Jumping out of TCP lineage's architectural deficiency, we propose Performance-oriented Congestion Control (PCC), a new congestion control architecture in which each sender controls its sending strategy based on empirically observed performance metrics. We show through preliminary experimental results that PCC achieves consistently high performance under various challenging network conditions.",,365–366,2,congestion control,,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
637,inproceedings,"Dong, Mo and Li, Qingxi and Zarchy, Doron and Godfrey, Brighten and Schapira, Michael",Rethinking Congestion Control Architecture: Performance-Oriented Congestion Control,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631456,10.1145/2619239.2631456,"After more than two decades of evolution, TCP and its end host based modifications can still suffer from severely degraded performance under real-world challenging network conditions. The reason, as we observe, is due to TCP family's fundamental architectural deficiency, which hardwires packet-level events to control responses and ignores emprical performance. Jumping out of TCP lineage's architectural deficiency, we propose Performance-oriented Congestion Control (PCC), a new congestion control architecture in which each sender controls its sending strategy based on empirically observed performance metrics. We show through preliminary experimental results that PCC achieves consistently high performance under various challenging network conditions.",Proceedings of the 2014 ACM Conference on SIGCOMM,365–366,2,congestion control,"Chicago, Illinois, USA",SIGCOMM '14,,,,,,
638,article,"Roy, Arup Raton and Bari, Md. Faizul and Zhani, Mohamed Faten and Ahmed, Reaz and Boutaba, Raouf",DOT: Distributed OpenFlow Testbed,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631457,10.1145/2740070.2631457,"With the growing adoption of Software Defined Networking (SDN) technology, there is a compelling need for an SDN emulator that can facilitate experimenting with new SDN solutions. Unfortunately, Mininet, the de facto standard emulator for software defined networks, fails to scale with network size and traffic volume. To address these limitations, we developed Distributed OpenFlow Testbed (DOT), a highly scalable emulator for SDN. It can emulate large SDN deployments by distributing the workload over a cluster of compute nodes. Moreover, DOT can emulate a wider range of network services compared to other publicly available SDN emulators and simulators. Our demonstration will illustrate several features of DOT including: (i) how easy it is to setup the emulator, (ii) how to deploy a topology using a single configuration file, (iii) how to run a connectivity test to ensure that the emulated network is properly deployed, and (iv) how to control and monitor the emulated components from a centralized location. We will also showcase DOT by emulating two applications: (i) policy based traffic steering through middleboxes and (ii) traffic monitoring.",,367–368,2,"emulator, software defined networking, testbed",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
639,inproceedings,"Roy, Arup Raton and Bari, Md. Faizul and Zhani, Mohamed Faten and Ahmed, Reaz and Boutaba, Raouf",DOT: Distributed OpenFlow Testbed,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631457,10.1145/2619239.2631457,"With the growing adoption of Software Defined Networking (SDN) technology, there is a compelling need for an SDN emulator that can facilitate experimenting with new SDN solutions. Unfortunately, Mininet, the de facto standard emulator for software defined networks, fails to scale with network size and traffic volume. To address these limitations, we developed Distributed OpenFlow Testbed (DOT), a highly scalable emulator for SDN. It can emulate large SDN deployments by distributing the workload over a cluster of compute nodes. Moreover, DOT can emulate a wider range of network services compared to other publicly available SDN emulators and simulators. Our demonstration will illustrate several features of DOT including: (i) how easy it is to setup the emulator, (ii) how to deploy a topology using a single configuration file, (iii) how to run a connectivity test to ensure that the emulated network is properly deployed, and (iv) how to control and monitor the emulated components from a centralized location. We will also showcase DOT by emulating two applications: (i) policy based traffic steering through middleboxes and (ii) traffic monitoring.",Proceedings of the 2014 ACM Conference on SIGCOMM,367–368,2,"software defined networking, emulator, testbed","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
640,article,"G\""{a",Evaluating the Effect of Centralization on Routing Convergence on a Hybrid BGP-SDN Emulation Framework,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631458,10.1145/2740070.2631458,,,369–370,2,"emulation, software defined networks, BGP",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
641,inproceedings,"G\""{a",Evaluating the Effect of Centralization on Routing Convergence on a Hybrid BGP-SDN Emulation Framework,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631458,10.1145/2619239.2631458,,Proceedings of the 2014 ACM Conference on SIGCOMM,369–370,2,"BGP, software defined networks, emulation","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
642,article,"Yuan, Zhenlong and Lu, Yongqiang and Wang, Zhaoguo and Xue, Yibo",Droid-Sec: Deep Learning in Android Malware Detection,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631434,10.1145/2740070.2631434,"As smartphones and mobile devices are rapidly becoming indispensable for many network users, mobile malware has become a serious threat in the network security and privacy. Especially on the popular Android platform, many malicious apps are hiding in a large number of normal apps, which makes the malware detection more challenging. In this paper, we propose a ML-based method that utilizes more than 200 features extracted from both static analysis and dynamic analysis of Android app for malware detection. The comparison of modeling results demonstrates that the deep learning technique is especially suitable for Android malware detection and can achieve a high level of 96% accuracy with real-world Android application sets.",,371–372,2,"detection, deep learning, android malware",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
643,inproceedings,"Yuan, Zhenlong and Lu, Yongqiang and Wang, Zhaoguo and Xue, Yibo",Droid-Sec: Deep Learning in Android Malware Detection,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631434,10.1145/2619239.2631434,"As smartphones and mobile devices are rapidly becoming indispensable for many network users, mobile malware has become a serious threat in the network security and privacy. Especially on the popular Android platform, many malicious apps are hiding in a large number of normal apps, which makes the malware detection more challenging. In this paper, we propose a ML-based method that utilizes more than 200 features extracted from both static analysis and dynamic analysis of Android app for malware detection. The comparison of modeling results demonstrates that the deep learning technique is especially suitable for Android malware detection and can achieve a high level of 96% accuracy with real-world Android application sets.",Proceedings of the 2014 ACM Conference on SIGCOMM,371–372,2,"deep learning, detection, android malware","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
644,article,"Samadi, Payman and Gupta, Varun and Birand, Berk and Wang, Howard and Zussman, Gil and Bergman, Keren",Accelerating Incast and Multicast Traffic Delivery for Data-Intensive Applications Using Physical Layer Optics,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631436,10.1145/2740070.2631436,We present a control plane architecture to accelerate multicast and incast traffic delivery for data-intensive applications in cluster-computing interconnection networks. The architecture is experimentally examined by enabling physical layer optical multicasting on-demand for the application layer to achieve non-blocking performance.,,373–374,2,"hybrid data center networks, incast, optics, multicast",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
645,inproceedings,"Samadi, Payman and Gupta, Varun and Birand, Berk and Wang, Howard and Zussman, Gil and Bergman, Keren",Accelerating Incast and Multicast Traffic Delivery for Data-Intensive Applications Using Physical Layer Optics,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631436,10.1145/2619239.2631436,We present a control plane architecture to accelerate multicast and incast traffic delivery for data-intensive applications in cluster-computing interconnection networks. The architecture is experimentally examined by enabling physical layer optical multicasting on-demand for the application layer to achieve non-blocking performance.,Proceedings of the 2014 ACM Conference on SIGCOMM,373–374,2,"multicast, optics, incast, hybrid data center networks","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
646,article,"Sathiaseelan, Arjuna and Seddiki, M. Said and Stoyanov, Stoyan and Trossen, Dirk",Social SDN: Online Social Networks Integration in Wireless Network Provisioning,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631437,10.1145/2740070.2631437,,,375–376,2,"home networks, software defined networking, online social networks",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
647,inproceedings,"Sathiaseelan, Arjuna and Seddiki, M. Said and Stoyanov, Stoyan and Trossen, Dirk",Social SDN: Online Social Networks Integration in Wireless Network Provisioning,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631437,10.1145/2619239.2631437,,Proceedings of the 2014 ACM Conference on SIGCOMM,375–376,2,"software defined networking, online social networks, home networks","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
648,article,"Moshref, Masoud and Bhargava, Apoorv and Gupta, Adhip and Yu, Minlan and Govindan, Ramesh",Flow-Level State Transition as a New Switch Primitive for SDN,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631439,10.1145/2740070.2631439,,,377–378,2,"state machine, software-defined network",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
649,inproceedings,"Moshref, Masoud and Bhargava, Apoorv and Gupta, Adhip and Yu, Minlan and Govindan, Ramesh",Flow-Level State Transition as a New Switch Primitive for SDN,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631439,10.1145/2619239.2631439,,Proceedings of the 2014 ACM Conference on SIGCOMM,377–378,2,"state machine, software-defined network","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
650,article,"Zhu, Liang and Hu, Zi and Heidemann, John and Wessels, Duane and Mankin, Allison and Somaiya, Nikita",T-DNS: Connection-Oriented DNS to Improve Privacy and Security (Poster Abstract),2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631442,10.1145/2740070.2631442,"DNS is the canonical protocol for connectionless UDP. Yet DNS today is challenged by eavesdropping that compromises privacy, source-address spoofing that results in denial-of-service (DoS) attacks on the server and third parties, injection attacks that exploit fragmentation, and size limitations that constrain policy and operational choices. We propose T-DNS to address these problems. It uses TCP to smoothly support large payloads and to mitigate spoofing and amplification for DoS. T-DNS uses transport-layer security (TLS) to provide privacy from users to their DNS resolvers and optionally to authoritative servers. Our model shows end-to-end latency from TLS to the recursive resolver is only about 9% slower when UDP is used to the authoritative server, and 22% slower with TCP to the authoritative. With diverse traces we show that frequent connection reuse is possible (60-95% for stub and recursive resolvers, although half that for authoritative servers). Our experiment shows that after connection establishment, TCP and TLS latency is equivalent to UDP. With conservative timeouts (20 s at authoritative servers and 60 s elsewhere) and conservative estimates of connection state memory requirements, we show that server memory requirements well within current, commodity server hardware. We identify the key design and implementation decisions needed to minimize overhead: query pipelining, out-of-order responses, TLS connection resumption, and plausible timeouts. This poster abstract summarizes work we describe in detail in ISI-TR-2014-693.",,379–380,2,"domain name system (DNS), network protocols, privacy, performance, transport layer security (TLS), security",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
651,inproceedings,"Zhu, Liang and Hu, Zi and Heidemann, John and Wessels, Duane and Mankin, Allison and Somaiya, Nikita",T-DNS: Connection-Oriented DNS to Improve Privacy and Security (Poster Abstract),2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631442,10.1145/2619239.2631442,"DNS is the canonical protocol for connectionless UDP. Yet DNS today is challenged by eavesdropping that compromises privacy, source-address spoofing that results in denial-of-service (DoS) attacks on the server and third parties, injection attacks that exploit fragmentation, and size limitations that constrain policy and operational choices. We propose T-DNS to address these problems. It uses TCP to smoothly support large payloads and to mitigate spoofing and amplification for DoS. T-DNS uses transport-layer security (TLS) to provide privacy from users to their DNS resolvers and optionally to authoritative servers. Our model shows end-to-end latency from TLS to the recursive resolver is only about 9% slower when UDP is used to the authoritative server, and 22% slower with TCP to the authoritative. With diverse traces we show that frequent connection reuse is possible (60-95% for stub and recursive resolvers, although half that for authoritative servers). Our experiment shows that after connection establishment, TCP and TLS latency is equivalent to UDP. With conservative timeouts (20 s at authoritative servers and 60 s elsewhere) and conservative estimates of connection state memory requirements, we show that server memory requirements well within current, commodity server hardware. We identify the key design and implementation decisions needed to minimize overhead: query pipelining, out-of-order responses, TLS connection resumption, and plausible timeouts. This poster abstract summarizes work we describe in detail in ISI-TR-2014-693.",Proceedings of the 2014 ACM Conference on SIGCOMM,379–380,2,"domain name system (DNS), network protocols, performance, privacy, transport layer security (TLS), security","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
652,article,"Michel, Oliver and Coughlin, Michael and Keller, Eric",Extending the Software-Defined Network Boundary,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631443,10.1145/2740070.2631443,"Given that Software-Defined Networking is highly successful in solving many of today's manageability, flexibility, and scalability issues in large-scale networks, in this paper we argue that the concept of SDN can be extended even further. Many applications (esp. stream processing and big-data applications) rely on graph-based inter-process communication patterns that are very similar to those in computer networks. To our mind, this network abstraction spanning over different types of entities is highly suitable for and would benefit from central (SDN-inspired) control for the same reasons classical networks do. In this work, we investigate the commonalities between such intra-host networks and classical computer networking. Based on this, we study the feasibility of a central network controller that manages both network traffic and intra-host communication over a custom bus system.",,381–382,2,"SDN, stream processing, multithreading",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
653,inproceedings,"Michel, Oliver and Coughlin, Michael and Keller, Eric",Extending the Software-Defined Network Boundary,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631443,10.1145/2619239.2631443,"Given that Software-Defined Networking is highly successful in solving many of today's manageability, flexibility, and scalability issues in large-scale networks, in this paper we argue that the concept of SDN can be extended even further. Many applications (esp. stream processing and big-data applications) rely on graph-based inter-process communication patterns that are very similar to those in computer networks. To our mind, this network abstraction spanning over different types of entities is highly suitable for and would benefit from central (SDN-inspired) control for the same reasons classical networks do. In this work, we investigate the commonalities between such intra-host networks and classical computer networking. Based on this, we study the feasibility of a central network controller that manages both network traffic and intra-host communication over a custom bus system.",Proceedings of the 2014 ACM Conference on SIGCOMM,381–382,2,"stream processing, multithreading, SDN","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
654,inproceedings,"Wroclawski, John",Session Details: Network Architecture (2),2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246778,10.1145/3246778,,Proceedings of the 2014 ACM Conference on SIGCOMM,,,,"Chicago, Illinois, USA",SIGCOMM '14,,,,,,
655,article,"Sharma, Abhigyan and Tie, Xiaozheng and Uppal, Hardeep and Venkataramani, Arun and Westbrook, David and Yadav, Aditya",A Global Name Service for a Highly Mobile Internetwork,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626331,10.1145/2740070.2626331,"Mobile devices dominate the Internet today, however the Internet rooted in its tethered origins continues to provide poor infrastructure support for mobility. Our position is that in order to address this problem, a key challenge that must be addressed is the design of a massively scalable global name service that rapidly resolves identities to network locations under high mobility. Our primary contribution is the design, implementation, and evaluation of auspice, a next-generation global name service that addresses this challenge. A key insight underlying auspice is a demand-aware replica {placement engine",,247–258,12,"mobility, distributed systems, network architecture",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
656,inproceedings,"Sharma, Abhigyan and Tie, Xiaozheng and Uppal, Hardeep and Venkataramani, Arun and Westbrook, David and Yadav, Aditya",A Global Name Service for a Highly Mobile Internetwork,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626331,10.1145/2619239.2626331,"Mobile devices dominate the Internet today, however the Internet rooted in its tethered origins continues to provide poor infrastructure support for mobility. Our position is that in order to address this problem, a key challenge that must be addressed is the design of a massively scalable global name service that rapidly resolves identities to network locations under high mobility. Our primary contribution is the design, implementation, and evaluation of auspice, a next-generation global name service that addresses this challenge. A key insight underlying auspice is a demand-aware replica {placement engine",Proceedings of the 2014 ACM Conference on SIGCOMM,247–258,12,"distributed systems, mobility, network architecture","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
657,article,"Gao, Zhaoyu and Venkataramani, Arun and Kurose, James F. and Heimlicher, Simon",Towards a Quantitative Comparison of Location-Independent Network Architectures,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626333,10.1145/2740070.2626333,This paper presents a quantitative methodology and results comparing different approaches for {it location-independent,,259–270,12,"location-independence, network architecture, mobility",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
658,inproceedings,"Gao, Zhaoyu and Venkataramani, Arun and Kurose, James F. and Heimlicher, Simon",Towards a Quantitative Comparison of Location-Independent Network Architectures,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626333,10.1145/2619239.2626333,This paper presents a quantitative methodology and results comparing different approaches for {it location-independent,Proceedings of the 2014 ACM Conference on SIGCOMM,259–270,12,"location-independence, network architecture, mobility","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
659,article,"Kim, Tiffany Hyun-Jin and Basescu, Cristina and Jia, Limin and Lee, Soo Bum and Hu, Yih-Chun and Perrig, Adrian",Lightweight Source Authentication and Path Validation,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626323,10.1145/2740070.2626323,"In-network source authentication and path validation are fundamental primitives to construct higher-level security mechanisms such as DDoS mitigation, path compliance, packet attribution, or protection against flow redirection. Unfortunately, currently proposed solutions either fall short of addressing important security concerns or require a substantial amount of router overhead. In this paper, we propose lightweight, scalable, and secure protocols for shared key setup, source authentication, and path validation. Our prototype implementation demonstrates the efficiency and scalability of the protocols, especially for software-based implementations.",,271–282,12,"source authentication, retroactive key setup, path validation",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
660,inproceedings,"Kim, Tiffany Hyun-Jin and Basescu, Cristina and Jia, Limin and Lee, Soo Bum and Hu, Yih-Chun and Perrig, Adrian",Lightweight Source Authentication and Path Validation,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626323,10.1145/2619239.2626323,"In-network source authentication and path validation are fundamental primitives to construct higher-level security mechanisms such as DDoS mitigation, path compliance, packet attribution, or protection against flow redirection. Unfortunately, currently proposed solutions either fall short of addressing important security concerns or require a substantial amount of router overhead. In this paper, we propose lightweight, scalable, and secure protocols for shared key setup, source authentication, and path validation. Our prototype implementation demonstrates the efficiency and scalability of the protocols, especially for software-based implementations.",Proceedings of the 2014 ACM Conference on SIGCOMM,271–282,12,"path validation, retroactive key setup, source authentication","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
661,inproceedings,"Fonseca, Rodrigo",Session Details: Scheduling in Datacenter Networks,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246779,10.1145/3246779,,Proceedings of the 2014 ACM Conference on SIGCOMM,,,,"Chicago, Illinois, USA",SIGCOMM '14,,,,,,
662,article,"Dogar, Fahad R. and Karagiannis, Thomas and Ballani, Hitesh and Rowstron, Antony",Decentralized Task-Aware Scheduling for Data Center Networks,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626322,10.1145/2740070.2626322,"Many data center applications perform rich and complex tasks (e.g., executing a search query or generating a user's news-feed). From a network perspective, these tasks typically comprise multiple flows, which traverse different parts of the network at potentially different times. Most network resource allocation schemes, however, treat all these flows in isolation -- rather than as part of a task -- and therefore only optimize flow-level metrics.In this paper, we show that task-aware network scheduling, which groups flows of a task and schedules them together, can reduce both the average as well as tail completion time for typical data center applications. To achieve these benefits in practice, we design and implement Baraat, a decentralized task-aware scheduling system. Baraat schedules tasks in a FIFO order but avoids head-of-line blocking by dynamically changing the level of multiplexing in the network. Through experiments with Memcached on a small testbed and large-scale simulations, we show that Baraat outperforms state-of-the-art decentralized schemes (e.g., pFabric) as well as centralized schedulers (e.g., Orchestra) for a wide range of workloads (e.g., search, analytics, etc).",,431–442,12,"scheduling, datacenter, response time, transport",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
663,inproceedings,"Dogar, Fahad R. and Karagiannis, Thomas and Ballani, Hitesh and Rowstron, Antony",Decentralized Task-Aware Scheduling for Data Center Networks,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626322,10.1145/2619239.2626322,"Many data center applications perform rich and complex tasks (e.g., executing a search query or generating a user's news-feed). From a network perspective, these tasks typically comprise multiple flows, which traverse different parts of the network at potentially different times. Most network resource allocation schemes, however, treat all these flows in isolation -- rather than as part of a task -- and therefore only optimize flow-level metrics.In this paper, we show that task-aware network scheduling, which groups flows of a task and schedules them together, can reduce both the average as well as tail completion time for typical data center applications. To achieve these benefits in practice, we design and implement Baraat, a decentralized task-aware scheduling system. Baraat schedules tasks in a FIFO order but avoids head-of-line blocking by dynamically changing the level of multiplexing in the network. Through experiments with Memcached on a small testbed and large-scale simulations, we show that Baraat outperforms state-of-the-art decentralized schemes (e.g., pFabric) as well as centralized schedulers (e.g., Orchestra) for a wide range of workloads (e.g., search, analytics, etc).",Proceedings of the 2014 ACM Conference on SIGCOMM,431–442,12,"response time, scheduling, transport, datacenter","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
664,article,"Chowdhury, Mosharaf and Zhong, Yuan and Stoica, Ion",Efficient Coflow Scheduling with Varys,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626315,10.1145/2740070.2626315,"Communication in data-parallel applications often involves a collection of parallel flows. Traditional techniques to optimize flow-level metrics do not perform well in optimizing such collections, because the network is largely agnostic to application-level requirements. The recently proposed coflow abstraction bridges this gap and creates new opportunities for network scheduling. In this paper, we address inter-coflow scheduling for two different objectives: decreasing communication time of data-intensive jobs and guaranteeing predictable communication time. We introduce the concurrent open shop scheduling with coupled resources problem, analyze its complexity, and propose effective heuristics to optimize either objective. We present Varys, a system that enables data-intensive frameworks to use coflows and the proposed algorithms while maintaining high network utilization and guaranteeing starvation freedom. EC2 deployments and trace-driven simulations show that communication stages complete up to 3.16X faster on average and up to 2X more coflows meet their deadlines using Varys in comparison to per-flow mechanisms. Moreover, Varys outperforms non-preemptive coflow schedulers by more than 5X.",,443–454,12,"data-intensive applications, coflow, datacenter networks",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
665,inproceedings,"Chowdhury, Mosharaf and Zhong, Yuan and Stoica, Ion",Efficient Coflow Scheduling with Varys,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626315,10.1145/2619239.2626315,"Communication in data-parallel applications often involves a collection of parallel flows. Traditional techniques to optimize flow-level metrics do not perform well in optimizing such collections, because the network is largely agnostic to application-level requirements. The recently proposed coflow abstraction bridges this gap and creates new opportunities for network scheduling. In this paper, we address inter-coflow scheduling for two different objectives: decreasing communication time of data-intensive jobs and guaranteeing predictable communication time. We introduce the concurrent open shop scheduling with coupled resources problem, analyze its complexity, and propose effective heuristics to optimize either objective. We present Varys, a system that enables data-intensive frameworks to use coflows and the proposed algorithms while maintaining high network utilization and guaranteeing starvation freedom. EC2 deployments and trace-driven simulations show that communication stages complete up to 3.16X faster on average and up to 2X more coflows meet their deadlines using Varys in comparison to per-flow mechanisms. Moreover, Varys outperforms non-preemptive coflow schedulers by more than 5X.",Proceedings of the 2014 ACM Conference on SIGCOMM,443–454,12,"datacenter networks, coflow, data-intensive applications","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
666,article,"Grandl, Robert and Ananthanarayanan, Ganesh and Kandula, Srikanth and Rao, Sriram and Akella, Aditya",Multi-Resource Packing for Cluster Schedulers,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626334,10.1145/2740070.2626334,"Tasks in modern data parallel clusters have highly diverse resource requirements, along CPU, memory, disk and network. Any of these resources may become bottlenecks and hence, the likelihood of wasting resources due to fragmentation is now larger. Today's schedulers do not explicitly reduce fragmentation. Worse, since they only allocate cores and memory, the resources that they ignore (disk and network) can be over-allocated leading to interference, failures and hogging of cores or memory that could have been used by other tasks. We present Tetris, a cluster scheduler that packs, i.e., matches multi-resource task requirements with resource availabilities of machines so as to increase cluster efficiency (makespan). Further, Tetris uses an analog of shortest-running-time-first to trade-off cluster efficiency for speeding up individual jobs. Tetris' packing heuristics seamlessly work alongside a large class of fairness policies. Trace-driven simulations and deployment of our prototype on a 250 node cluster shows median gains of 30% in job completion time while achieving nearly perfect fairness.",,455–466,12,"fairness, multi-dimensional, completion time, makespan, cluster schedulers, packing",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
667,inproceedings,"Grandl, Robert and Ananthanarayanan, Ganesh and Kandula, Srikanth and Rao, Sriram and Akella, Aditya",Multi-Resource Packing for Cluster Schedulers,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626334,10.1145/2619239.2626334,"Tasks in modern data parallel clusters have highly diverse resource requirements, along CPU, memory, disk and network. Any of these resources may become bottlenecks and hence, the likelihood of wasting resources due to fragmentation is now larger. Today's schedulers do not explicitly reduce fragmentation. Worse, since they only allocate cores and memory, the resources that they ignore (disk and network) can be over-allocated leading to interference, failures and hogging of cores or memory that could have been used by other tasks. We present Tetris, a cluster scheduler that packs, i.e., matches multi-resource task requirements with resource availabilities of machines so as to increase cluster efficiency (makespan). Further, Tetris uses an analog of shortest-running-time-first to trade-off cluster efficiency for speeding up individual jobs. Tetris' packing heuristics seamlessly work alongside a large class of fairness policies. Trace-driven simulations and deployment of our prototype on a 250 node cluster shows median gains of 30% in job completion time while achieving nearly perfect fairness.",Proceedings of the 2014 ACM Conference on SIGCOMM,455–466,12,"makespan, cluster schedulers, multi-dimensional, packing, fairness, completion time","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
668,article,"Lee, Jeongkeun and Turner, Yoshio and Lee, Myungjin and Popa, Lucian and Banerjee, Sujata and Kang, Joon-Myung and Sharma, Puneet",Application-Driven Bandwidth Guarantees in Datacenters,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626326,10.1145/2740070.2626326,"Providing bandwidth guarantees to specific applications is becoming increasingly important as applications compete for shared cloud network resources. We present CloudMirror, a solution that provides bandwidth guarantees to cloud applications based on a new network abstraction and workload placement algorithm. An effective network abstraction should enable applications to easily and accurately specify their requirements, while simultaneously enabling the infrastructure to provision resources efficiently for deployed applications. Prior research has approached the bandwidth guarantee specification by using abstractions that resemble physical network topologies. We present a contrasting approach of deriving a network abstraction based on application communication structure, called Tenant Application Graph or TAG. CloudMirror also incorporates a new workload placement algorithm that efficiently meets bandwidth requirements specified by TAGs while factoring in high availability considerations. Extensive simulations using real application traces and datacenter topologies show that CloudMirror can handle 40% more bandwidth demand than the state of the art (e.g., the Oktopus system), while improving high availability from 20% to 70%.",,467–478,12,"application, cloud, bandwidth, virtual network, datacenter, availability",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
669,inproceedings,"Lee, Jeongkeun and Turner, Yoshio and Lee, Myungjin and Popa, Lucian and Banerjee, Sujata and Kang, Joon-Myung and Sharma, Puneet",Application-Driven Bandwidth Guarantees in Datacenters,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626326,10.1145/2619239.2626326,"Providing bandwidth guarantees to specific applications is becoming increasingly important as applications compete for shared cloud network resources. We present CloudMirror, a solution that provides bandwidth guarantees to cloud applications based on a new network abstraction and workload placement algorithm. An effective network abstraction should enable applications to easily and accurately specify their requirements, while simultaneously enabling the infrastructure to provision resources efficiently for deployed applications. Prior research has approached the bandwidth guarantee specification by using abstractions that resemble physical network topologies. We present a contrasting approach of deriving a network abstraction based on application communication structure, called Tenant Application Graph or TAG. CloudMirror also incorporates a new workload placement algorithm that efficiently meets bandwidth requirements specified by TAGs while factoring in high availability considerations. Extensive simulations using real application traces and datacenter topologies show that CloudMirror can handle 40% more bandwidth demand than the state of the art (e.g., the Oktopus system), while improving high availability from 20% to 70%.",Proceedings of the 2014 ACM Conference on SIGCOMM,467–478,12,"bandwidth, availability, cloud, virtual network, datacenter, application","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
670,inproceedings,"Jamieson, Kyle",Session Details: Wireless 2,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246780,10.1145/3246780,,Proceedings of the 2014 ACM Conference on SIGCOMM,,,,"Chicago, Illinois, USA",SIGCOMM '14,,,,,,
671,article,"Yenamandra, Vivek and Srinivasan, Kannan",Vidyut: Exploiting Power Line Infrastructure for Enterprise Wireless Networks,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626329,10.1145/2740070.2626329,"Global synchronization across time and frequency domains significantly benefits wireless communications. Multi-Cell (Network) MIMO, interference alignment solutions, opportunistic routing techniques in ad-hoc networks, OFDMA etc. all necessitate synchronization in either time or frequency domain or both. This paper presents sysname, a system that exploits the easily accessible and ubiquitous power line infrastructure to achieve synchronization in time and frequency domains across nodes distributed beyond a single-collision domain. sysname uses the power lines to transmit a reference frequency tone to which each node locks its frequency. sysname exploits the steady periodicity of delivered power signal itself to synchronize distributed nodes in time.We validate the extent of sysname's synchronization and evaluate its effectiveness. We verify sysname's suitability for wireless applications such as OFDMA and multi-cell MIMO by validating the benefits of global synchronization in an enterprise wireless network. Our experiments show a throughput gain of 8.2x over MegaMIMO, 7x over NemoX and 2.5x over OFDMA systems.Enterprise wireless networks are supported by an Ethernet backbone. Researchers have been exploring techniques over the backbone to enable and assist in improving the performance of the wireless networks. Recent work also showed the benefit of sharing information in the air between the nodes to enable higher performance of the network. Even sharing information as little as synchronization information has been demonstrated to open new avenues (such as MU-MIMO, Physical Network Coding, Opportunistic routing, etc.) for the wireless networks to enhance its performance. However, another medium shared by majority of the nodes in the enterprise network - the power line infrastructure - has been largely left untapped to assist the wireless network. While the power lines are noisy and have a frequency selective transmission characteristic, its uniqueness is that its range can extend beyond that over air while it is unburdened by switching and other responsibilities of the Ethernet backbone. This paper poses the following question: How best to exploit the opportunity presented by the power lines to further enhance enterprise wireless networks?The key contributions of this paper are the following: Identify and demonstrate the feasibility of utilizing power lines as a medium to achieve synchronization (in time and frequency domains) between nodes in the network; Demonstrate the scalability of this technique by achieving synchronization between nodes beyond the transmission range of any of the individual nodes. The paper presents empirical results pertaining to the accuracy of synchronization and the benefit of the proposed synchronization method to existing distributed wireless techniques by virtue of extension across multiple collision domains.",,595–606,12,"power line communications, network mimo, frequency synchronization, wireless networks, time synchronization",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
672,inproceedings,"Yenamandra, Vivek and Srinivasan, Kannan",Vidyut: Exploiting Power Line Infrastructure for Enterprise Wireless Networks,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626329,10.1145/2619239.2626329,"Global synchronization across time and frequency domains significantly benefits wireless communications. Multi-Cell (Network) MIMO, interference alignment solutions, opportunistic routing techniques in ad-hoc networks, OFDMA etc. all necessitate synchronization in either time or frequency domain or both. This paper presents sysname, a system that exploits the easily accessible and ubiquitous power line infrastructure to achieve synchronization in time and frequency domains across nodes distributed beyond a single-collision domain. sysname uses the power lines to transmit a reference frequency tone to which each node locks its frequency. sysname exploits the steady periodicity of delivered power signal itself to synchronize distributed nodes in time.We validate the extent of sysname's synchronization and evaluate its effectiveness. We verify sysname's suitability for wireless applications such as OFDMA and multi-cell MIMO by validating the benefits of global synchronization in an enterprise wireless network. Our experiments show a throughput gain of 8.2x over MegaMIMO, 7x over NemoX and 2.5x over OFDMA systems.Enterprise wireless networks are supported by an Ethernet backbone. Researchers have been exploring techniques over the backbone to enable and assist in improving the performance of the wireless networks. Recent work also showed the benefit of sharing information in the air between the nodes to enable higher performance of the network. Even sharing information as little as synchronization information has been demonstrated to open new avenues (such as MU-MIMO, Physical Network Coding, Opportunistic routing, etc.) for the wireless networks to enhance its performance. However, another medium shared by majority of the nodes in the enterprise network - the power line infrastructure - has been largely left untapped to assist the wireless network. While the power lines are noisy and have a frequency selective transmission characteristic, its uniqueness is that its range can extend beyond that over air while it is unburdened by switching and other responsibilities of the Ethernet backbone. This paper poses the following question: How best to exploit the opportunity presented by the power lines to further enhance enterprise wireless networks?The key contributions of this paper are the following: Identify and demonstrate the feasibility of utilizing power lines as a medium to achieve synchronization (in time and frequency domains) between nodes in the network; Demonstrate the scalability of this technique by achieving synchronization between nodes beyond the transmission range of any of the individual nodes. The paper presents empirical results pertaining to the accuracy of synchronization and the benefit of the proposed synchronization method to existing distributed wireless techniques by virtue of extension across multiple collision domains.",Proceedings of the 2014 ACM Conference on SIGCOMM,595–606,12,"wireless networks, network mimo, power line communications, frequency synchronization, time synchronization","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
673,article,"Kellogg, Bryce and Parks, Aaron and Gollakota, Shyamnath and Smith, Joshua R. and Wetherall, David",Wi-Fi Backscatter: Internet Connectivity for RF-Powered Devices,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626319,10.1145/2740070.2626319,"RF-powered computers are small devices that compute and communicate using only the power that they harvest from RF signals. While existing technologies have harvested power from ambient RF sources (e.g., TV broadcasts), they require a dedicated gateway (like an RFID reader) for Internet connectivity. We present Wi-Fi Backscatter, a novel communication system that bridges RF-powered devices with the Internet. Specifically, we show that it is possible to reuse existing Wi-Fi infrastructure to provide Internet connectivity to RF-powered devices. To show Wi-Fi Backscatter's feasibility, we build a hardware prototype and demonstrate the first communication link between an RF-powered device and commodity Wi-Fi devices. We use off-the-shelf Wi-Fi devices including Intel Wi-Fi cards, Linksys Routers, and our organization's Wi-Fi infrastructure, and achieve communication rates of up to 1 kbps and ranges of up to 2.1 meters. We believe that this new capability can pave the way for the rapid deployment and adoption of RF-powered devices and achieve ubiquitous connectivity via nearby mobile devices that are Wi-Fi enabled.",,607–618,12,"energy harvesting, internet of things, backscatter, wireless",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
674,inproceedings,"Kellogg, Bryce and Parks, Aaron and Gollakota, Shyamnath and Smith, Joshua R. and Wetherall, David",Wi-Fi Backscatter: Internet Connectivity for RF-Powered Devices,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626319,10.1145/2619239.2626319,"RF-powered computers are small devices that compute and communicate using only the power that they harvest from RF signals. While existing technologies have harvested power from ambient RF sources (e.g., TV broadcasts), they require a dedicated gateway (like an RFID reader) for Internet connectivity. We present Wi-Fi Backscatter, a novel communication system that bridges RF-powered devices with the Internet. Specifically, we show that it is possible to reuse existing Wi-Fi infrastructure to provide Internet connectivity to RF-powered devices. To show Wi-Fi Backscatter's feasibility, we build a hardware prototype and demonstrate the first communication link between an RF-powered device and commodity Wi-Fi devices. We use off-the-shelf Wi-Fi devices including Intel Wi-Fi cards, Linksys Routers, and our organization's Wi-Fi infrastructure, and achieve communication rates of up to 1 kbps and ranges of up to 2.1 meters. We believe that this new capability can pave the way for the rapid deployment and adoption of RF-powered devices and achieve ubiquitous connectivity via nearby mobile devices that are Wi-Fi enabled.",Proceedings of the 2014 ACM Conference on SIGCOMM,607–618,12,"backscatter, wireless, internet of things, energy harvesting","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
675,article,"Parks, Aaron N. and Liu, Angli and Gollakota, Shyamnath and Smith, Joshua R.",Turbocharging Ambient Backscatter Communication,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626312,10.1145/2740070.2626312,"Communication primitives such as coding and multiple antenna processing have provided significant benefits for traditional wireless systems. Existing designs, however, consume significant power and computational resources, and hence cannot be run on low complexity, power constrained backscatter devices. This paper makes two main contributions: (1) we introduce the first multi-antenna cancellation design that operates on backscatter devices while retaining a small form factor and power footprint, (2) we introduce a novel coding mechanism that enables long range communication as well as concurrent transmissions and can be decoded on backscatter devices. We build hardware prototypes of the above designs that can be powered solely using harvested energy from TV and solar sources. The results show that our designs provide benefits for both RFID and ambient backscatter systems: they enable RFID tags to communicate directly with each other at distances of tens of meters and through multiple walls. They also increase the communication rate and range achieved by ambient backscatter systems by 100X and 40X respectively. We believe that this paper represents a substantial leap in the capabilities of backscatter communication.",,619–630,12,"energy harvesting, wireless, internet of things, backscatter",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
676,inproceedings,"Parks, Aaron N. and Liu, Angli and Gollakota, Shyamnath and Smith, Joshua R.",Turbocharging Ambient Backscatter Communication,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626312,10.1145/2619239.2626312,"Communication primitives such as coding and multiple antenna processing have provided significant benefits for traditional wireless systems. Existing designs, however, consume significant power and computational resources, and hence cannot be run on low complexity, power constrained backscatter devices. This paper makes two main contributions: (1) we introduce the first multi-antenna cancellation design that operates on backscatter devices while retaining a small form factor and power footprint, (2) we introduce a novel coding mechanism that enables long range communication as well as concurrent transmissions and can be decoded on backscatter devices. We build hardware prototypes of the above designs that can be powered solely using harvested energy from TV and solar sources. The results show that our designs provide benefits for both RFID and ambient backscatter systems: they enable RFID tags to communicate directly with each other at distances of tens of meters and through multiple walls. They also increase the communication rate and range achieved by ambient backscatter systems by 100X and 40X respectively. We believe that this paper represents a substantial leap in the capabilities of backscatter communication.",Proceedings of the 2014 ACM Conference on SIGCOMM,619–630,12,"wireless, energy harvesting, internet of things, backscatter","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
677,article,"Nikitopoulos, Konstantinos and Zhou, Juan and Congdon, Ben and Jamieson, Kyle",Geosphere: Consistently Turning MIMO Capacity into Throughput,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626301,10.1145/2740070.2626301,"This paper presents the design and implementation of Geosphere, a physical- and link-layer design for access point-based MIMO wireless networks that consistently improves network throughput. To send multiple streams of data in a MIMO system, prior designs rely on a technique called zero-forcing, a way of ""nulling"" the interference between data streams by mathematically inverting the wireless channel matrix. In general, zero-forcing is highly effective, significantly improving throughput. But in certain physical situations, the MIMO channel matrix can become ""poorly conditioned,"" harming performance. With these situations in mind, Geosphere uses sphere decoding, a more computationally demanding technique that can achieve higher throughput in such channels. To overcome the sphere decoder's computational complexity when sending dense wireless constellations at a high rate, Geosphere introduces search and pruning techniques that incorporate novel geometric reasoning about the wireless constellation. These techniques reduce computational complexity of 256-QAM systems by almost one order of magnitude, bringing computational demands in line with current 16- and 64-QAM systems already realized in ASIC. Geosphere thus makes the sphere decoder practical for the first time in a 4 \texttimes{",,631–642,12,"MIMO, distributed MIMO, sphere decoder",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
678,inproceedings,"Nikitopoulos, Konstantinos and Zhou, Juan and Congdon, Ben and Jamieson, Kyle",Geosphere: Consistently Turning MIMO Capacity into Throughput,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626301,10.1145/2619239.2626301,"This paper presents the design and implementation of Geosphere, a physical- and link-layer design for access point-based MIMO wireless networks that consistently improves network throughput. To send multiple streams of data in a MIMO system, prior designs rely on a technique called zero-forcing, a way of ""nulling"" the interference between data streams by mathematically inverting the wireless channel matrix. In general, zero-forcing is highly effective, significantly improving throughput. But in certain physical situations, the MIMO channel matrix can become ""poorly conditioned,"" harming performance. With these situations in mind, Geosphere uses sphere decoding, a more computationally demanding technique that can achieve higher throughput in such channels. To overcome the sphere decoder's computational complexity when sending dense wireless constellations at a high rate, Geosphere introduces search and pruning techniques that incorporate novel geometric reasoning about the wireless constellation. These techniques reduce computational complexity of 256-QAM systems by almost one order of magnitude, bringing computational demands in line with current 16- and 64-QAM systems already realized in ASIC. Geosphere thus makes the sphere decoder practical for the first time in a 4 \texttimes{",Proceedings of the 2014 ACM Conference on SIGCOMM,631–642,12,"distributed MIMO, MIMO, sphere decoder","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
679,inproceedings,"Moore, Andrew",Session Details: Network Operations,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246781,10.1145/3246781,,Proceedings of the 2014 ACM Conference on SIGCOMM,,,,"Chicago, Illinois, USA",SIGCOMM '14,,,,,,
680,article,"Kandula, Srikanth and Menache, Ishai and Schwartz, Roy and Babbula, Spandana Raj",Calendaring for Wide Area Networks,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626336,10.1145/2740070.2626336,"Datacenter WAN traffic consists of high priority transfers that have to be carried as soon as they arrive alongside large transfers with pre-assigned deadlines on their completion (ranging from minutes to hours). The ability to offer guarantees to large transfers is crucial for business needs and impacts overall cost-of-business. State-of-the-art traffic engineering solutions only consider the current time epoch and hence cannot provide pre-facto promises for long-lived transfers. We present Tempus, an online traffic engineering scheme that exploits information on transfer size and deadlines to appropriately pack long-running transfers across network paths and time, thereby leaving enough capacity slack for future high-priority requests. Tempus builds on a tailored approximate solution to a mixed packing-covering linear program, which is parallelizable and scales well in both running time and memory usage. Consequently, Tempus is able to quickly and effectively update its solution when new transfers arrive or unexpected changes happen. These updates involve only small edits to existing transfers. Therefore, as experiments on traces from a large production WAN show, Tempus can offer and keep promises to long-lived transfers well in advance of their actual deadline; the promise on minimal transfer size is comparable with an offline optimal solution and outperforms state-of-the-art solutions by 2-3X.",,515–526,12,"software-defined networking, deadlines, inter-datacenter, mixed packing covering, online temporal planning, wide area network",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
681,inproceedings,"Kandula, Srikanth and Menache, Ishai and Schwartz, Roy and Babbula, Spandana Raj",Calendaring for Wide Area Networks,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626336,10.1145/2619239.2626336,"Datacenter WAN traffic consists of high priority transfers that have to be carried as soon as they arrive alongside large transfers with pre-assigned deadlines on their completion (ranging from minutes to hours). The ability to offer guarantees to large transfers is crucial for business needs and impacts overall cost-of-business. State-of-the-art traffic engineering solutions only consider the current time epoch and hence cannot provide pre-facto promises for long-lived transfers. We present Tempus, an online traffic engineering scheme that exploits information on transfer size and deadlines to appropriately pack long-running transfers across network paths and time, thereby leaving enough capacity slack for future high-priority requests. Tempus builds on a tailored approximate solution to a mixed packing-covering linear program, which is parallelizable and scales well in both running time and memory usage. Consequently, Tempus is able to quickly and effectively update its solution when new transfers arrive or unexpected changes happen. These updates involve only small edits to existing transfers. Therefore, as experiments on traces from a large production WAN show, Tempus can offer and keep promises to long-lived transfers well in advance of their actual deadline; the promise on minimal transfer size is comparable with an offline optimal solution and outperforms state-of-the-art solutions by 2-3X.",Proceedings of the 2014 ACM Conference on SIGCOMM,515–526,12,"deadlines, wide area network, inter-datacenter, online temporal planning, mixed packing covering, software-defined networking","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
682,article,"Liu, Hongqiang Harry and Kandula, Srikanth and Mahajan, Ratul and Zhang, Ming and Gelernter, David",Traffic Engineering with Forward Fault Correction,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626314,10.1145/2740070.2626314,"Faults such as link failures and high switch configuration delays can cause heavy congestion and packet loss. Because it takes time to detect and react to faults, these conditions can last long---even tens of seconds. We propose forward fault correction (FFC), a proactive approach to handling faults. FFC spreads network traffic such that freedom from congestion is guaranteed under arbitrary combinations of up to k faults. We show how FFC can be practically realized by compactly encoding the constraints that arise from this large number of possible faults and solving them efficiently using sorting networks. Experiments with data from real networks show that, with negligible loss in overall network throughput, FFC can reduce data loss by a factor of 7--130 in well-provisioned networks, and reduce the loss of high-priority traffic to almost zero in well-utilized networks.",,527–538,12,"fault tolerance, traffic engineering, congestion-free",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
683,inproceedings,"Liu, Hongqiang Harry and Kandula, Srikanth and Mahajan, Ratul and Zhang, Ming and Gelernter, David",Traffic Engineering with Forward Fault Correction,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626314,10.1145/2619239.2626314,"Faults such as link failures and high switch configuration delays can cause heavy congestion and packet loss. Because it takes time to detect and react to faults, these conditions can last long---even tens of seconds. We propose forward fault correction (FFC), a proactive approach to handling faults. FFC spreads network traffic such that freedom from congestion is guaranteed under arbitrary combinations of up to k faults. We show how FFC can be practically realized by compactly encoding the constraints that arise from this large number of possible faults and solving them efficiently using sorting networks. Experiments with data from real networks show that, with negligible loss in overall network throughput, FFC can reduce data loss by a factor of 7--130 in well-provisioned networks, and reduce the loss of high-priority traffic to almost zero in well-utilized networks.",Proceedings of the 2014 ACM Conference on SIGCOMM,527–538,12,"traffic engineering, fault tolerance, congestion-free","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
684,article,"Jin, Xin and Liu, Hongqiang Harry and Gandhi, Rohan and Kandula, Srikanth and Mahajan, Ratul and Zhang, Ming and Rexford, Jennifer and Wattenhofer, Roger",Dynamic Scheduling of Network Updates,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626307,10.1145/2740070.2626307,"We present Dionysus, a system for fast, consistent network updates in software-defined networks. Dionysus encodes as a graph the consistency-related dependencies among updates at individual switches, and it then dynamically schedules these updates based on runtime differences in the update speeds of different switches. This dynamic scheduling is the key to its speed; prior update methods are slow because they pre-determine a schedule, which does not adapt to runtime conditions. Testbed experiments and data-driven simulations show that Dionysus improves the median update speed by 53--88% in both wide area and data center networks compared to prior methods.",,539–550,12,"software-defined networking, network update",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
685,inproceedings,"Jin, Xin and Liu, Hongqiang Harry and Gandhi, Rohan and Kandula, Srikanth and Mahajan, Ratul and Zhang, Ming and Rexford, Jennifer and Wattenhofer, Roger",Dynamic Scheduling of Network Updates,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626307,10.1145/2619239.2626307,"We present Dionysus, a system for fast, consistent network updates in software-defined networks. Dionysus encodes as a graph the consistency-related dependencies among updates at individual switches, and it then dynamically schedules these updates based on runtime differences in the update speeds of different switches. This dynamic scheduling is the key to its speed; prior update methods are slow because they pre-determine a schedule, which does not adapt to runtime conditions. Testbed experiments and data-driven simulations show that Dionysus improves the median update speed by 53--88% in both wide area and data center networks compared to prior methods.",Proceedings of the 2014 ACM Conference on SIGCOMM,539–550,12,"software-defined networking, network update","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
686,article,"Gupta, Arpit and Vanbever, Laurent and Shahbaz, Muhammad and Donovan, Sean P. and Schlinker, Brandon and Feamster, Nick and Rexford, Jennifer and Shenker, Scott and Clark, Russ and Katz-Bassett, Ethan",SDX: A Software Defined Internet Exchange,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626300,10.1145/2740070.2626300,"BGP severely constrains how networks can deliver traffic over the Internet. Today's networks can only forward traffic based on the destination IP prefix, by selecting among routes offered by their immediate neighbors. We believe Software Defined Networking (SDN) could revolutionize wide-area traffic delivery, by offering direct control over packet-processing rules that match on multiple header fields and perform a variety of actions. Internet exchange points (IXPs) are a compelling place to start, given their central role in interconnecting many networks and their growing importance in bringing popular content closer to end users.To realize a Software Defined IXP (an ""SDX""), we must create compelling applications, such as ""application-specific peering""---where two networks peer only for (say) streaming video traffic. We also need new programming abstractions that allow participating networks to create and run these applications and a runtime that both behaves correctly when interacting with BGP and ensures that applications do not interfere with each other. Finally, we must ensure that the system scales, both in rule-table size and computational overhead. In this paper, we tackle these challenges and demonstrate the flexibility and scalability of our solutions through controlled and in-the-wild experiments. Our experiments demonstrate that our SDX implementation can implement representative policies for hundreds of participants who advertise full routing tables while achieving sub-second convergence in response to configuration changes and routing updates.",,551–562,12,"software defined networking (SDN), internet exchange point (IXP), BGP",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
687,inproceedings,"Gupta, Arpit and Vanbever, Laurent and Shahbaz, Muhammad and Donovan, Sean P. and Schlinker, Brandon and Feamster, Nick and Rexford, Jennifer and Shenker, Scott and Clark, Russ and Katz-Bassett, Ethan",SDX: A Software Defined Internet Exchange,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626300,10.1145/2619239.2626300,"BGP severely constrains how networks can deliver traffic over the Internet. Today's networks can only forward traffic based on the destination IP prefix, by selecting among routes offered by their immediate neighbors. We believe Software Defined Networking (SDN) could revolutionize wide-area traffic delivery, by offering direct control over packet-processing rules that match on multiple header fields and perform a variety of actions. Internet exchange points (IXPs) are a compelling place to start, given their central role in interconnecting many networks and their growing importance in bringing popular content closer to end users.To realize a Software Defined IXP (an ""SDX""), we must create compelling applications, such as ""application-specific peering""---where two networks peer only for (say) streaming video traffic. We also need new programming abstractions that allow participating networks to create and run these applications and a runtime that both behaves correctly when interacting with BGP and ensures that applications do not interfere with each other. Finally, we must ensure that the system scales, both in rule-table size and computational overhead. In this paper, we tackle these challenges and demonstrate the flexibility and scalability of our solutions through controlled and in-the-wild experiments. Our experiments demonstrate that our SDX implementation can implement representative policies for hundreds of participants who advertise full routing tables while achieving sub-second convergence in response to configuration changes and routing updates.",Proceedings of the 2014 ACM Conference on SIGCOMM,551–562,12,"software defined networking (SDN), internet exchange point (IXP), BGP","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
688,article,"Sun, Peng and Mahajan, Ratul and Rexford, Jennifer and Yuan, Lihua and Zhang, Ming and Arefin, Ahsan",A Network-State Management Service,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626298,10.1145/2740070.2626298,"We present Statesman, a network-state management service that allows multiple network management applications to operate independently, while maintaining network-wide safety and performance invariants. Network state captures various aspects of the network such as which links are alive and how switches are forwarding traffic. Statesman uses three views of the network state. In observed state, it maintains an up-to-date view of the actual network state. Applications read this state and propose state changes based on their individual goals. Using a model of dependencies among state variables, Statesman merges these proposed states into a target state that is guaranteed to maintain the safety and performance invariants. It then updates the network to the target state. Statesman has been deployed in ten Microsoft Azure datacenters for several months, and three distinct applications have been built on it. We use the experience from this deployment to demonstrate how Statesman enables each application to meet its goals, while maintaining network-wide invariants.",,563–574,12,"datacenter network, software-defined networking, network state",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
689,inproceedings,"Sun, Peng and Mahajan, Ratul and Rexford, Jennifer and Yuan, Lihua and Zhang, Ming and Arefin, Ahsan",A Network-State Management Service,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626298,10.1145/2619239.2626298,"We present Statesman, a network-state management service that allows multiple network management applications to operate independently, while maintaining network-wide safety and performance invariants. Network state captures various aspects of the network such as which links are alive and how switches are forwarding traffic. Statesman uses three views of the network state. In observed state, it maintains an up-to-date view of the actual network state. Applications read this state and propose state changes based on their individual goals. Using a model of dependencies among state variables, Statesman merges these proposed states into a target state that is guaranteed to maintain the safety and performance invariants. It then updates the network to the target state. Statesman has been deployed in ten Microsoft Azure datacenters for several months, and three distinct applications have been built on it. We use the experience from this deployment to demonstrate how Statesman enables each application to meet its goals, while maintaining network-wide invariants.",Proceedings of the 2014 ACM Conference on SIGCOMM,563–574,12,"network state, software-defined networking, datacenter network","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
690,article,"Mushtaq, Aisha and Ismail, Asad Khalid and Wasay, Abdul and Mahmood, Bilal and Qazi, Ihsan Ayyub and Uzmi, Zartash Afzal",Rethinking Buffer Management in Data Center Networks,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631462,10.1145/2740070.2631462,"Data center operators face extreme challenges in simultaneously providing low latency for short flows, high throughput for long flows, and high burst tolerance. We propose a buffer management strategy that addresses these challenges by isolating short and long flows into separate buffers, sizing these buffers based on flow requirements, and scheduling packets to meet different flow-level objectives. Our design provides new opportunities for performance improvements that complement transport layer optimisations.",,575–576,2,"TCP, buffer management, data center",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
691,inproceedings,"Mushtaq, Aisha and Ismail, Asad Khalid and Wasay, Abdul and Mahmood, Bilal and Qazi, Ihsan Ayyub and Uzmi, Zartash Afzal",Rethinking Buffer Management in Data Center Networks,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631462,10.1145/2619239.2631462,"Data center operators face extreme challenges in simultaneously providing low latency for short flows, high throughput for long flows, and high burst tolerance. We propose a buffer management strategy that addresses these challenges by isolating short and long flows into separate buffers, sizing these buffers based on flow requirements, and scheduling packets to meet different flow-level objectives. Our design provides new opportunities for performance improvements that complement transport layer optimisations.",Proceedings of the 2014 ACM Conference on SIGCOMM,575–576,2,"buffer management, data center, TCP","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
692,article,"Obstfeld, Joel and Knight, Simon and Kern, Ed and Wang, Qiang Sheng and Bryan, Tom and Bourque, Dan",VIRL: The Virtual Internet Routing Lab,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631463,10.1145/2740070.2631463,"The increasing demand to provide new network services in a timely and efficient manner is driving the need to design, test and deploy networks quickly and consistently. Testing and verifying at scale is a challenge: network equipment is expensive, requires space, power and cooling, and there is never enough test equipment for everyone who wants to use it! Network virtualization technologies enable a flexible environment for educators, researchers, and operators to create functional models of current, planned, or theoretical networks. This demonstration will show VIRL --- the Virtual Internet Routing Lab --- a platform that can be used for network change validation, training, education, research, or network-aware applications development. The platform combines network virtualization technologies with virtual machines (VMs) running open-source and commercial operating systems; VM orchestration capabilities; a context-aware configuration engine; and an extensible data-collection framework. The system simplifies the process to create both simple and complex environments, run simulations, and collect measurement data.",,577–578,2,"network design, simulation, emulation, network modelling",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
693,inproceedings,"Obstfeld, Joel and Knight, Simon and Kern, Ed and Wang, Qiang Sheng and Bryan, Tom and Bourque, Dan",VIRL: The Virtual Internet Routing Lab,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631463,10.1145/2619239.2631463,"The increasing demand to provide new network services in a timely and efficient manner is driving the need to design, test and deploy networks quickly and consistently. Testing and verifying at scale is a challenge: network equipment is expensive, requires space, power and cooling, and there is never enough test equipment for everyone who wants to use it! Network virtualization technologies enable a flexible environment for educators, researchers, and operators to create functional models of current, planned, or theoretical networks. This demonstration will show VIRL --- the Virtual Internet Routing Lab --- a platform that can be used for network change validation, training, education, research, or network-aware applications development. The platform combines network virtualization technologies with virtual machines (VMs) running open-source and commercial operating systems; VM orchestration capabilities; a context-aware configuration engine; and an extensible data-collection framework. The system simplifies the process to create both simple and complex environments, run simulations, and collect measurement data.",Proceedings of the 2014 ACM Conference on SIGCOMM,577–578,2,"network design, network modelling, simulation, emulation","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
694,article,"Gupta, Arpit and Vanbever, Laurent and Shahbaz, Muhammad and Donovan, Sean Patrick and Schlinker, Brandon and Feamster, Nick and Rexford, Jennifer and Shenker, Scott and Clark, Russ and Katz-Bassett, Ethan",SDX: A Software Defined Internet Exchange,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631473,10.1145/2740070.2631473,"BGP severely constrains how networks can deliver traffic over the Internet. Today's networks can only forward traffic based on the destination IP prefix, by selecting among routes offered by their immediate neighbors. We believe Software Defined Networking (SDN) could revolutionize wide-area traffic delivery, by offering direct control over packet-processing rules that match on multiple header fields and perform a variety of actions. Internet exchange points (IXPs) are a compelling place to start, given their central role in interconnecting many networks and their growing importance in bringing popular content closer to end users. To realize a Software Defined IXP (an ""SDX""), we need new programming abstractions that allow participating networks to create and run these applications and a runtime that both behaves correctly when interacting with BGP and ensures that applications do not interfere with each other. We must also ensure that the system scales, both in rule-table size and computational overhead. In this demo, we show how we tackle these challenges demonstrating the flexibility and scalability of our SDX platform. The paper also appears in the main program.",,579–580,2,"internet exchange point (IXP), BGP, software defined networking (SDN)",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
695,inproceedings,"Gupta, Arpit and Vanbever, Laurent and Shahbaz, Muhammad and Donovan, Sean Patrick and Schlinker, Brandon and Feamster, Nick and Rexford, Jennifer and Shenker, Scott and Clark, Russ and Katz-Bassett, Ethan",SDX: A Software Defined Internet Exchange,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631473,10.1145/2619239.2631473,"BGP severely constrains how networks can deliver traffic over the Internet. Today's networks can only forward traffic based on the destination IP prefix, by selecting among routes offered by their immediate neighbors. We believe Software Defined Networking (SDN) could revolutionize wide-area traffic delivery, by offering direct control over packet-processing rules that match on multiple header fields and perform a variety of actions. Internet exchange points (IXPs) are a compelling place to start, given their central role in interconnecting many networks and their growing importance in bringing popular content closer to end users. To realize a Software Defined IXP (an ""SDX""), we need new programming abstractions that allow participating networks to create and run these applications and a runtime that both behaves correctly when interacting with BGP and ensures that applications do not interfere with each other. We must also ensure that the system scales, both in rule-table size and computational overhead. In this demo, we show how we tackle these challenges demonstrating the flexibility and scalability of our SDX platform. The paper also appears in the main program.",Proceedings of the 2014 ACM Conference on SIGCOMM,579–580,2,"internet exchange point (IXP), BGP, software defined networking (SDN)","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
696,article,"Hu, Han and Jin, Yichao and Wen, Yonggang and Chua, Tat-Seng and Li, Xuelong",Toward a Biometric-Aware Cloud Service Engine for Multi-Screen Video Applications,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631430,10.1145/2740070.2631430,"The emergence of portable devices and online social networks (OSNs) has changed the traditional video consumption paradigm by simultaneously providing multi-screen video watching, social networking engagement, etc. One challenge is to design a unified solution to support ever-growing features while guarantee system performance. In this demo, we design and implement a multi-screen technology to provide multi-screen interactions over wide area network (WAN). Furthermore, we incorporate face-detection technology into our system to identify users' bio-features and employ a machine learning based traffic scheduling mechanism to improve the system performance.",,581–582,2,"cloud, second screen, internet video",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
697,inproceedings,"Hu, Han and Jin, Yichao and Wen, Yonggang and Chua, Tat-Seng and Li, Xuelong",Toward a Biometric-Aware Cloud Service Engine for Multi-Screen Video Applications,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631430,10.1145/2619239.2631430,"The emergence of portable devices and online social networks (OSNs) has changed the traditional video consumption paradigm by simultaneously providing multi-screen video watching, social networking engagement, etc. One challenge is to design a unified solution to support ever-growing features while guarantee system performance. In this demo, we design and implement a multi-screen technology to provide multi-screen interactions over wide area network (WAN). Furthermore, we incorporate face-detection technology into our system to identify users' bio-features and employ a machine learning based traffic scheduling mechanism to improve the system performance.",Proceedings of the 2014 ACM Conference on SIGCOMM,581–582,2,"second screen, cloud, internet video","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
698,article,"Liu, Jiaqiang and Li, Yong and Jin, Depeng",SDN-Based Live VM Migration across Datacenters,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631431,10.1145/2740070.2631431,,,583–584,2,"virtual machine migration, software defined network",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
699,inproceedings,"Liu, Jiaqiang and Li, Yong and Jin, Depeng",SDN-Based Live VM Migration across Datacenters,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631431,10.1145/2619239.2631431,,Proceedings of the 2014 ACM Conference on SIGCOMM,583–584,2,"software defined network, virtual machine migration","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
700,article,"Chang, Wentao and Wang, An and Mohaisen, Aziz and Chen, Songqing",Characterizing Botnets-as-a-Service,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631464,10.1145/2740070.2631464,,,585–586,2,"botnet, collaborations, measurement",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
701,inproceedings,"Chang, Wentao and Wang, An and Mohaisen, Aziz and Chen, Songqing",Characterizing Botnets-as-a-Service,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631464,10.1145/2619239.2631464,,Proceedings of the 2014 ACM Conference on SIGCOMM,585–586,2,"botnet, collaborations, measurement","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
702,article,"Chen, Shaofeng and Fang, Dingyi and Chen, Xiaojiang and Xia, Tingting and Jin, Meng",Aerial Wireless Localization Using Target-Guided Flight Route,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631466,10.1145/2740070.2631466,"This poster presents GuideLoc, a highly efficient aerial wireless localization system that uses directional antennas mounted on a mini Multi-rotor Unmanned Aerial Vehicle (UAV), to enable detecting and positioning of targets. Taking advantage of angle and signal strength information of frames transmitted from targets, GuideLoc can directly fly towards the targets with the minimum flight route and time. We implement a prototype of GuideLoc using ArduCopter and evaluate the performance by simulations and experiments. Experimental results show that GuideLoc achieves an average location accuracy of 2.7 meters and reduces flight distance more than 50% compared with other known wireless localization approaches using UAV.",,587–588,2,"wireless localization, flight routes, multi-rotor UAV",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
703,inproceedings,"Chen, Shaofeng and Fang, Dingyi and Chen, Xiaojiang and Xia, Tingting and Jin, Meng",Aerial Wireless Localization Using Target-Guided Flight Route,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631466,10.1145/2619239.2631466,"This poster presents GuideLoc, a highly efficient aerial wireless localization system that uses directional antennas mounted on a mini Multi-rotor Unmanned Aerial Vehicle (UAV), to enable detecting and positioning of targets. Taking advantage of angle and signal strength information of frames transmitted from targets, GuideLoc can directly fly towards the targets with the minimum flight route and time. We implement a prototype of GuideLoc using ArduCopter and evaluate the performance by simulations and experiments. Experimental results show that GuideLoc achieves an average location accuracy of 2.7 meters and reduces flight distance more than 50% compared with other known wireless localization approaches using UAV.",Proceedings of the 2014 ACM Conference on SIGCOMM,587–588,2,"wireless localization, multi-rotor UAV, flight routes","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
704,article,"Lee, Keunhong and Kim, Joongi and Moon, Sue",An Educational Networking Framework for Full Layer Implementation and Testing,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631467,10.1145/2740070.2631467,,,589–590,2,"full layer implementation, educational networking framework, automated test suite",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
705,inproceedings,"Lee, Keunhong and Kim, Joongi and Moon, Sue",An Educational Networking Framework for Full Layer Implementation and Testing,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631467,10.1145/2619239.2631467,,Proceedings of the 2014 ACM Conference on SIGCOMM,589–590,2,"educational networking framework, automated test suite, full layer implementation","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
706,article,"Li, Jun and Berg, Skyler and Zhang, Mingwei and Reiher, Peter and Wei, Tao",Drawbridge: Software-Defined DDoS-Resistant Traffic Engineering,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631469,10.1145/2740070.2631469,"End hosts in today's Internet have the best knowledge of the type of traffic they should receive, but they play no active role in traffic engineering. Traffic engineering is conducted by ISPs, which unfortunately are blind to specific user needs. End hosts are therefore subject to unwanted traffic, particularly from Distributed Denial of Service (DDoS) attacks. This research proposes a new system called DrawBridge to address this traffic engineering dilemma. By realizing the potential of software-defined networking (SDN), in this research we investigate a solution that enables end hosts to use their knowledge of desired traffic to improve traffic engineering during DDoS attacks.",,591–592,2,"traffic engineering, DDoS, software-defined networking",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
707,inproceedings,"Li, Jun and Berg, Skyler and Zhang, Mingwei and Reiher, Peter and Wei, Tao",Drawbridge: Software-Defined DDoS-Resistant Traffic Engineering,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631469,10.1145/2619239.2631469,"End hosts in today's Internet have the best knowledge of the type of traffic they should receive, but they play no active role in traffic engineering. Traffic engineering is conducted by ISPs, which unfortunately are blind to specific user needs. End hosts are therefore subject to unwanted traffic, particularly from Distributed Denial of Service (DDoS) attacks. This research proposes a new system called DrawBridge to address this traffic engineering dilemma. By realizing the potential of software-defined networking (SDN), in this research we investigate a solution that enables end hosts to use their knowledge of desired traffic to improve traffic engineering during DDoS attacks.",Proceedings of the 2014 ACM Conference on SIGCOMM,591–592,2,"DDoS, software-defined networking, traffic engineering","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
708,article,"Hu, Chengchen and Yang, Ji and Gong, Zhimin and Deng, Shuoling and Zhao, Hongbo",DesktopDC: Setting All Programmable Data Center Networking Testbed on Desk,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2631472,10.1145/2740070.2631472,,,593–594,2,"data center, programmable, openflow",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
709,inproceedings,"Hu, Chengchen and Yang, Ji and Gong, Zhimin and Deng, Shuoling and Zhao, Hongbo",DesktopDC: Setting All Programmable Data Center Networking Testbed on Desk,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2631472,10.1145/2619239.2631472,,Proceedings of the 2014 ACM Conference on SIGCOMM,593–594,2,"openflow, programmable, data center","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
710,inproceedings,"Padhye, Jitendra",Session Details: Transport and Congestion Control,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246782,10.1145/3246782,,Proceedings of the 2014 ACM Conference on SIGCOMM,,,,"Chicago, Illinois, USA",SIGCOMM '14,,,,,,
711,article,"Sivaraman, Anirudh and Winstein, Keith and Thaker, Pratiksha and Balakrishnan, Hari",An Experimental Study of the Learnability of Congestion Control,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626324,10.1145/2740070.2626324,"When designing a distributed network protocol, typically it is infeasible to fully define the target network where the protocol is intended to be used. It is therefore natural to ask: How faithfully do protocol designers really need to understand the networks they design for? What are the important signals that endpoints should listen to? How can researchers gain confidence that systems that work well on well-characterized test networks during development will also perform adequately on real networks that are inevitably more complex, or future networks yet to be developed? Is there a tradeoff between the performance of a protocol and the breadth of its intended operating range of networks? What is the cost of playing fairly with cross-traffic that is governed by another protocol?We examine these questions quantitatively in the context of congestion control, by using an automated protocol-design tool to approximate the best possible congestion-control scheme given imperfect prior knowledge about the network. We found only weak evidence of a tradeoff between operating range in link speeds and performance, even when the operating range was extended to cover a thousand-fold range of link speeds. We found that it may be acceptable to simplify some characteristics of the network---such as its topology---when modeling for design purposes. Some other features, such as the degree of multiplexing and the aggressiveness of contending endpoints, are important to capture in a model.",,479–490,12,"protocol, learnability, machine learning, simulation, measurement, congestion control",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
712,inproceedings,"Sivaraman, Anirudh and Winstein, Keith and Thaker, Pratiksha and Balakrishnan, Hari",An Experimental Study of the Learnability of Congestion Control,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626324,10.1145/2619239.2626324,"When designing a distributed network protocol, typically it is infeasible to fully define the target network where the protocol is intended to be used. It is therefore natural to ask: How faithfully do protocol designers really need to understand the networks they design for? What are the important signals that endpoints should listen to? How can researchers gain confidence that systems that work well on well-characterized test networks during development will also perform adequately on real networks that are inevitably more complex, or future networks yet to be developed? Is there a tradeoff between the performance of a protocol and the breadth of its intended operating range of networks? What is the cost of playing fairly with cross-traffic that is governed by another protocol?We examine these questions quantitatively in the context of congestion control, by using an automated protocol-design tool to approximate the best possible congestion-control scheme given imperfect prior knowledge about the network. We found only weak evidence of a tradeoff between operating range in link speeds and performance, even when the operating range was extended to cover a thousand-fold range of link speeds. We found that it may be acceptable to simplify some characteristics of the network---such as its topology---when modeling for design purposes. Some other features, such as the degree of multiplexing and the aggressiveness of contending endpoints, are important to capture in a model.",Proceedings of the 2014 ACM Conference on SIGCOMM,479–490,12,"protocol, simulation, machine learning, measurement, congestion control, learnability","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
713,article,"Munir, Ali and Baig, Ghufran and Irteza, Syed M. and Qazi, Ihsan A. and Liu, Alex X. and Dogar, Fahad R.","Friends, Not Foes: Synthesizing Existing Transport Strategies for Data Center Networks",2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626305,10.1145/2740070.2626305,"Many data center transports have been proposed in recent times (e.g., DCTCP, PDQ, pFabric, etc). Contrary to the common perception that they are competitors (i.e., protocol A vs. protocol B), we claim that the underlying strategies used in these protocols are, in fact, complementary. Based on this insight, we design PASE, a transport framework that synthesizes existing transport strategies, namely, self-adjusting endpoints (used in TCP style protocols), innetwork prioritization (used in pFabric), and arbitration (used in PDQ). PASE is deployment friendly: it does not require any changes to the network fabric; yet, its performance is comparable to, or better than, the state-of-the-art protocols that require changes to network elements (e.g., pFabric). We evaluate PASE using simulations and testbed experiments. Our results show that PASE performs well for a wide range of application workloads and network settings.",,491–502,12,"datacenter, scheduling, transport",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
714,inproceedings,"Munir, Ali and Baig, Ghufran and Irteza, Syed M. and Qazi, Ihsan A. and Liu, Alex X. and Dogar, Fahad R.","Friends, Not Foes: Synthesizing Existing Transport Strategies for Data Center Networks",2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626305,10.1145/2619239.2626305,"Many data center transports have been proposed in recent times (e.g., DCTCP, PDQ, pFabric, etc). Contrary to the common perception that they are competitors (i.e., protocol A vs. protocol B), we claim that the underlying strategies used in these protocols are, in fact, complementary. Based on this insight, we design PASE, a transport framework that synthesizes existing transport strategies, namely, self-adjusting endpoints (used in TCP style protocols), innetwork prioritization (used in pFabric), and arbitration (used in PDQ). PASE is deployment friendly: it does not require any changes to the network fabric; yet, its performance is comparable to, or better than, the state-of-the-art protocols that require changes to network elements (e.g., pFabric). We evaluate PASE using simulations and testbed experiments. Our results show that PASE performs well for a wide range of application workloads and network settings.",Proceedings of the 2014 ACM Conference on SIGCOMM,491–502,12,"datacenter, transport, scheduling","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
715,article,"Alizadeh, Mohammad and Edsall, Tom and Dharmapurikar, Sarang and Vaidyanathan, Ramanan and Chu, Kevin and Fingerhut, Andy and Lam, Vinh The and Matus, Francis and Pan, Rong and Yadav, Navindra and Varghese, George",CONGA: Distributed Congestion-Aware Load Balancing for Datacenters,2014,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2740070.2626316,10.1145/2740070.2626316,"We present the design, implementation, and evaluation of CONGA, a network-based distributed congestion-aware load balancing mechanism for datacenters. CONGA exploits recent trends including the use of regular Clos topologies and overlays for network virtualization. It splits TCP flows into flowlets, estimates real-time congestion on fabric paths, and allocates flowlets to paths based on feedback from remote switches. This enables CONGA to efficiently balance load and seamlessly handle asymmetry, without requiring any TCP modifications. CONGA has been implemented in custom ASICs as part of a new datacenter fabric. In testbed experiments, CONGA has 5x better flow completion times than ECMP even with a single link failure and achieves 2-8x better throughput than MPTCP in Incast scenarios. Further, the Price of Anarchy for CONGA is provably small in Leaf-Spine topologies; hence CONGA is nearly as effective as a centralized scheduler while being able to react to congestion in microseconds. Our main thesis is that datacenter fabric load balancing is best done in the network, and requires global schemes such as CONGA to handle asymmetry.",,503–514,12,"datacenter fabric, distributed, load balancing",,,October 2014,44,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
716,inproceedings,"Alizadeh, Mohammad and Edsall, Tom and Dharmapurikar, Sarang and Vaidyanathan, Ramanan and Chu, Kevin and Fingerhut, Andy and Lam, Vinh The and Matus, Francis and Pan, Rong and Yadav, Navindra and Varghese, George",CONGA: Distributed Congestion-Aware Load Balancing for Datacenters,2014,9781450328364,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2619239.2626316,10.1145/2619239.2626316,"We present the design, implementation, and evaluation of CONGA, a network-based distributed congestion-aware load balancing mechanism for datacenters. CONGA exploits recent trends including the use of regular Clos topologies and overlays for network virtualization. It splits TCP flows into flowlets, estimates real-time congestion on fabric paths, and allocates flowlets to paths based on feedback from remote switches. This enables CONGA to efficiently balance load and seamlessly handle asymmetry, without requiring any TCP modifications. CONGA has been implemented in custom ASICs as part of a new datacenter fabric. In testbed experiments, CONGA has 5x better flow completion times than ECMP even with a single link failure and achieves 2-8x better throughput than MPTCP in Incast scenarios. Further, the Price of Anarchy for CONGA is provably small in Leaf-Spine topologies; hence CONGA is nearly as effective as a centralized scheduler while being able to react to congestion in microseconds. Our main thesis is that datacenter fabric load balancing is best done in the network, and requires global schemes such as CONGA to handle asymmetry.",Proceedings of the 2014 ACM Conference on SIGCOMM,503–514,12,"load balancing, distributed, datacenter fabric","Chicago, Illinois, USA",SIGCOMM '14,,,,,,
717,article,"Peterson, Larry",Zen and the Art of Network Architecture,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2494259,10.1145/2534169.2494259,,,1–2,2,network architecture,,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
718,inproceedings,"Peterson, Larry",Zen and the Art of Network Architecture,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2494259,10.1145/2486001.2494259,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,1–2,2,network architecture,"Hong Kong, China",SIGCOMM '13,,,,,,
719,inproceedings,"Akella, Aditya",Session Details: Software Defined Networks,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246325,10.1145/3246325,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,,,,"Hong Kong, China",SIGCOMM '13,,,,,,
720,article,"Jain, Sushant and Kumar, Alok and Mandal, Subhasree and Ong, Joon and Poutievski, Leon and Singh, Arjun and Venkata, Subbaiah and Wanderer, Jim and Zhou, Junlan and Zhu, Min and Zolla, Jon and H\""{o",B4: Experience with a Globally-Deployed Software Defined Wan,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486019,10.1145/2534169.2486019,"We present the design, implementation, and evaluation of B4, a private WAN connecting Google's data centers across the planet. B4 has a number of unique characteristics: i) massive bandwidth requirements deployed to a modest number of sites, ii) elastic traffic demand that seeks to maximize average bandwidth, and iii) full control over the edge servers and network, which enables rate limiting and demand measurement at the edge.These characteristics led to a Software Defined Networking architecture using OpenFlow to control relatively simple switches built from merchant silicon. B4's centralized traffic engineering service drives links to near 100% utilization, while splitting application flows among multiple paths to balance capacity against application priority/demands. We describe experience with three years of B4 production deployment, lessons learned, and areas for future work.",,3–14,12,"centralized traffic engineering, openflow, routing, software- defined networking, wide-area networks",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
721,inproceedings,"Jain, Sushant and Kumar, Alok and Mandal, Subhasree and Ong, Joon and Poutievski, Leon and Singh, Arjun and Venkata, Subbaiah and Wanderer, Jim and Zhou, Junlan and Zhu, Min and Zolla, Jon and H\""{o",B4: Experience with a Globally-Deployed Software Defined Wan,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486019,10.1145/2486001.2486019,"We present the design, implementation, and evaluation of B4, a private WAN connecting Google's data centers across the planet. B4 has a number of unique characteristics: i) massive bandwidth requirements deployed to a modest number of sites, ii) elastic traffic demand that seeks to maximize average bandwidth, and iii) full control over the edge servers and network, which enables rate limiting and demand measurement at the edge.These characteristics led to a Software Defined Networking architecture using OpenFlow to control relatively simple switches built from merchant silicon. B4's centralized traffic engineering service drives links to near 100% utilization, while splitting application flows among multiple paths to balance capacity against application priority/demands. We describe experience with three years of B4 production deployment, lessons learned, and areas for future work.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,3–14,12,"wide-area networks, routing, software- defined networking, centralized traffic engineering, openflow","Hong Kong, China",SIGCOMM '13,,,,,,
722,article,"Hong, Chi-Yao and Kandula, Srikanth and Mahajan, Ratul and Zhang, Ming and Gill, Vijay and Nanduri, Mohan and Wattenhofer, Roger",Achieving High Utilization with Software-Driven WAN,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486012,10.1145/2534169.2486012,"We present SWAN, a system that boosts the utilization of inter-datacenter networks by centrally controlling when and how much traffic each service sends and frequently re-configuring the network's data plane to match current traffic demand. But done simplistically, these re-configurations can also cause severe, transient congestion because different switches may apply updates at different times. We develop a novel technique that leverages a small amount of scratch capacity on links to apply updates in a provably congestion-free manner, without making any assumptions about the order and timing of updates at individual switches. Further, to scale to large networks in the face of limited forwarding table capacity, SWAN greedily selects a small set of entries that can best satisfy current demand. It updates this set without disrupting traffic by leveraging a small amount of scratch capacity in forwarding tables. Experiments using a testbed prototype and data-driven simulations of two production networks show that SWAN carries 60% more traffic than the current practice.",,15–26,12,"inter-dc wan, software-defined networking",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
723,inproceedings,"Hong, Chi-Yao and Kandula, Srikanth and Mahajan, Ratul and Zhang, Ming and Gill, Vijay and Nanduri, Mohan and Wattenhofer, Roger",Achieving High Utilization with Software-Driven WAN,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486012,10.1145/2486001.2486012,"We present SWAN, a system that boosts the utilization of inter-datacenter networks by centrally controlling when and how much traffic each service sends and frequently re-configuring the network's data plane to match current traffic demand. But done simplistically, these re-configurations can also cause severe, transient congestion because different switches may apply updates at different times. We develop a novel technique that leverages a small amount of scratch capacity on links to apply updates in a provably congestion-free manner, without making any assumptions about the order and timing of updates at individual switches. Further, to scale to large networks in the face of limited forwarding table capacity, SWAN greedily selects a small set of entries that can best satisfy current demand. It updates this set without disrupting traffic by leveraging a small amount of scratch capacity in forwarding tables. Experiments using a testbed prototype and data-driven simulations of two production networks show that SWAN carries 60% more traffic than the current practice.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,15–26,12,"software-defined networking, inter-dc wan","Hong Kong, China",SIGCOMM '13,,,,,,
724,article,"Qazi, Zafar Ayyub and Tu, Cheng-Chun and Chiang, Luis and Miao, Rui and Sekar, Vyas and Yu, Minlan",SIMPLE-Fying Middlebox Policy Enforcement Using SDN,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486022,10.1145/2534169.2486022,"Networks today rely on middleboxes to provide critical performance, security, and policy compliance capabilities. Achieving these benefits and ensuring that the traffic is directed through the desired sequence of middleboxes requires significant manual effort and operator expertise. In this respect, Software-Defined Networking (SDN) offers a promising alternative. Middleboxes, however, introduce new aspects (e.g., policy composition, resource management, packet modifications) that fall outside the purvey of traditional L2/L3 functions that SDN supports (e.g., access control or routing). This paper presents SIMPLE, a SDN-based policy enforcement layer for efficient middlebox-specific ""traffic steering''. In designing SIMPLE, we take an explicit stance to work within the constraints of legacy middleboxes and existing SDN interfaces. To this end, we address algorithmic and system design challenges to demonstrate the feasibility of using SDN to simplify middlebox traffic steering. In doing so, we also take a significant step toward addressing industry concerns surrounding the ability of SDN to integrate with existing infrastructure and support L4-L7 capabilities.",,27–38,12,"middlebox, network management, software-defined networking",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
725,inproceedings,"Qazi, Zafar Ayyub and Tu, Cheng-Chun and Chiang, Luis and Miao, Rui and Sekar, Vyas and Yu, Minlan",SIMPLE-Fying Middlebox Policy Enforcement Using SDN,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486022,10.1145/2486001.2486022,"Networks today rely on middleboxes to provide critical performance, security, and policy compliance capabilities. Achieving these benefits and ensuring that the traffic is directed through the desired sequence of middleboxes requires significant manual effort and operator expertise. In this respect, Software-Defined Networking (SDN) offers a promising alternative. Middleboxes, however, introduce new aspects (e.g., policy composition, resource management, packet modifications) that fall outside the purvey of traditional L2/L3 functions that SDN supports (e.g., access control or routing). This paper presents SIMPLE, a SDN-based policy enforcement layer for efficient middlebox-specific ""traffic steering''. In designing SIMPLE, we take an explicit stance to work within the constraints of legacy middleboxes and existing SDN interfaces. To this end, we address algorithmic and system design challenges to demonstrate the feasibility of using SDN to simplify middlebox traffic steering. In doing so, we also take a significant step toward addressing industry concerns surrounding the ability of SDN to integrate with existing infrastructure and support L4-L7 capabilities.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,27–38,12,"middlebox, network management, software-defined networking","Hong Kong, China",SIGCOMM '13,,,,,,
726,inproceedings,"Tan, Kun",Session Details: Wireless Communication 1,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246326,10.1145/3246326,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,,,,"Hong Kong, China",SIGCOMM '13,,,,,,
727,inproceedings,"Liu, Vincent and Parks, Aaron and Talla, Vamsi and Gollakota, Shyamnath and Wetherall, David and Smith, Joshua R.",Ambient Backscatter: Wireless Communication out of Thin Air,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486015,10.1145/2486001.2486015,"We present the design of a communication system that enables two devices to communicate using ambient RF as the only source of power. Our approach leverages existing TV and cellular transmissions to eliminate the need for wires and batteries, thus enabling ubiquitous communication where devices can communicate among themselves at unprecedented scales and in locations that were previously inaccessible.To achieve this, we introduce ambient backscatter, a new communication primitive where devices communicate by backscattering ambient RF signals. Our design avoids the expensive process of generating radio waves; backscatter communication is orders of magnitude more power-efficient than traditional radio communication. Further, since it leverages the ambient RF signals that are already around us, it does not require a dedicated power infrastructure as in traditional backscatter communication. To show the feasibility of our design, we prototype ambient backscatter devices in hardware and achieve information rates of 1 kbps over distances of 2.5 feet and 1.5 feet, while operating outdoors and indoors respectively. We use our hardware prototype to implement proof-of-concepts for two previously infeasible ubiquitous communication applications.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,39–50,12,"energy harvesting, internet of things, backscatter, wireless","Hong Kong, China",SIGCOMM '13,,,,,,
728,article,"Liu, Vincent and Parks, Aaron and Talla, Vamsi and Gollakota, Shyamnath and Wetherall, David and Smith, Joshua R.",Ambient Backscatter: Wireless Communication out of Thin Air,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486015,10.1145/2534169.2486015,"We present the design of a communication system that enables two devices to communicate using ambient RF as the only source of power. Our approach leverages existing TV and cellular transmissions to eliminate the need for wires and batteries, thus enabling ubiquitous communication where devices can communicate among themselves at unprecedented scales and in locations that were previously inaccessible.To achieve this, we introduce ambient backscatter, a new communication primitive where devices communicate by backscattering ambient RF signals. Our design avoids the expensive process of generating radio waves; backscatter communication is orders of magnitude more power-efficient than traditional radio communication. Further, since it leverages the ambient RF signals that are already around us, it does not require a dedicated power infrastructure as in traditional backscatter communication. To show the feasibility of our design, we prototype ambient backscatter devices in hardware and achieve information rates of 1 kbps over distances of 2.5 feet and 1.5 feet, while operating outdoors and indoors respectively. We use our hardware prototype to implement proof-of-concepts for two previously infeasible ubiquitous communication applications.",,39–50,12,"energy harvesting, backscatter, internet of things, wireless",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
729,inproceedings,"Nandakumar, Rajalakshmi and Chintalapudi, Krishna Kant and Padmanabhan, Venkat and Venkatesan, Ramarathnam",Dhwani: Secure Peer-to-Peer Acoustic NFC,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486037,10.1145/2486001.2486037,"Near Field Communication (NFC) enables physically proximate devices to communicate over very short ranges in a peer-to-peer manner without incurring complex network configuration overheads. However, adoption of NFC-enabled applications has been stymied by the low levels of penetration of NFC hardware. In this paper, we address the challenge of enabling NFC-like capability on the existing base of mobile phones. To this end, we develop Dhwani, a novel, acoustics-based NFC system that uses the microphone and speakers on mobile phones, thus eliminating the need for any specialized NFC hardware. A key feature of Dhwani is the JamSecure technique, which uses self-jamming coupled with self-interference cancellation at the receiver, to provide an information-theoretically secure communication channel between the devices. Our current implementation of Dhwani achieves data rates of up to 2.4 Kbps, which is sufficient for most existing NFC applications.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,63–74,12,"wireless, security, nfc","Hong Kong, China",SIGCOMM '13,,,,,,
730,article,"Nandakumar, Rajalakshmi and Chintalapudi, Krishna Kant and Padmanabhan, Venkat and Venkatesan, Ramarathnam",Dhwani: Secure Peer-to-Peer Acoustic NFC,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486037,10.1145/2534169.2486037,"Near Field Communication (NFC) enables physically proximate devices to communicate over very short ranges in a peer-to-peer manner without incurring complex network configuration overheads. However, adoption of NFC-enabled applications has been stymied by the low levels of penetration of NFC hardware. In this paper, we address the challenge of enabling NFC-like capability on the existing base of mobile phones. To this end, we develop Dhwani, a novel, acoustics-based NFC system that uses the microphone and speakers on mobile phones, thus eliminating the need for any specialized NFC hardware. A key feature of Dhwani is the JamSecure technique, which uses self-jamming coupled with self-interference cancellation at the receiver, to provide an information-theoretically secure communication channel between the devices. Our current implementation of Dhwani achieves data rates of up to 2.4 Kbps, which is sufficient for most existing NFC applications.",,63–74,12,"nfc, wireless, security",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
731,inproceedings,"Adib, Fadel and Katabi, Dina",See through Walls with WiFi!,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486039,10.1145/2486001.2486039,"Wi-Fi signals are typically information carriers between a transmitter and a receiver. In this paper, we show that Wi-Fi can also extend our senses, enabling us to see moving objects through walls and behind closed doors. In particular, we can use such signals to identify the number of people in a closed room and their relative locations. We can also identify simple gestures made behind a wall, and combine a sequence of gestures to communicate messages to a wireless receiver without carrying any transmitting device. The paper introduces two main innovations. First, it shows how one can use MIMO interference nulling to eliminate reflections off static objects and focus the receiver on a moving target. Second, it shows how one can track a human by treating the motion of a human body as an antenna array and tracking the resulting RF beam. We demonstrate the validity of our design by building it into USRP software radios and testing it in office buildings.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,75–86,12,"wireless, seeing through walls, gesture-based user interface, mimo","Hong Kong, China",SIGCOMM '13,,,,,,
732,article,"Adib, Fadel and Katabi, Dina",See through Walls with WiFi!,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486039,10.1145/2534169.2486039,"Wi-Fi signals are typically information carriers between a transmitter and a receiver. In this paper, we show that Wi-Fi can also extend our senses, enabling us to see moving objects through walls and behind closed doors. In particular, we can use such signals to identify the number of people in a closed room and their relative locations. We can also identify simple gestures made behind a wall, and combine a sequence of gestures to communicate messages to a wireless receiver without carrying any transmitting device. The paper introduces two main innovations. First, it shows how one can use MIMO interference nulling to eliminate reflections off static objects and focus the receiver on a moving target. Second, it shows how one can track a human by treating the motion of a human body as an antenna array and tracking the resulting RF beam. We demonstrate the validity of our design by building it into USRP software radios and testing it in office buildings.",,75–86,12,"seeing through walls, wireless, gesture-based user interface, mimo",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
733,inproceedings,"Bhagwan, Ranjita",Session Details: Fast and Scalable Network Designs,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246327,10.1145/3246327,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,,,,"Hong Kong, China",SIGCOMM '13,,,,,,
734,article,"Voellmy, Andreas and Wang, Junchang and Yang, Y Richard and Ford, Bryan and Hudak, Paul",Maple: Simplifying SDN Programming Using Algorithmic Policies,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486030,10.1145/2534169.2486030,"Software-Defined Networking offers the appeal of a simple, centralized programming model for managing complex networks. However, challenges in managing low-level details, such as setting up and maintaining correct and efficient forwarding tables on distributed switches, often compromise this conceptual simplicity. In this pa- per, we present Maple, a system that simplifies SDN programming by (1) allowing a programmer to use a standard programming language to design an arbitrary, centralized algorithm, which we call an algorithmic policy, to decide the behaviors of an entire network, and (2) providing an abstraction that the programmer-defined, centralized policy runs, conceptually, ""afresh"" on every packet entering a network, and hence is oblivious to the challenge of translating a high-level policy into sets of rules on distributed individual switches. To implement algorithmic policies efficiently, Maple includes not only a highly-efficient multicore scheduler that can scale efficiently to controllers with 40+ cores, but more importantly a novel tracing runtime optimizer that can automatically record reusable policy decisions, offload work to switches when possible, and keep switch flow tables up-to-date by dynamically tracing the dependency of policy decisions on packet contents as well as the environment (system state). Evaluations using real HP switches show that Maple optimizer reduces HTTP connection time by a factor of 100 at high load. During simulated benchmarking, Maple scheduler, when not running the optimizer, achieves a throughput of over 20 million new flow requests per second on a single machine, with 95-percentile latency under 10 ms.",,87–98,12,"software-defined networking, algorithmic policies, openflow",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
735,inproceedings,"Voellmy, Andreas and Wang, Junchang and Yang, Y Richard and Ford, Bryan and Hudak, Paul",Maple: Simplifying SDN Programming Using Algorithmic Policies,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486030,10.1145/2486001.2486030,"Software-Defined Networking offers the appeal of a simple, centralized programming model for managing complex networks. However, challenges in managing low-level details, such as setting up and maintaining correct and efficient forwarding tables on distributed switches, often compromise this conceptual simplicity. In this pa- per, we present Maple, a system that simplifies SDN programming by (1) allowing a programmer to use a standard programming language to design an arbitrary, centralized algorithm, which we call an algorithmic policy, to decide the behaviors of an entire network, and (2) providing an abstraction that the programmer-defined, centralized policy runs, conceptually, ""afresh"" on every packet entering a network, and hence is oblivious to the challenge of translating a high-level policy into sets of rules on distributed individual switches. To implement algorithmic policies efficiently, Maple includes not only a highly-efficient multicore scheduler that can scale efficiently to controllers with 40+ cores, but more importantly a novel tracing runtime optimizer that can automatically record reusable policy decisions, offload work to switches when possible, and keep switch flow tables up-to-date by dynamically tracing the dependency of policy decisions on packet contents as well as the environment (system state). Evaluations using real HP switches show that Maple optimizer reduces HTTP connection time by a factor of 100 at high load. During simulated benchmarking, Maple scheduler, when not running the optimizer, achieves a throughput of over 20 million new flow requests per second on a single machine, with 95-percentile latency under 10 ms.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,87–98,12,"algorithmic policies, openflow, software-defined networking","Hong Kong, China",SIGCOMM '13,,,,,,
736,article,"Bosshart, Pat and Gibb, Glen and Kim, Hun-Seok and Varghese, George and McKeown, Nick and Izzard, Martin and Mujica, Fernando and Horowitz, Mark",Forwarding Metamorphosis: Fast Programmable Match-Action Processing in Hardware for SDN,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486011,10.1145/2534169.2486011,"In Software Defined Networking (SDN) the control plane is physically separate from the forwarding plane. Control software programs the forwarding plane (e.g., switches and routers) using an open interface, such as OpenFlow. This paper aims to overcomes two limitations in current switching chips and the OpenFlow protocol: i) current hardware switches are quite rigid, allowing ``Match-Action'' processing on only a fixed set of fields, and ii) the OpenFlow specification only defines a limited repertoire of packet processing actions. We propose the RMT (reconfigurable match tables) model, a new RISC-inspired pipelined architecture for switching chips, and we identify the essential minimal set of action primitives to specify how headers are processed in hardware. RMT allows the forwarding plane to be changed in the field without modifying hardware. As in OpenFlow, the programmer can specify multiple match tables of arbitrary width and depth, subject only to an overall resource limit, with each table configurable for matching on arbitrary fields. However, RMT allows the programmer to modify all header fields much more comprehensively than in OpenFlow. Our paper describes the design of a 64 port by 10 Gb/s switch chip implementing the RMT model. Our concrete design demonstrates, contrary to concerns within the community, that flexible OpenFlow hardware switch implementations are feasible at almost no additional cost or power.",,99–110,12,"rmt model, sdn, reconfigurable match tables",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
737,inproceedings,"Bosshart, Pat and Gibb, Glen and Kim, Hun-Seok and Varghese, George and McKeown, Nick and Izzard, Martin and Mujica, Fernando and Horowitz, Mark",Forwarding Metamorphosis: Fast Programmable Match-Action Processing in Hardware for SDN,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486011,10.1145/2486001.2486011,"In Software Defined Networking (SDN) the control plane is physically separate from the forwarding plane. Control software programs the forwarding plane (e.g., switches and routers) using an open interface, such as OpenFlow. This paper aims to overcomes two limitations in current switching chips and the OpenFlow protocol: i) current hardware switches are quite rigid, allowing ``Match-Action'' processing on only a fixed set of fields, and ii) the OpenFlow specification only defines a limited repertoire of packet processing actions. We propose the RMT (reconfigurable match tables) model, a new RISC-inspired pipelined architecture for switching chips, and we identify the essential minimal set of action primitives to specify how headers are processed in hardware. RMT allows the forwarding plane to be changed in the field without modifying hardware. As in OpenFlow, the programmer can specify multiple match tables of arbitrary width and depth, subject only to an overall resource limit, with each table configurable for matching on arbitrary fields. However, RMT allows the programmer to modify all header fields much more comprehensively than in OpenFlow. Our paper describes the design of a 64 port by 10 Gb/s switch chip implementing the RMT model. Our concrete design demonstrates, contrary to concerns within the community, that flexible OpenFlow hardware switch implementations are feasible at almost no additional cost or power.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,99–110,12,"rmt model, sdn, reconfigurable match tables","Hong Kong, China",SIGCOMM '13,,,,,,
738,article,R\'{e,Compressing IP Forwarding Tables: Towards Entropy Bounds and Beyond,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486009,10.1145/2534169.2486009,"Lately, there has been an upsurge of interest in compressed data structures, aiming to pack ever larger quantities of information into constrained memory without sacrificing the efficiency of standard operations, like random access, search, or update. The main goal of this paper is to demonstrate how data compression can benefit the networking community, by showing how to squeeze the IP Forwarding Information Base (FIB), the giant table consulted by IP routers to make forwarding decisions, into information-theoretical entropy bounds, with essentially zero cost on longest prefix match and FIB update. First, we adopt the state-of-the-art in compressed data structures, yielding a static entropy-compressed FIB representation with asymptotically optimal lookup. Then, we re-design the venerable prefix tree, used commonly for IP lookup for at least 20 years in IP routers, to also admit entropy bounds and support lookup in optimal time and update in nearly optimal time. Evaluations on a Linux kernel prototype indicate that our compressors encode a FIB comprising more than 440K prefixes to just about 100--400 KBytes of memory, with a threefold increase in lookup throughput and no penalty on FIB updates.",,111–122,12,"data compression, ip forwarding table lookup, prefix tree",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
739,inproceedings,R\'{e,Compressing IP Forwarding Tables: Towards Entropy Bounds and Beyond,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486009,10.1145/2486001.2486009,"Lately, there has been an upsurge of interest in compressed data structures, aiming to pack ever larger quantities of information into constrained memory without sacrificing the efficiency of standard operations, like random access, search, or update. The main goal of this paper is to demonstrate how data compression can benefit the networking community, by showing how to squeeze the IP Forwarding Information Base (FIB), the giant table consulted by IP routers to make forwarding decisions, into information-theoretical entropy bounds, with essentially zero cost on longest prefix match and FIB update. First, we adopt the state-of-the-art in compressed data structures, yielding a static entropy-compressed FIB representation with asymptotically optimal lookup. Then, we re-design the venerable prefix tree, used commonly for IP lookup for at least 20 years in IP routers, to also admit entropy bounds and support lookup in optimal time and update in nearly optimal time. Evaluations on a Linux kernel prototype indicate that our compressors encode a FIB comprising more than 440K prefixes to just about 100--400 KBytes of memory, with a threefold increase in lookup throughput and no penalty on FIB updates.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,111–122,12,"prefix tree, data compression, ip forwarding table lookup","Hong Kong, China",SIGCOMM '13,,,,,,
740,inproceedings,"Smaragdakis, Georgios",Session Details: Content Delivery and Congestion Control 1,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246328,10.1145/3246328,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,,,,"Hong Kong, China",SIGCOMM '13,,,,,,
741,inproceedings,"Winstein, Keith and Balakrishnan, Hari",TCP Ex Machina: Computer-Generated Congestion Control,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486020,10.1145/2486001.2486020,"This paper describes a new approach to end-to-end congestion control on a multi-user network. Rather than manually formulate each endpoint's reaction to congestion signals, as in traditional protocols, we developed a program called Remy that generates congestion-control algorithms to run at the endpoints.In this approach, the protocol designer specifies their prior knowledge or assumptions about the network and an objective that the algorithm will try to achieve, e.g., high throughput and low queueing delay. Remy then produces a distributed algorithm---the control rules for the independent endpoints---that tries to achieve this objective.In simulations with ns-2, Remy-generated algorithms outperformed human-designed end-to-end techniques, including TCP Cubic, Compound, and Vegas. In many cases, Remy's algorithms also outperformed methods that require intrusive in-network changes, including XCP and Cubic-over-sfqCoDel (stochastic fair queueing with CoDel for active queue management). Remy can generate algorithms both for networks where some parameters are known tightly a priori, e.g. datacenters, and for networks where prior knowledge is less precise, such as cellular networks. We characterize the sensitivity of the resulting performance to the specificity of the prior knowledge, and the consequences when real-world conditions contradict the assumptions supplied at design-time.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,123–134,12,"computer-designed algorithms, congestion control","Hong Kong, China",SIGCOMM '13,,,,,,
742,article,"Winstein, Keith and Balakrishnan, Hari",TCP Ex Machina: Computer-Generated Congestion Control,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486020,10.1145/2534169.2486020,"This paper describes a new approach to end-to-end congestion control on a multi-user network. Rather than manually formulate each endpoint's reaction to congestion signals, as in traditional protocols, we developed a program called Remy that generates congestion-control algorithms to run at the endpoints.In this approach, the protocol designer specifies their prior knowledge or assumptions about the network and an objective that the algorithm will try to achieve, e.g., high throughput and low queueing delay. Remy then produces a distributed algorithm---the control rules for the independent endpoints---that tries to achieve this objective.In simulations with ns-2, Remy-generated algorithms outperformed human-designed end-to-end techniques, including TCP Cubic, Compound, and Vegas. In many cases, Remy's algorithms also outperformed methods that require intrusive in-network changes, including XCP and Cubic-over-sfqCoDel (stochastic fair queueing with CoDel for active queue management). Remy can generate algorithms both for networks where some parameters are known tightly a priori, e.g. datacenters, and for networks where prior knowledge is less precise, such as cellular networks. We characterize the sensitivity of the resulting performance to the specificity of the prior knowledge, and the consequences when real-world conditions contradict the assumptions supplied at design-time.",,123–134,12,"congestion control, computer-designed algorithms",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
743,inproceedings,"Han, Dongsu and Grandl, Robert and Akella, Aditya and Seshan, Srinivasan",FCP: A Flexible Transport Framework for Accommodating Diversity,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486004,10.1145/2486001.2486004,"Transport protocols must accommodate diverse application and network requirements. As a result, TCP has evolved over time with new congestion control algorithms such as support for generalized AIMD, background flows, and multipath. On the other hand, explicit congestion control algorithms have been shown to be more efficient. However, they are inherently more rigid because they rely on in-network components. Therefore, it is not clear whether they can be made flexible enough to support diverse application requirements. This paper presents a flexible framework for network resource allocation, called FCP, that accommodates diversity by exposing a simple abstraction for resource allocation. FCP incorporates novel primitives for end-point flexibility (aggregation and preloading) into a single framework and makes economics-based congestion control practical by explicitly handling load variations and by decoupling it from actual billing. We show that FCP allows evolution by accommodating diversity and ensuring coexistence, while being as efficient as existing explicit congestion control algorithms.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,135–146,12,"end-point flexibility, congestion control, transport protocol","Hong Kong, China",SIGCOMM '13,,,,,,
744,article,"Han, Dongsu and Grandl, Robert and Akella, Aditya and Seshan, Srinivasan",FCP: A Flexible Transport Framework for Accommodating Diversity,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486004,10.1145/2534169.2486004,"Transport protocols must accommodate diverse application and network requirements. As a result, TCP has evolved over time with new congestion control algorithms such as support for generalized AIMD, background flows, and multipath. On the other hand, explicit congestion control algorithms have been shown to be more efficient. However, they are inherently more rigid because they rely on in-network components. Therefore, it is not clear whether they can be made flexible enough to support diverse application requirements. This paper presents a flexible framework for network resource allocation, called FCP, that accommodates diversity by exposing a simple abstraction for resource allocation. FCP incorporates novel primitives for end-point flexibility (aggregation and preloading) into a single framework and makes economics-based congestion control practical by explicitly handling load variations and by decoupling it from actual billing. We show that FCP allows evolution by accommodating diversity and ensuring coexistence, while being as efficient as existing explicit congestion control algorithms.",,135–146,12,"end-point flexibility, congestion control, transport protocol",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
745,inproceedings,"Fayazbakhsh, Seyed Kaveh and Lin, Yin and Tootoonchian, Amin and Ghodsi, Ali and Koponen, Teemu and Maggs, Bruce and Ng, K.C. and Sekar, Vyas and Shenker, Scott","Less Pain, Most of the Gain: Incrementally Deployable ICN",2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486023,10.1145/2486001.2486023,"Information-Centric Networking (ICN) has seen a significant resurgence in recent years. ICN promises benefits to users and service providers along several dimensions (e.g., performance, security, and mobility). These benefits, however, come at a non-trivial cost as many ICN proposals envision adding significant complexity to the network by having routers serve as content caches and support nearest-replica routing. This paper is driven by the simple question of whether this additional complexity is justified and if we can achieve these benefits in an incrementally deployable fashion. To this end, we use trace-driven simulations to analyze the quantitative benefits attributed to ICN (e.g., lower latency and congestion). Somewhat surprisingly, we find that pervasive caching and nearest-replica routing are not fundamentally necessary---most of the performance benefits can be achieved with simpler caching architectures. We also discuss how the qualitative benefits of ICN (e.g., security, mobility) can be achieved without any changes to the network. Building on these insights, we present a proof-of-concept design of an incrementally deployable ICN architecture.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,147–158,12,"internet architecture, information-centric networking","Hong Kong, China",SIGCOMM '13,,,,,,
746,article,"Fayazbakhsh, Seyed Kaveh and Lin, Yin and Tootoonchian, Amin and Ghodsi, Ali and Koponen, Teemu and Maggs, Bruce and Ng, K.C. and Sekar, Vyas and Shenker, Scott","Less Pain, Most of the Gain: Incrementally Deployable ICN",2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486023,10.1145/2534169.2486023,"Information-Centric Networking (ICN) has seen a significant resurgence in recent years. ICN promises benefits to users and service providers along several dimensions (e.g., performance, security, and mobility). These benefits, however, come at a non-trivial cost as many ICN proposals envision adding significant complexity to the network by having routers serve as content caches and support nearest-replica routing. This paper is driven by the simple question of whether this additional complexity is justified and if we can achieve these benefits in an incrementally deployable fashion. To this end, we use trace-driven simulations to analyze the quantitative benefits attributed to ICN (e.g., lower latency and congestion). Somewhat surprisingly, we find that pervasive caching and nearest-replica routing are not fundamentally necessary---most of the performance benefits can be achieved with simpler caching architectures. We also discuss how the qualitative benefits of ICN (e.g., security, mobility) can be achieved without any changes to the network. Building on these insights, we present a proof-of-concept design of an incrementally deployable ICN architecture.",,147–158,12,"information-centric networking, internet architecture",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
747,inproceedings,"Flach, Tobias and Dukkipati, Nandita and Terzis, Andreas and Raghavan, Barath and Cardwell, Neal and Cheng, Yuchung and Jain, Ankur and Hao, Shuai and Katz-Bassett, Ethan and Govindan, Ramesh",Reducing Web Latency: The Virtue of Gentle Aggression,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486014,10.1145/2486001.2486014,"To serve users quickly, Web service providers build infrastructure closer to clients and use multi-stage transport connections. Although these changes reduce client-perceived round-trip times, TCP's current mechanisms fundamentally limit latency improvements. We performed a measurement study of a large Web service provider and found that, while connections with no loss complete close to the ideal latency of one round-trip time, TCP's timeout-driven recovery causes transfers with loss to take five times longer on average.In this paper, we present the design of novel loss recovery mechanisms for TCP that judiciously use redundant transmissions to minimize timeout-driven recovery. Proactive, Reactive, and Corrective are three qualitatively-different, easily-deployable mechanisms that (1) proactively recover from losses, (2) recover from them as quickly as possible, and (3) reconstruct packets to mask loss. Crucially, the mechanisms are compatible both with middleboxes and with TCP's existing congestion control and loss recovery. Our large-scale experiments on Google's production network that serves billions of flows demonstrate a 23% decrease in the mean and 47% in 99th percentile latency over today's TCP.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,159–170,12,"internet measurements, redundancy, packet loss, tcp, web latency, congestion control, recovery","Hong Kong, China",SIGCOMM '13,,,,,,
748,article,"Flach, Tobias and Dukkipati, Nandita and Terzis, Andreas and Raghavan, Barath and Cardwell, Neal and Cheng, Yuchung and Jain, Ankur and Hao, Shuai and Katz-Bassett, Ethan and Govindan, Ramesh",Reducing Web Latency: The Virtue of Gentle Aggression,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486014,10.1145/2534169.2486014,"To serve users quickly, Web service providers build infrastructure closer to clients and use multi-stage transport connections. Although these changes reduce client-perceived round-trip times, TCP's current mechanisms fundamentally limit latency improvements. We performed a measurement study of a large Web service provider and found that, while connections with no loss complete close to the ideal latency of one round-trip time, TCP's timeout-driven recovery causes transfers with loss to take five times longer on average.In this paper, we present the design of novel loss recovery mechanisms for TCP that judiciously use redundant transmissions to minimize timeout-driven recovery. Proactive, Reactive, and Corrective are three qualitatively-different, easily-deployable mechanisms that (1) proactively recover from losses, (2) recover from them as quickly as possible, and (3) reconstruct packets to mask loss. Crucially, the mechanisms are compatible both with middleboxes and with TCP's existing congestion control and loss recovery. Our large-scale experiments on Google's production network that serves billions of flows demonstrate a 23% decrease in the mean and 47% in 99th percentile latency over today's TCP.",,159–170,12,"congestion control, tcp, packet loss, redundancy, web latency, internet measurements, recovery",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
749,inproceedings,"Heidemann, John",Session Details: Security and Diagnosis,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246329,10.1145/3246329,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,,,,"Hong Kong, China",SIGCOMM '13,,,,,,
750,article,"Lychev, Robert and Goldberg, Sharon and Schapira, Michael",BGP Security in Partial Deployment: Is the Juice Worth the Squeeze?,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486010,10.1145/2534169.2486010,"As the rollout of secure route origin authentication with the RPKI slowly gains traction among network operators, there is a push to standardize secure path validation for BGP (i.e., S*BGP: S-BGP, soBGP, BGPSEC, etc.). Origin authentication already does much to improve routing security. Moreover, the transition to S*BGP is expected to be long and slow, with S*BGP coexisting in ""partial deployment"" alongside BGP for a long time. We therefore use theoretical and experimental approach to study the security benefits provided by partially-deployed S*BGP, vis-a-vis those already provided by origin authentication. Because routing policies have a profound impact on routing security, we use a survey of 100 network operators to find the policies that are likely to be most popular during partial S*BGP deployment. We find that S*BGP provides only meagre benefits over origin authentication when these popular policies are used. We also study the security benefits of other routing policies, provide prescriptive guidelines for partially-deployed S*BGP, and show how interactions between S*BGP and BGP can introduce new vulnerabilities into the routing system.",,171–182,12,"partial deployment, bgp, security, routing",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
751,inproceedings,"Lychev, Robert and Goldberg, Sharon and Schapira, Michael",BGP Security in Partial Deployment: Is the Juice Worth the Squeeze?,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486010,10.1145/2486001.2486010,"As the rollout of secure route origin authentication with the RPKI slowly gains traction among network operators, there is a push to standardize secure path validation for BGP (i.e., S*BGP: S-BGP, soBGP, BGPSEC, etc.). Origin authentication already does much to improve routing security. Moreover, the transition to S*BGP is expected to be long and slow, with S*BGP coexisting in ""partial deployment"" alongside BGP for a long time. We therefore use theoretical and experimental approach to study the security benefits provided by partially-deployed S*BGP, vis-a-vis those already provided by origin authentication. Because routing policies have a profound impact on routing security, we use a survey of 100 network operators to find the policies that are likely to be most popular during partial S*BGP deployment. We find that S*BGP provides only meagre benefits over origin authentication when these popular policies are used. We also study the security benefits of other routing policies, provide prescriptive guidelines for partially-deployed S*BGP, and show how interactions between S*BGP and BGP can introduce new vulnerabilities into the routing system.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,171–182,12,"bgp, routing, partial deployment, security","Hong Kong, China",SIGCOMM '13,,,,,,
752,article,"Javed, Umar and Cunha, Italo and Choffnes, David and Katz-Bassett, Ethan and Anderson, Thomas and Krishnamurthy, Arvind",PoiRoot: Investigating the Root Cause of Interdomain Path Changes,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486036,10.1145/2534169.2486036,"Interdomain path changes occur frequently. Because routing protocols expose insufficient information to reason about all changes, the general problem of identifying the root cause remains unsolved. In this work, we design and evaluate PoiRoot, a real-time system that allows a provider to accurately isolate the root cause (the network responsible) of path changes affecting its prefixes. First, we develop a new model describing path changes and use it to provably identify the set of all potentially responsible networks. Next, we develop a recursive algorithm that accurately isolates the root cause of any path change. We observe that the algorithm requires monitoring paths that are generally not visible using standard measurement tools. To address this limitation, we combine existing measurement tools in new ways to acquire path information required for isolating the root cause of a path change. We evaluate PoiRoot on path changes obtained through controlled Internet experiments, simulations, and ""in-the-wild"" measurements. We demonstrate that PoiRoot is highly accurate, works well even with partial information, and generally narrows down the root cause to a single network or two neighboring ones. On controlled experiments PoiRoot is 100% accurate, as opposed to prior work which is accurate only 61.7% of the time.",,183–194,12,"root cause analysis, path changes, bgp, monitoring, measurement",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
753,inproceedings,"Javed, Umar and Cunha, Italo and Choffnes, David and Katz-Bassett, Ethan and Anderson, Thomas and Krishnamurthy, Arvind",PoiRoot: Investigating the Root Cause of Interdomain Path Changes,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486036,10.1145/2486001.2486036,"Interdomain path changes occur frequently. Because routing protocols expose insufficient information to reason about all changes, the general problem of identifying the root cause remains unsolved. In this work, we design and evaluate PoiRoot, a real-time system that allows a provider to accurately isolate the root cause (the network responsible) of path changes affecting its prefixes. First, we develop a new model describing path changes and use it to provably identify the set of all potentially responsible networks. Next, we develop a recursive algorithm that accurately isolates the root cause of any path change. We observe that the algorithm requires monitoring paths that are generally not visible using standard measurement tools. To address this limitation, we combine existing measurement tools in new ways to acquire path information required for isolating the root cause of a path change. We evaluate PoiRoot on path changes obtained through controlled Internet experiments, simulations, and ""in-the-wild"" measurements. We demonstrate that PoiRoot is highly accurate, works well even with partial information, and generally narrows down the root cause to a single network or two neighboring ones. On controlled experiments PoiRoot is 100% accurate, as opposed to prior work which is accurate only 61.7% of the time.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,183–194,12,"monitoring, bgp, root cause analysis, measurement, path changes","Hong Kong, China",SIGCOMM '13,,,,,,
754,article,"Angel, Sebastian and Walfish, Michael",Verifiable Auctions for Online Ad Exchanges,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486038,10.1145/2534169.2486038,"This paper treats a critical component of the Web ecosystem that has so far received little attention in our community: ad exchanges. Ad exchanges run auctions to sell publishers' inventory-space on Web pages-to advertisers who want to display ads in those spaces. Unfortunately, under the status quo, the parties to an auction cannot check that the auction was carried out correctly, which raises the following more general question: how can we create verifiability in low-latency, high-frequency auctions where the parties do not know each other? We address this question with the design, prototype implementation, and experimental evaluation of VEX. VEX introduces a technique for efficient, privacy-preserving integer comparisons; couples these with careful protocol design; and adds little latency and tolerable overhead.",,195–206,12,"verifiable auctions, online advertising, ad exchanges",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
755,inproceedings,"Angel, Sebastian and Walfish, Michael",Verifiable Auctions for Online Ad Exchanges,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486038,10.1145/2486001.2486038,"This paper treats a critical component of the Web ecosystem that has so far received little attention in our community: ad exchanges. Ad exchanges run auctions to sell publishers' inventory-space on Web pages-to advertisers who want to display ads in those spaces. Unfortunately, under the status quo, the parties to an auction cannot check that the auction was carried out correctly, which raises the following more general question: how can we create verifiability in low-latency, high-frequency auctions where the parties do not know each other? We address this question with the design, prototype implementation, and experimental evaluation of VEX. VEX introduces a technique for efficient, privacy-preserving integer comparisons; couples these with careful protocol design; and adds little latency and tolerable overhead.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,195–206,12,"ad exchanges, verifiable auctions, online advertising","Hong Kong, China",SIGCOMM '13,,,,,,
756,inproceedings,"Byers, John",Session Details: Data Center Networks 1,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246330,10.1145/3246330,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,,,,"Hong Kong, China",SIGCOMM '13,,,,,,
757,article,"Patel, Parveen and Bansal, Deepak and Yuan, Lihua and Murthy, Ashwin and Greenberg, Albert and Maltz, David A. and Kern, Randy and Kumar, Hemant and Zikos, Marios and Wu, Hongyu and Kim, Changhoon and Karri, Naveen",Ananta: Cloud Scale Load Balancing,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486026,10.1145/2534169.2486026,"Layer-4 load balancing is fundamental to creating scale-out web services. We designed and implemented Ananta, a scale-out layer-4 load balancer that runs on commodity hardware and meets the performance, reliability and operational requirements of multi-tenant cloud computing environments. Ananta combines existing techniques in routing and distributed systems in a unique way and splits the components of a load balancer into a consensus-based reliable control plane and a decentralized scale-out data plane. A key component of Ananta is an agent in every host that can take over the packet modification function from the load balancer, thereby enabling the load balancer to naturally scale with the size of the data center. Due to its distributed architecture, Ananta provides direct server return (DSR) and network address translation (NAT) capabilities across layer-2 boundaries. Multiple instances of Ananta have been deployed in the Windows Azure public cloud with combined bandwidth capacity exceeding 1Tbps. It is serving traffic needs of a diverse set of tenants, including the blob, table and relational storage services. With its scale-out data plane we can easily achieve more than 100Gbps throughput for a single public IP address. In this paper, we describe the requirements of a cloud-scale load balancer, the design of Ananta and lessons learnt from its implementation and operation in the Windows Azure public cloud.",,207–218,12,"software defined networking, server load balancing, distributed systems",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
758,inproceedings,"Patel, Parveen and Bansal, Deepak and Yuan, Lihua and Murthy, Ashwin and Greenberg, Albert and Maltz, David A. and Kern, Randy and Kumar, Hemant and Zikos, Marios and Wu, Hongyu and Kim, Changhoon and Karri, Naveen",Ananta: Cloud Scale Load Balancing,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486026,10.1145/2486001.2486026,"Layer-4 load balancing is fundamental to creating scale-out web services. We designed and implemented Ananta, a scale-out layer-4 load balancer that runs on commodity hardware and meets the performance, reliability and operational requirements of multi-tenant cloud computing environments. Ananta combines existing techniques in routing and distributed systems in a unique way and splits the components of a load balancer into a consensus-based reliable control plane and a decentralized scale-out data plane. A key component of Ananta is an agent in every host that can take over the packet modification function from the load balancer, thereby enabling the load balancer to naturally scale with the size of the data center. Due to its distributed architecture, Ananta provides direct server return (DSR) and network address translation (NAT) capabilities across layer-2 boundaries. Multiple instances of Ananta have been deployed in the Windows Azure public cloud with combined bandwidth capacity exceeding 1Tbps. It is serving traffic needs of a diverse set of tenants, including the blob, table and relational storage services. With its scale-out data plane we can easily achieve more than 100Gbps throughput for a single public IP address. In this paper, we describe the requirements of a cloud-scale load balancer, the design of Ananta and lessons learnt from its implementation and operation in the Windows Azure public cloud.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,207–218,12,"software defined networking, server load balancing, distributed systems","Hong Kong, China",SIGCOMM '13,,,,,,
759,article,"Jalaparti, Virajith and Bodik, Peter and Kandula, Srikanth and Menache, Ishai and Rybalkin, Mikhail and Yan, Chenyu",Speeding up Distributed Request-Response Workflows,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486028,10.1145/2534169.2486028,"We found that interactive services at Bing have highly variable datacenter-side processing latencies because their processing consists of many sequential stages, parallelization across 10s-1000s of servers and aggregation of responses across the network. To improve the tail latency of such services, we use a few building blocks: reissuing laggards elsewhere in the cluster, new policies to return incomplete results and speeding up laggards by giving them more resources. Combining these building blocks to reduce the overall latency is non-trivial because for the same amount of resource (e.g., number of reissues), different stages improve their latency by different amounts. We present Kwiken, a framework that takes an end-to-end view of latency improvements and costs. It decomposes the problem of minimizing latency over a general processing DAG into a manageable optimization over individual stages. Through simulations with production traces, we show sizable gains; the 99th percentile of latency improves by over 50% when just 0.1% of the responses are allowed to have partial results and by over 40% for 25% of the services when just 5% extra resources are used for reissues.",,219–230,12,"incomplete results, reissues, optimization, tail latency, distributed services., interactive services",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
760,inproceedings,"Jalaparti, Virajith and Bodik, Peter and Kandula, Srikanth and Menache, Ishai and Rybalkin, Mikhail and Yan, Chenyu",Speeding up Distributed Request-Response Workflows,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486028,10.1145/2486001.2486028,"We found that interactive services at Bing have highly variable datacenter-side processing latencies because their processing consists of many sequential stages, parallelization across 10s-1000s of servers and aggregation of responses across the network. To improve the tail latency of such services, we use a few building blocks: reissuing laggards elsewhere in the cluster, new policies to return incomplete results and speeding up laggards by giving them more resources. Combining these building blocks to reduce the overall latency is non-trivial because for the same amount of resource (e.g., number of reissues), different stages improve their latency by different amounts. We present Kwiken, a framework that takes an end-to-end view of latency improvements and costs. It decomposes the problem of minimizing latency over a general processing DAG into a manageable optimization over individual stages. Through simulations with production traces, we show sizable gains; the 99th percentile of latency improves by over 50% when just 0.1% of the responses are allowed to have partial results and by over 40% for 25% of the services when just 5% extra resources are used for reissues.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,219–230,12,"incomplete results, tail latency, reissues, interactive services, distributed services., optimization","Hong Kong, China",SIGCOMM '13,,,,,,
761,article,"Chowdhury, Mosharaf and Kandula, Srikanth and Stoica, Ion",Leveraging Endpoint Flexibility in Data-Intensive Clusters,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486021,10.1145/2534169.2486021,"Many applications do not constrain the destinations of their network transfers. New opportunities emerge when such transfers contribute a large amount of network bytes. By choosing the endpoints to avoid congested links, completion times of these transfers as well as that of others without similar flexibility can be improved. In this paper, we focus on leveraging the flexibility in replica placement during writes to cluster file systems (CFSes), which account for almost half of all cross-rack traffic in data-intensive clusters. The replicas of a CFS write can be placed in any subset of machines as long as they are in multiple fault domains and ensure a balanced use of storage throughout the cluster.We study CFS interactions with the cluster network, analyze optimizations for replica placement, and propose Sinbad -- a system that identifies imbalance and adapts replica destinations to navigate around congested links. Experiments on EC2 and trace-driven simulations show that block writes complete 1.3X (respectively, 1.58X) faster as the network becomes more balanced. As a collateral benefit, end-to-end completion times of data-intensive jobs improve as well. Sinbad does so with little impact on the long-term storage balance.",,231–242,12,"cluster file systems, constrained anycast, data-intensive applications, datacenter networks, replica placement",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
762,inproceedings,"Chowdhury, Mosharaf and Kandula, Srikanth and Stoica, Ion",Leveraging Endpoint Flexibility in Data-Intensive Clusters,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486021,10.1145/2486001.2486021,"Many applications do not constrain the destinations of their network transfers. New opportunities emerge when such transfers contribute a large amount of network bytes. By choosing the endpoints to avoid congested links, completion times of these transfers as well as that of others without similar flexibility can be improved. In this paper, we focus on leveraging the flexibility in replica placement during writes to cluster file systems (CFSes), which account for almost half of all cross-rack traffic in data-intensive clusters. The replicas of a CFS write can be placed in any subset of machines as long as they are in multiple fault domains and ensure a balanced use of storage throughout the cluster.We study CFS interactions with the cluster network, analyze optimizations for replica placement, and propose Sinbad -- a system that identifies imbalance and adapts replica destinations to navigate around congested links. Experiments on EC2 and trace-driven simulations show that block writes complete 1.3X (respectively, 1.58X) faster as the network becomes more balanced. As a collateral benefit, end-to-end completion times of data-intensive jobs improve as well. Sinbad does so with little impact on the long-term storage balance.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,231–242,12,"constrained anycast, replica placement, cluster file systems, data-intensive applications, datacenter networks","Hong Kong, China",SIGCOMM '13,,,,,,
763,inproceedings,"Sekar, Vyas",Session Details: Network Measurement,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246331,10.1145/3246331,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,,,,"Hong Kong, China",SIGCOMM '13,,,,,,
764,article,"Chen, Yingying and Mahajan, Ratul and Sridharan, Baskar and Zhang, Zhi-Li",A Provider-Side View of Web Search Response Time,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486035,10.1145/2534169.2486035,"Using a large Web search service as a case study, we highlight the challenges that modern Web services face in understanding and diagnosing the response time experienced by users. We show that search response time (SRT) varies widely over time and also exhibits counter-intuitive behavior. It is actually higher during off-peak hours, when the query load is lower, than during peak hours. To resolve this paradox and explain SRT variations in general, we develop an analysis framework that separates systemic variations due to periodic changes in service usage and anomalous variations due to unanticipated events such as failures and denial-of-service attacks. We find that systemic SRT variations are primarily caused by systemic changes in aggregate network characteristics, nature of user queries, and browser types. For instance, one reason for higher SRTs during off-peak hours is that during those hours a greater fraction of queries come from slower, mainly-residential networks. We also develop a technique that, by factoring out the impact of such variations, robustly detects and diagnoses performance anomalies in SRT. Deployment experience shows that our technique detects three times more true (operator-verified) anomalies than existing techniques.",,243–254,12,"search response time, anomaly detection and diagnosis, web services, performance monitoring",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
765,inproceedings,"Chen, Yingying and Mahajan, Ratul and Sridharan, Baskar and Zhang, Zhi-Li",A Provider-Side View of Web Search Response Time,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486035,10.1145/2486001.2486035,"Using a large Web search service as a case study, we highlight the challenges that modern Web services face in understanding and diagnosing the response time experienced by users. We show that search response time (SRT) varies widely over time and also exhibits counter-intuitive behavior. It is actually higher during off-peak hours, when the query load is lower, than during peak hours. To resolve this paradox and explain SRT variations in general, we develop an analysis framework that separates systemic variations due to periodic changes in service usage and anomalous variations due to unanticipated events such as failures and denial-of-service attacks. We find that systemic SRT variations are primarily caused by systemic changes in aggregate network characteristics, nature of user queries, and browser types. For instance, one reason for higher SRTs during off-peak hours is that during those hours a greater fraction of queries come from slower, mainly-residential networks. We also develop a technique that, by factoring out the impact of such variations, robustly detects and diagnoses performance anomalies in SRT. Deployment experience shows that our technique detects three times more true (operator-verified) anomalies than existing techniques.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,243–254,12,"anomaly detection and diagnosis, web services, search response time, performance monitoring","Hong Kong, China",SIGCOMM '13,,,,,,
766,article,"Quan, Lin and Heidemann, John and Pradkin, Yuri",Trinocular: Understanding Internet Reliability through Adaptive Probing,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486017,10.1145/2534169.2486017,"Natural and human factors cause Internet outages---from big events like Hurricane Sandy in 2012 and the Egyptian Internet shutdown in Jan. 2011 to small outages every day that go unpublicized. We describe Trinocular, an outage detection system that uses active probing to understand reliability of edge networks. Trinocular is principled: deriving a simple model of the Internet that captures the information pertinent to outages, and populating that model through long-term data, and learning current network state through ICMP probes. It is parsimonious, using Bayesian inference to determine how many probes are needed. On average, each Trinocular instance sends fewer than 20 probes per hour to each /24 network block under study, increasing Internet ""background radiation"" by less than 0.7%. Trinocular is also predictable and precise: we provide known precision in outage timing and duration. Probing in rounds of 11 minutes, we detect 100% of outages one round or longer, and estimate outage duration within one-half round. Since we require little traffic, a single machine can track 3.4M /24 IPv4 blocks, all of the Internet currently suitable for analysis. We show that our approach is significantly more accurate than the best current methods, with about one-third fewer false conclusions, and about 30% greater coverage at constant accuracy. We validate our approach using controlled experiments, use Trinocular to analyze two days of Internet outages observed from three sites, and re-analyze three years of existing data to develop trends for the Internet.",,255–266,12,"internet reliability, bayesian inference, adaptive probing, network outages",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
767,inproceedings,"Quan, Lin and Heidemann, John and Pradkin, Yuri",Trinocular: Understanding Internet Reliability through Adaptive Probing,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486017,10.1145/2486001.2486017,"Natural and human factors cause Internet outages---from big events like Hurricane Sandy in 2012 and the Egyptian Internet shutdown in Jan. 2011 to small outages every day that go unpublicized. We describe Trinocular, an outage detection system that uses active probing to understand reliability of edge networks. Trinocular is principled: deriving a simple model of the Internet that captures the information pertinent to outages, and populating that model through long-term data, and learning current network state through ICMP probes. It is parsimonious, using Bayesian inference to determine how many probes are needed. On average, each Trinocular instance sends fewer than 20 probes per hour to each /24 network block under study, increasing Internet ""background radiation"" by less than 0.7%. Trinocular is also predictable and precise: we provide known precision in outage timing and duration. Probing in rounds of 11 minutes, we detect 100% of outages one round or longer, and estimate outage duration within one-half round. Since we require little traffic, a single machine can track 3.4M /24 IPv4 blocks, all of the Internet currently suitable for analysis. We show that our approach is significantly more accurate than the best current methods, with about one-third fewer false conclusions, and about 30% greater coverage at constant accuracy. We validate our approach using controlled experiments, use Trinocular to analyze two days of Internet outages observed from three sites, and re-analyze three years of existing data to develop trends for the Internet.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,255–266,12,"bayesian inference, adaptive probing, network outages, internet reliability","Hong Kong, China",SIGCOMM '13,,,,,,
768,article,"Gao, Hongyu and Yegneswaran, Vinod and Chen, Yan and Porras, Phillip and Ghosh, Shalini and Jiang, Jian and Duan, Haixin",An Empirical Reexamination of Global DNS Behavior,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486018,10.1145/2534169.2486018,"The performance and operational characteristics of the DNS protocol are of deep interest to the research and network operations community. In this paper, we present measurement results from a unique dataset containing more than 26 billion DNS query-response pairs collected from more than 600 globally distributed recursive DNS resolvers. We use this dataset to reaffirm findings in published work and notice some significant differences that could be attributed both to the evolving nature of DNS traffic and to our differing perspective. For example, we find that although characteristics of DNS traffic vary greatly across networks, the resolvers within an organization tend to exhibit similar behavior. We further find that more than 50% of DNS queries issued to root servers do not return successful answers, and that the primary cause of lookup failures at root servers is malformed queries with invalid TLDs. Furthermore, we propose a novel approach that detects malicious domain groups using temporal correlation in DNS queries. Our approach requires no comprehensive labeled training set, which can be difficult to build in practice. Instead, it uses a known malicious domain as anchor, and identifies the set of previously unknown malicious domains that are related to the anchor domain. Experimental results illustrate the viability of this approach, i.e. , we attain a true positive rate of more than 96%, and each malicious anchor domain results in a malware domain group with more than 53 previously unknown malicious domains on average.",,267–278,12,"measurement, malicious domain detection, dns",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
769,inproceedings,"Gao, Hongyu and Yegneswaran, Vinod and Chen, Yan and Porras, Phillip and Ghosh, Shalini and Jiang, Jian and Duan, Haixin",An Empirical Reexamination of Global DNS Behavior,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486018,10.1145/2486001.2486018,"The performance and operational characteristics of the DNS protocol are of deep interest to the research and network operations community. In this paper, we present measurement results from a unique dataset containing more than 26 billion DNS query-response pairs collected from more than 600 globally distributed recursive DNS resolvers. We use this dataset to reaffirm findings in published work and notice some significant differences that could be attributed both to the evolving nature of DNS traffic and to our differing perspective. For example, we find that although characteristics of DNS traffic vary greatly across networks, the resolvers within an organization tend to exhibit similar behavior. We further find that more than 50% of DNS queries issued to root servers do not return successful answers, and that the primary cause of lookup failures at root servers is malformed queries with invalid TLDs. Furthermore, we propose a novel approach that detects malicious domain groups using temporal correlation in DNS queries. Our approach requires no comprehensive labeled training set, which can be difficult to build in practice. Instead, it uses a known malicious domain as anchor, and identifies the set of previously unknown malicious domains that are related to the anchor domain. Experimental results illustrate the viability of this approach, i.e. , we attain a true positive rate of more than 96%, and each malicious anchor domain results in a malware domain group with more than 53 previously unknown malicious domains on average.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,267–278,12,"measurement, dns, malicious domain detection","Hong Kong, China",SIGCOMM '13,,,,,,
770,inproceedings,"Maennel, Olaf",Session Details: Privacy,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246332,10.1145/3246332,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,,,,"Hong Kong, China",SIGCOMM '13,,,,,,
771,article,"Xia, Ning and Song, Han Hee and Liao, Yong and Iliofotou, Marios and Nucci, Antonio and Zhang, Zhi-Li and Kuzmanovic, Aleksandar",Mosaic: Quantifying Privacy Leakage in Mobile Networks,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486008,10.1145/2534169.2486008,"With the proliferation of online social networking (OSN) and mobile devices, preserving user privacy has become a great challenge. While prior studies have directly focused on OSN services, we call attention to the privacy leakage in mobile network data. This concern is motivated by two factors. First, the prevalence of OSN usage leaves identifiable digital footprints that can be traced back to users in the real-world. Second, the association between users and their mobile devices makes it easier to associate traffic to its owners. These pose a serious threat to user privacy as they enable an adversary to attribute significant portions of data traffic including the ones with NO identity leaks to network users' true identities. To demonstrate its feasibility, we develop the Tessellation methodology. By applying Tessellation on traffic from a cellular service provider (CSP), we show that up to 50% of the traffic can be attributed to the names of users. In addition to revealing the user identity, the reconstructed profile, dubbed as ""mosaic,"" associates personal information such as political views, browsing habits, and favorite apps to the users. We conclude by discussing approaches for preventing and mitigating the alarming leakage of sensitive user information.",,279–290,12,"online social network, privacy, user profile, mobile network, security",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
772,inproceedings,"Xia, Ning and Song, Han Hee and Liao, Yong and Iliofotou, Marios and Nucci, Antonio and Zhang, Zhi-Li and Kuzmanovic, Aleksandar",Mosaic: Quantifying Privacy Leakage in Mobile Networks,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486008,10.1145/2486001.2486008,"With the proliferation of online social networking (OSN) and mobile devices, preserving user privacy has become a great challenge. While prior studies have directly focused on OSN services, we call attention to the privacy leakage in mobile network data. This concern is motivated by two factors. First, the prevalence of OSN usage leaves identifiable digital footprints that can be traced back to users in the real-world. Second, the association between users and their mobile devices makes it easier to associate traffic to its owners. These pose a serious threat to user privacy as they enable an adversary to attribute significant portions of data traffic including the ones with NO identity leaks to network users' true identities. To demonstrate its feasibility, we develop the Tessellation methodology. By applying Tessellation on traffic from a cellular service provider (CSP), we show that up to 50% of the traffic can be attributed to the names of users. In addition to revealing the user identity, the reconstructed profile, dubbed as ""mosaic,"" associates personal information such as political views, browsing habits, and favorite apps to the users. We conclude by discussing approaches for preventing and mitigating the alarming leakage of sensitive user information.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,279–290,12,"security, online social network, privacy, user profile, mobile network","Hong Kong, China",SIGCOMM '13,,,,,,
773,article,"Han, Seungyeop and Liu, Vincent and Pu, Qifan and Peter, Simon and Anderson, Thomas and Krishnamurthy, Arvind and Wetherall, David",Expressive Privacy Control with Pseudonyms,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486032,10.1145/2534169.2486032,"As personal information increases in value, the incentives for remote services to collect as much of it as possible increase as well. In the current Internet, the default assumption is that all behavior can be correlated using a variety of identifying information, not the least of which is a user's IP address. Tools like Tor, Privoxy, and even NATs, are located at the opposite end of the spectrum and prevent any behavior from being linked. Instead, our goal is to provide users with more control over linkability---which activites of the user can be correlated at the remote services---not necessarily more anonymity. We design a cross-layer architecture that provides users with a pseudonym abstraction. To the user, a pseudonym represents a set of activities that the user is fine with linking, and to the outside world, a pseudonym gives the illusion of a single machine. We provide this abstraction by associating each pseudonym with a unique, random address drawn from the IPv6 address space, which is large enough to provide each device with multiple globally-routable addresses. We have implemented and evaluated a prototype that is able to provide unlinkable pseudonyms within the Chrome web browser in order to demonstrate the feasibility, efficacy, and expressiveness of our approach.",,291–302,12,"ipv6, privacy, web tracking, pseudonym",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
774,inproceedings,"Han, Seungyeop and Liu, Vincent and Pu, Qifan and Peter, Simon and Anderson, Thomas and Krishnamurthy, Arvind and Wetherall, David",Expressive Privacy Control with Pseudonyms,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486032,10.1145/2486001.2486032,"As personal information increases in value, the incentives for remote services to collect as much of it as possible increase as well. In the current Internet, the default assumption is that all behavior can be correlated using a variety of identifying information, not the least of which is a user's IP address. Tools like Tor, Privoxy, and even NATs, are located at the opposite end of the spectrum and prevent any behavior from being linked. Instead, our goal is to provide users with more control over linkability---which activites of the user can be correlated at the remote services---not necessarily more anonymity. We design a cross-layer architecture that provides users with a pseudonym abstraction. To the user, a pseudonym represents a set of activities that the user is fine with linking, and to the outside world, a pseudonym gives the illusion of a single machine. We provide this abstraction by associating each pseudonym with a unique, random address drawn from the IPv6 address space, which is large enough to provide each device with multiple globally-routable addresses. We have implemented and evaluated a prototype that is able to provide unlinkable pseudonyms within the Chrome web browser in order to demonstrate the feasibility, efficacy, and expressiveness of our approach.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,291–302,12,"ipv6, web tracking, pseudonym, privacy","Hong Kong, China",SIGCOMM '13,,,,,,
775,article,"Le Blond, Stevens and Choffnes, David and Zhou, Wenxuan and Druschel, Peter and Ballani, Hitesh and Francis, Paul",Towards Efficient Traffic-Analysis Resistant Anonymity Networks,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486002,10.1145/2534169.2486002,"Existing IP anonymity systems tend to sacrifice one of low latency, high bandwidth, or resistance to traffic-analysis. High-latency mix-nets like Mixminion batch messages to resist traffic-analysis at the expense of low latency. Onion routing schemes like Tor deliver low latency and high bandwidth, but are not designed to withstand traffic analysis. Designs based on DC-nets or broadcast channels resist traffic analysis and provide low latency, but are limited to low bandwidth communication. In this paper, we present the design, implementation, and evaluation of Aqua, a high-bandwidth anonymity system that resists traffic analysis. We focus on providing strong anonymity for BitTorrent, and evaluate the performance of Aqua using traces from hundreds of thousands of actual BitTorrent users. We show that Aqua achieves latency low enough for efficient bulk TCP flows, bandwidth sufficient to carry BitTorrent traffic with reasonable efficiency, and resistance to traffic analysis within anonymity sets of hundreds of clients. We conclude that Aqua represents an interesting new point in the space of anonymity network designs.",,303–314,12,"strong anonymity, p2p file sharing, anonymity networks",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
776,inproceedings,"Le Blond, Stevens and Choffnes, David and Zhou, Wenxuan and Druschel, Peter and Ballani, Hitesh and Francis, Paul",Towards Efficient Traffic-Analysis Resistant Anonymity Networks,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486002,10.1145/2486001.2486002,"Existing IP anonymity systems tend to sacrifice one of low latency, high bandwidth, or resistance to traffic-analysis. High-latency mix-nets like Mixminion batch messages to resist traffic-analysis at the expense of low latency. Onion routing schemes like Tor deliver low latency and high bandwidth, but are not designed to withstand traffic analysis. Designs based on DC-nets or broadcast channels resist traffic analysis and provide low latency, but are limited to low bandwidth communication. In this paper, we present the design, implementation, and evaluation of Aqua, a high-bandwidth anonymity system that resists traffic analysis. We focus on providing strong anonymity for BitTorrent, and evaluate the performance of Aqua using traces from hundreds of thousands of actual BitTorrent users. We show that Aqua achieves latency low enough for efficient bulk TCP flows, bandwidth sufficient to carry BitTorrent traffic with reasonable efficiency, and resistance to traffic analysis within anonymity sets of hundreds of clients. We conclude that Aqua represents an interesting new point in the space of anonymity network designs.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,303–314,12,"p2p file sharing, anonymity networks, strong anonymity","Hong Kong, China",SIGCOMM '13,,,,,,
777,article,"Chen, Ruichuan and Akkus, Istemi Ekin and Francis, Paul",SplitX: High-Performance Private Analytics,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486013,10.1145/2534169.2486013,"There is a growing body of research on mechanisms for preserving online user privacy while still allowing aggregate queries over private user data. A common approach is to store user data at users' devices, and to query the data in such a way that a differentially private noisy result is produced without exposing individual user data to any system component. A particular challenge is to design a system that scales well while limiting how much the malicious users can distort the result. This paper presents SplitX, a high-performance analytics system for making differentially private queries over distributed user data. SplitX is typically two to three orders of magnitude more efficient in bandwidth, and from three to five orders of magnitude more efficient in computation than previous comparable systems, while operating under a similar trust model. SplitX accomplishes this performance by replacing public-key operations with exclusive-or operations. This paper presents the design of SplitX, analyzes its security and performance, and describes its implementation and deployment across 416 users.",,315–326,12,"differential privacy, xor cryptography, analytics",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
778,inproceedings,"Chen, Ruichuan and Akkus, Istemi Ekin and Francis, Paul",SplitX: High-Performance Private Analytics,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486013,10.1145/2486001.2486013,"There is a growing body of research on mechanisms for preserving online user privacy while still allowing aggregate queries over private user data. A common approach is to store user data at users' devices, and to query the data in such a way that a differentially private noisy result is produced without exposing individual user data to any system component. A particular challenge is to design a system that scales well while limiting how much the malicious users can distort the result. This paper presents SplitX, a high-performance analytics system for making differentially private queries over distributed user data. SplitX is typically two to three orders of magnitude more efficient in bandwidth, and from three to five orders of magnitude more efficient in computation than previous comparable systems, while operating under a similar trust model. SplitX accomplishes this performance by replacing public-key operations with exclusive-or operations. This paper presents the design of SplitX, analyzes its security and performance, and describes its implementation and deployment across 416 users.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,315–326,12,"differential privacy, analytics, xor cryptography","Hong Kong, China",SIGCOMM '13,,,,,,
779,inproceedings,"Oran, Dave",Session Details: Applications and Resource Allocation,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246333,10.1145/3246333,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,,,,"Hong Kong, China",SIGCOMM '13,,,,,,
780,article,"Ferguson, Andrew D. and Guha, Arjun and Liang, Chen and Fonseca, Rodrigo and Krishnamurthi, Shriram",Participatory Networking: An API for Application Control of SDNs,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486003,10.1145/2534169.2486003,"We present the design, implementation, and evaluation of an API for applications to control a software-defined network (SDN). Our API is implemented by an OpenFlow controller that delegates read and write authority from the network's administrators to end users, or applications and devices acting on their behalf. Users can then work with the network, rather than around it, to achieve better performance, security, or predictable behavior. Our API serves well as the next layer atop current SDN stacks. Our design addresses the two key challenges: how to safely decompose control and visibility of the network, and how to resolve conflicts between untrusted users and across requests, while maintaining baseline levels of fairness and security. Using a real OpenFlow testbed, we demonstrate our API's feasibility through microbenchmarks, and its usefulness by experiments with four real applications modified to take advantage of it.",,327–338,12,"participatory networking, openflow, software-defined networks",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
781,inproceedings,"Ferguson, Andrew D. and Guha, Arjun and Liang, Chen and Fonseca, Rodrigo and Krishnamurthi, Shriram",Participatory Networking: An API for Application Control of SDNs,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486003,10.1145/2486001.2486003,"We present the design, implementation, and evaluation of an API for applications to control a software-defined network (SDN). Our API is implemented by an OpenFlow controller that delegates read and write authority from the network's administrators to end users, or applications and devices acting on their behalf. Users can then work with the network, rather than around it, to achieve better performance, security, or predictable behavior. Our API serves well as the next layer atop current SDN stacks. Our design addresses the two key challenges: how to safely decompose control and visibility of the network, and how to resolve conflicts between untrusted users and across requests, while maintaining baseline levels of fairness and security. Using a real OpenFlow testbed, we demonstrate our API's feasibility through microbenchmarks, and its usefulness by experiments with four real applications modified to take advantage of it.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,327–338,12,"participatory networking, software-defined networks, openflow","Hong Kong, China",SIGCOMM '13,,,,,,
782,article,"Balachandran, Athula and Sekar, Vyas and Akella, Aditya and Seshan, Srinivasan and Stoica, Ion and Zhang, Hui",Developing a Predictive Model of Quality of Experience for Internet Video,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486025,10.1145/2534169.2486025,"Improving users' quality of experience (QoE) is crucial for sustaining the advertisement and subscription based revenue models that enable the growth of Internet video. Despite the rich literature on video and QoE measurement, our understanding of Internet video QoE is limited because of the shift from traditional methods of measuring video quality (e.g., Peak Signal-to-Noise Ratio) and user experience (e.g., opinion scores). These have been replaced by new quality metrics (e.g., rate of buffering, bitrate) and new engagement centric measures of user experience (e.g., viewing time and number of visits). The goal of this paper is to develop a predictive model of Internet video QoE. To this end, we identify two key requirements for the QoE model: (1) it has to be tied in to observable user engagement and (2) it should be actionable to guide practical system design decisions. Achieving this goal is challenging because the quality metrics are interdependent, they have complex and counter-intuitive relationships to engagement measures, and there are many external factors that confound the relationship between quality and engagement (e.g., type of video, user connectivity). To address these challenges, we present a data-driven approach to model the metric interdependencies and their complex relationships to engagement, and propose a systematic framework to identify and account for the confounding factors. We show that a delivery infrastructure that uses our proposed model to choose CDN and bitrates can achieve more than 20% improvement in overall user engagement compared to strawman approaches.",,339–350,12,"video quality, human factors, measurement, peformance",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
783,inproceedings,"Balachandran, Athula and Sekar, Vyas and Akella, Aditya and Seshan, Srinivasan and Stoica, Ion and Zhang, Hui",Developing a Predictive Model of Quality of Experience for Internet Video,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486025,10.1145/2486001.2486025,"Improving users' quality of experience (QoE) is crucial for sustaining the advertisement and subscription based revenue models that enable the growth of Internet video. Despite the rich literature on video and QoE measurement, our understanding of Internet video QoE is limited because of the shift from traditional methods of measuring video quality (e.g., Peak Signal-to-Noise Ratio) and user experience (e.g., opinion scores). These have been replaced by new quality metrics (e.g., rate of buffering, bitrate) and new engagement centric measures of user experience (e.g., viewing time and number of visits). The goal of this paper is to develop a predictive model of Internet video QoE. To this end, we identify two key requirements for the QoE model: (1) it has to be tied in to observable user engagement and (2) it should be actionable to guide practical system design decisions. Achieving this goal is challenging because the quality metrics are interdependent, they have complex and counter-intuitive relationships to engagement measures, and there are many external factors that confound the relationship between quality and engagement (e.g., type of video, user connectivity). To address these challenges, we present a data-driven approach to model the metric interdependencies and their complex relationships to engagement, and propose a systematic framework to identify and account for the confounding factors. We show that a delivery infrastructure that uses our proposed model to choose CDN and bitrates can achieve more than 20% improvement in overall user engagement compared to strawman approaches.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,339–350,12,"video quality, human factors, peformance, measurement","Hong Kong, China",SIGCOMM '13,,,,,,
784,article,"Popa, Lucian and Yalagandula, Praveen and Banerjee, Sujata and Mogul, Jeffrey C. and Turner, Yoshio and Santos, Jose Renato",ElasticSwitch: Practical Work-Conserving Bandwidth Guarantees for Cloud Computing,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486027,10.1145/2534169.2486027,"While cloud computing providers offer guaranteed allocations for resources such as CPU and memory, they do not offer any guarantees for network resources. The lack of network guarantees prevents tenants from predicting lower bounds on the performance of their applications. The research community has recognized this limitation but, unfortunately, prior solutions have significant limitations: either they are inefficient, because they are not work-conserving, or they are impractical, because they require expensive switch support or congestion-free network cores.In this paper, we propose ElasticSwitch, an efficient and practical approach for providing bandwidth guarantees. ElasticSwitch is efficient because it utilizes the spare bandwidth from unreserved capacity or underutilized reservations. ElasticSwitch is practical because it can be fully implemented in hypervisors, without requiring a specific topology or any support from switches. Because hypervisors operate mostly independently, there is no need for complex coordination between them or with a central controller. Our experiments, with a prototype implementation on a 100-server testbed, demonstrate that ElasticSwitch provides bandwidth guarantees and is work-conserving, even in challenging situations.",,351–362,12,"cloud computing, work-conserving, bandwidth guarantees",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
785,inproceedings,"Popa, Lucian and Yalagandula, Praveen and Banerjee, Sujata and Mogul, Jeffrey C. and Turner, Yoshio and Santos, Jose Renato",ElasticSwitch: Practical Work-Conserving Bandwidth Guarantees for Cloud Computing,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486027,10.1145/2486001.2486027,"While cloud computing providers offer guaranteed allocations for resources such as CPU and memory, they do not offer any guarantees for network resources. The lack of network guarantees prevents tenants from predicting lower bounds on the performance of their applications. The research community has recognized this limitation but, unfortunately, prior solutions have significant limitations: either they are inefficient, because they are not work-conserving, or they are impractical, because they require expensive switch support or congestion-free network cores.In this paper, we propose ElasticSwitch, an efficient and practical approach for providing bandwidth guarantees. ElasticSwitch is efficient because it utilizes the spare bandwidth from unreserved capacity or underutilized reservations. ElasticSwitch is practical because it can be fully implemented in hypervisors, without requiring a specific topology or any support from switches. Because hypervisors operate mostly independently, there is no need for complex coordination between them or with a central controller. Our experiments, with a prototype implementation on a 100-server testbed, demonstrate that ElasticSwitch provides bandwidth guarantees and is work-conserving, even in challenging situations.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,351–362,12,"bandwidth guarantees, cloud computing, work-conserving","Hong Kong, China",SIGCOMM '13,,,,,,
786,inproceedings,"Karp, Brad",Session Details: Wireless Communication 2,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246334,10.1145/3246334,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,,,,"Hong Kong, China",SIGCOMM '13,,,,,,
787,article,"Huang, Junxian and Qian, Feng and Guo, Yihua and Zhou, Yuanyuan and Xu, Qiang and Mao, Z. Morley and Sen, Subhabrata and Spatscheck, Oliver",An In-Depth Study of LTE: Effect of Network Protocol and Application Behavior on Performance,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486006,10.1145/2534169.2486006,"With lower latency and higher bandwidth than its predecessor 3G networks, the latest cellular technology 4G LTE has been attracting many new users. However, the interactions among applications, network transport protocol, and the radio layer still remain unexplored. In this work, we conduct an in-depth study of these interactions and their impact on performance, using a combination of active and passive measurements. We observed that LTE has significantly shorter state promotion delays and lower RTTs than those of 3G networks. We discovered various inefficiencies in TCP over LTE such as undesired slow start. We further developed a novel and lightweight passive bandwidth estimation technique for LTE networks. Using this tool, we discovered that many TCP connections significantly under-utilize the available bandwidth. On average, the actually used bandwidth is less than 50% of the available bandwidth. This causes data downloads to be longer, and incur additional energy overhead. We found that the under-utilization can be caused by both application behavior and TCP parameter setting. We found that 52.6% of all downlink TCP flows have been throttled by limited TCP receive window, and that data transfer patterns for some popular applications are both energy and network unfriendly. All these findings highlight the need to develop transport protocol mechanisms and applications that are more LTE-friendly.",,363–374,12,"bandwidth estimation, 4g, resource underutilization, lte, tcp performance",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
788,inproceedings,"Huang, Junxian and Qian, Feng and Guo, Yihua and Zhou, Yuanyuan and Xu, Qiang and Mao, Z. Morley and Sen, Subhabrata and Spatscheck, Oliver",An In-Depth Study of LTE: Effect of Network Protocol and Application Behavior on Performance,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486006,10.1145/2486001.2486006,"With lower latency and higher bandwidth than its predecessor 3G networks, the latest cellular technology 4G LTE has been attracting many new users. However, the interactions among applications, network transport protocol, and the radio layer still remain unexplored. In this work, we conduct an in-depth study of these interactions and their impact on performance, using a combination of active and passive measurements. We observed that LTE has significantly shorter state promotion delays and lower RTTs than those of 3G networks. We discovered various inefficiencies in TCP over LTE such as undesired slow start. We further developed a novel and lightweight passive bandwidth estimation technique for LTE networks. Using this tool, we discovered that many TCP connections significantly under-utilize the available bandwidth. On average, the actually used bandwidth is less than 50% of the available bandwidth. This causes data downloads to be longer, and incur additional energy overhead. We found that the under-utilization can be caused by both application behavior and TCP parameter setting. We found that 52.6% of all downlink TCP flows have been throttled by limited TCP receive window, and that data transfer patterns for some popular applications are both energy and network unfriendly. All these findings highlight the need to develop transport protocol mechanisms and applications that are more LTE-friendly.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,363–374,12,"bandwidth estimation, lte, 4g, resource underutilization, tcp performance","Hong Kong, China",SIGCOMM '13,,,,,,
789,article,"Bharadia, Dinesh and McMilin, Emily and Katti, Sachin",Full Duplex Radios,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486033,10.1145/2534169.2486033,"This paper presents the design and implementation of the first in-band full duplex WiFi radios that can simultaneously transmit and receive on the same channel using standard WiFi 802.11ac PHYs and achieves close to the theoretical doubling of throughput in all practical deployment scenarios. Our design uses a single antenna for simultaneous TX/RX (i.e., the same resources as a standard half duplex system). We also propose novel analog and digital cancellation techniques that cancel the self interference to the receiver noise floor, and therefore ensure that there is no degradation to the received signal. We prototype our design by building our own analog circuit boards and integrating them with a fully WiFi-PHY compatible software radio implementation. We show experimentally that our design works robustly in noisy indoor environments, and provides close to the expected theoretical doubling of throughput in practice.",,375–386,12,"interference cancellation, full duplex, non-linear cancellation",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
790,inproceedings,"Bharadia, Dinesh and McMilin, Emily and Katti, Sachin",Full Duplex Radios,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486033,10.1145/2486001.2486033,"This paper presents the design and implementation of the first in-band full duplex WiFi radios that can simultaneously transmit and receive on the same channel using standard WiFi 802.11ac PHYs and achieves close to the theoretical doubling of throughput in all practical deployment scenarios. Our design uses a single antenna for simultaneous TX/RX (i.e., the same resources as a standard half duplex system). We also propose novel analog and digital cancellation techniques that cancel the self interference to the receiver noise floor, and therefore ensure that there is no degradation to the received signal. We prototype our design by building our own analog circuit boards and integrating them with a fully WiFi-PHY compatible software radio implementation. We show experimentally that our design works robustly in noisy indoor environments, and provides close to the expected theoretical doubling of throughput in practice.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,375–386,12,"full duplex, interference cancellation, non-linear cancellation","Hong Kong, China",SIGCOMM '13,,,,,,
791,article,"Kumar, Swarun and Cifuentes, Diego and Gollakota, Shyamnath and Katabi, Dina",Bringing Cross-Layer MIMO to Today's Wireless LANs,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486034,10.1145/2534169.2486034,"Recent years have seen major innovations in cross-layer wireless designs. Despite demonstrating significant throughput gains, hardly any of these technologies have made it into real networks. Deploying cross-layer innovations requires adoption from Wi-Fi chip manufacturers. Yet, manufacturers hesitate to undertake major investments without a better understanding of how these designs interact with real networks and applications.This paper presents the first step towards breaking this stalemate, by enabling the adoption of cross-layer designs in today's networks with commodity Wi-Fi cards and actual applications. We present OpenRF, a cross-layer architecture for managing MIMO signal processing. OpenRF enables access points on the same channel to cancel their interference at each other's clients, while beamforming their signal to their own clients. OpenRF is self-configuring, so that network administrators need not understand MIMO or physical layer techniques.We patch the iwlwifi driver to support OpenRF on off-the-shelf Intel cards. We deploy OpenRF on a 20-node network, showing how it manages the complex interaction of cross-layer design with a real network stack, TCP, bursty traffic, and real applications. Our results demonstrate an average gain of 1.6x for TCP traffic and a significant reduction in response time for real-time applications, like remote desktop.",,387–398,12,"wireless, cross-layer, mimo, sdn",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
792,inproceedings,"Kumar, Swarun and Cifuentes, Diego and Gollakota, Shyamnath and Katabi, Dina",Bringing Cross-Layer MIMO to Today's Wireless LANs,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486034,10.1145/2486001.2486034,"Recent years have seen major innovations in cross-layer wireless designs. Despite demonstrating significant throughput gains, hardly any of these technologies have made it into real networks. Deploying cross-layer innovations requires adoption from Wi-Fi chip manufacturers. Yet, manufacturers hesitate to undertake major investments without a better understanding of how these designs interact with real networks and applications.This paper presents the first step towards breaking this stalemate, by enabling the adoption of cross-layer designs in today's networks with commodity Wi-Fi cards and actual applications. We present OpenRF, a cross-layer architecture for managing MIMO signal processing. OpenRF enables access points on the same channel to cancel their interference at each other's clients, while beamforming their signal to their own clients. OpenRF is self-configuring, so that network administrators need not understand MIMO or physical layer techniques.We patch the iwlwifi driver to support OpenRF on off-the-shelf Intel cards. We deploy OpenRF on a 20-node network, showing how it manages the complex interaction of cross-layer design with a real network stack, TCP, bursty traffic, and real applications. Our results demonstrate an average gain of 1.6x for TCP traffic and a significant reduction in response time for real-time applications, like remote desktop.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,387–398,12,"mimo, cross-layer, wireless, sdn","Hong Kong, China",SIGCOMM '13,,,,,,
793,article,"Yang, Qing and Li, Xiaoxiao and Yao, Hongyi and Fang, Ji and Tan, Kun and Hu, Wenjun and Zhang, Jiansong and Zhang, Yongguang",BigStation: Enabling Scalable Real-Time Signal Processingin Large Mu-Mimo Systems,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486016,10.1145/2534169.2486016,"Multi-user multiple-input multiple-output (MU-MIMO) is the latest communication technology that promises to linearly increase the wireless capacity by deploying more antennas on access points (APs). However, the large number of MIMO antennas will generate a huge amount of digital signal samples in real time. This imposes a grand challenge on the AP design by multiplying the computation and the I/O requirements to process the digital samples. This paper presents BigStation, a scalable architecture that enables realtime signal processing in large-scale MIMO systems which may have tens or hundreds of antennas. Our strategy to scale is to extensively parallelize the MU-MIMO processing on many simple and low-cost commodity computing devices. Our design can incrementally support more antennas by proportionally adding more computing devices. To reduce the overall processing latency, which is a critical constraint for wireless communication, we parallelize the MU-MIMO processing with a distributed pipeline based on its computation and communication patterns. At each stage of the pipeline, we further use data partitioning and computation partitioning to increase the processing speed. As a proof of concept, we have built a BigStation prototype based on commodity PC servers and standard Ethernet switches. Our prototype employs 15 PC servers and can support real-time processing of 12 software radio antennas. Our results show that the BigStation architecture is able to scale to tens to hundreds of antennas. With 12 antennas, our BigStation prototype can increase wireless capacity by 6.8x with a low mean processing delay of 860μs. While this latency is not yet low enough for the 802.11 MAC, it already satisfies the real-time requirements of many existing wireless standards, e.g., LTE and WCDMA.",,399–410,12,"mu-mimo, bigstation, software radio, parallel signal processing",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
794,inproceedings,"Yang, Qing and Li, Xiaoxiao and Yao, Hongyi and Fang, Ji and Tan, Kun and Hu, Wenjun and Zhang, Jiansong and Zhang, Yongguang",BigStation: Enabling Scalable Real-Time Signal Processingin Large Mu-Mimo Systems,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486016,10.1145/2486001.2486016,"Multi-user multiple-input multiple-output (MU-MIMO) is the latest communication technology that promises to linearly increase the wireless capacity by deploying more antennas on access points (APs). However, the large number of MIMO antennas will generate a huge amount of digital signal samples in real time. This imposes a grand challenge on the AP design by multiplying the computation and the I/O requirements to process the digital samples. This paper presents BigStation, a scalable architecture that enables realtime signal processing in large-scale MIMO systems which may have tens or hundreds of antennas. Our strategy to scale is to extensively parallelize the MU-MIMO processing on many simple and low-cost commodity computing devices. Our design can incrementally support more antennas by proportionally adding more computing devices. To reduce the overall processing latency, which is a critical constraint for wireless communication, we parallelize the MU-MIMO processing with a distributed pipeline based on its computation and communication patterns. At each stage of the pipeline, we further use data partitioning and computation partitioning to increase the processing speed. As a proof of concept, we have built a BigStation prototype based on commodity PC servers and standard Ethernet switches. Our prototype employs 15 PC servers and can support real-time processing of 12 software radio antennas. Our results show that the BigStation architecture is able to scale to tens to hundreds of antennas. With 12 antennas, our BigStation prototype can increase wireless capacity by 6.8x with a low mean processing delay of 860μs. While this latency is not yet low enough for the 802.11 MAC, it already satisfies the real-time requirements of many existing wireless standards, e.g., LTE and WCDMA.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,399–410,12,"parallel signal processing, mu-mimo, software radio, bigstation","Hong Kong, China",SIGCOMM '13,,,,,,
795,inproceedings,"Padmanabhan, Venkat",Session Details: Data Center Networks 2,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3246335,10.1145/3246335,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,,,,"Hong Kong, China",SIGCOMM '13,,,,,,
796,article,"Liu, Hongqiang Harry and Wu, Xin and Zhang, Ming and Yuan, Lihua and Wattenhofer, Roger and Maltz, David",ZUpdate: Updating Data Center Networks with Zero Loss,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486005,10.1145/2534169.2486005,"Datacenter networks (DCNs) are constantly evolving due to various updates such as switch upgrades and VM migrations. Each update must be carefully planned and executed in order to avoid disrupting many of the mission-critical, interactive applications hosted in DCNs. The key challenge arises from the inherent difficulty in synchronizing the changes to many devices, which may result in unforeseen transient link load spikes or even congestions. We present one primitive, zUpdate, to perform congestion-free network updates under asynchronous switch and traffic matrix changes. We formulate the update problem using a network model and apply our model to a variety of representative update scenarios in DCNs. We develop novel techniques to handle several practical challenges in realizing zUpdate as well as implement the zUpdate prototype on OpenFlow switches and deploy it on a testbed that resembles real DCN topology. Our results, from both real-world experiments and large-scale trace-driven simulations, show that zUpdate can effectively perform congestion-free updates in production DCNs.",,411–422,12,"congestion, data center network, network update",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
797,inproceedings,"Liu, Hongqiang Harry and Wu, Xin and Zhang, Ming and Yuan, Lihua and Wattenhofer, Roger and Maltz, David",ZUpdate: Updating Data Center Networks with Zero Loss,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486005,10.1145/2486001.2486005,"Datacenter networks (DCNs) are constantly evolving due to various updates such as switch upgrades and VM migrations. Each update must be carefully planned and executed in order to avoid disrupting many of the mission-critical, interactive applications hosted in DCNs. The key challenge arises from the inherent difficulty in synchronizing the changes to many devices, which may result in unforeseen transient link load spikes or even congestions. We present one primitive, zUpdate, to perform congestion-free network updates under asynchronous switch and traffic matrix changes. We formulate the update problem using a network model and apply our model to a variety of representative update scenarios in DCNs. We develop novel techniques to handle several practical challenges in realizing zUpdate as well as implement the zUpdate prototype on OpenFlow switches and deploy it on a testbed that resembles real DCN topology. Our results, from both real-world experiments and large-scale trace-driven simulations, show that zUpdate can effectively perform congestion-free updates in production DCNs.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,411–422,12,"network update, data center network, congestion","Hong Kong, China",SIGCOMM '13,,,,,,
798,article,"Crisan, Daniel and Birke, Robert and Cressier, Gilles and Minkenberg, Cyriel and Gusat, Mitch",Got Loss?  Get ZOVN!,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486024,10.1145/2534169.2486024,"Datacenter networking is currently dominated by two major trends. One aims toward lossless, flat layer-2 fabrics based on Converged Enhanced Ethernet or InfiniBand, with benefits in efficiency and performance. The other targets flexibility based on Software Defined Networking, which enables Overlay Virtual Networking. Although clearly complementary, these trends also exhibit some conflicts: In contrast to physical fabrics, which avoid packet drops by means of flow control, practically all current virtual networks are lossy. We quantify these losses for several common combinations of hypervisors and virtual switches, and show their detrimental effect on application performance. Moreover, we propose a zero-loss Overlay Virtual Network (zOVN) designed to reduce the query and flow completion time of latency-sensitive datacenter applications. We describe its architecture and detail the design of its key component, the zVALE lossless virtual switch. As proof of concept, we implemented a zOVN prototype and benchmark it with Partition-Aggregate in two testbeds, achieving an up to 15-fold reduction of the mean completion time with three widespread TCP versions. For larger-scale validation and deeper introspection into zOVN, we developed an OMNeT++ model for accurate cross-layer simulations of a virtualized datacenter, which confirm the validity of our results.",,423–434,12,"datacenter networking, overlay networks, virtualization, lossless, partition-aggregate",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
799,inproceedings,"Crisan, Daniel and Birke, Robert and Cressier, Gilles and Minkenberg, Cyriel and Gusat, Mitch",Got Loss?  Get ZOVN!,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486024,10.1145/2486001.2486024,"Datacenter networking is currently dominated by two major trends. One aims toward lossless, flat layer-2 fabrics based on Converged Enhanced Ethernet or InfiniBand, with benefits in efficiency and performance. The other targets flexibility based on Software Defined Networking, which enables Overlay Virtual Networking. Although clearly complementary, these trends also exhibit some conflicts: In contrast to physical fabrics, which avoid packet drops by means of flow control, practically all current virtual networks are lossy. We quantify these losses for several common combinations of hypervisors and virtual switches, and show their detrimental effect on application performance. Moreover, we propose a zero-loss Overlay Virtual Network (zOVN) designed to reduce the query and flow completion time of latency-sensitive datacenter applications. We describe its architecture and detail the design of its key component, the zVALE lossless virtual switch. As proof of concept, we implemented a zOVN prototype and benchmark it with Partition-Aggregate in two testbeds, achieving an up to 15-fold reduction of the mean completion time with three widespread TCP versions. For larger-scale validation and deeper introspection into zOVN, we developed an OMNeT++ model for accurate cross-layer simulations of a virtualized datacenter, which confirm the validity of our results.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,423–434,12,"overlay networks, partition-aggregate, datacenter networking, virtualization, lossless","Hong Kong, China",SIGCOMM '13,,,,,,
800,article,"Alizadeh, Mohammad and Yang, Shuang and Sharif, Milad and Katti, Sachin and McKeown, Nick and Prabhakar, Balaji and Shenker, Scott",PFabric: Minimal near-Optimal Datacenter Transport,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486031,10.1145/2534169.2486031,"In this paper we present pFabric, a minimalistic datacenter transport design that provides near theoretically optimal flow completion times even at the 99th percentile for short flows, while still minimizing average flow completion time for long flows. Moreover, pFabric delivers this performance with a very simple design that is based on a key conceptual insight: datacenter transport should decouple flow scheduling from rate control. For flow scheduling, packets carry a single priority number set independently by each flow; switches have very small buffers and implement a very simple priority-based scheduling/dropping mechanism. Rate control is also correspondingly simpler; flows start at line rate and throttle back only under high and persistent packet loss. We provide theoretical intuition and show via extensive simulations that the combination of these two simple mechanisms is sufficient to provide near-optimal performance.",,435–446,12,"flow scheduling, datacenter network, packet transport",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
801,inproceedings,"Alizadeh, Mohammad and Yang, Shuang and Sharif, Milad and Katti, Sachin and McKeown, Nick and Prabhakar, Balaji and Shenker, Scott",PFabric: Minimal near-Optimal Datacenter Transport,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486031,10.1145/2486001.2486031,"In this paper we present pFabric, a minimalistic datacenter transport design that provides near theoretically optimal flow completion times even at the 99th percentile for short flows, while still minimizing average flow completion time for long flows. Moreover, pFabric delivers this performance with a very simple design that is based on a key conceptual insight: datacenter transport should decouple flow scheduling from rate control. For flow scheduling, packets carry a single priority number set independently by each flow; switches have very small buffers and implement a very simple priority-based scheduling/dropping mechanism. Rate control is also correspondingly simpler; flows start at line rate and throttle back only under high and persistent packet loss. We provide theoretical intuition and show via extensive simulations that the combination of these two simple mechanisms is sufficient to provide near-optimal performance.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,435–446,12,"packet transport, flow scheduling, datacenter network","Hong Kong, China",SIGCOMM '13,,,,,,
802,article,"Porter, George and Strong, Richard and Farrington, Nathan and Forencich, Alex and Chen-Sun, Pang and Rosing, Tajana and Fainman, Yeshaiahu and Papen, George and Vahdat, Amin",Integrating Microsecond Circuit Switching into the Data Center,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2486007,10.1145/2534169.2486007,"Recent proposals have employed optical circuit switching (OCS) to reduce the cost of data center networks. However, the relatively slow switching times (10--100 ms) assumed by these approaches, and the accompanying latencies of their control planes, has limited its use to only the largest data center networks with highly aggregated and constrained workloads. As faster switch technologies become available, designing a control plane capable of supporting them becomes a key challenge.In this paper, we design and implement an OCS prototype capable of switching in 11.5 us, and we use this prototype to expose a set of challenges that arise when supporting switching at microsecond time scales. In response, we propose a microsecond-latency control plane based on a circuit scheduling approach we call Traffic Matrix Scheduling (TMS) that proactively communicates circuit assignments to communicating entities so that circuit bandwidth can be used efficiently.",,447–458,12,"data center networks, optical networks",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
803,inproceedings,"Porter, George and Strong, Richard and Farrington, Nathan and Forencich, Alex and Chen-Sun, Pang and Rosing, Tajana and Fainman, Yeshaiahu and Papen, George and Vahdat, Amin",Integrating Microsecond Circuit Switching into the Data Center,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2486007,10.1145/2486001.2486007,"Recent proposals have employed optical circuit switching (OCS) to reduce the cost of data center networks. However, the relatively slow switching times (10--100 ms) assumed by these approaches, and the accompanying latencies of their control planes, has limited its use to only the largest data center networks with highly aggregated and constrained workloads. As faster switch technologies become available, designing a control plane capable of supporting them becomes a key challenge.In this paper, we design and implement an OCS prototype capable of switching in 11.5 us, and we use this prototype to expose a set of challenges that arise when supporting switching at microsecond time scales. In response, we propose a microsecond-latency control plane based on a circuit scheduling approach we call Traffic Matrix Scheduling (TMS) that proactively communicates circuit assignments to communicating entities so that circuit bandwidth can be used efficiently.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,447–458,12,"optical networks, data center networks","Hong Kong, China",SIGCOMM '13,,,,,,
804,article,"Biscuitwala, Kanak and Bult, Willem and L\'{e","Dispatch: Secure, Resilient Mobile Reporting",2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491697,10.1145/2534169.2491697,,,459–460,2,"disconnection resilience, mobile publishing",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
805,inproceedings,"Biscuitwala, Kanak and Bult, Willem and L\'{e","Dispatch: Secure, Resilient Mobile Reporting",2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491697,10.1145/2486001.2491697,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,459–460,2,"mobile publishing, disconnection resilience","Hong Kong, China",SIGCOMM '13,,,,,,
806,article,"Biswas, Trisha and Chakraborti, Asit and Ravindran, Ravishankar and Zhang, Xinwen and Wang, Guoqiang",Contextualized Information-Centric Home Network,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491691,10.1145/2534169.2491691,"We deploy information-centric networks (ICN) to serve several applications including content distribution, vehicle-to-vehicle communication (V2V), home networks (homenet), and sensor networks. These applications require policy and context-based interaction between service producers and consumers. We visualize the ICN service layer as a contextualized information-centric bus (CIBUS), over which diverse sets of service producers and consumers co-exist. We develop a prototype and demonstrate several desirable features of ICN for homenets such as contextual service publishing and subscription, zero-configuration based node and service discovery, policy based routing and forwarding with name-based firewall, and device-to-device communication. Furthermore the prototype is applicable to both ad hoc and infrastructure settings, and can deal with diverse devices and services.",,461–462,2,"service discovery, named data networks, zero-configuration, node discovery, home networks, policy based routing, information-centric networks, content centric networking",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
807,inproceedings,"Biswas, Trisha and Chakraborti, Asit and Ravindran, Ravishankar and Zhang, Xinwen and Wang, Guoqiang",Contextualized Information-Centric Home Network,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491691,10.1145/2486001.2491691,"We deploy information-centric networks (ICN) to serve several applications including content distribution, vehicle-to-vehicle communication (V2V), home networks (homenet), and sensor networks. These applications require policy and context-based interaction between service producers and consumers. We visualize the ICN service layer as a contextualized information-centric bus (CIBUS), over which diverse sets of service producers and consumers co-exist. We develop a prototype and demonstrate several desirable features of ICN for homenets such as contextual service publishing and subscription, zero-configuration based node and service discovery, policy based routing and forwarding with name-based firewall, and device-to-device communication. Furthermore the prototype is applicable to both ad hoc and infrastructure settings, and can deal with diverse devices and services.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,461–462,2,"information-centric networks, service discovery, home networks, policy based routing, node discovery, named data networks, zero-configuration, content centric networking","Hong Kong, China",SIGCOMM '13,,,,,,
808,article,"Chen, Yuanfang and Crespi, Noel and Lv, Lin and Li, Mingchu and Ortiz, Antonio M. and Shu, Lei",Locating Using Prior Information: Wireless Indoor Localization Algorithm,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491688,10.1145/2534169.2491688,"Most indoor localization algorithms are based on Received Signal Strength (RSS), in which RSS signatures of an interested area are annotated with their real recorded locations. However, according to our experiments, RSS signatures are not suitable as the unique annotations (like Fingerprints) of recorded locations. In this study, we investigate the characteristics of RSS (e.g., how the RSS values change as time goes on and between consecutive positions?). On this basis, we design LuPI (Locating using Prior Information) that exploits the characteristics of RSS: with user motion, LuPI uses novel sensors integrated in smartphones to construct the RSS variation space (like radio map) of a floor plan as prior information. The deployment of LuPI is easy and rapid since little human intervention is needed. In LuPI, the calibration of ``radio map'' is crowd-sourced, automatic and scheduled. Experimental results show that LuPI achieves comparable location accuracy to previous approaches, even without the statistical information of site survey.",,463–464,2,"indoor localization, floor plan, wireless networks, smart devices",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
809,inproceedings,"Chen, Yuanfang and Crespi, Noel and Lv, Lin and Li, Mingchu and Ortiz, Antonio M. and Shu, Lei",Locating Using Prior Information: Wireless Indoor Localization Algorithm,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491688,10.1145/2486001.2491688,"Most indoor localization algorithms are based on Received Signal Strength (RSS), in which RSS signatures of an interested area are annotated with their real recorded locations. However, according to our experiments, RSS signatures are not suitable as the unique annotations (like Fingerprints) of recorded locations. In this study, we investigate the characteristics of RSS (e.g., how the RSS values change as time goes on and between consecutive positions?). On this basis, we design LuPI (Locating using Prior Information) that exploits the characteristics of RSS: with user motion, LuPI uses novel sensors integrated in smartphones to construct the RSS variation space (like radio map) of a floor plan as prior information. The deployment of LuPI is easy and rapid since little human intervention is needed. In LuPI, the calibration of ``radio map'' is crowd-sourced, automatic and scheduled. Experimental results show that LuPI achieves comparable location accuracy to previous approaches, even without the statistical information of site survey.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,463–464,2,"smart devices, indoor localization, floor plan, wireless networks","Hong Kong, China",SIGCOMM '13,,,,,,
810,article,"Dietrich, David and Rizk, Amr and Papadimitriou, Panagiotis",AutoEmbed: Automated Multi-Provider Virtual Network Embedding,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491690,10.1145/2534169.2491690,"We present AutoEmbed, a fully-automated framework for VN embedding across multiple substrate networks. To automate VN embedding, AutoEmbed deploys functions over three layers: (i) Service Providers, (ii) VN Providers, and (iii) Infrastructure Providers (InPs). AutoEmbed enables VN Providers to partition VN requests among multiple substrate networks based on resource and network topology information that is not treated as confidential by InPs. Subsequently, each VN segment is mapped by the corresponding InP onto its substrate network. AutoEmbed enables the evaluation of various aspects of multi-provider VN embedding, such as the efficiency and scalability of embedding algorithms, the impact of different levels of information disclosure on VN embedding efficiency, and the suitability of VN request specifications.",,465–466,2,"performance evaluation, resource assignment, network virtualization",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
811,inproceedings,"Dietrich, David and Rizk, Amr and Papadimitriou, Panagiotis",AutoEmbed: Automated Multi-Provider Virtual Network Embedding,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491690,10.1145/2486001.2491690,"We present AutoEmbed, a fully-automated framework for VN embedding across multiple substrate networks. To automate VN embedding, AutoEmbed deploys functions over three layers: (i) Service Providers, (ii) VN Providers, and (iii) Infrastructure Providers (InPs). AutoEmbed enables VN Providers to partition VN requests among multiple substrate networks based on resource and network topology information that is not treated as confidential by InPs. Subsequently, each VN segment is mapped by the corresponding InP onto its substrate network. AutoEmbed enables the evaluation of various aspects of multi-provider VN embedding, such as the efficiency and scalability of embedding algorithms, the impact of different levels of information disclosure on VN embedding efficiency, and the suitability of VN request specifications.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,465–466,2,"performance evaluation, resource assignment, network virtualization","Hong Kong, China",SIGCOMM '13,,,,,,
812,article,"Gember, Aaron and Grandl, Robert and Khalid, Junaid and Akella, Aditya",Design and Implementation of a Framework for Software-Defined Middlebox Networking,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491686,10.1145/2534169.2491686,,,467–468,2,"middlebox, software-defined networking",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
813,inproceedings,"Gember, Aaron and Grandl, Robert and Khalid, Junaid and Akella, Aditya",Design and Implementation of a Framework for Software-Defined Middlebox Networking,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491686,10.1145/2486001.2491686,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,467–468,2,"software-defined networking, middlebox","Hong Kong, China",SIGCOMM '13,,,,,,
814,article,"Keller, Matthias and Robbert, Christoph and Peuster, Manuel","An Evaluation Testbed for Adaptive, Topology-Aware Deployment of Elastic Applications",2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491689,10.1145/2534169.2491689,,,469–470,2,"testbed, something, geographically distributed, adaptive deployment",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
815,inproceedings,"Keller, Matthias and Robbert, Christoph and Peuster, Manuel","An Evaluation Testbed for Adaptive, Topology-Aware Deployment of Elastic Applications",2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491689,10.1145/2486001.2491689,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,469–470,2,"adaptive deployment, something, geographically distributed, testbed","Hong Kong, China",SIGCOMM '13,,,,,,
816,article,"Knight, Simon",Automated Configuration and Measurement of Emulated Networks with AutoNetkit,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491692,10.1145/2534169.2491692,"Emulated networks enable educators, researchers, and operators to conduct realistic network scenarios on commodity hardware. However each network device must be configured, typically in a low-level syntax. This time-consuming and error-prone process limits scalability and discourages repeated experimentation.This demonstration will show a platform to automate emulated network configuration and measurement, making large-scale network experimentation accessible.",,471–472,2,"configuration management, emulation",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
817,inproceedings,"Knight, Simon",Automated Configuration and Measurement of Emulated Networks with AutoNetkit,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491692,10.1145/2486001.2491692,"Emulated networks enable educators, researchers, and operators to conduct realistic network scenarios on commodity hardware. However each network device must be configured, typically in a low-level syntax. This time-consuming and error-prone process limits scalability and discourages repeated experimentation.This demonstration will show a platform to automate emulated network configuration and measurement, making large-scale network experimentation accessible.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,471–472,2,"configuration management, emulation","Hong Kong, China",SIGCOMM '13,,,,,,
818,article,"Levin, Dan and Canini, Marco and Schmid, Stefan and Feldmann, Anja",Incremental SDN Deployment in Enterprise Networks,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491694,10.1145/2534169.2491694,,,473–474,2,"incremental deployment, software defined network",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
819,inproceedings,"Levin, Dan and Canini, Marco and Schmid, Stefan and Feldmann, Anja",Incremental SDN Deployment in Enterprise Networks,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491694,10.1145/2486001.2491694,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,473–474,2,"incremental deployment, software defined network","Hong Kong, China",SIGCOMM '13,,,,,,
820,article,"Lin, Pingping and Hart, Jonathan and Krishnaswamy, Umesh and Murakami, Tetsuya and Kobayashi, Masayoshi and Al-Shabibi, Ali and Wang, Kuang-Ching and Bi, Jun",Seamless Interworking of SDN and IP,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491703,10.1145/2534169.2491703,,,475–476,2,"sdn-ip network peering, software defined networking",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
821,inproceedings,"Lin, Pingping and Hart, Jonathan and Krishnaswamy, Umesh and Murakami, Tetsuya and Kobayashi, Masayoshi and Al-Shabibi, Ali and Wang, Kuang-Ching and Bi, Jun",Seamless Interworking of SDN and IP,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491703,10.1145/2486001.2491703,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,475–476,2,"sdn-ip network peering, software defined networking","Hong Kong, China",SIGCOMM '13,,,,,,
822,article,"Liu, Bo and Zhao, Baokang and Wei, Ziling and Wu, Chunqing and Su, Jinshu and Yu, Wanrong and Wang, Fei and Sun, Shihai",Qphone: A Quantum Security VoIP Phone,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491696,10.1145/2534169.2491696,"This work presents a novel quantum security VoIP phone, called Qphone. Qphone integrates quantum key distribution (QKD) and VoIP steganography, and achieves peer-to-peer communication with information-theoretical security (ITS) guaranteeing. Qphone consists of three parts, a real-time QKD system, RT-QKD, a steganography software, VS-Phone, and an audio encryption and authentication hardware, AE-KEY. RT-QKD explores QKD technologies, and is able establish a shared key between two peers ensuring ITS. VS-Phone utilizes VoIP steganography to protect transmission channels of sensitive information. Qphone can provide efficient and real-time security protections to meet different security demands.",,477–478,2,"voip, security, steganography, quantum communication",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
823,inproceedings,"Liu, Bo and Zhao, Baokang and Wei, Ziling and Wu, Chunqing and Su, Jinshu and Yu, Wanrong and Wang, Fei and Sun, Shihai",Qphone: A Quantum Security VoIP Phone,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491696,10.1145/2486001.2491696,"This work presents a novel quantum security VoIP phone, called Qphone. Qphone integrates quantum key distribution (QKD) and VoIP steganography, and achieves peer-to-peer communication with information-theoretical security (ITS) guaranteeing. Qphone consists of three parts, a real-time QKD system, RT-QKD, a steganography software, VS-Phone, and an audio encryption and authentication hardware, AE-KEY. RT-QKD explores QKD technologies, and is able establish a shared key between two peers ensuring ITS. VS-Phone utilizes VoIP steganography to protect transmission channels of sensitive information. Qphone can provide efficient and real-time security protections to meet different security demands.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,477–478,2,"voip, quantum communication, steganography, security","Hong Kong, China",SIGCOMM '13,,,,,,
824,article,"Liu, Yang and Liu, Jiahe and Liu, Ting and Guan, Xiaohong and Sun, Yanan",Security Risks Evaluation Toolbox for Smart Grid Devices,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491693,10.1145/2534169.2491693,"Numerous smart devices are deployed in smart grid for state measurement, decision-making and remote control. The security issues of smart devices attract more and more attention. In our work, the communication protocol, storage mechanism and authentication of smart devices are analyzed and a toolbox is developed to evaluate the security risks of smart devices. In this demo, our toolbox is applied to scan 3 smart meters/power monitor systems. A potential risk list is generated and the vulnerabilities are further verified.",,479–480,2,"smart device, smart grid, security risk evaluation",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
825,inproceedings,"Liu, Yang and Liu, Jiahe and Liu, Ting and Guan, Xiaohong and Sun, Yanan",Security Risks Evaluation Toolbox for Smart Grid Devices,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491693,10.1145/2486001.2491693,"Numerous smart devices are deployed in smart grid for state measurement, decision-making and remote control. The security issues of smart devices attract more and more attention. In our work, the communication protocol, storage mechanism and authentication of smart devices are analyzed and a toolbox is developed to evaluate the security risks of smart devices. In this demo, our toolbox is applied to scan 3 smart meters/power monitor systems. A potential risk list is generated and the vulnerabilities are further verified.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,479–480,2,"security risk evaluation, smart grid, smart device","Hong Kong, China",SIGCOMM '13,,,,,,
826,article,N\'{e,A Large-Scale Multipath Playground for Experimenters and Early Adopters,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491698,10.1145/2534169.2491698,"Multipath TCP is an experimental transport protocol with remarkable recent past and non-negligible future potential. However the lack of available large-scale testbeds and publicly accessible multiple paths grossly prohibits the adoption of the technology. Here, we demonstrate a large-scale multipath playground deployed on PlanetLab Europe, which can be used either by experimenters and researchers to test and verify their multipath-related ideas (e.g. enhancing congestion control, fairness or even the arrangement of multiple paths) and also by early adopters to enhance their Internet connection even if single-homed.",,481–482,2,"planetlab, sdn, multipath tcp, openflow",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
827,inproceedings,N\'{e,A Large-Scale Multipath Playground for Experimenters and Early Adopters,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491698,10.1145/2486001.2491698,"Multipath TCP is an experimental transport protocol with remarkable recent past and non-negligible future potential. However the lack of available large-scale testbeds and publicly accessible multiple paths grossly prohibits the adoption of the technology. Here, we demonstrate a large-scale multipath playground deployed on PlanetLab Europe, which can be used either by experimenters and researchers to test and verify their multipath-related ideas (e.g. enhancing congestion control, fairness or even the arrangement of multiple paths) and also by early adopters to enhance their Internet connection even if single-homed.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,481–482,2,"planetlab, openflow, multipath tcp, sdn","Hong Kong, China",SIGCOMM '13,,,,,,
828,article,"P N, Shankaranarayanan and Sivakumar, Ashiwan and Rao, Sanjay and Tawarmalani, Mohit",D-Tunes: Self Tuning Datastores for Geo-Distributed Interactive Applications,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491684,10.1145/2534169.2491684,"Modern internet applications have resulted in users sharing data with each other in an interactive fashion. These applications have very stringent service level agreements (SLAs) which place tight constraints on the performance of the underlying geo-distributed datastores. Deploying these systems in the cloud to meet such constraints is a challenging task, as application architects have to strike an optimal balance among different contrasting objectives such as maintaining consistency between multiple replicas, minimizing access latency and ensuring high availability. Achieving these objectives requires carefully configuring a number of low-level parameters of the datastores, such as the number of replicas, which DCs contain which data, and the underlying consistency protocol parameters. In this work, we adopt a systematic approach where we develop analytical models that capture the performance of a datastore based on application workload and build a system that can automatically configure the datastore for optimal performance.",,483–484,2,"wide-area replication, storage networks",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
829,inproceedings,"P N, Shankaranarayanan and Sivakumar, Ashiwan and Rao, Sanjay and Tawarmalani, Mohit",D-Tunes: Self Tuning Datastores for Geo-Distributed Interactive Applications,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491684,10.1145/2486001.2491684,"Modern internet applications have resulted in users sharing data with each other in an interactive fashion. These applications have very stringent service level agreements (SLAs) which place tight constraints on the performance of the underlying geo-distributed datastores. Deploying these systems in the cloud to meet such constraints is a challenging task, as application architects have to strike an optimal balance among different contrasting objectives such as maintaining consistency between multiple replicas, minimizing access latency and ensuring high availability. Achieving these objectives requires carefully configuring a number of low-level parameters of the datastores, such as the number of replicas, which DCs contain which data, and the underlying consistency protocol parameters. In this work, we adopt a systematic approach where we develop analytical models that capture the performance of a datastore based on application workload and build a system that can automatically configure the datastore for optimal performance.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,483–484,2,"wide-area replication, storage networks","Hong Kong, China",SIGCOMM '13,,,,,,
830,article,"Pu, Qifan and Jiang, Siyu and Gollakota, Shyamnath",Whole-Home Gesture Recognition Using Wireless Signals (Demo),2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491687,10.1145/2534169.2491687,"This demo presents WiSee, a novel human-computer interaction system that leverages wireless networks (e.g., Wi-Fi), to enable sensing and recognition of human gestures and motion. Since wire- less signals do not require line-of-sight and can traverse through walls, WiSee enables novel human-computer interfaces for remote device control and building automation. Further, it achieves this goal without requiring instrumentation of the human body with sensing devices. We integrate WiSee with applications and demonstrate how WiSee enables users to use gestures and control applications including music players and gaming systems. Specifically, our demo will allow SIGCOMM attendees to control a music player and a lighting control device using gestures.",,485–486,2,"wireless, gestures, user interface",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
831,inproceedings,"Pu, Qifan and Jiang, Siyu and Gollakota, Shyamnath",Whole-Home Gesture Recognition Using Wireless Signals (Demo),2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491687,10.1145/2486001.2491687,"This demo presents WiSee, a novel human-computer interaction system that leverages wireless networks (e.g., Wi-Fi), to enable sensing and recognition of human gestures and motion. Since wire- less signals do not require line-of-sight and can traverse through walls, WiSee enables novel human-computer interfaces for remote device control and building automation. Further, it achieves this goal without requiring instrumentation of the human body with sensing devices. We integrate WiSee with applications and demonstrate how WiSee enables users to use gestures and control applications including music players and gaming systems. Specifically, our demo will allow SIGCOMM attendees to control a music player and a lighting control device using gestures.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,485–486,2,"gestures, wireless, user interface","Hong Kong, China",SIGCOMM '13,,,,,,
832,article,"Qazi, Zafar Ayyub and Lee, Jeongkeun and Jin, Tao and Bellala, Gowtham and Arndt, Manfred and Noubir, Guevara",Application-Awareness in SDN,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491700,10.1145/2534169.2491700,"We present a framework, Atlas, which incorporates application-awareness into Software-Defined Networking (SDN), which is currently capable of L2/3/4-based policy enforcement but agnostic to higher layers. Atlas enables fine-grained, accurate and scalable application classification in SDN. It employs a machine learning (ML) based traffic classification technique, a crowd-sourcing approach to obtain ground truth data and leverages SDN's data reporting mechanism and centralized control. We prototype Atlas on HP Labs wireless networks and observe 94% accuracy on average, for top 40 Android applications.",,487–488,2,"software-defined networking (sdn), application awareness",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
833,inproceedings,"Qazi, Zafar Ayyub and Lee, Jeongkeun and Jin, Tao and Bellala, Gowtham and Arndt, Manfred and Noubir, Guevara",Application-Awareness in SDN,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491700,10.1145/2486001.2491700,"We present a framework, Atlas, which incorporates application-awareness into Software-Defined Networking (SDN), which is currently capable of L2/3/4-based policy enforcement but agnostic to higher layers. Atlas enables fine-grained, accurate and scalable application classification in SDN. It employs a machine learning (ML) based traffic classification technique, a crowd-sourcing approach to obtain ground truth data and leverages SDN's data reporting mechanism and centralized control. We prototype Atlas on HP Labs wireless networks and observe 94% accuracy on average, for top 40 Android applications.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,487–488,2,"application awareness, software-defined networking (sdn)","Hong Kong, China",SIGCOMM '13,,,,,,
834,article,"Roverso, Roberto and El-Ansary, Sameh and H\""{o",On HTTP Live Streaming in Large Enterprises,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491685,10.1145/2534169.2491685,"In this work, we present a distributed caching solution which addresses the problem of efficient delivery of HTTP live streams in large private networks. With our system, we have conducted tests on a number of pilot deployments. The largest of them, with 3000 concurrent viewers, consistently showed that our system saves more than 90% of traffic towards the source of the stream while providing the same quality of user experience of a CDN. Another result is that our solution was able to reduce the load on the bottlenecks in the network by an average of 91.6%.",,489–490,2,"distributed caching, private networks, content delivery network, http live",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
835,inproceedings,"Roverso, Roberto and El-Ansary, Sameh and H\""{o",On HTTP Live Streaming in Large Enterprises,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491685,10.1145/2486001.2491685,"In this work, we present a distributed caching solution which addresses the problem of efficient delivery of HTTP live streams in large private networks. With our system, we have conducted tests on a number of pilot deployments. The largest of them, with 3000 concurrent viewers, consistently showed that our system saves more than 90% of traffic towards the source of the stream while providing the same quality of user experience of a CDN. Another result is that our solution was able to reduce the load on the bottlenecks in the network by an average of 91.6%.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,489–490,2,"distributed caching, private networks, content delivery network, http live","Hong Kong, China",SIGCOMM '13,,,,,,
836,article,"Sharma, Sachin and Staessens, Dimitri and Colle, Didier and Pickavet, Mario and Demeester, Piet",Automatic Configuration of Routing Control Platforms in OpenFlow Networks,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491695,10.1145/2534169.2491695,"RouteFlow provides a way to run routing control platforms (e.g. Quagga) in OpenFlow networks. One of the issues of RouteFlow is that an administrator needs to devote a lot of time (typically 7 hours for 28 switches) in manual configurations. We propose and demonstrate a framework that can automatically configure RouteFlow. For this demonstration, we use an emulated pan-European topology of 28 switches. In the demonstration, we stream a video clip from a server to a remote client, and show that the video clip reaches at the remote client within 4 minutes (including the configuration time). In addition, we show automatic configuration of RouteFlow using a GUI (Graphical User Interface).",,491–492,2,"quagga, virtualization, openflow",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
837,inproceedings,"Sharma, Sachin and Staessens, Dimitri and Colle, Didier and Pickavet, Mario and Demeester, Piet",Automatic Configuration of Routing Control Platforms in OpenFlow Networks,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491695,10.1145/2486001.2491695,"RouteFlow provides a way to run routing control platforms (e.g. Quagga) in OpenFlow networks. One of the issues of RouteFlow is that an administrator needs to devote a lot of time (typically 7 hours for 28 switches) in manual configurations. We propose and demonstrate a framework that can automatically configure RouteFlow. For this demonstration, we use an emulated pan-European topology of 28 switches. In the demonstration, we stream a video clip from a server to a remote client, and show that the video clip reaches at the remote client within 4 minutes (including the configuration time). In addition, we show automatic configuration of RouteFlow using a GUI (Graphical User Interface).",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,491–492,2,"openflow, virtualization, quagga","Hong Kong, China",SIGCOMM '13,,,,,,
838,article,"Singh, Rayman Preet and Brush, A.J. Bernheim and Filippov, Evgeni and Huang, Danny and Mahajan, Ratul and Mazhar, Khurshed and Phanishayee, Amar and Samuel, Arjmand",HomeLab: A Platform for Conducting Experiments with Connected Devices in the Home,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491701,10.1145/2534169.2491701,,,493–494,2,"test beds, domestic technology, smart home, devices, home automation",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
839,inproceedings,"Singh, Rayman Preet and Brush, A.J. Bernheim and Filippov, Evgeni and Huang, Danny and Mahajan, Ratul and Mazhar, Khurshed and Phanishayee, Amar and Samuel, Arjmand",HomeLab: A Platform for Conducting Experiments with Connected Devices in the Home,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491701,10.1145/2486001.2491701,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,493–494,2,"smart home, devices, domestic technology, test beds, home automation","Hong Kong, China",SIGCOMM '13,,,,,,
840,article,"So, Won and Narayanan, Ashok and Oran, David and Stapp, Mark",Named Data Networking on a Router: Forwarding at 20gbps and Beyond,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491699,10.1145/2534169.2491699,"Named data networking (NDN) is a new networking paradigm using named data instead of named hosts for communication. Implementation of scalable NDN packet forwarding remains a challenge because NDN requires fast variable-length hierarchical name-based lookup, per-packet data plane state update, and large-scale forwarding tables. We have designed and implemented an NDN data plane with a software forwarding engine on an Intel Xeon-based line card in a Cisco ASR9000 router. In order to achieve high-speed forwarding, our design features (1) name lookup via hash tables with fast collision-resistant hash computation, (2) an efficient and secure FIB lookup algorithm that provides good average and bounded worst-case FIB lookup time, (3) PIT partitioning that enables linear multi-core speedup, and (4) an optimized data structure and software prefetching to maximize data cache utilization. In this demonstration, we showcase our NDN router implementation on the ASR9000 and demonstrate that it can forward real NDN traffic at 20Gbps or higher.",,495–496,2,"router, packet forwarding engine, named data networking",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
841,inproceedings,"So, Won and Narayanan, Ashok and Oran, David and Stapp, Mark",Named Data Networking on a Router: Forwarding at 20gbps and Beyond,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491699,10.1145/2486001.2491699,"Named data networking (NDN) is a new networking paradigm using named data instead of named hosts for communication. Implementation of scalable NDN packet forwarding remains a challenge because NDN requires fast variable-length hierarchical name-based lookup, per-packet data plane state update, and large-scale forwarding tables. We have designed and implemented an NDN data plane with a software forwarding engine on an Intel Xeon-based line card in a Cisco ASR9000 router. In order to achieve high-speed forwarding, our design features (1) name lookup via hash tables with fast collision-resistant hash computation, (2) an efficient and secure FIB lookup algorithm that provides good average and bounded worst-case FIB lookup time, (3) PIT partitioning that enables linear multi-core speedup, and (4) an optimized data structure and software prefetching to maximize data cache utilization. In this demonstration, we showcase our NDN router implementation on the ASR9000 and demonstrate that it can forward real NDN traffic at 20Gbps or higher.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,495–496,2,"named data networking, packet forwarding engine, router","Hong Kong, China",SIGCOMM '13,,,,,,
842,article,"Xue, Lei and Mok, Ricky K.P. and Chang, Rocky K.C.",OMware: An Open Measurement Ware for Stable Residential Broadband Measurement,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491702,10.1145/2534169.2491702,"A number of home-installed middleboxes, e.g., BISMark and SamKnows, and web-based tools, e.g., Netalyzr and Ookla's speedtest service, have been developed recently to enable residential broadband users to gauge their network service quality. One challenge to designing these systems is to provide stable network measurement. That is, the measurement results will not be fluctuated by sporadic overheads incurred inside the middlebox or web browser. In this poster, we propose a network measurement ware, OMware, to increase the stability of residential broadband measurement. The key feature is to implement the send and receive functions for measurement packets in the kernel. Our preliminary evaluation for an OpenWrt implementation shows that OMware provides very stable throughput and delay measurement, compared with typical socket-based measurement at the user level.",,497–498,2,"high performance, network measurement, openwrt kernel module",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
843,inproceedings,"Xue, Lei and Mok, Ricky K.P. and Chang, Rocky K.C.",OMware: An Open Measurement Ware for Stable Residential Broadband Measurement,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491702,10.1145/2486001.2491702,"A number of home-installed middleboxes, e.g., BISMark and SamKnows, and web-based tools, e.g., Netalyzr and Ookla's speedtest service, have been developed recently to enable residential broadband users to gauge their network service quality. One challenge to designing these systems is to provide stable network measurement. That is, the measurement results will not be fluctuated by sporadic overheads incurred inside the middlebox or web browser. In this poster, we propose a network measurement ware, OMware, to increase the stability of residential broadband measurement. The key feature is to implement the send and receive functions for measurement packets in the kernel. Our preliminary evaluation for an OpenWrt implementation shows that OMware provides very stable throughput and delay measurement, compared with typical socket-based measurement at the user level.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,497–498,2,"openwrt kernel module, high performance, network measurement","Hong Kong, China",SIGCOMM '13,,,,,,
844,article,"Yin, Jianxiong and Sun, Peng and Wen, Yonggang and Gong, Haigang and Liu, Ming and Li, Xuelong and You, Haipeng and Gao, Jinqi and Lin, Cynthia",Cloud3DView: An Interactive Tool for Cloud Data Center Operations,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491704,10.1145/2534169.2491704,"The emergence of cloud computing has promoted growing demand and rapid deployment of data centers. However, data center operations require a set of sophisticated skills (e.g., command-line-interface), resulting in a high operational cost. In this demo, to reduce the data center operational cost, we design and build a novel cloud data center management system, based on the concept of 3D gamification. In particular, we apply data visualization techniques to overlay operational status upon a data center 3D model, allowing the operators to monitor the real-time situation and control the data center from a friendly user interface. This demo highlights: (1)a data center 3D view from a First Person Shooter (FPS) camera, (2)a run-time presentation of visualized infrastructures information. Moreover, to improve the user experience, we employ cutting-edge HCI technologies from multi-touch, for remote access to Cloud3DView.",,499–500,2,"data visualization, data center operation",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
845,inproceedings,"Yin, Jianxiong and Sun, Peng and Wen, Yonggang and Gong, Haigang and Liu, Ming and Li, Xuelong and You, Haipeng and Gao, Jinqi and Lin, Cynthia",Cloud3DView: An Interactive Tool for Cloud Data Center Operations,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491704,10.1145/2486001.2491704,"The emergence of cloud computing has promoted growing demand and rapid deployment of data centers. However, data center operations require a set of sophisticated skills (e.g., command-line-interface), resulting in a high operational cost. In this demo, to reduce the data center operational cost, we design and build a novel cloud data center management system, based on the concept of 3D gamification. In particular, we apply data visualization techniques to overlay operational status upon a data center 3D model, allowing the operators to monitor the real-time situation and control the data center from a friendly user interface. This demo highlights: (1)a data center 3D view from a First Person Shooter (FPS) camera, (2)a run-time presentation of visualized infrastructures information. Moreover, to improve the user experience, we employ cutting-edge HCI technologies from multi-touch, for remote access to Cloud3DView.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,499–500,2,"data visualization, data center operation","Hong Kong, China",SIGCOMM '13,,,,,,
846,article,"Awara, Karim and Jamjoom, Hani and Kanlis, Panos","To 4,000 Compute Nodes and beyond: Network-Aware Vertex Placement in Large-Scale Graph Processing Systems",2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491726,10.1145/2534169.2491726,"The explosive growth of ""big data"" is giving rise to a new breed of large scale graph systems, such as Pregel. This poster describes our ongoing work in characterizing and minimizing the communication cost of Bulk Synchronous Parallel (BSP) graph mining systems, like Pregel, when scaling to 4,096 compute nodes. Existing implementations generally assume a fixed communication cost. This is sufficient in small deployments as the BSP programming model (i.e., overlapping computation and communication) masks small variations in the underlying network. In large scale deployments, such variations can dominate the overall runtime characteristics. In this poster, we first quantify the impact of network communication on the total compute time of a Pregel system. We then propose an efficient vertex placement strategy that subsamples highly connected vertices and applies the Reverse Cuthill-McKee (RCM) algorithm to efficiently partition the input graph and place partitions closer to each other based on their expected communication patterns. We finally describe a vertex replication strategy to further reduce communication overhead.",,501–502,2,"extreme scaling, network topology, vertex placement, bulk synchronous parallel, graph mining systems",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
847,inproceedings,"Awara, Karim and Jamjoom, Hani and Kanlis, Panos","To 4,000 Compute Nodes and beyond: Network-Aware Vertex Placement in Large-Scale Graph Processing Systems",2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491726,10.1145/2486001.2491726,"The explosive growth of ""big data"" is giving rise to a new breed of large scale graph systems, such as Pregel. This poster describes our ongoing work in characterizing and minimizing the communication cost of Bulk Synchronous Parallel (BSP) graph mining systems, like Pregel, when scaling to 4,096 compute nodes. Existing implementations generally assume a fixed communication cost. This is sufficient in small deployments as the BSP programming model (i.e., overlapping computation and communication) masks small variations in the underlying network. In large scale deployments, such variations can dominate the overall runtime characteristics. In this poster, we first quantify the impact of network communication on the total compute time of a Pregel system. We then propose an efficient vertex placement strategy that subsamples highly connected vertices and applies the Reverse Cuthill-McKee (RCM) algorithm to efficiently partition the input graph and place partitions closer to each other based on their expected communication patterns. We finally describe a vertex replication strategy to further reduce communication overhead.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,501–502,2,"network topology, bulk synchronous parallel, extreme scaling, vertex placement, graph mining systems","Hong Kong, China",SIGCOMM '13,,,,,,
848,article,"Calder, Matt and Miao, Rui and Zarifis, Kyriakos and Katz-Bassett, Ethan and Yu, Minlan and Padhye, Jitendra","Don't Drop, Detour!",2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491718,10.1145/2534169.2491718,"Today's data centers must support a range of workloads with different demands. While existing approaches handle routine traffic smoothly, ephemeral but intense hotspots cause excessive packet loss and severely degrade performance. This loss occurs even though the congestion is typically highly localized, with spare buffer capacity available at nearby switches.We argue that switches should share buffer capacity to effectively handle this spot congestion without the latency or monetary hit of deploying large buffers at individual switches. We present detour-induced buffer sharing (DIBS), a mechanism that achieves a near lossless network without requiring additional buffers. Using DIBS, a congested switch detours packets randomly to neighboring switches to avoid dropping the packets. We implement DIBS in hardware, on software routers in a testbed, and in simulation, and we demonstrate that it reduces the 99th percentile of query completion time by 85%, with very little impact on background traffic.",,503–504,2,"data center, buffers, packet loss",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
849,inproceedings,"Calder, Matt and Miao, Rui and Zarifis, Kyriakos and Katz-Bassett, Ethan and Yu, Minlan and Padhye, Jitendra","Don't Drop, Detour!",2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491718,10.1145/2486001.2491718,"Today's data centers must support a range of workloads with different demands. While existing approaches handle routine traffic smoothly, ephemeral but intense hotspots cause excessive packet loss and severely degrade performance. This loss occurs even though the congestion is typically highly localized, with spare buffer capacity available at nearby switches.We argue that switches should share buffer capacity to effectively handle this spot congestion without the latency or monetary hit of deploying large buffers at individual switches. We present detour-induced buffer sharing (DIBS), a mechanism that achieves a near lossless network without requiring additional buffers. Using DIBS, a congested switch detours packets randomly to neighboring switches to avoid dropping the packets. We implement DIBS in hardware, on software routers in a testbed, and in simulation, and we demonstrate that it reduces the 99th percentile of query completion time by 85%, with very little impact on background traffic.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,503–504,2,"data center, buffers, packet loss","Hong Kong, China",SIGCOMM '13,,,,,,
850,article,"Chen, Bo-Si and Lin, Kate Ching-Ju and Wei, Hung-Yu",Harnessing Receive Diversity in Distributed Multi-User MIMO Networks,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491720,10.1145/2534169.2491720,"In existing multiuser MIMO (MU-MIMO) MAC protocols, a multi-antenna node sends as many concurrent streams as possible once it wins the contention. Though such a scheme allows nodes to utilize the multiplex gain of a MIMO system, it however fails to leverage receive diversity gains provided by multiple receive antennas across nodes. We introduce Multiplex-Diversity Medium Access (MDMA), a MU-MIMO MAC protocol that achieves both the multiplex gain and the receive diversity gain at the same time. Instead of letting a node pair use all the available degrees of freedom, MDMA allows as many contending node pairs to communicate concurrently as possible and share all the degrees of freedom. It hence can exploit the antennas equipped on different receivers to further provide some of concurrent streams more receive diversity, without losing the achievable multiplex gain. We implement a prototype on software radios to demonstrate the throughput gain of MDMA.",,505–506,2,"diversity gain, multi-user mimo, medium access control",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
851,inproceedings,"Chen, Bo-Si and Lin, Kate Ching-Ju and Wei, Hung-Yu",Harnessing Receive Diversity in Distributed Multi-User MIMO Networks,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491720,10.1145/2486001.2491720,"In existing multiuser MIMO (MU-MIMO) MAC protocols, a multi-antenna node sends as many concurrent streams as possible once it wins the contention. Though such a scheme allows nodes to utilize the multiplex gain of a MIMO system, it however fails to leverage receive diversity gains provided by multiple receive antennas across nodes. We introduce Multiplex-Diversity Medium Access (MDMA), a MU-MIMO MAC protocol that achieves both the multiplex gain and the receive diversity gain at the same time. Instead of letting a node pair use all the available degrees of freedom, MDMA allows as many contending node pairs to communicate concurrently as possible and share all the degrees of freedom. It hence can exploit the antennas equipped on different receivers to further provide some of concurrent streams more receive diversity, without losing the achievable multiplex gain. We implement a prototype on software radios to demonstrate the throughput gain of MDMA.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,505–506,2,"multi-user mimo, medium access control, diversity gain","Hong Kong, China",SIGCOMM '13,,,,,,
852,article,"Grosvenor, Matthew P. and Schwarzkopf, Malte and Moore, Andrew W.","R2D2: Bufferless, Switchless Data Center Networks Using Commodity Ethernet Hardware",2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491722,10.1145/2534169.2491722,"Modern data centers commonly run distributed applications that require low-latency communication, and whose performance is critical to service revenue. If as little as one machine in 10,000 is a latency outlier, around 18% of requests will experience high latency. The sacrifice of latency determinism for bandwidth, however, is not an inevitable one. In our R2D2 architecture, we conceptually split the data centre network into an unbuffered, unswitched low-latency network (LLNet) and a deeply buffered bandwidth centric network (BBNet). Through explicitly scheduling network multiplexing in software, our prototype implementation achieves 99.995% and 99.999% messaging latencies of 35us and 75us respectively for 1514-byte packets on a fully loaded network. Furthermore, we show that it is possible to merge the conceptually separate LLNet and BBNet networks onto the same physical infrastructure using commodity switched Ethernet hardware.",,507–508,2,"scheduling, ethernet, broadcast, latency, data centers",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
853,inproceedings,"Grosvenor, Matthew P. and Schwarzkopf, Malte and Moore, Andrew W.","R2D2: Bufferless, Switchless Data Center Networks Using Commodity Ethernet Hardware",2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491722,10.1145/2486001.2491722,"Modern data centers commonly run distributed applications that require low-latency communication, and whose performance is critical to service revenue. If as little as one machine in 10,000 is a latency outlier, around 18% of requests will experience high latency. The sacrifice of latency determinism for bandwidth, however, is not an inevitable one. In our R2D2 architecture, we conceptually split the data centre network into an unbuffered, unswitched low-latency network (LLNet) and a deeply buffered bandwidth centric network (BBNet). Through explicitly scheduling network multiplexing in software, our prototype implementation achieves 99.995% and 99.999% messaging latencies of 35us and 75us respectively for 1514-byte packets on a fully loaded network. Furthermore, we show that it is possible to merge the conceptually separate LLNet and BBNet networks onto the same physical infrastructure using commodity switched Ethernet hardware.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,507–508,2,"broadcast, data centers, ethernet, latency, scheduling","Hong Kong, China",SIGCOMM '13,,,,,,
854,article,"Hua, Yu and Liu, Xue and Feng, Dan",Smart In-Network Deduplication for Storage-Aware SDN,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491714,10.1145/2534169.2491714,"In order to efficiently handle the rapid growth of data and reduce the overhead of network transmission, we propose an in-network deduplication for storage-aware Software Defined Network (SDN), called SMIND. Unlike conventional source or destination deduplication schemes, SMIND implements in-network deduplication via SDN. Moreover, to address the performance bottleneck of accessing and indexing SDN controller, we implement an SDN-enabled Flash Translation Layer (FTL) in a real prototype of Solid State Disk (SSD). Experimental results demonstrate the efficiency and efficacy of SMIND.",,509–510,2,"storage systems, software defined network, deduplication",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
855,inproceedings,"Hua, Yu and Liu, Xue and Feng, Dan",Smart In-Network Deduplication for Storage-Aware SDN,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491714,10.1145/2486001.2491714,"In order to efficiently handle the rapid growth of data and reduce the overhead of network transmission, we propose an in-network deduplication for storage-aware Software Defined Network (SDN), called SMIND. Unlike conventional source or destination deduplication schemes, SMIND implements in-network deduplication via SDN. Moreover, to address the performance bottleneck of accessing and indexing SDN controller, we implement an SDN-enabled Flash Translation Layer (FTL) in a real prototype of Solid State Disk (SSD). Experimental results demonstrate the efficiency and efficacy of SMIND.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,509–510,2,"deduplication, storage systems, software defined network","Hong Kong, China",SIGCOMM '13,,,,,,
856,article,"Huang, He and Liao, Xiangke and Li, Shanshan and Peng, Shaoliang and Liu, Xiaodong and Lin, Bin",The Architecture and Traffic Management of Wireless Collaborated Hybrid Data Center Network,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491724,10.1145/2534169.2491724,"This paper introduces a novel wireless collaborated hybrid data center architecture called RF-HYBRID that could optimize the effect of wireless transmission while reduce the complexity of wired network. RF-HYBRID improves throughput and packet delivery latency through flexible wireless detours and shortcuts, with a comprehensive routing and congestion control method.",,511–512,2,"data center network, wireless technology",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
857,inproceedings,"Huang, He and Liao, Xiangke and Li, Shanshan and Peng, Shaoliang and Liu, Xiaodong and Lin, Bin",The Architecture and Traffic Management of Wireless Collaborated Hybrid Data Center Network,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491724,10.1145/2486001.2491724,"This paper introduces a novel wireless collaborated hybrid data center architecture called RF-HYBRID that could optimize the effect of wireless transmission while reduce the complexity of wired network. RF-HYBRID improves throughput and packet delivery latency through flexible wireless detours and shortcuts, with a comprehensive routing and congestion control method.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,511–512,2,"wireless technology, data center network","Hong Kong, China",SIGCOMM '13,,,,,,
858,article,"Ion, Mihaela and Zhang, Jianqing and Schooler, Eve M.",Toward Content-Centric Privacy in ICN: Attribute-Based Encryption and Routing,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491717,10.1145/2534169.2491717,"We design a content-centric privacy scheme for Information-Centric Networking (ICN). We enhance ICN's ability to support data confidentiality by introducing attribute-based encryption into ICN and making it specific to the data attributes. Our approach is unusual in that it preserves ICN's goal to decouple publishers and subscribers for greater data accessibility, scalable multiparty communication and efficient data distribution. Inspired by application-layer publish-subscribe, we enable fine-grained access control with more expressive policies. Moreover, we propose an attribute-based routing scheme that offers interest confidentiality. A prototype system is implemented based on CCNx, a popular open source version of ICN, to showcase privacy preservation in Smart Neighborhood and Smart City applications.",,513–514,2,"security, privacy, icn, attribute-based encryption",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
859,inproceedings,"Ion, Mihaela and Zhang, Jianqing and Schooler, Eve M.",Toward Content-Centric Privacy in ICN: Attribute-Based Encryption and Routing,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491717,10.1145/2486001.2491717,"We design a content-centric privacy scheme for Information-Centric Networking (ICN). We enhance ICN's ability to support data confidentiality by introducing attribute-based encryption into ICN and making it specific to the data attributes. Our approach is unusual in that it preserves ICN's goal to decouple publishers and subscribers for greater data accessibility, scalable multiparty communication and efficient data distribution. Inspired by application-layer publish-subscribe, we enable fine-grained access control with more expressive policies. Moreover, we propose an attribute-based routing scheme that offers interest confidentiality. A prototype system is implemented based on CCNx, a popular open source version of ICN, to showcase privacy preservation in Smart Neighborhood and Smart City applications.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,513–514,2,"icn, privacy, attribute-based encryption, security","Hong Kong, China",SIGCOMM '13,,,,,,
860,article,"Jiang, Xiaoke and Bi, Jun",Interest Set Mechanism to Improve the Transport of Named Data Networking,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491723,10.1145/2534169.2491723,"In this paper, we proposal an Interest Set mechanism which aggregate similar Interest packets from same flow to one packet to improve the efficient of transport of NDN. The trick here is to reset lifetime of corresponding PIT entry in the immediate routers every time when valid Data packet is passed by. This mechanism covers the time and space uncertainty of data generating, reduce the cost of maintaining the pipeline and improve the transport of NDN.",,515–516,2,"icn, transport, ndn",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
861,inproceedings,"Jiang, Xiaoke and Bi, Jun",Interest Set Mechanism to Improve the Transport of Named Data Networking,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491723,10.1145/2486001.2491723,"In this paper, we proposal an Interest Set mechanism which aggregate similar Interest packets from same flow to one packet to improve the efficient of transport of NDN. The trick here is to reset lifetime of corresponding PIT entry in the immediate routers every time when valid Data packet is passed by. This mechanism covers the time and space uncertainty of data generating, reduce the cost of maintaining the pipeline and improve the transport of NDN.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,515–516,2,"icn, ndn, transport","Hong Kong, China",SIGCOMM '13,,,,,,
862,article,"Liu, Zhongjin and Li, Yong and Su, Li and Jin, Depeng and Zeng, Lieguang",M2cloud: Software Defined Multi-Site Data Center Network Control Framework for Multi-Tenant,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491725,10.1145/2534169.2491725,"A significant concern for cloud operators is to provide global network performance isolation for concurrent tenants. To address this, we propose M2cloud, a software defined framework providing scalable network control for multi-site data centers (DCs). M2cloud employs two-level controllers with decoupled functions, providing each tenant with flexible virtualization support in both intra- and inter-DC networks.",,517–518,2,"multi-site, multi-tenant, sdn, data center networks",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
863,inproceedings,"Liu, Zhongjin and Li, Yong and Su, Li and Jin, Depeng and Zeng, Lieguang",M2cloud: Software Defined Multi-Site Data Center Network Control Framework for Multi-Tenant,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491725,10.1145/2486001.2491725,"A significant concern for cloud operators is to provide global network performance isolation for concurrent tenants. To address this, we propose M2cloud, a software defined framework providing scalable network control for multi-site data centers (DCs). M2cloud employs two-level controllers with decoupled functions, providing each tenant with flexible virtualization support in both intra- and inter-DC networks.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,517–518,2,"sdn, multi-site, data center networks, multi-tenant","Hong Kong, China",SIGCOMM '13,,,,,,
864,article,"Mai, Luo and Rupprecht, Lukas and Costa, Paolo and Migliavacca, Matteo and Pietzuch, Peter and Wolf, Alexander L.",Supporting Application-Specific in-Network Processing in Data Centres,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491733,10.1145/2534169.2491733,,,519–520,2,"in-network processing, data centres, network as a service",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
865,inproceedings,"Mai, Luo and Rupprecht, Lukas and Costa, Paolo and Migliavacca, Matteo and Pietzuch, Peter and Wolf, Alexander L.",Supporting Application-Specific in-Network Processing in Data Centres,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491733,10.1145/2486001.2491733,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,519–520,2,"data centres, network as a service, in-network processing","Hong Kong, China",SIGCOMM '13,,,,,,
866,article,"Ming, Zhongxing and Xu, Mingwei and Wang, Dan",In-Network Caching Assisted Wireless AP Storage Management: Challenges and Algorithms,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491706,10.1145/2534169.2491706,"The goal of this paper is to improve wireless AP caching by leveraging in-network caching. We observe that by treating routers as an in-network storage extension, we can relieve the storage limitation of APs. The unique challenge is that APs and routers cannot have a full collaboration, which makes the problem different from traditional cooperative caching problems. We study how APs can optimize caching decisions by using in-network caching information without controlling routers.",,521–522,2,"wireless caching, algorithm, information-centric networking",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
867,inproceedings,"Ming, Zhongxing and Xu, Mingwei and Wang, Dan",In-Network Caching Assisted Wireless AP Storage Management: Challenges and Algorithms,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491706,10.1145/2486001.2491706,"The goal of this paper is to improve wireless AP caching by leveraging in-network caching. We observe that by treating routers as an in-network storage extension, we can relieve the storage limitation of APs. The unique challenge is that APs and routers cannot have a full collaboration, which makes the problem different from traditional cooperative caching problems. We study how APs can optimize caching decisions by using in-network caching information without controlling routers.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,521–522,2,"wireless caching, algorithm, information-centric networking","Hong Kong, China",SIGCOMM '13,,,,,,
868,article,"Pupatwibul, Pakawat and Banjar, Ameen and Braun, Robin",Using DAIM as a Reactive Interpreter for Openflow Networks to Enable Autonomic Functionality,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491721,10.1145/2534169.2491721,"OpenFlow is the first standardization of Software Defined Networks. OpenFlow approach, however, has number of limitations: it restricts its use within a single-domain, it is not scalable, and it does not adapt well to changes in local environments. We evaluate the number of approaches to solve these limitations, and propose DAIM model (Distributed Active information Model) which can be integrated into the OpenFlow structure at the level of the switches to provide a reactive interpreter that will manage the flow tables autonomically.",,523–524,2,"distributed systems, autonomic functionality, openflow",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
869,inproceedings,"Pupatwibul, Pakawat and Banjar, Ameen and Braun, Robin",Using DAIM as a Reactive Interpreter for Openflow Networks to Enable Autonomic Functionality,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491721,10.1145/2486001.2491721,"OpenFlow is the first standardization of Software Defined Networks. OpenFlow approach, however, has number of limitations: it restricts its use within a single-domain, it is not scalable, and it does not adapt well to changes in local environments. We evaluate the number of approaches to solve these limitations, and propose DAIM model (Distributed Active information Model) which can be integrated into the OpenFlow structure at the level of the switches to provide a reactive interpreter that will manage the flow tables autonomically.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,523–524,2,"autonomic functionality, distributed systems, openflow","Hong Kong, China",SIGCOMM '13,,,,,,
870,article,"Roy, Swati and Feamster, Nick",Characterizing Correlated Latency Anomalies in Broadband Access Networks,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491734,10.1145/2534169.2491734,"The growing prevalence of broadband Internet access around the world has made understanding the performance and reliability of broadband access networks extremely important. To better understand the performance anomalies that arise in broadband access networks, we have deployed hundreds of routers in home broadband access networks around the world and are studying the performance of these networks. One of the performance pathologies that we have observed is correlated, sudden latency increases simultaneously and to multiple destinations. In this work, we provide an preliminary glimpse into these sudden latency increases and attempt to understand their causes. Although we do not isolate root cause in this study, observing the sets of destinations that experience correlated latency increases can provide important clues as to the locations in the network that may be inducing these pathologies. We present an algorithm to better identify the network locations that are likely responsible for these pathologies. We then analyze latency data from one month across our home router deployment to determine where in the network latency issues are arising, and how those pathologies differ across regions, ISPs, and countries. Our preliminary analysis suggests that most latency pathologies are to a single destination and a relatively small percentage of these pathologies are likely in the last mile, suggesting that peering within the network may be a more likely culprit for these pathologies than access link problems.",,525–526,2,"measurement, performance, active probing",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
871,inproceedings,"Roy, Swati and Feamster, Nick",Characterizing Correlated Latency Anomalies in Broadband Access Networks,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491734,10.1145/2486001.2491734,"The growing prevalence of broadband Internet access around the world has made understanding the performance and reliability of broadband access networks extremely important. To better understand the performance anomalies that arise in broadband access networks, we have deployed hundreds of routers in home broadband access networks around the world and are studying the performance of these networks. One of the performance pathologies that we have observed is correlated, sudden latency increases simultaneously and to multiple destinations. In this work, we provide an preliminary glimpse into these sudden latency increases and attempt to understand their causes. Although we do not isolate root cause in this study, observing the sets of destinations that experience correlated latency increases can provide important clues as to the locations in the network that may be inducing these pathologies. We present an algorithm to better identify the network locations that are likely responsible for these pathologies. We then analyze latency data from one month across our home router deployment to determine where in the network latency issues are arising, and how those pathologies differ across regions, ISPs, and countries. Our preliminary analysis suggests that most latency pathologies are to a single destination and a relatively small percentage of these pathologies are likely in the last mile, suggesting that peering within the network may be a more likely culprit for these pathologies than access link problems.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,525–526,2,"active probing, measurement, performance","Hong Kong, China",SIGCOMM '13,,,,,,
872,article,"Ruchansky, Natali and Proserpio, Davide",A (Not) NICE Way to Verify the Openflow Switch Specification: Formal Modelling of the Openflow Switch Using Alloy,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491711,10.1145/2534169.2491711,,,527–528,2,"openflow switch, alloy",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
873,inproceedings,"Ruchansky, Natali and Proserpio, Davide",A (Not) NICE Way to Verify the Openflow Switch Specification: Formal Modelling of the Openflow Switch Using Alloy,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491711,10.1145/2486001.2491711,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,527–528,2,"openflow switch, alloy","Hong Kong, China",SIGCOMM '13,,,,,,
874,article,"Shen, Wei-Liang and Lin, Kate Ching-Ju and Chen, Ming-Syan",An Empirical Study of Analog Channel Feedback,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491716,10.1145/2534169.2491716,"Exchanging the channel state information (CSI) in a multiuser WLAN is considered an extremely expensive overhead. A possible solution to reduce the overhead is to notify the analog value of the CSI, which is also known as analog channel feedback. It however only allows nodes to overhear an imperfect channel information. While some previous studies have theoretically analyzed the performance of analog channel feedback, this work aims at addressing issues of realizing it in practice and empirically demonstrating its effectiveness. Our prototype implementation using USRP-N200 shows that analog channel feedback produces a small error comparable to that of estimating CSI using reciprocity, but however can be applied to more general scenarios.",,529–530,2,"analog channel feedback, mimo",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
875,inproceedings,"Shen, Wei-Liang and Lin, Kate Ching-Ju and Chen, Ming-Syan",An Empirical Study of Analog Channel Feedback,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491716,10.1145/2486001.2491716,"Exchanging the channel state information (CSI) in a multiuser WLAN is considered an extremely expensive overhead. A possible solution to reduce the overhead is to notify the analog value of the CSI, which is also known as analog channel feedback. It however only allows nodes to overhear an imperfect channel information. While some previous studies have theoretically analyzed the performance of analog channel feedback, this work aims at addressing issues of realizing it in practice and empirically demonstrating its effectiveness. Our prototype implementation using USRP-N200 shows that analog channel feedback produces a small error comparable to that of estimating CSI using reciprocity, but however can be applied to more general scenarios.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,529–530,2,"analog channel feedback, mimo","Hong Kong, China",SIGCOMM '13,,,,,,
876,article,"Wang, Kai and Li, Jun",Towards Fast Regular Expression Matching in Practice,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491705,10.1145/2534169.2491705,"Regular expression matching is popular in today's network devices with deep inspection function, but due to lack of algorithmic scalability, it is still the performance bottleneck in practical network processing. To address this problem, our method first partition regular expression patterns into simple segments to avoid state explosion, and then compile these segments into a compact data structure to achieve fast matching. Preliminary experiments illustrate that our matching engine scales linearly with the size of the real-world pattern set, and outperforms state-of-the-art solutions.",,531–532,2,"dfa, regular expression matching, deep inspection",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
877,inproceedings,"Wang, Kai and Li, Jun",Towards Fast Regular Expression Matching in Practice,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491705,10.1145/2486001.2491705,"Regular expression matching is popular in today's network devices with deep inspection function, but due to lack of algorithmic scalability, it is still the performance bottleneck in practical network processing. To address this problem, our method first partition regular expression patterns into simple segments to avoid state explosion, and then compile these segments into a compact data structure to achieve fast matching. Preliminary experiments illustrate that our matching engine scales linearly with the size of the real-world pattern set, and outperforms state-of-the-art solutions.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,531–532,2,"deep inspection, dfa, regular expression matching","Hong Kong, China",SIGCOMM '13,,,,,,
878,article,"Wang, Liang and Bayhan, Suzan and Kangasharju, Jussi",Cooperation Policies for Efficient In-Network Caching,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491713,10.1145/2534169.2491713,"Caching is a key component of information-centric networking, but most of the work in the area focuses on simple en-route caching with limited cooperation between the caches. In this paper we model cache cooperation under a game theoretical framework and show how cache cooperation policy can allow the system to converge to a Pareto optimal configuration. Our work shows how cooperation impacts network caching performance and how it takes advantage of the structural properties of the underlying network.",,533–534,2,"in-network caching, game theory, cooperative caching",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
879,inproceedings,"Wang, Liang and Bayhan, Suzan and Kangasharju, Jussi",Cooperation Policies for Efficient In-Network Caching,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491713,10.1145/2486001.2491713,"Caching is a key component of information-centric networking, but most of the work in the area focuses on simple en-route caching with limited cooperation between the caches. In this paper we model cache cooperation under a game theoretical framework and show how cache cooperation policy can allow the system to converge to a Pareto optimal configuration. Our work shows how cooperation impacts network caching performance and how it takes advantage of the structural properties of the underlying network.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,533–534,2,"in-network caching, cooperative caching, game theory","Hong Kong, China",SIGCOMM '13,,,,,,
880,article,"Wang, Sen and Bi, Jun and Wu, Jianping",Collaborative Caching Based on Hash-Routing for Information-Centric Networking,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491727,10.1145/2534169.2491727,,,535–536,2,"information-centric networking, collaborative caching, hash-routing",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
881,inproceedings,"Wang, Sen and Bi, Jun and Wu, Jianping",Collaborative Caching Based on Hash-Routing for Information-Centric Networking,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491727,10.1145/2486001.2491727,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,535–536,2,"hash-routing, collaborative caching, information-centric networking","Hong Kong, China",SIGCOMM '13,,,,,,
882,article,"Wang, Tianyi and Wang, Gang and Li, Xing and Zheng, Haitao and Zhao, Ben Y.",Characterizing and Detecting Malicious Crowdsourcing,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491719,10.1145/2534169.2491719,"Popular Internet services in recent years have shown that remarkable things can be achieved by harnessing the power of the masses. However, crowd-sourcing systems also pose a real challenge to existing security mechanisms deployed to protect Internet services, particularly those tools that identify malicious activity by detecting activities of automated programs such as CAPTCHAs.In this work, we leverage access to two large crowdturfing sites to gather a large corpus of ground-truth data generated by crowdturfing campaigns. We compare and contrast this data with ""organic"" content generated by normal users to identify unique characteristics and potential signatures for use in real-time detectors. This poster describes first steps taken focused on crowdturfing campaigns targeting the Sina Weibo microblogging system. We describe our methodology, our data (over 290K campaigns, 34K worker accounts, 61 million tweets...), and some initial results.",,537–538,2,"user behavior, malicious crowdsourcing, crowdturfing",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
883,inproceedings,"Wang, Tianyi and Wang, Gang and Li, Xing and Zheng, Haitao and Zhao, Ben Y.",Characterizing and Detecting Malicious Crowdsourcing,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491719,10.1145/2486001.2491719,"Popular Internet services in recent years have shown that remarkable things can be achieved by harnessing the power of the masses. However, crowd-sourcing systems also pose a real challenge to existing security mechanisms deployed to protect Internet services, particularly those tools that identify malicious activity by detecting activities of automated programs such as CAPTCHAs.In this work, we leverage access to two large crowdturfing sites to gather a large corpus of ground-truth data generated by crowdturfing campaigns. We compare and contrast this data with ""organic"" content generated by normal users to identify unique characteristics and potential signatures for use in real-time detectors. This poster describes first steps taken focused on crowdturfing campaigns targeting the Sina Weibo microblogging system. We describe our methodology, our data (over 290K campaigns, 34K worker accounts, 61 million tweets...), and some initial results.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,537–538,2,"user behavior, crowdturfing, malicious crowdsourcing","Hong Kong, China",SIGCOMM '13,,,,,,
884,article,"Wang, Xiang and Chen, Chang and Li, Jun",Replication Free Rule Grouping for Packet Classification,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491709,10.1145/2534169.2491709,"Most recent works demonstrate that grouping methodology could bring significant reduction of memory usage to decision-tree packet classification algorithms, with insignificant impact on throughput. However, these grouping techniques can hardly eliminate rule-replication completely. This work proposes a novel rule grouping algorithm without any replication. At each space decomposition step, all rules projecting on the split dimension form the maximum number of non-overlapped ranges, which guarantees the modest memory usage and grouping speed. Evaluation shows that the proposed algorithm achieves comparable memory size with less pre-processing time.",,539–540,2,"packet classification, rule replication, algorithms",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
885,inproceedings,"Wang, Xiang and Chen, Chang and Li, Jun",Replication Free Rule Grouping for Packet Classification,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491709,10.1145/2486001.2491709,"Most recent works demonstrate that grouping methodology could bring significant reduction of memory usage to decision-tree packet classification algorithms, with insignificant impact on throughput. However, these grouping techniques can hardly eliminate rule-replication completely. This work proposes a novel rule grouping algorithm without any replication. At each space decomposition step, all rules projecting on the split dimension form the maximum number of non-overlapped ranges, which guarantees the modest memory usage and grouping speed. Evaluation shows that the proposed algorithm achieves comparable memory size with less pre-processing time.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,539–540,2,"packet classification, algorithms, rule replication","Hong Kong, China",SIGCOMM '13,,,,,,
886,article,"Wette, Philip and Karl, Holger",Which Flows Are Hiding behind My Wildcard Rule? Adding Packet Sampling to Openflow,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491710,10.1145/2534169.2491710,"In OpenFlow, multiple switches share the same control plane which is centralized at what is called the OpenFlow controller. A switch only consists of a forwarding plane. Rules for forwarding individual packets (called flow entries in OpenFlow) are pushed from the controller to the switches.In a network with a high arrival rate of new flows, such as in a data center, the control traffic between the switch and controller can become very high. As a consequence, routing of new flows will be slow. One way to reduce control traffic is to use wildcarded flow entries. Wildcard flow entries can be used to create default routes in the network. However, since switches do not keep track of flows covered by a wildcard flow entry, the controller no longer has knowledge about individual flows. To find out about these individual flows we propose an extension to the current OpenFlow standard to enable packet sampling of wildcard flow entries.",,541–542,2,openflow,,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
887,inproceedings,"Wette, Philip and Karl, Holger",Which Flows Are Hiding behind My Wildcard Rule? Adding Packet Sampling to Openflow,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491710,10.1145/2486001.2491710,"In OpenFlow, multiple switches share the same control plane which is centralized at what is called the OpenFlow controller. A switch only consists of a forwarding plane. Rules for forwarding individual packets (called flow entries in OpenFlow) are pushed from the controller to the switches.In a network with a high arrival rate of new flows, such as in a data center, the control traffic between the switch and controller can become very high. As a consequence, routing of new flows will be slow. One way to reduce control traffic is to use wildcarded flow entries. Wildcard flow entries can be used to create default routes in the network. However, since switches do not keep track of flows covered by a wildcard flow entry, the controller no longer has knowledge about individual flows. To find out about these individual flows we propose an extension to the current OpenFlow standard to enable packet sampling of wildcard flow entries.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,541–542,2,openflow,"Hong Kong, China",SIGCOMM '13,,,,,,
888,article,"Woo, Jiyoung and Kang, Ah Reum and Kim, Huy Kang",The Contagion of Malicious Behaviors in Online Games,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491712,10.1145/2534169.2491712,"This article investigates whether individual users are more likely to display malicious behavior after receiving social reinforcement from friends in their online social networks. We analyze the dynamics of game bot diffusion on the basis of real data supplied by a major massively multiplayer online role-playing game company. We find that the social reinforcement, measured by the ratio of bot friends over total friends, affects the likelihood of game bot adoption and the commitment in terms of usage time.",,543–544,2,"diffusion model, game bot, online game, social contagion",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
889,inproceedings,"Woo, Jiyoung and Kang, Ah Reum and Kim, Huy Kang",The Contagion of Malicious Behaviors in Online Games,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491712,10.1145/2486001.2491712,"This article investigates whether individual users are more likely to display malicious behavior after receiving social reinforcement from friends in their online social networks. We analyze the dynamics of game bot diffusion on the basis of real data supplied by a major massively multiplayer online role-playing game company. We find that the social reinforcement, measured by the ratio of bot friends over total friends, affects the likelihood of game bot adoption and the commitment in terms of usage time.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,543–544,2,"social contagion, online game, game bot, diffusion model","Hong Kong, China",SIGCOMM '13,,,,,,
890,article,"Wu, Zhe and Butkiewicz, Michael and Perkins, Dorian and Katz-Bassett, Ethan and Madhyastha, Harsha V.",CSPAN: Cost-Effective Geo-Replicated Storage Spanning Multiple Cloud Services,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491707,10.1145/2534169.2491707,"Existing cloud computing platforms leave it up to applications to deal with the complexities associated with data replication and propagation across data centers. In our work, we propose the CSPAN key-value store to instead export a unified view of storage services in several geographically distributed data centers. To minimize the cost incurred by application providers, we combine two principles. First, CSPAN spans the data centers of multiple cloud providers. Second, CSPAN judiciously trades off the lower latencies and the higher storage and data propagation costs based on an application's anticipated workload, latency goals, and consistency requirements.",,545–546,2,"storage system;, cloud services, optimization",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
891,inproceedings,"Wu, Zhe and Butkiewicz, Michael and Perkins, Dorian and Katz-Bassett, Ethan and Madhyastha, Harsha V.",CSPAN: Cost-Effective Geo-Replicated Storage Spanning Multiple Cloud Services,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491707,10.1145/2486001.2491707,"Existing cloud computing platforms leave it up to applications to deal with the complexities associated with data replication and propagation across data centers. In our work, we propose the CSPAN key-value store to instead export a unified view of storage services in several geographically distributed data centers. To minimize the cost incurred by application providers, we combine two principles. First, CSPAN spans the data centers of multiple cloud providers. Second, CSPAN judiciously trades off the lower latencies and the higher storage and data propagation costs based on an application's anticipated workload, latency goals, and consistency requirements.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,545–546,2,"cloud services, optimization, storage system;","Hong Kong, China",SIGCOMM '13,,,,,,
892,article,"Xia, Wenfeng and Tsou, Tina and Lopez, Diego R. and Sun, Qiong and Lu, Felix and Xie, Haiyong",A Software Defined Approach to Unified IPv6 Transition,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491715,10.1145/2534169.2491715,"The IPv6 transition has been an ongoing process throughout the world due to the exhaustion of the IPv4 address space. However, this transition leads to costly end-to-end network upgrades and poses new challenges of managing a large number of devices with a variety of transitioning protocols. Recognizing these difficulties, we propose an software defined approach to unifying the deployment of IPv6 in a cost-effective, flexible manner. Our deployment and experiments demonstrate significant benefits of this approach, including low complexity, low cost and high flexibility of adopting different existing transition mechanisms.",,547–548,2,"ipv6 transition, software defined network",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
893,inproceedings,"Xia, Wenfeng and Tsou, Tina and Lopez, Diego R. and Sun, Qiong and Lu, Felix and Xie, Haiyong",A Software Defined Approach to Unified IPv6 Transition,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491715,10.1145/2486001.2491715,"The IPv6 transition has been an ongoing process throughout the world due to the exhaustion of the IPv4 address space. However, this transition leads to costly end-to-end network upgrades and poses new challenges of managing a large number of devices with a variety of transitioning protocols. Recognizing these difficulties, we propose an software defined approach to unifying the deployment of IPv6 in a cost-effective, flexible manner. Our deployment and experiments demonstrate significant benefits of this approach, including low complexity, low cost and high flexibility of adopting different existing transition mechanisms.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,547–548,2,"ipv6 transition, software defined network","Hong Kong, China",SIGCOMM '13,,,,,,
894,article,"Yang, Mao and Li, Yong and Jin, Depeng and Su, Li and Ma, Shaowu and Zeng, Lieguang",OpenRAN: A Software-Defined Ran Architecture via Virtualization,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491732,10.1145/2534169.2491732,"With the rapid growth of the demands for mobile data, wireless network faces several challenges, such as lack of efficient interconnection among heterogeneous wireless networks, and shortage of customized QoS guarantees between services. The fundamental reason for these challenges is that the radio access network (RAN) is closed and ossified. We propose OpenRAN, an architecture for software-defined RAN via virtualization. It achieves complete virtualization and programmability vertically, and benefits the convergence of heterogeneous network horizontally. It provides open, controllable, flexible and evolvable wireless networks.",,549–550,2,"radio access network, wireless virtualization, software-defined network",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
895,inproceedings,"Yang, Mao and Li, Yong and Jin, Depeng and Su, Li and Ma, Shaowu and Zeng, Lieguang",OpenRAN: A Software-Defined Ran Architecture via Virtualization,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491732,10.1145/2486001.2491732,"With the rapid growth of the demands for mobile data, wireless network faces several challenges, such as lack of efficient interconnection among heterogeneous wireless networks, and shortage of customized QoS guarantees between services. The fundamental reason for these challenges is that the radio access network (RAN) is closed and ossified. We propose OpenRAN, an architecture for software-defined RAN via virtualization. It achieves complete virtualization and programmability vertically, and benefits the convergence of heterogeneous network horizontally. It provides open, controllable, flexible and evolvable wireless networks.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,549–550,2,"radio access network, software-defined network, wireless virtualization","Hong Kong, China",SIGCOMM '13,,,,,,
896,article,"Yen, Yu-Chuan and Chu, Cing-Yu and Chen, Chien-Nan and Yeh, Su-Ling and Chu, Hao-Hua and Huang, Polly",Exponential Quantization: User-Centric Rate Control for Skype Calls,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491708,10.1145/2534169.2491708,"As Skype has become popular and a profitable business, the long-standing problem of how to deliver Skype calls deserves a serious revisit from an economic viewpoint. This study proposes a rate control mechanism for Skype calls that satisfies more users and satisfies users more than the greedy-na\""{\i",,551–552,2,"voip, qoe, skype, proportional fairness, rate control",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
897,inproceedings,"Yen, Yu-Chuan and Chu, Cing-Yu and Chen, Chien-Nan and Yeh, Su-Ling and Chu, Hao-Hua and Huang, Polly",Exponential Quantization: User-Centric Rate Control for Skype Calls,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491708,10.1145/2486001.2491708,"As Skype has become popular and a profitable business, the long-standing problem of how to deliver Skype calls deserves a serious revisit from an economic viewpoint. This study proposes a rate control mechanism for Skype calls that satisfies more users and satisfies users more than the greedy-na\""{\i",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,551–552,2,"voip, skype, rate control, qoe, proportional fairness","Hong Kong, China",SIGCOMM '13,,,,,,
898,article,"Zhang, Baobao and Bi, Jun and Wu, Jianping",Making Intra-Domain Traffic Engineering Resistant to Failures,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491730,10.1145/2534169.2491730,,,553–554,2,"traffic engineering, failure recovery",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
899,inproceedings,"Zhang, Baobao and Bi, Jun and Wu, Jianping",Making Intra-Domain Traffic Engineering Resistant to Failures,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491730,10.1145/2486001.2491730,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,553–554,2,"traffic engineering, failure recovery","Hong Kong, China",SIGCOMM '13,,,,,,
900,article,"Zhang, Jingwei",Greedy Forwarding for Mobile Social Networks Embedded in Hyperbolic Spaces,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491728,10.1145/2534169.2491728,"In this work, we design and evaluate a novel greedy forwarding algorithm using metrics in hyperbolic spaces. Hyperbolic geometry has a natural topological reflection of scale-free networks, and greedy algorithm failed in Euclidean space becomes possible in hyperbolic one. We show that mobile social networks can be successfully embedded in such spaces, and obtains competitive performance in terms of message delivery ratio and cost. Under this result, we thus intuitively reveal the fundamental reason that why the famous BUBBLE Rap achieves the optimal performance.",,555–556,2,"mobile social networks, hyperbolic spaces, greedy forwarding",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
901,inproceedings,"Zhang, Jingwei",Greedy Forwarding for Mobile Social Networks Embedded in Hyperbolic Spaces,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491728,10.1145/2486001.2491728,"In this work, we design and evaluate a novel greedy forwarding algorithm using metrics in hyperbolic spaces. Hyperbolic geometry has a natural topological reflection of scale-free networks, and greedy algorithm failed in Euclidean space becomes possible in hyperbolic one. We show that mobile social networks can be successfully embedded in such spaces, and obtains competitive performance in terms of message delivery ratio and cost. Under this result, we thus intuitively reveal the fundamental reason that why the famous BUBBLE Rap achieves the optimal performance.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,555–556,2,"greedy forwarding, mobile social networks, hyperbolic spaces","Hong Kong, China",SIGCOMM '13,,,,,,
902,article,"Zhang, Liang and Wu, Weijie and Wang, Dan",The Effectiveness of Time Dependent Pricing in Controlling Usage Incentives in Wireless Data Network,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491731,10.1145/2534169.2491731,,,557–558,2,"time-dependent bundling, time-dependent metering, stackelberg game",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
903,inproceedings,"Zhang, Liang and Wu, Weijie and Wang, Dan",The Effectiveness of Time Dependent Pricing in Controlling Usage Incentives in Wireless Data Network,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491731,10.1145/2486001.2491731,,Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,557–558,2,"time-dependent metering, time-dependent bundling, stackelberg game","Hong Kong, China",SIGCOMM '13,,,,,,
904,article,"Zhang, Xinggong and Niu, Tong and Lao, Feng and Guo, Zongming",Topology-Aware Content-Centric Networking,2013,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2534169.2491729,10.1145/2534169.2491729,"Making data the first class entity, Information-Centric Networking (ICN) replaces conventional host-to-host model with content sharing model. However, the huge amount of content and the volatility of replicas cached across the Internet pose significant challenges for addressing content only by name. In this paper, we propose a topology-aware name-based routing protocol which combines the benefits of location-oriented routing and content-centric routing together. We adopt a URL-like naming scheme, which defines register locations and content identifier. Node with copies sends Register messages towards a register using location-oriented routing protocols. All en-path routers record forwarding entries in forwarding table (FIB) as the ""bread crumb"" to this content. Following the bread crumb, routers know the ""best"" topology path to the available copies. An Interest is either forwarded towards a ""known"" copy by the content identifier, or towards the register nodes where it would find the bread crumb to the ""best"" copies. Compared with the existing flooding or name resolution methods, Our design shows a good potential in terms of scalability, availability and overhead.",,559–560,2,"topology-aware fib, name-based routing, url-like naming, distributed registration, information-centric networking",,,October 2013,43,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
905,inproceedings,"Zhang, Xinggong and Niu, Tong and Lao, Feng and Guo, Zongming",Topology-Aware Content-Centric Networking,2013,9781450320566,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2486001.2491729,10.1145/2486001.2491729,"Making data the first class entity, Information-Centric Networking (ICN) replaces conventional host-to-host model with content sharing model. However, the huge amount of content and the volatility of replicas cached across the Internet pose significant challenges for addressing content only by name. In this paper, we propose a topology-aware name-based routing protocol which combines the benefits of location-oriented routing and content-centric routing together. We adopt a URL-like naming scheme, which defines register locations and content identifier. Node with copies sends Register messages towards a register using location-oriented routing protocols. All en-path routers record forwarding entries in forwarding table (FIB) as the ""bread crumb"" to this content. Following the bread crumb, routers know the ""best"" topology path to the available copies. An Interest is either forwarded towards a ""known"" copy by the content identifier, or towards the register nodes where it would find the bread crumb to the ""best"" copies. Compared with the existing flooding or name resolution methods, Our design shows a good potential in terms of scalability, availability and overhead.",Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM,559–560,2,"distributed registration, information-centric networking, name-based routing, url-like naming, topology-aware fib","Hong Kong, China",SIGCOMM '13,,,,,,
906,inproceedings,"Kompella, Ramana",Session Details: Middleboxes and Middleware,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3259301,10.1145/3259301,,"Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",,,,"Helsinki, Finland",SIGCOMM '12,,,,,,
907,inproceedings,"Ghodsi, Ali and Sekar, Vyas and Zaharia, Matei and Stoica, Ion",Multi-Resource Fair Queueing for Packet Processing,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342358,10.1145/2342356.2342358,"Middleboxes are ubiquitous in today's networks and perform a variety of important functions, including IDS, VPN, firewalling, and WAN optimization. These functions differ vastly in their requirements for hardware resources (e.g., CPU cycles and memory bandwidth). Thus, depending on the functions they go through, different flows can consume different amounts of a middlebox's resources. While there is much literature on weighted fair sharing of link bandwidth to isolate flows, it is unclear how to schedule multiple resources in a middlebox to achieve similar guarantees. In this paper, we analyze several natural packet scheduling algorithms for multiple resources and show that they have undesirable properties. We propose a new algorithm, Dominant Resource Fair Queuing (DRFQ), that retains the attractive properties that fair sharing provides for one resource. In doing so, we generalize the concept of virtual time in classical fair queuing to multi-resource settings. The resulting algorithm is also applicable in other contexts where several resources need to be multiplexed in the time domain.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",1–12,12,"middleboxes, fairness, scheduling, fair queueing","Helsinki, Finland",SIGCOMM '12,,,,,,
908,inproceedings,"Sherry, Justine and Hasan, Shaddi and Scott, Colin and Krishnamurthy, Arvind and Ratnasamy, Sylvia and Sekar, Vyas",Making Middleboxes Someone Else's Problem: Network Processing as a Cloud Service,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342359,10.1145/2342356.2342359,"Modern enterprises almost ubiquitously deploy middlebox processing services to improve security and performance in their networks. Despite this, we find that today's middlebox infrastructure is expensive, complex to manage, and creates new failure modes for the networks that use them. Given the promise of cloud computing to decrease costs, ease management, and provide elasticity and fault-tolerance, we argue that middlebox processing can benefit from outsourcing the cloud. Arriving at a feasible implementation, however, is challenging due to the need to achieve functional equivalence with traditional middlebox deployments without sacrificing performance or increasing network complexity.In this paper, we motivate, design, and implement APLOMB, a practical service for outsourcing enterprise middlebox processing to the cloud.Our discussion of APLOMB is data-driven, guided by a survey of 57 enterprise networks, the first large-scale academic study of middlebox deployment. We show that APLOMB solves real problems faced by network administrators, can outsource over 90% of middlebox hardware in a typical large enterprise network, and, in a case study of a real enterprise, imposes an average latency penalty of 1.1ms and median bandwidth inflation of 3.8%.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",13–24,12,"outsourcing, middlebox, cloud","Helsinki, Finland",SIGCOMM '12,,,,,,
909,inproceedings,"Escriva, Robert and Wong, Bernard and Sirer, Emin G\""{u","HyperDex: A Distributed, Searchable Key-Value Store",2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342360,10.1145/2342356.2342360,"Distributed key-value stores are now a standard component of high-performance web services and cloud computing applications. While key-value stores offer significant performance and scalability advantages compared to traditional databases, they achieve these properties through a restricted API that limits object retrieval---an object can only be retrieved by the (primary and only) key under which it was inserted. This paper presents HyperDex, a novel distributed key-value store that provides a unique search primitive that enables queries on secondary attributes. The key insight behind HyperDex is the concept of hyperspace hashing in which objects with multiple attributes are mapped into a multidimensional hyperspace. This mapping leads to efficient implementations not only for retrieval by primary key, but also for partially-specified secondary attribute searches and range queries. A novel chaining protocol enables the system to achieve strong consistency, maintain availability and guarantee fault tolerance. An evaluation of the full system shows that HyperDex is 12-13x faster than Cassandra and MongoDB for finding partially specified objects. Additionally, HyperDex achieves 2-4x higher throughput for get/put operations.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",25–36,12,"key-value store, strong consistency, performance, fault-tolerance, nosql","Helsinki, Finland",SIGCOMM '12,,,,,,
910,inproceedings,"Crowcroft, Jon",Session Details: Wireless Communication,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3259302,10.1145/3259302,,"Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",,,,"Helsinki, Finland",SIGCOMM '12,,,,,,
911,inproceedings,"Hong, Steven S. and Mehlman, Jeffrey and Katti, Sachin",Picasso: Flexible RF and Spectrum Slicing,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342362,10.1145/2342356.2342362,"This paper presents the design, implementation and evaluation of Picasso, a novel radio design that allows simultaneous transmission and reception on separate and arbitrary spectrum fragments using a single RF frontend and antenna. Picasso leverages this capability to flexibly partition fragmented spectrum into multiple slices that share the RF frontend and antenna, yet operate concurrent and independent PHY/MAC protocols. We show how this capability provides a general and clean abstraction to exploit fragmented spectrum in WiFi networks, handle coexistence in dense deployments as well as many other applications. We prototype Picasso, and demonstrate experimentally that a Picasso radio partitioned into four slices, each concurrently operating four standard WiFi OFDM PHY and CSMA MAC stacks, can achieve the same sum throughput as four physically separate radios individually configured to operate on the spectrum fragments. We also demonstrate experimentally how Picasso's slicing abstraction provides a clean mechanism to enable multiple diverse networks to coexist and achieve higher throughput, better video quality and latency than the best known state of the art approaches.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",37–48,12,"interference cancellation, radio virtualization","Helsinki, Finland",SIGCOMM '12,,,,,,
912,inproceedings,"Perry, Jonathan and Iannucci, Peter A. and Fleming, Kermin E. and Balakrishnan, Hari and Shah, Devavrat",Spinal Codes,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342363,10.1145/2342356.2342363,"Spinal codes are a new class of rateless codes that enable wireless networks to cope with time-varying channel conditions in a natural way, without requiring any explicit bit rate selection. The key idea in the code is the sequential application of a pseudo-random hash function to the message bits to produce a sequence of coded symbols for transmission. This encoding ensures that two input messages that differ in even one bit lead to very different coded sequences after the point at which they differ, providing good resilience to noise and bit errors. To decode spinal codes, this paper develops an approximate maximum-likelihood decoder, called the bubble decoder, which runs in time polynomial in the message size and achieves the Shannon capacity over both additive white Gaussian noise (AWGN) and binary symmetric channel (BSC) models. Experimental results obtained from a software implementation of a linear-time decoder show that spinal codes achieve higher throughput than fixed-rate LDPC codes, rateless Raptor codes, and the layered rateless coding approach of Strider, across a range of channel conditions and message sizes. An early hardware prototype that can decode at 10 Mbits/s in FPGA demonstrates that spinal codes are a practical construction.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",49–60,12,"spinal code, practical decoder, capacity, rateless, wireless, channel code","Helsinki, Finland",SIGCOMM '12,,,,,,
913,inproceedings,"Wang, Jue and Hassanieh, Haitham and Katabi, Dina and Indyk, Piotr",Efficient and Reliable Low-Power Backscatter Networks,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342364,10.1145/2342356.2342364,"There is a long-standing vision of embedding backscatter nodes like RFIDs into everyday objects to build ultra-low power ubiquitous networks. A major problem that has challenged this vision is that backscatter communication is neither reliable nor efficient. Backscatter nodes cannot sense each other, and hence tend to suffer from colliding transmissions. Further, they are ineffective at adapting the bit rate to channel conditions, and thus miss opportunities to increase throughput, or transmit above capacity causing errors. This paper introduces a new approach to backscatter communication. The key idea is to treat all nodes as if they were a single virtual sender. One can then view collisions as a code across the bits transmitted by the nodes. By ensuring only a few nodes collide at any time, we make collisions act as a sparse code and decode them using a new customized compressive sensing algorithm. Further, we can make these collisions act as a rateless code to automatically adapt the bit rate to channel quality --i.e., nodes can keep colliding until the base station has collected enough collisions to decode. Results from a network of backscatter nodes communicating with a USRP backscatter base station demonstrate that the new design produces a 3.5\texttimes{","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",61–72,12,"wireless, rfid, compressive sensing, backscatter","Helsinki, Finland",SIGCOMM '12,,,,,,
914,inproceedings,"Steiner, Moritz and Gaglianello, Bob Gaglianello and Gurbani, Vijay and Hilt, Volker and Roome, W.D. and Scharf, Michael and Voith, Thomas",Network-Aware Service Placement in a Distributed Cloud Environment,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342366,10.1145/2342356.2342366,"We consider a system of compute and storage resources geographically distributed over a large number of locations connected via a wide-area network. By distributing the resources, latency to users can be decreased, bandwidth costs reduced and availablility increased. The challenge is to distribute services with varying characteristics among the data centers optimally. Some services are very latency sensitive, others need vast amounts of storage, and yet others are computationally complex but do not require hard deadlines on execution. We propose efficient algorithms for the placement of services to get the maximum benefit from a distributed cloud systems. The algorithms need input on the status of the network, compute resources and data resources, which are matched to application requirements.This demonstration shows how a network-aware cloud can combine all three resource types - computation, storage, and network connectivity - in distributed cloud environments. Our dynamic service placement algorithm monitors the network and data center resources in real-time. Our prototype uses the information gathered to place or migrate services to provide the best user experience for a service.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",73–74,2,"cloud, service placement","Helsinki, Finland",SIGCOMM '12,,,,,,
915,inproceedings,"Gill, Harjot and Lin, Dong and Sarna, Lohit and Mead, Robert and Lee, Kenton C.T. and Loo, Boon Thau",SP4: Scalable Programmable Packet Processing Platform,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342367,10.1145/2342356.2342367,"We propose the demonstration of SP4, a software-based programmable packet processing platform that supports (1) stateful packet processing useful for analyzing traffic flows with session semantics, (2) uses a task-stealing architecture that automatically leverages multi-core processing capabilities in a load-balanced manner without the need for explicit performance profiling, and (3) a declarative language for rapidly specifying and composing new packet processing functionalities from reusable modules. Our demonstration showcases the use of SP4 for performing high-throughput analysis of traffic traces for a variety of applications, such as filtering out unwanted traffic and detection of DDoS attacks using machine learning based analysis.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",75–76,2,"multicore, packet analysis, declarative networking","Helsinki, Finland",SIGCOMM '12,,,,,,
916,inproceedings,"Dandapat, Sourav Kumar and Jain, Sanyam and Choudhury, Romit Roy and Ganguly, Niloy",Distributed Content Storage for Just-in-Time Streaming,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342368,10.1145/2342356.2342368,"We propose a content distribution strategy over municipal WiFi networks where Access Points (APs) collaboratively cache popular multimedia content, and disseminate them in a manner that each mobile device has the portion of the content just-in-time for playback. If successful, we envision that a child will be able to seamlessly watch a movie in a car, as her tablet downloads different parts of the movie over different WiFi APs at different times.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",77–78,2,"municipal wifi network, content distribution, distributed content storage","Helsinki, Finland",SIGCOMM '12,,,,,,
917,inproceedings,"Huici, Felipe and di Pietro, Andrea and Trammell, Brian and Gomez Hidalgo, Jose Maria and Martinez Ruiz, Daniel and d'Heureuse, Nico",Blockmon: A High-Performance Composable Network Traffic Measurement System,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342369,10.1145/2342356.2342369,"Passive network monitoring and data analysis, crucial to the correct operation of networks and the systems that rely on them, has become an increasingly difficult task given continued growth and diversification of the Internet. In this demo we present Blockmon, a novel composable measurement system with the flexibility to allow for a wide range of traffic monitoring and data analysis, as well as the necessary mechanisms to yield high performance on today's modern multi-core hardware. In this demo we use Blockmon's GUI to show how to easily create Blockmon applications and display data exported by them. We present a simple flow meter application and a more involved VoIP nomaly detection one.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",79–80,2,"network monitoring, data processing, high performance","Helsinki, Finland",SIGCOMM '12,,,,,,
918,inproceedings,"Poese, Ingmar and Frank, Benjamin and Knight, Simon and Semmler, Niklas and Smaragdakis, Georgios",PaDIS Emulator: An Emulator to Evaluate CDN-ISP Collaboration,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342370,10.1145/2342356.2342370,"We present PaDIS Emulator, a fully automated platform to evaluate CDN-ISP collaboration for better content delivery, traffic engineering, and cost reduction. The PaDIS Emulator enables researchers as well as CDN and ISP operators to evaluate the benefits of collaboration using their own operational networks, configuration, and cost functions.The PaDIS Emulator consists of three components: the network emulation, the collaboration mechanism, and the performance monitor. These layers provide scalable emulation of the interaction between an ISP or a number of ISPs with multiple CDNs and vice versa. PaDIS Emulator design is flexible in order to implement a wide range of collaboration mechanisms on virtualized or real hardware, and evaluate them before introduction to operational networks.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",81–82,2,"cdn-isp collaboration, traffic engineering","Helsinki, Finland",SIGCOMM '12,,,,,,
919,inproceedings,"Chaudhry, Amir and Madhavapeddy, Anil and Rotsos, Charalampos and Mortier, Richard and Aucinas, Andrius and Crowcroft, Jon and Eide, Sebastian Probst and Hand, Steven and Moore, Andrew W. and Vallina-Rodriguez, Narseo",Signposts: End-to-End Networking in a World of Middleboxes,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342371,10.1145/2342356.2342371,"This demo presents Signposts, a system to provide users with a secure, simple mechanism to establish and maintain communication channels between their personal cloud of named devices. Signpost names exist in the DNSSEC hierarchy, and resolve to secure end-points when accessed by existing DNS clients. Signpost clients intercept user connection intentions while adding privacy and multipath support. Signpost servers co-ordinate clients to dynamically discover routes and overcome the middleboxes that pervade modern edge networks. The demo will show a simple scenario where an individual's personal devices (phone, laptop) are interconnected via Signposts while sitting on different networks behind various middleboxes. As a result they will be able to fetch and push data between each other, demonstrated by, e.g., simple web browsing, even as the network configuration changes.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",83–84,2,"user-centered, middlebox, naming, edge network, dns","Helsinki, Finland",SIGCOMM '12,,,,,,
920,inproceedings,N\'{e,Towards SmartFlow: Case Studies on Enhanced Programmable Forwarding in OpenFlow Switches,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342372,10.1145/2342356.2342372,"The limited capabilities of the switches renders the implementation of unorthodox routing and forwarding mechanisms as a hard task in OpenFlow. Our high level goal is therefore to inspect the possibilities of slightly smartening up the OpenFlow switches. As a first step in this direction we demonstrate (with Bloom filters, greedy routing and network coding) that a very limited computational capability enables us to natively support experimental technologies while preserving performance. We distribute the demos in source files and as a ready-to-experiment VM image to promote further improvements and evaluations.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",85–86,2,"sdn, network coding, greedy routing, openflow, bloom filters","Helsinki, Finland",SIGCOMM '12,,,,,,
921,inproceedings,"Jarschel, Michael and Pries, Rastin",An OpenFlow-Based Energy-Efficient Data Center Approach,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342373,10.1145/2342356.2342373,,"Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",87–88,2,"data center, openflow, energy-efficiency","Helsinki, Finland",SIGCOMM '12,,,,,,
922,inproceedings,"Wang, Anduo and Gurney, Alexander J.T. and Han, Xianglong and Cao, Jinyan and Talcot, Carolyn and Loo, Boon Thau and Scedrov, Andre",Reduction-Based Analysis of BGP Systems with BGPVerif,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342374,10.1145/2342356.2342374,"Today's inter-domain routing protocol, the Border Gateway Protocol (BGP), is increasingly complicated and fragile due to policy misconfiguration by individual autonomous systems (ASes). Existing configuration analysis techniques are either manual and tedious, or do not scale beyond a small number of nodes due to the state explosion problem. To aid the diagnosis of misconfigurations in real-world large BGP systems, this paper presents BGPVerif , a reduction based analysis toolkit. The key idea is to reduce BGP system size prior to analysis while preserving crucial correctness properties. BGPVerif consists of two components, NetReducer that simplifies BGP configurations, and NetAnalyzer that automatically detects routing oscillation. BGPVerif accepts a wide range of BGP configuration inputs ranging from real-world traces (Rocketfuel network topologies), randomly generated BGP networks (GT-ITM), Cisco configuration guidelines, as well as arbitrary user-defined networks. BGPVerif illustrates the applicability, efficiency, and benefits of the reduction technique, it also introduces an infrastructure that enables networking researchers to interact with advanced formal method tool.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",89–90,2,"formal analysis, reduction, border gateway protocol","Helsinki, Finland",SIGCOMM '12,,,,,,
923,inproceedings,"Gurney, Alexander J.T. and Han, Xianglong and Li, Yang and Loo, Boon Thau",Route Shepherd: Stability Hints for the Control Plane,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342375,10.1145/2342356.2342375,"The Route Shepherd tool demonstrates applications of choosing between routing protocol configurations on the basis of rigorously-supported theory. Splitting the configuration space into equivalence classes allows the identification of which parameter combinations lead to protocol stability, and which do not. This ahead-of-time analysis generates a predicate, in the form of a combination of linear integer inequalities, which can be used in several complementary ways by downstream applications. Examples presented include warning operators about errors in advance, recovery from protocol oscillation, plotting a series of safe parameter changes, and understanding the dynamics of the routing system.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",91–92,2,"stable path problems, border gateway protocol, routing policy, partial specification","Helsinki, Finland",SIGCOMM '12,,,,,,
924,inproceedings,"Olteanu, Vladimir Andrei and Raiciu, Costin",Efficiently Migrating Stateful Middleboxes,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342376,10.1145/2342356.2342376,,"Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",93–94,2,"migration, middlebox","Helsinki, Finland",SIGCOMM '12,,,,,,
925,inproceedings,"Farrington, Nathan and Porter, George and Sun, Pang-Chen and Forencich, Alex and Ford, Joseph and Fainman, Yeshaiahu and Papen, George and Vahdat, Amin",A Demonstration of Ultra-Low-Latency Data Center Optical Circuit Switching,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342377,10.1145/2342356.2342377,"We designed and constructed a 24x24-port optical circuit switch (OCS) prototype with a programming time of 68.5 μs, a switching time of 2.8 μs, and a receiver electronics initialization time of 8.7 μs [1]. We demonstrate the operation of this prototype switch in a data center testbed under various workloads.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",95–96,2,"data center networks, optical circuit switching","Helsinki, Finland",SIGCOMM '12,,,,,,
926,inproceedings,"Knight, Simon and Jaboldinov, Askar and Maennel, Olaf and Phillips, Iain and Roughan, Matthew","AutoNetkit: Simplifying Large Scale, Open-Source Network Experimentation",2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342378,10.1145/2342356.2342378,"We present a methodology that brings simplicity to large and complex test labs by using abstraction. The networking community has appreciated the value of large scale test labs to explore complex network interactions, as seen in projects such as PlanetLab, GENI, DETER, Emulab, and SecSI. Virtualization has enabled the creation of many more such labs. However, one problem remains: it is time consuming, tedious and error prone to setup and configure large scale test networks. Separate devices need to be configured in a coordinated way, even in a virtual lab.AutoNetkit, an open source tool, uses abstractions and defaults to achieve both configuration and deployment and create such large-scale virtual labs. This allows researchers and operators to explore new protocols, create complex models of networks and predict consequences of configuration changes. However, our abstractions could also allow the discussion of the broader configuration management problem. Abstractions that currently configure networks in a test lab can, in the future, be employed in configuration management tools for real networks.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",97–98,2,"automated configuration, emulation, network management","Helsinki, Finland",SIGCOMM '12,,,,,,
927,inproceedings,"W\""{a",Bulk of Interest: Performance Measurement of Content-Centric Routing,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342379,10.1145/2342356.2342379,"The paradigm of information-centric networking subsumes recent approaches to integrate content replication services into a future Internet layer. Current concepts foster either a dynamic mapping that directs content requests to a nearby copy, or an immediate routing on content identifiers. In this paper, we evaluate in practical experiments the performance of content routing, which we analyze with a focus on conceptual aspects. Our findings indicate that the performance of the content distribution system is threatened by a heavy management of states that arise from the strong coupling of the control to the data plane in the underlying routing infrastructure.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",99–100,2,"experimental evaluation, performance, routing","Helsinki, Finland",SIGCOMM '12,,,,,,
928,inproceedings,"Twigg, Neil Alexander and Fayed, Marwan and Perkins, Colin and Pezaros, Dimitrios and Tso, Posco",User-Level Data Center Tomography,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342380,10.1145/2342356.2342380,"Measurement and inference in data centers present a set of opportunities and challenges distinct from the Internet domain. Existing toolsets may be perturbed or be mislead by issues related to virtualization. Yet, while equally confronted by scale, data centers are relatively homogenous and symmetric. We believe these may be attributes to be exploited. However, data is required to better evaluate our hypotheses. Therefore, we introduce our efforts to gather data using a single framework from which we can launch tests of our choosing. Our observations reinforce recent claims, but indicate changes in the network. They also reveal additional obfuscations stemming from virtualization.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",101–102,2,"tomography, network measurement, data centers","Helsinki, Finland",SIGCOMM '12,,,,,,
929,inproceedings,"W\""{a",Towards Detecting BGP Route Hijacking Using the RPKI,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342381,10.1145/2342356.2342381,"Prefix hijacking has always been a big concern in the Internet. Some events made it into the international world-news, but most of them remain unreported or even unnoticed. The scale of the problem can only be estimated.The Resource Publication Infrastructure (RPKI) is an effort by the IETF to secure the inter-domain routing system. It includes a formally verifiable way of identifying who owns legitimately which portion of the IP address space. The RPKI has been standardized and prototype implementations are tested by Internet Service Providers (ISPs). Currently the system holds already about 2% of the Internet routing table.Therefore, in theory, it should be easy to detect hijacking of prefixes within that address space. We take an early look at BGP update data and check those updates against the RPKI---in the same way a router would do, once the system goes operational. We find many interesting dynamics, not all can be easily explained as hijacking, but a significant number are likely operational testing or misconfigurations.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",103–104,2,"secure inter-domain routing, bgp, rpki, deployment","Helsinki, Finland",SIGCOMM '12,,,,,,
930,inproceedings,"Wolf, Tilman and Griffioen, James and Calvert, Kenneth L. and Dutta, Rudra and Rouskas, George N. and Baldine, Ilia and Nagurney, Anna",Choice as a Principle in Network Architecture,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342382,10.1145/2342356.2342382,"There has been a great interest in defining a new network architecture that can meet the needs of a future Internet. One of the main challenges in this context is how to realize the many different technical solutions that have developed in recent years in a single coherent architecture. In addition, it is necessary to consider how to ensure economic viability of architecture solutions. In this work, we discuss how to design a network architecture where choices at different layers of the protocol stack are explicitly exposed to users. This approach ensures that innovative technical solutions can be used and rewarded, which is essential to encourage wide deployment of this architecture.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",105–106,2,network architecture; innovation; economics,"Helsinki, Finland",SIGCOMM '12,,,,,,
931,inproceedings,"Fu, Wenliang and Song, Tian",A Frequency Adjustment Architecture for Energy Efficient Router,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342383,10.1145/2342356.2342383,"With the rapid expansion of customer population and link bandwidth, energy expenditures of the Internet have been rising dramatically. To gain energy efficiency, we propose a novel router architecture, which allows each of its modules to adjust frequency according to traffic loads. Several modulation strategies are also discussed to ensure dwell time on low energy states and reduce blind switches. Our preliminary results show that the frequency adjustment router could save up to 40% of the total energy consumption.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",107–108,2,"energy efficient strategy, frequency adjustment, energy efficient router architecture","Helsinki, Finland",SIGCOMM '12,,,,,,
932,inproceedings,"Marchetta, Pietro and de Donato, Walter and Pescap\'{e",Detecting Third-Party Addresses in Traceroute IP Paths,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342384,10.1145/2342356.2342384,"Traceroute is probably the most famous computer networks diagnostic tool, widely adopted for both performance troubleshooting and research. Unfortunately, traceroute is not free of inaccuracies.In this poster, we present our ongoing work to address the inaccuracy caused by third-party addresses.We discuss the impact of third-party addresses on traceroute applications and present a novel active probing technique able to identify such addresses in traceroute traces. Finally, we detail preliminary results suggesting how this phenomenon has been largely underestimated.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",109–110,2,"as-level path, traceroute, internet topology","Helsinki, Finland",SIGCOMM '12,,,,,,
933,inproceedings,"Lee, Changhyun and Jang, Keon and Moon, Sue",Reviving Delay-Based TCP for Data Centers,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342385,10.1145/2342356.2342385,"With the rapid growth of data centers, minimizing the queueing delay at network switches has been one of the key challenges. In this work, we analyze the shortcomings of the current TCP algorithm when used in data center networks, and we propose to use latency-based congestion detection and rate-based transfer to achieve ultra-low queueing delay in data centers.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",111–112,2,"data centers, tcp, latency","Helsinki, Finland",SIGCOMM '12,,,,,,
934,inproceedings,"Liu, Bingyang and Bi, Jun and Yang, Xiaowei",FaaS: Filtering IP Spoofing Traffic as a Service,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342386,10.1145/2342356.2342386,,"Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",113–114,2,"ingress filtering, ip spoofing, economics","Helsinki, Finland",SIGCOMM '12,,,,,,
935,inproceedings,"Katti, Sachin",Session Details: Data Centers: Latency,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3259303,10.1145/3259303,,"Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",,,,"Helsinki, Finland",SIGCOMM '12,,,,,,
936,inproceedings,"Vamanan, Balajee and Hasan, Jahangir and Vijaykumar, T.N.",Deadline-Aware Datacenter Tcp (D2TCP),2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342388,10.1145/2342356.2342388,"An important class of datacenter applications, called Online Data-Intensive (OLDI) applications, includes Web search, online retail, and advertisement. To achieve good user experience, OLDI applications operate under soft-real-time constraints (e.g., 300 ms latency) which imply deadlines for network communication within the applications. Further, OLDI applications typically employ tree-based algorithms which, in the common case, result in bursts of children-to-parent traffic with tight deadlines. Recent work on datacenter network protocols is either deadline-agnostic (DCTCP) or is deadline-aware (D3) but suffers under bursts due to race conditions. Further, D3 has the practical drawbacks of requiring changes to the switch hardware and not being able to coexist with legacy TCP. We propose Deadline-Aware Datacenter TCP (D2TCP), a novel transport protocol, which handles bursts, is deadline-aware, and is readily deployable. In designing D2TCP, we make two contributions: (1) D2TCP uses a distributed and reactive approach for bandwidth allocation which fundamentally enables D2TCP's properties. (2) D2TCP employs a novel congestion avoidance algorithm, which uses ECN feedback and deadlines to modulate the congestion window via a gamma-correction function. Using a small-scale implementation and at-scale simulations, we show that D2TCP reduces the fraction of missed deadlines compared to DCTCP and D3 by 75% and 50%, respectively.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",115–126,12,"datacenter, sla, oldi, cloud services, tcp, deadline, ecn","Helsinki, Finland",SIGCOMM '12,,,,,,
937,inproceedings,"Hong, Chi-Yao and Caesar, Matthew and Godfrey, P. Brighten",Finishing Flows Quickly with Preemptive Scheduling,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342389,10.1145/2342356.2342389,"Today's data centers face extreme challenges in providing low latency. However, fair sharing, a principle commonly adopted in current congestion control protocols, is far from optimal for satisfying latency requirements.We propose Preemptive Distributed Quick (PDQ) flow scheduling, a protocol designed to complete flows quickly and meet flow deadlines. PDQ enables flow preemption to approximate a range of scheduling disciplines. For example, PDQ can emulate a shortest job first algorithm to give priority to the short flows by pausing the contending flows. PDQ borrows ideas from centralized scheduling disciplines and implements them in a fully distributed manner, making it scalable to today's data centers. Further, we develop a multipath version of PDQ to exploit path diversity.Through extensive packet-level and flow-level simulation, we demonstrate that PDQ significantly outperforms TCP, RCP and D3 in data center environments. We further show that PDQ is stable, resilient to packet loss, and preserves nearly all its performance gains even given inaccurate flow information.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",127–138,12,"deadline, flow scheduling, data center","Helsinki, Finland",SIGCOMM '12,,,,,,
938,inproceedings,"Zats, David and Das, Tathagata and Mohan, Prashanth and Borthakur, Dhruba and Katz, Randy",DeTail: Reducing the Flow Completion Time Tail in Datacenter Networks,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342390,10.1145/2342356.2342390,"Web applications have now become so sophisticated that rendering a typical page may require hundreds of intra-datacenter flows. At the same time, web sites must meet strict page creation deadlines of 200-300ms to satisfy user demands for interactivity. Long-tailed flow completion times make it challenging for web sites to meet these constraints. They are forced to choose between rendering a subset of the complex page, or delay its rendering, thus missing deadlines and sacrificing either quality or responsiveness. Either option leads to potential financial loss.In this paper, we present a new cross-layer network stack aimed at reducing the long tail of flow completion times. The approach exploits cross-layer information to reduce packet drops, prioritize latency-sensitive flows, and evenly distribute network load, effectively reducing the long tail of flow completion times. We evaluate our approach through NS-3 based simulation and Click-based implementation demonstrating our ability to consistently reduce the tail across a wide range of workloads. We often achieve reductions of over 50% in 99.9th percentile flow completion times.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",139–150,12,"multi-path, flow statistics, datacenter network","Helsinki, Finland",SIGCOMM '12,,,,,,
939,inproceedings,"Gorinsky, Sergey",Session Details: Measuring Networks,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3259304,10.1145/3259304,,"Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",,,,"Helsinki, Finland",SIGCOMM '12,,,,,,
940,inproceedings,"G\""{u",Inferring Visibility: Who's (Not) Talking to Whom?,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342392,10.1145/2342356.2342392,"Consider this simple question: how can a network operator identify the set of routes that pass through its network? Answering this question is surprisingly hard: BGP only informs an operator about a limited set of routes. By observing traffic, an operator can only conclude that a particular route passes through its network -- but not that a route does not pass through its network. We approach this problem as one of statistical inference, bringing varying levels of additional information to bear: (1) the existence of traffic, and (2) the limited set of publicly available routing tables. We show that the difficulty depends critically on the position of the network in the overall Internet topology, and that the operators with the greatest incentive to solve this problem are those for which the problem is hardest. Nonetheless, we show that suitable application of nonparametric inference techniques can solve this problem quite accurately. For certain networks, traffic existence information yields good accuracy, while for other networks an accurate approach uses the ""distance"" between prefixes, according to a new network distance metric that we define. We then show how solving this problem leads to improved solutions for a particular application: traffic matrix completion.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",151–162,12,"bgp, matrix completion","Helsinki, Finland",SIGCOMM '12,,,,,,
941,inproceedings,"Ager, Bernhard and Chatzis, Nikolaos and Feldmann, Anja and Sarrar, Nadi and Uhlig, Steve and Willinger, Walter",Anatomy of a Large European IXP,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342393,10.1145/2342356.2342393,"The largest IXPs carry on a daily basis traffic volumes in the petabyte range, similar to what some of the largest global ISPs reportedly handle. This little-known fact is due to a few hundreds of member ASes exchanging traffic with one another over the IXP's infrastructure. This paper reports on a first-of-its-kind and in-depth analysis of one of the largest IXPs worldwide based on nine months' worth of sFlow records collected at that IXP in 2011.A main finding of our study is that the number of actual peering links at this single IXP exceeds the number of total AS links of the peer-peer type in the entire Internet known as of 2010! To explain such a surprisingly rich peering fabric, we examine in detail this IXP's ecosystem and highlight the diversity of networks that are members at this IXP and connect there with other member ASes for reasons that are similarly diverse, but can be partially inferred from their business types and observed traffic patterns. In the process, we investigate this IXP's traffic matrix and illustrate what its temporal and structural properties can tell us about the member ASes that generated the traffic in the first place. While our results suggest that these large IXPs can be viewed as a microcosm of the Internet ecosystem itself, they also argue for a re-assessment of the mental picture that our community has about this ecosystem.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",163–174,12,"internet exchange points, traffic characterization, internet topology","Helsinki, Finland",SIGCOMM '12,,,,,,
942,inproceedings,"Dave, Vacha and Guha, Saikat and Zhang, Yin",Measuring and Fingerprinting Click-Spam in Ad Networks,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342394,10.1145/2342356.2342394,"Advertising plays a vital role in supporting free websites and smartphone apps. Click-spam, i.e., fraudulent or invalid clicks on online ads where the user has no actual interest in the advertiser's site, results in advertising revenue being misappropriated by click-spammers. While ad networks take active measures to block click-spam today, the effectiveness of these measures is largely unknown. Moreover, advertisers and third parties have no way of independently estimating or defending against click-spam.In this paper, we take the first systematic look at click-spam. We propose the first methodology for advertisers to independently measure click-spam rates on their ads. We also develop an automated methodology for ad networks to proactively detect different simultaneous click-spam attacks. We validate both methodologies using data from major ad networks. We then conduct a large-scale measurement study of click-spam across ten major ad networks and four types of ads. In the process, we identify and perform in-depth analysis on seven ongoing click-spam attacks not blocked by major ad networks at the time of this writing. Our findings highlight the severity of the click-spam problem, especially for mobile ads.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",175–186,12,"click-spam, click fraud, advertising fraud, traffic quality, invalid clicks","Helsinki, Finland",SIGCOMM '12,,,,,,
943,inproceedings,"Sirer, Emin G\""{u",Session Details: Data Centers: Resource Management,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3259305,10.1145/3259305,,"Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",,,,"Helsinki, Finland",SIGCOMM '12,,,,,,
944,inproceedings,"Popa, Lucian and Kumar, Gautam and Chowdhury, Mosharaf and Krishnamurthy, Arvind and Ratnasamy, Sylvia and Stoica, Ion",FairCloud: Sharing the Network in Cloud Computing,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342396,10.1145/2342356.2342396,"The network, similar to CPU and memory, is a critical and shared resource in the cloud. However, unlike other resources, it is neither shared proportionally to payment, nor do cloud providers offer minimum guarantees on network bandwidth. The reason networks are more difficult to share is because the network allocation of a virtual machine (VM) X depends not only on the VMs running on the same machine with X, but also on the other VMs that X communicates with and the cross-traffic on each link used by X. In this paper, we start from the above requirements--payment proportionality and minimum guarantees--and show that the network-specific challenges lead to fundamental tradeoffs when sharing cloud networks. We then propose a set of properties to explicitly express these tradeoffs. Finally, we present three allocation policies that allow us to navigate the tradeoff space. We evaluate their characteristics through simulation and testbed experiments to show that they can provide minimum guarantees and achieve better proportionality than existing solutions.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",187–198,12,"cloud computing, network sharing","Helsinki, Finland",SIGCOMM '12,,,,,,
945,inproceedings,"Xie, Di and Ding, Ning and Hu, Y. Charlie and Kompella, Ramana",The Only Constant is Change: Incorporating Time-Varying Network Reservations in Data Centers,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342397,10.1145/2342356.2342397,"In multi-tenant datacenters, jobs of different tenants compete for the shared datacenter network and can suffer poor performance and high cost from varying, unpredictable network performance. Recently, several virtual network abstractions have been proposed to provide explicit APIs for tenant jobs to specify and reserve virtual clusters (VC) with both explicit VMs and required network bandwidth between the VMs. However, all of the existing proposals reserve a fixed bandwidth throughout the entire execution of a job.In the paper, we first profile the traffic patterns of several popular cloud applications, and find that they generate substantial traffic during only 30%-60% of the entire execution, suggesting existing simple VC models waste precious networking resources. We then propose a fine-grained virtual network abstraction, Time-Interleaved Virtual Clusters (TIVC), that models the time-varying nature of the networking requirement of cloud applications. To demonstrate the effectiveness of TIVC, we develop Proteus, a system that implements the new abstraction. Using large-scale simulations of cloud application workloads and prototype implementation running actual cloud applications, we show the new abstraction significantly increases the utilization of the entire datacenter and reduces the cost to the tenants, compared to previous fixed-bandwidth abstractions.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",199–210,12,"allocation, datacenter, profiling, network reservation, bandwidth","Helsinki, Finland",SIGCOMM '12,,,,,,
946,inproceedings,"Gao, Peter Xiang and Curtis, Andrew R. and Wong, Bernard and Keshav, Srinivasan",It's Not Easy Being Green,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342398,10.1145/2342356.2342398,"Large-scale Internet applications, such as content distribution networks, are deployed across multiple datacenters and consume massive amounts of electricity. To provide uniformly low access latencies, these datacenters are geographically distributed and the deployment size at each location reflects the regional demand for the application. Consequently, an application's environmental impact can vary significantly depending on the geographical distribution of end-users, as electricity cost and carbon footprint per watt is location specific. In this paper, we describe FORTE: Flow Optimization based framework for request-Routing and Traffic Engineering. FORTE dynamically controls the fraction of user traffic directed to each datacenter in response to changes in both request workload and carbon footprint. It allows an operator to navigate the three-way tradeoff between access latency, carbon footprint, and electricity costs and to determine an optimal datacenter upgrade plan in response to increases in traffic load. We use FORTE to show that carbon taxes or credits are impractical in incentivizing carbon output reduction by providers of large-scale Internet applications. However, they can reduce carbon emissions by 10% without increasing the mean latency nor the electricity bill.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",211–222,12,"green computing, energy","Helsinki, Finland",SIGCOMM '12,,,,,,
947,inproceedings,"Seshan, Srinivasan",Session Details: Wireless and Mobile Networking,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3259306,10.1145/3259306,,"Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",,,,"Helsinki, Finland",SIGCOMM '12,,,,,,
948,inproceedings,"Cidon, Asaf and Nagaraj, Kanthi and Katti, Sachin and Viswanath, Pramod",Flashback: Decoupled Lightweight Wireless Control,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342400,10.1145/2342356.2342400,"Unlike their cellular counterparts, Wi-Fi networks do not have the luxury of a dedicated control plane that is decoupled from the data plane. Consequently, Wi-Fi struggles to provide many of the capabilities that are taken for granted in cellular networks, including efficient and fair resource allocation, QoS and handoffs. The reason for the lack of a control plane with designated spectrum is that it would impose significant overhead. This is at odds with Wi-Fi's goal of providing a simple, plug-and-play network.In this paper we present Flashback, a novel technique that provides a decoupled low overhead control plane for wireless networks that retains the simplicity of Wi-Fi's distributed asynchronous operation. Flashback allows nodes to reliably send short control messages concurrently with data transmissions, while ensuring that data packets are decoded correctly without harming throughput. We utilize Flashback's novel messaging capability to design, implement and experimentally evaluate a reliable control plane for Wi-Fi with rates from 175Kbps to 400Kbps depending on the environment. Moreover, to demonstrate its broad applicability, we design and implement a novel resource allocation mechanism that utilizes Flashback to provide efficient, QoS-aware and fair medium access, while eliminating control overheads including data plane contention, RTS/CTS and random back offs.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",223–234,12,wireless control,"Helsinki, Finland",SIGCOMM '12,,,,,,
949,inproceedings,"Rahul, Hariharan Shankar and Kumar, Swarun and Katabi, Dina",JMB: Scaling Wireless Capacity with User Demands,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342401,10.1145/2342356.2342401,"We present joint multi-user beamforming (JMB), a system that enables independent access points (APs) to beamform their signals, and communicate with their clients on the same channel as if they were one large MIMO transmitter. The key enabling technology behind JMB is a new low-overhead technique for synchronizing the phase of multiple transmitters in a distributed manner. The design allows a wireless LAN to scale its throughput by continually adding more APs on the same channel. JMB is implemented and tested with both software radio clients and off-the-shelf 802.11n cards, and evaluated in a dense congested deployment resembling a conference room. Results from a 10-AP software-radio testbed show a linear increase in network throughput with a median gain of 8.1 to 9.4x. Our results also demonstrate that JMB's joint multi-user beamforming can provide throughput gains with unmodified 802.11n cards.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",235–246,12,"distributed mimo, wireless networks, multi-user mimo","Helsinki, Finland",SIGCOMM '12,,,,,,
950,inproceedings,"Ha, Sangtae and Sen, Soumya and Joe-Wong, Carlee and Im, Youngbin and Chiang, Mung",TUBE: Time-Dependent Pricing for Mobile Data,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342402,10.1145/2342356.2342402,"The two largest U.S. wireless ISPs have recently moved towards usage-based pricing to better manage the growing demand on their networks. Yet usage-based pricing still requires ISPs to over-provision capacity for demand at peak times of the day. Time-dependent pricing (TDP) addresses this problem by considering when a user consumes data, in addition to how much is used. We present the architecture, implementation, and a user trial of an end-to-end TDP system called TUBE. TUBE creates a price-based feedback control loop between an ISP and its end users. On the ISP side, it computes TDP prices so as to balance the cost of congestion during peak periods with that of offering lower prices in less congested periods. On mobile devices, it provides a graphical user interface that allows users to respond to the offered prices either by themselves or using an ""autopilot"" mode. We conducted a pilot TUBE trial with 50 iPhone or iPad 3G data users, who were charged according to our TDP algorithms. Our results show that TDP benefits both operators and customers, flattening the temporal fluctuation of demand while allowing users to save money by choosing the time and volume of their usage.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",247–258,12,"time-dependent pricing, wireless, user trial","Helsinki, Finland",SIGCOMM '12,,,,,,
951,inproceedings,"Kumar, Swarun and Shi, Lixin and Ahmed, Nabeel and Gil, Stephanie and Katabi, Dina and Rus, Daniela",CarSpeak: A Content-Centric Network for Autonomous Driving,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342403,10.1145/2342356.2342403,"This paper introduces CarSpeak, a communication system for autonomous driving. CarSpeak enables a car to query and access sensory information captured by other cars in a manner similar to how it accesses information from its local sensors. CarSpeak adopts a content-centric approach where information objects -- i.e., regions along the road -- are first class citizens. It names and accesses road regions using a multi-resolution system, which allows it to scale the amount of transmitted data with the available bandwidth. CarSpeak also changes the MAC protocol so that, instead of having nodes contend for the medium, contention is between road regions, and the medium share assigned to any region depends on the number of cars interested in that region.CarSpeak is implemented in a state-of-the-art autonomous driving system and tested on indoor and outdoor hardware testbeds including an autonomous golf car and 10 iRobot Create robots. In comparison with a baseline that directly uses 802.11, CarSpeak reduces the time for navigating around obstacles by 2.4x, and reduces the probability of a collision due to limited visibility by 14x.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",259–270,12,"autonomous vehicles, wireless, content-centric","Helsinki, Finland",SIGCOMM '12,,,,,,
952,inproceedings,"W\""{a",Vitamin C for Your Smartphone: The SKIMS Approach for Cooperativeand Lightweight Security at Mobiles,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342405,10.1145/2342356.2342405,"Smartphones are popular attack targets, but usually too weak in applying common protection concepts. SKIMS designs and implements a cooperative, cross-layer security system for mobile devices. Detection mechanisms as well as a proactive and reactive defense of attacks are core components of this project. In this demo, we show a comprehensive proof-of-concept of our approaches, which include entropy-based malware detection, a mobile honeypot, and spontaneous, socio-inspired trust establishment.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",271–272,2,"mobile honeypot, malware detection, ad hoc trust, mobile security","Helsinki, Finland",SIGCOMM '12,,,,,,
953,inproceedings,"Riggio, Roberto and Sengul, Cigdem and Mabell Gomez, Karina and Rasheed, Tinku",Energino: Energy Saving Tips for Your Wireless Network,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342406,10.1145/2342356.2342406,"The energy wasted in wireless networks is a serious concern and the main challenge lies in determining when and where the energy is wasted. In this demo, we present Energino, an energy measurement and control system designed to deliver high performance while remaining a cheap solution.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",273–274,2,"arduino, energy consumption monitoring, open hardware, wireless","Helsinki, Finland",SIGCOMM '12,,,,,,
954,inproceedings,"Brown, Anthony and Mortier, Richard and Rodden, Tom",MultiNet: Usable and Secure WiFi Device Association,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342407,10.1145/2342356.2342407,"This demo presents MultiNet, a novel method for joining devices to a domestic Wi-Fi network. MultiNet dynamically reconfigures the network to accept each device, rather than configuring each device to fit the network as is the norm. It does so by assuming that each device is pre-configured with a cryptographically generated WPA2 network SSID/passphrase pair, and then providing a lightweight interaction through which the user creates a new network for each device. This approach makes securely adding devices to a wireless network straightforward without compromising security or burdening the user, and maintaining backward compatibility with existing deployed standards and protocols.The demo deploys a MultiNet Access Point (AP) and a number of Wi-Fi enabled consumer devices to allow viewers to dynamically construct and deconstruct the network via the MultiNet controller currently implemented as an app on an Android phone (Figure 1). The code for MultiNet is publicly available under open-source licenses.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",275–276,2,usable security; domestic environments; 802.11; infrastructure intervention,"Helsinki, Finland",SIGCOMM '12,,,,,,
955,inproceedings,"Zhang, Xi and Ansari, Junaid and M\""{a",Demo: Runtime MAC Reconfiguration Using a Meta-Compiler Assisted Toolchain,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342408,10.1145/2342356.2342408,"A rapid reconfiguration of medium access scheme is required in order to achieve runtime performance optimization for dynamic spectrum access and fulfilling varying Quality of Service (QoS) demands. We have developed TRUMP, a toolchain which allows composing MAC solutions at runtime. In this demonstration, we will show how MAC reconfiguration can be achieved efficiently using TRUMP. Inspired by the optimum route calculation method used in car navigation systems, the compiler toolchain in TRUMP realizes an appropriate MAC solution at runtime. TRUMP allows expressing various types of constraints and options such as speed, energy consumption and packet delivery rate which leads to different MAC compositions. The live demonstration of MAC reconfiguration will be carried out on WARP SDR platform.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",277–278,2,"mac, compiler assisted, sdr platform, reconfiguration","Helsinki, Finland",SIGCOMM '12,,,,,,
956,inproceedings,"Suresh, Lalith and Schulz-Zander, Julius and Merz, Ruben and Feldmann, Anja",Demo: Programming Enterprise WLANs with Odin,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342409,10.1145/2342356.2342409,"We present a demo of Odin, an SDN framework to program enterprise wireless local area networks (WLANs). Enterprise WLANs need to support a wide range of services and functionalities. This includes authentication, authorization and accounting, policy, mobility and interference management, and load balancing. WLANs also exhibit unique challenges. In particular, access point (AP) association decisions are not made by the infrastructure, but by clients. In addition, the association state machine combined with the broadcast nature of the wireless medium requires keeping track of a large amount of state changes. To this end, Odin builds on a light virtual AP abstraction that greatly simplifies client management. Odin does not require any client side modifications and its design supports WPA2 Enterprise. With Odin, a network operator can implement enterprise WLAN services as network applications.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",279–280,2,"enterprise wlans, odin, sdn","Helsinki, Finland",SIGCOMM '12,,,,,,
957,inproceedings,"Grandl, Robert and Han, Dongsu and Lee, Suk-Bok and Lim, Hyeontaek and Machado, Michel and Mukerjee, Matthew and Naylor, David",Supporting Network Evolution and Incremental Deployment with XIA,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342410,10.1145/2342356.2342410,"eXpressive Internet Architecture (XIA) [1] is an architecture that natively supports multiple communication types and allows networks to evolve their abstractions and functionality to accommodate new styles of communication over time. XIA embeds an elegant mechanism for handling unforeseen communication types for legacy routers.In this demonstration, we show that XIA overcomes three key barriers in network evolution (outlined below) by (1) allowing end-hosts and applications to start using new communication types (e.g., service and content) before the network supports them, (2) ensuring that upgrading a subset of routers to support new functionalities immediately benefits applications, and (3) using the same mechanisms we employ for 1 and 2 to incrementally deploy XIA in IP networks.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",281–282,2,"internet architecture, evolution, multiple communication styles","Helsinki, Finland",SIGCOMM '12,,,,,,
958,inproceedings,"Hong, Steven and Mehlman, Jeffrey and Katti, Sachin",Picasso: Flexible RF and Spectrum Slicing,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342411,10.1145/2342356.2342411,"Many applications can benefit from the capability to simultaneously and independently use arbitrarily sized but separate spectrum fragments with a single radio and antenna. By this capability we mean that the radio can simultaneously transmit, simultaneously receive, or simultaneously transmit and receive on arbitrary but separate spectrum fragments. For example, we can use it for spectrum aggregation in fragmented ISM bands as shown in as shown in Fig. 2(A). A WiFi AP can run independent OFDM PHY and CSMA MAC protocols on two WiFi channels to simultaneously serve two legacy WiFi clients assigned to different channels and achieve significantly higher throughput than a legacy AP that is restricted to use only one channel at a time. Similarly, a WiFi client radio with such a capability can use it to simultaneously connect to multiple WiFi APs on different channels and obtain a much higher aggregate throughput than current radios that can transmit or receive on only one channel at a time.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",283–284,2,"interference cancellation, radio virtualization","Helsinki, Finland",SIGCOMM '12,,,,,,
959,inproceedings,"Niemi, Olli-Pekka and Levom\""{a",Dismantling Intrusion Prevention Systems,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342412,10.1145/2342356.2342412,"This paper introduces a serious security problem that people believe has been fixed, but which is still very much existing and evolving, namely evasions. We describe how protocols can still be misused to fool network security devices, such as intrusion prevention systems.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",285–286,2,"ips, evasion, intrusion prevention, network, ids","Helsinki, Finland",SIGCOMM '12,,,,,,
960,inproceedings,"Otto, John S. and S\'{a",Namehelp: Intelligent Client-Side DNS Resolution,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342413,10.1145/2342356.2342413,"The Domain Name System (DNS) is a fundamental component of today's Internet. Recent years have seen radical changes to DNS with increases in usage of remote DNS and public DNS services such as OpenDNS. Given the close relationship between DNS and Content Delivery Networks (CDNs) and the pervasive use of CDNs by many popular applications including web browsing and real-time entertainment services, it is important to understand the impact of remote and public DNS services on users' overall experience on the Web. This work presents a tool, namehelp, which comparatively evaluates DNS services in terms of the web performance they provide, and implements an end-host solution to address the performance impact of remote DNS on CDNs. The demonstration will show the functionality of namehelp with online results for its performance improvements.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",287–288,2,"content delivery networks, domain name system","Helsinki, Finland",SIGCOMM '12,,,,,,
961,inproceedings,"Voellmy, Andreas and Wang, Junchang",Scalable Software Defined Network Controllers,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342414,10.1145/2342356.2342414,"Software defined networking (SDN) introduces centralized controllers to dramatically increase network programmability. The simplicity of a logical centralized controller, however, can come at the cost of control-plane scalability. In this demo, we present McNettle, an extensible SDN control system whose control event processing throughput scales with the number of system CPU cores and which supports control algorithms requiring globally visible state changes occurring at flow arrival rates. Programmers extend McNettle by writing event handlers and background programs in a high-level functional programming language extended with shared state and memory transactions. We implement our framework in Haskell and leverage the multicore facilities of the Glasgow Haskell Compiler (GHC) and runtime system. Our implementation schedules event handlers, allocates memory, optimizes message parsing and serialization, and reduces system calls in order to optimize cache usage, OS processing, and runtime system overhead. Our experiments show that McNettle can serve up to 5000 switches using a single controller with 46 cores, achieving throughput of over 14 million flows per second, near-linear scaling up to 46 cores, and latency under 200 μs for light loads and 10 ms with loads consisting of up to 5000 switches.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",289–290,2,"multicore, openflow, software-defined networking, haskell","Helsinki, Finland",SIGCOMM '12,,,,,,
962,inproceedings,"Eittenberger, Philipp M.",RaptorStream: Boosting Mobile Peer-to-Peer Streaming with Raptor Codes,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342415,10.1145/2342356.2342415,"As mobile devices and cellular networks become ubiquitous, first apps for popular P2P video streaming networks emerge. We have observed that when these applications operate in cellular networks, they don't upload video traffic back to other peers. This paper presents a reason for this behavior and proposes a viable solution to exploit the uplink capacity of mobile devices more efficiently. To the best of our knowledge, this paper is the first to propose the usage of Raptor codes to increase the upload throughput of mobile P2P applications.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",291–292,2,"raptor codes, mobile p2p streaming, android","Helsinki, Finland",SIGCOMM '12,,,,,,
963,inproceedings,"Ahmed, Mohamed and Huici, Felipe and Jahanpanah, Armin",Enabling Dynamic Network Processing with ClickOS,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342416,10.1145/2342356.2342416,,"Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",293–294,2,"sdn, isolation, virtualization, minimalistic, xen, network performance","Helsinki, Finland",SIGCOMM '12,,,,,,
964,inproceedings,"Zhang, Daqiang and Vasilakos, Athanasios V. and Xiong, Haoyi",Predicting Location Using Mobile Phone Calls,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342417,10.1145/2342356.2342417,"Location prediction using mobile phone traces has attracted increasing attention. Owing to the irregular user mobility patterns, it still remains challenging to predict user location. Our empirical study in this paper shows that the call patterns are strongly correlated with co-locate patterns (i.e., visiting the same cell tower at the same period), and the call patterns mainly affect user short-time mobility. On top of these findings, we propose NextMe --- a novel scheme to enhance the location prediction accuracy by leveraging the social interplay revealed in the cellular calls. To identify when the social interplay will affect user mobility, we introduce the concepts of the Critical Call Pattern (CCP), and the Critical Call (CC). We validate NextMe with the MIT Reality Mining dataset, involving 350,000-hour activity logs of 106 persons, and 112,508 cellular calls. Experimental results show that the social interplay significantly improves the accuracy.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",295–296,2,"social networks, social interplay, mobile phone calls","Helsinki, Finland",SIGCOMM '12,,,,,,
965,inproceedings,"Saarinen, Aki and Siekkinen, Matti and Xiao, Yu and Nurminen, Jukka K. and Kemppainen, Matti and Hui, Pan",SmartDiet: Offloading Popular Apps to Save Energy,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342418,10.1145/2342356.2342418,"Offloading computation to cloud has been widely used for extending battery life of mobile devices. However, little effort has been invested in applying the offloading techniques to communication-related tasks. We propose SmartDiet, a toolkit to identify the constraints that reduce offloading opportunities and to calculate the energy-saving potential of offloading communication-related tasks. SmartDiet traces the method-level application execution and estimates the allocation of communication energy cost from traffic traces. We discuss key features of SmartDiet and show some preliminary results using a prototype implementation.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",297–298,2,"constraint analysis, offloading, energy consumption","Helsinki, Finland",SIGCOMM '12,,,,,,
966,inproceedings,"Li, Yong and Jin, Depeng and Hui, Pan and Su, Li and Zeng, Lieguang",Revealing Contact Interval Patterns in Large Scale Urban Vehicular Ad Hoc Networks,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342419,10.1145/2342356.2342419,"Contact interval between moving vehicles is one of the key metrics in vehicular ad hoc networks (VANETs), which is important to routing schemes and network capacity. In this work, by carrying out an extensive experiment involving tens of thousands of operational taxis in Beijing city, we find an invariant character that the contact interval can be modeled by a three-segmented distribution, and there exists a characteristic time point, up to which the contact interval obeys a power law distribution, while beyond which it decays as an exponential one. This property is in sharp contrast to the recent empirical data studies based on Shanghai vehicular mobility, where the contact interval exhibits only exponential distribution.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",299–300,2,"vehicular networks, mobility trace, contact interval patterns","Helsinki, Finland",SIGCOMM '12,,,,,,
967,inproceedings,"Mineraud, Julien and Balasubramaniam, Sasitharan and Kangasharju, Jussi and Donnelly, William",Fs-PGBR: A Scalable and Delay Sensitive Cloud Routing Protocol,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342420,10.1145/2342356.2342420,"This paper proposes an improved version of a fully distributed routing protocol, that is applicable for cloud computing infrastructure. Simulation results shows the protocol is ideal for discovering cloud services in a scalable manner with minimum latency.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",301–302,2,"cloud computing infrastructure, scalable route discovery","Helsinki, Finland",SIGCOMM '12,,,,,,
968,inproceedings,"Sundaresan, Srikanth and Magharei, Nazanin and Feamster, Nick and Teixeira, Renata",Accelerating Last-Mile Web Performance with Popularity-Based Prefetching,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342421,10.1145/2342356.2342421,,"Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",303–304,2,"pre-fetching, broadband networks, web performance","Helsinki, Finland",SIGCOMM '12,,,,,,
969,inproceedings,"W\""{a",First Insights from a Mobile Honeypot,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342422,10.1145/2342356.2342422,"Computer systems are commonly attacked by malicious transport contacts. We present a comparative study that analyzes to what extent those attacks depend on the network access, in particular if an adversary targets specifically on mobile or non-mobile devices. Based on a mobile honeypot that extracts first statistical results, our findings indicate that a few topological domains of the Internet have started to place particular focus on attacking mobile networks.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",305–306,2,"mobile vs. non-mobile attacks, mobile honeypot","Helsinki, Finland",SIGCOMM '12,,,,,,
970,inproceedings,"Grosvenor, Matthew P.",UvNIC: Rapid Prototyping Network Interface Controller Device Drivers,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342423,10.1145/2342356.2342423,,"Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",307–308,2,"device driver, hardware, virtualisation, emulation, userspace","Helsinki, Finland",SIGCOMM '12,,,,,,
971,inproceedings,"Kang, Nanxi and Reich, Joshua and Rexford, Jennifer and Walker, David",Policy Transformation in Software Defined Networks,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342424,10.1145/2342356.2342424,"A Software Defined Network (SDN) enforces network-wide policies by installing packet-handling rules across a distributed collection of switches. Today's SDN platforms force programmers to decide how to decompose a high-level policy into the low-level rules in each switch. We argue that future SDN platforms should support automatic transformation of policies by moving, merging, or splitting rules across multiple switches. This would simplify programming by allowing programs written on one abstract switch to run over a more complex network topology, and simplify analysis by consolidating a policy spread over multiple switches into a single list of rules. This poster presents our ongoing work on a sound and complete set of axioms for policy transformation, to enable rewriting of rules across multiple switches while preserving the forwarding policy. These axioms are invaluable for creating and analyzing algorithms for optimizing the rewriting of rules.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",309–310,2,"network virtualization, software defined networks, openflow","Helsinki, Finland",SIGCOMM '12,,,,,,
972,inproceedings,"Caesar, Matt",Session Details: Network Formalism and Algorithmics,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3259307,10.1145/3259307,,"Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",,,,"Helsinki, Finland",SIGCOMM '12,,,,,,
973,inproceedings,"Ciucu, Florin and Schmitt, Jens","Perspectives on Network Calculus: No Free Lunch, but Still Good Value",2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342426,10.1145/2342356.2342426,"ACM Sigcomm 2006 published a paper [26] which was perceived to unify the deterministic and stochastic branches of the network calculus (abbreviated throughout as DNC and SNC) [39]. Unfortunately, this seemingly fundamental unification---which has raised the hope of a straightforward transfer of all results from DNC to SNC---is invalid. To substantiate this claim, we demonstrate that for the class of stationary and ergodic processes, which is prevalent in traffic modelling, the probabilistic arrival model from [26] is quasi-deterministic, i.e., the underlying probabilities are either zero or one. Thus, the probabilistic framework from [26] is unable to account for statistical multiplexing gain, which is in fact the raison d'\^{e","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",311–322,12,"network calculus, statistical multiplexing gain, queueing theory","Helsinki, Finland",SIGCOMM '12,,,,,,
974,inproceedings,"Reitblatt, Mark and Foster, Nate and Rexford, Jennifer and Schlesinger, Cole and Walker, David",Abstractions for Network Update,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342427,10.1145/2342356.2342427,"Configuration changes are a common source of instability in networks, leading to outages, performance disruptions, and security vulnerabilities. Even when the initial and final configurations are correct, the update process itself often steps through intermediate configurations that exhibit incorrect behaviors. This paper introduces the notion of consistent network updates---updates that are guaranteed to preserve well-defined behaviors when transitioning mbetween configurations. We identify two distinct consistency levels, per-packet and per-flow, and we present general mechanisms for implementing them in Software-Defined Networks using switch APIs like OpenFlow. We develop a formal model of OpenFlow networks, and prove that consistent updates preserve a large class of properties. We describe our prototype implementation, including several optimizations that reduce the overhead required to perform consistent updates. We present a verification tool that leverages consistent updates to significantly reduce the complexity of checking the correctness of network control software. Finally, we describe the results of some simple experiments demonstrating the effectiveness of these optimizations on example applications.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",323–334,12,"software-defined networking, consistency, openflow, frenetic, planned change, network programming languages","Helsinki, Finland",SIGCOMM '12,,,,,,
975,inproceedings,"Ma, Yadi and Banerjee, Suman",A Smart Pre-Classifier to Reduce Power Consumption of TCAMs for Multi-Dimensional Packet Classification,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342428,10.1145/2342356.2342428,"Ternary Content-Addressable Memories (TCAMs) has become the industrial standard for high-throughput packet classification. However, one major drawback of TCAMs is their high power consumption, which is becoming critical with the boom of data centers, the growing classifiers and the deployment of IPv6. In this paper, we propose a practical and efficient solution which introduces a smart pre-classifier to reduce power consumption of TCAMs for multi-dimensional packet classification. We reduce the dimension of the problem through the pre-classifier which pre-classifies a packet on two header fields, source and destination IP addresses. We then return to the high dimension problem where only a small portion of a TCAM is activated and searched for a given packet. The smart pre-classifier is built in a way such that a given packet matches at most one entry in the pre-classifier, which make commodity TCAMs sufficient to implement the pre-classifier. Furthermore, each rule is stored only once in one of the TCAM blocks, which avoids rule replication. The presented solution uses commodity TCAMs, and the proposed algorithms are easy to implement. Our scheme achieves a median power reduction of 91% and an average power reduction of 88% on real and synthetic classifiers respectively.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",335–346,12,"packet classification, smartpc, power consumption","Helsinki, Finland",SIGCOMM '12,,,,,,
976,inproceedings,"Bustamante, Fabi\'{a",Session Details: Streaming and Content Networking,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3259308,10.1145/3259308,,"Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",,,,"Helsinki, Finland",SIGCOMM '12,,,,,,
977,inproceedings,"Tian, Chen and Alimi, Richard and Yang, Yang Richard and Zhang, David",ShadowStream: Performance Evaluation as a Capability in Production Internet Live Streaming Networks,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342430,10.1145/2342356.2342430,"As live streaming networks grow in scale and complexity, they are becoming increasingly difficult to evaluate. Existing evaluation methods including lab/testbed testing, simulation, and theoretical modeling, lack either scale or realism. The industrial practice of gradually-rolling-out in a testing channel is lacking in controllability and protection when experimental algorithms fail, due to its passive approach. In this paper, we design a novel system called ShadowStream that introduces evaluation as a built-in capability in production Internet live streaming networks. ShadowStream introduces a simple, novel, transparent embedding of experimental live streaming algorithms to achieve safe evaluations of the algorithms during large-scale, real production live streaming, despite the possibility of large performance failures of the tested algorithms. ShadowStream also introduces transparent, scalable, distributed experiment orchestration to resolve the mismatch between desired viewer behaviors and actual production viewer behaviors, achieving experimental scenario controllability. We implement ShadowStream based on a major Internet live streaming network, build additional evaluation tools such as deterministic replay, and demonstrate the benefits of ShadowStream through extensive evaluations.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",347–358,12,"live testing, streaming, performance evaluation","Helsinki, Finland",SIGCOMM '12,,,,,,
978,inproceedings,"Liu, Xi and Dobrian, Florin and Milner, Henry and Jiang, Junchen and Sekar, Vyas and Stoica, Ion and Zhang, Hui",A Case for a Coordinated Internet Video Control Plane,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342431,10.1145/2342356.2342431,"Video traffic already represents a significant fraction of today's traffic and is projected to exceed 90% in the next five years. In parallel, user expectations for a high quality viewing experience (e.g., low startup delays, low buffering, and high bitrates) are continuously increasing. Unlike traditional workloads that either require low latency (e.g., short web transfers) or high average throughput (e.g., large file transfers), a high quality video viewing experience requires sustained performance over extended periods of time (e.g., tens of minutes). This imposes fundamentally different demands on content delivery infrastructures than those envisioned for traditional traffic patterns. Our large-scale measurements over 200 million video sessions show that today's delivery infrastructure fails to meet these requirements: more than 20% of sessions have a rebuffering ratio ≥ 10% and more than 14% of sessions have a video startup delay ≥ 10s. Using measurement-driven insights, we make a case for a video control plane that can use a global view of client and network conditions to dynamically optimize the video delivery in order to provide a high quality viewing experience despite an unreliable delivery infrastructure. Our analysis shows that such a control plane can potentially improve the rebuffering ratio by up to 2\texttimes{","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",359–370,12,"cdns, control plane, video","Helsinki, Finland",SIGCOMM '12,,,,,,
979,inproceedings,"Liu, Hongqiang Harry and Wang, Ye and Yang, Yang Richard and Wang, Hao and Tian, Chen",Optimizing Cost and Performance for Content Multihoming,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342432,10.1145/2342356.2342432,"Many large content publishers use multiple content distribution networks to deliver their content, and many commercial systems have become available to help a broader set of content publishers to benefit from using multiple distribution networks, which we refer to as content multihoming. In this paper, we conduct the first systematic study on optimizing content multihoming, by introducing novel algorithms to optimize both performance and cost for content multihoming. In particular, we design a novel, efficient algorithm to compute assignments of content objects to content distribution networks for content publishers, considering both cost and performance. We also design a novel, lightweight client adaptation algorithm executing at individual content viewers to achieve scalable, fine-grained, fast online adaptation to optimize the quality of experience (QoE) for individual viewers. We prove the optimality of our optimization algorithms and conduct systematic, extensive evaluations, using real charging data, content viewer demands, and performance data, to demonstrate the effectiveness of our algorithms. We show that our content multihoming algorithms reduce publishing cost by up to 40%. Our client algorithm executing in browsers reduces viewer QoE degradation by 51%.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",371–382,12,"optimization., multiple cdns, content delivery","Helsinki, Finland",SIGCOMM '12,,,,,,
980,inproceedings,"Partridge, Craig",Session Details: Routing,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3259309,10.1145/3259309,,"Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",,,,"Helsinki, Finland",SIGCOMM '12,,,,,,
981,inproceedings,"Zhao, Mingchen and Zhou, Wenchao and Gurney, Alexander J.T. and Haeberlen, Andreas and Sherr, Micah and Loo, Boon Thau",Private and Verifiable Interdomain Routing Decisions,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342434,10.1145/2342356.2342434,"Existing secure interdomain routing protocols can verify validity properties about individual routes, such as whether they correspond to a real network path. It is often useful to verify more complex properties relating to the route decision procedure - for example, whether the chosen route was the best one available, or whether it was consistent with the network's peering agreements. However, this is difficult to do without knowing a network's routing policy and full routing state, which are not normally disclosed. In this paper, we show how a network can allow its peers to verify a number of nontrivial properties of its interdomain routing decisions without revealing any additional information. If all the properties hold, the peers learn nothing beyond what the interdomain routing protocol already reveals; if a property does not hold, at least one peer can detect this and prove the violation. We present SPIDeR, a practical system that applies this approach to the Border Gateway Protocol, and we report results from an experimental evaluation to demonstrate that SPIDeR has a reasonable overhead.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",383–394,12,"privacy, fault detection, security, routing, accountability","Helsinki, Finland",SIGCOMM '12,,,,,,
982,inproceedings,"Katz-Bassett, Ethan and Scott, Colin and Choffnes, David R. and Cunha, \'{I",LIFEGUARD: Practical Repair of Persistent Route Failures,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342435,10.1145/2342356.2342435,"The Internet was designed to always find a route if there is a policy-compliant path. However, in many cases, connectivity is disrupted despite the existence of an underlying valid path. The research community has focused on short-term outages that occur during route convergence. There has been less progress on addressing avoidable long-lasting outages. Our measurements show that long-lasting events contribute significantly to overall unavailability.To address these problems, we develop LIFEGUARD, a system for automatic failure localization and remediation. LIFEGUARD uses active measurements and a historical path atlas to locate faults, even in the presence of asymmetric paths and failures. Given the ability to locate faults, we argue that the Internet protocols should allow edge ISPs to steer traffic to them around failures, without requiring the involvement of the network causing the failure. Although the Internet does not explicitly support this functionality today, we show how to approximate it using carefully crafted BGP messages. LIFEGUARD employs a set of techniques to reroute around failures with low impact on working routes. Deploying LIFEGUARD on the Internet, we find that it can effectively route traffic around an AS without causing widespread disruption.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",395–406,12,"outages, bgp, internet, availability, repair, routing, measurement","Helsinki, Finland",SIGCOMM '12,,,,,,
983,inproceedings,"Nychis, George P. and Fallin, Chris and Moscibroda, Thomas and Mutlu, Onur and Seshan, Srinivasan",On-Chip Networks from a Networking Perspective: Congestion and Scalability in Many-Core Interconnects,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342436,10.1145/2342356.2342436,"In this paper, we present network-on-chip (NoC) design and contrast it to traditional network design, highlighting similarities and differences between the two. As an initial case study, we examine network congestion in bufferless NoCs. We show that congestion manifests itself differently in a NoC than in traditional networks. Network congestion reduces system throughput in congested workloads for smaller NoCs (16 and 64 nodes), and limits the scalability of larger bufferless NoCs (256 to 4096 nodes) even when traffic has locality (e.g., when an application's required data is mapped nearby to its core in the network). We propose a new source throttling-based congestion control mechanism with application-level awareness that reduces network congestion to improve system performance. Our mechanism improves system performance by up to 28% (15% on average in congested workloads) in smaller NoCs, achieves linear throughput scaling in NoCs up to 4096 cores (attaining similar performance scalability to a NoC with large buffers), and reduces power consumption by up to 20%. Thus, we show an effective application of a network-level concept, congestion control, to a class of networks -- bufferless on-chip networks -- that has not been studied before by the networking community.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",407–418,12,"congestion control, multi-core, on-chip networks","Helsinki, Finland",SIGCOMM '12,,,,,,
984,inproceedings,"Maggs, Bruce",Session Details: Data Centers: Network Resilience,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3259310,10.1145/3259310,,"Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",,,,"Helsinki, Finland",SIGCOMM '12,,,,,,
985,inproceedings,"Wu, Xin and Turner, Daniel and Chen, Chao-Chih and Maltz, David A. and Yang, Xiaowei and Yuan, Lihua and Zhang, Ming",NetPilot: Automating Datacenter Network Failure Mitigation,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342438,10.1145/2342356.2342438,"Driven by the soaring demands for always-on and fast-response online services, modern datacenter networks have recently undergone tremendous growth. These networks often rely on commodity hardware to reach immense scale while keeping capital expenses under check. The downside is that commodity devices are prone to failures, raising a formidable challenge for network operators to promptly handle these failures with minimal disruptions to the hosted services. Recent research efforts have focused on automatic failure localization. Yet, resolving failures still requires significant human interventions, resulting in prolonged failure recovery time. Unlike previous work, NetPilot aims to quickly mitigate rather than resolve failures. NetPilot mitigates failures in much the same way operators do -- by deactivating or restarting suspected offending components. NetPilot circumvents the need for knowing the exact root cause of a failure by taking an intelligent trial-and-error approach. The core of NetPilot is comprised of an Impact Estimator that helps guard against overly disruptive mitigation actions and a failure-specific mitigation planner that minimizes the number of trials. We demonstrate that NetPilot can effectively mitigate several types of critical failures commonly encountered in production datacenter networks.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",419–430,12,"automated failure mitigation, datacenter networks","Helsinki, Finland",SIGCOMM '12,,,,,,
986,inproceedings,Bod\'{\i,Surviving Failures in Bandwidth-Constrained Datacenters,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342439,10.1145/2342356.2342439,"Datacenter networks have been designed to tolerate failures of network equipment and provide sufficient bandwidth. In practice, however, failures and maintenance of networking and power equipment often make tens to thousands of servers unavailable, and network congestion can increase service latency. Unfortunately, there exists an inherent tradeoff between achieving high fault tolerance and reducing bandwidth usage in network core; spreading servers across fault domains improves fault tolerance, but requires additional bandwidth, while deploying servers together reduces bandwidth usage, but also decreases fault tolerance. We present a detailed analysis of a large-scale Web application and its communication patterns. Based on that, we propose and evaluate a novel optimization framework that achieves both high fault tolerance and significantly reduces bandwidth usage in the network core by exploiting the skewness in the observed communication patterns.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",431–442,12,"fault tolerance, bandwidth, datacenter networks","Helsinki, Finland",SIGCOMM '12,,,,,,
987,inproceedings,"Zhou, Xia and Zhang, Zengbin and Zhu, Yibo and Li, Yubo and Kumar, Saipriya and Vahdat, Amin and Zhao, Ben Y. and Zheng, Haitao",Mirror Mirror on the Ceiling: Flexible Wireless Links for Data Centers,2012,9781450314190,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2342356.2342440,10.1145/2342356.2342440,"Modern data centers are massive, and support a range of distributed applications across potentially hundreds of server racks. As their utilization and bandwidth needs continue to grow, traditional methods of augmenting bandwidth have proven complex and costly in time and resources. Recent measurements show that data center traffic is often limited by congestion loss caused by short traffic bursts. Thus an attractive alternative to adding physical bandwidth is to augment wired links with wireless links in the 60 GHz band.We address two limitations with current 60 GHz wireless proposals. First, 60 GHz wireless links are limited by line-of-sight, and can be blocked by even small obstacles. Second, even beamforming links leak power, and potential interference will severely limit concurrent transmissions in dense data centers. We propose and evaluate a new wireless primitive for data centers, 3D beamforming, where 60 GHz signals bounce off data center ceilings, thus establishing indirect line-of-sight between any two racks in a data center. We build a small 3D beamforming testbed to demonstrate its ability to address both link blockage and link interference, thus improving link range and number of concurrent transmissions in the data center. In addition, we propose a simple link scheduler and use traffic simulations to show that these 3D links significantly expand wireless capacity compared to their 2D counterparts.","Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",443–454,12,"data centers, 60 ghz wireless, wireless beamforming","Helsinki, Finland",SIGCOMM '12,,,,,,
988,inproceedings,"Krishnamurthy, Balachander",Session Details: Security,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256188,10.1145/3256188,,Proceedings of the ACM SIGCOMM 2011 Conference,,,,"Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
989,article,"Gollakota, Shyamnath and Hassanieh, Haitham and Ransford, Benjamin and Katabi, Dina and Fu, Kevin",They Can Hear Your Heartbeats: Non-Invasive Security for Implantable Medical Devices,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018438,10.1145/2043164.2018438,"Wireless communication has become an intrinsic part of modern implantable medical devices (IMDs). Recent work, however, has demonstrated that wireless connectivity can be exploited to compromise the confidentiality of IMDs' transmitted data or to send unauthorized commands to IMDs---even commands that cause the device to deliver an electric shock to the patient. The key challenge in addressing these attacks stems from the difficulty of modifying or replacing already-implanted IMDs. Thus, in this paper, we explore the feasibility of protecting an implantable device from such attacks without modifying the device itself. We present a physical-layer solution that delegates the security of an IMD to a personal base station called the shield. The shield uses a novel radio design that can act as a jammer-cum-receiver. This design allows it to jam the IMD's messages, preventing others from decoding them while being able to decode them itself. It also allows the shield to jam unauthorized commands---even those that try to alter the shield's own transmissions. We implement our design in a software radio and evaluate it with commercial IMDs. We find that it effectively provides confidentiality for private data and protects the IMD from unauthorized commands.",,2–13,12,"wireless, full-duplex, implanted medical devices",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
990,inproceedings,"Gollakota, Shyamnath and Hassanieh, Haitham and Ransford, Benjamin and Katabi, Dina and Fu, Kevin",They Can Hear Your Heartbeats: Non-Invasive Security for Implantable Medical Devices,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018438,10.1145/2018436.2018438,"Wireless communication has become an intrinsic part of modern implantable medical devices (IMDs). Recent work, however, has demonstrated that wireless connectivity can be exploited to compromise the confidentiality of IMDs' transmitted data or to send unauthorized commands to IMDs---even commands that cause the device to deliver an electric shock to the patient. The key challenge in addressing these attacks stems from the difficulty of modifying or replacing already-implanted IMDs. Thus, in this paper, we explore the feasibility of protecting an implantable device from such attacks without modifying the device itself. We present a physical-layer solution that delegates the security of an IMD to a personal base station called the shield. The shield uses a novel radio design that can act as a jammer-cum-receiver. This design allows it to jam the IMD's messages, preventing others from decoding them while being able to decode them itself. It also allows the shield to jam unauthorized commands---even those that try to alter the shield's own transmissions. We implement our design in a software radio and evaluate it with commercial IMDs. We find that it effectively provides confidentiality for private data and protects the IMD from unauthorized commands.",Proceedings of the ACM SIGCOMM 2011 Conference,2–13,12,"full-duplex, implanted medical devices, wireless","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
991,article,"Gill, Phillipa and Schapira, Michael and Goldberg, Sharon",Let the Market Drive Deployment: A Strategy for Transitioning to BGP Security,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018439,10.1145/2043164.2018439,"With a cryptographic root-of-trust for Internet routing(RPKI [17]) on the horizon, we can finally start planning the deployment of one of the secure interdomain routing protocols proposed over a decade ago (Secure BGP [22], secure origin BGP [37]). However, if experience with IPv6 is any indicator, this will be no easy task. Security concerns alone seem unlikely to provide sufficient local incentive to drive the deployment process forward. Worse yet, the security benefits provided by the S*BGP protocols do not even kick in until a large number of ASes have deployed them.Instead, we appeal to ISPs' interest in increasing revenue-generating traffic. We propose a strategy that governments and industry groups can use to harness ISPs' local business objectives and drive global S*BGP deployment. We evaluate our deployment strategy using theoretical analysis and large-scale simulations on empirical data. Our results give evidence that the market dynamics created by our proposal can transition the majority of the Internet to S*BGP.",,14–25,12,"routing, security, bgp",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
992,inproceedings,"Gill, Phillipa and Schapira, Michael and Goldberg, Sharon",Let the Market Drive Deployment: A Strategy for Transitioning to BGP Security,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018439,10.1145/2018436.2018439,"With a cryptographic root-of-trust for Internet routing(RPKI [17]) on the horizon, we can finally start planning the deployment of one of the secure interdomain routing protocols proposed over a decade ago (Secure BGP [22], secure origin BGP [37]). However, if experience with IPv6 is any indicator, this will be no easy task. Security concerns alone seem unlikely to provide sufficient local incentive to drive the deployment process forward. Worse yet, the security benefits provided by the S*BGP protocols do not even kick in until a large number of ASes have deployed them.Instead, we appeal to ISPs' interest in increasing revenue-generating traffic. We propose a strategy that governments and industry groups can use to harness ISPs' local business objectives and drive global S*BGP deployment. We evaluate our deployment strategy using theoretical analysis and large-scale simulations on empirical data. Our results give evidence that the market dynamics created by our proposal can transition the majority of the Internet to S*BGP.",Proceedings of the ACM SIGCOMM 2011 Conference,14–25,12,"security, routing, bgp","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
993,article,"Kothari, Nupur and Mahajan, Ratul and Millstein, Todd and Govindan, Ramesh and Musuvathi, Madanlal",Finding Protocol Manipulation Attacks,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018440,10.1145/2043164.2018440,"We develop a method to help discover manipulation attacks in protocol implementations. In these attacks, adversaries induce honest nodes to exhibit undesirable behaviors by misrepresenting their intent or network conditions. Our method is based on a novel combination of static analysis with symbolic execution and dynamic analysis with concrete execution. The former finds code paths that are likely vulnerable, and the latter emulates adversarial actions that lead to effective attacks. Our method is precise (i.e., no false positives) and we show that it scales to complex protocol implementations. We apply it to four diverse protocols, including TCP, the 802.11 MAC, ECN, and SCTP, and show that it is able to find all manipulation attacks that have been previously reported for these protocols. We also find a previously unreported attack for SCTP. This attack is a variant of a TCP attack but must be mounted differently in SCTP because of subtle semantic differences between the two protocols.",,26–37,12,"symbolic execution, manipulation attacks",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
994,inproceedings,"Kothari, Nupur and Mahajan, Ratul and Millstein, Todd and Govindan, Ramesh and Musuvathi, Madanlal",Finding Protocol Manipulation Attacks,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018440,10.1145/2018436.2018440,"We develop a method to help discover manipulation attacks in protocol implementations. In these attacks, adversaries induce honest nodes to exhibit undesirable behaviors by misrepresenting their intent or network conditions. Our method is based on a novel combination of static analysis with symbolic execution and dynamic analysis with concrete execution. The former finds code paths that are likely vulnerable, and the latter emulates adversarial actions that lead to effective attacks. Our method is precise (i.e., no false positives) and we show that it scales to complex protocol implementations. We apply it to four diverse protocols, including TCP, the 802.11 MAC, ECN, and SCTP, and show that it is able to find all manipulation attacks that have been previously reported for these protocols. We also find a previously unreported attack for SCTP. This attack is a variant of a TCP attack but must be mounted differently in SCTP because of subtle semantic differences between the two protocols.",Proceedings of the ACM SIGCOMM 2011 Conference,26–37,12,"manipulation attacks, symbolic execution","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
995,inproceedings,"Padhye, Jitendra",Session Details: Novel Data Center Architectures,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256189,10.1145/3256189,,Proceedings of the ACM SIGCOMM 2011 Conference,,,,"Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
996,article,"Halperin, Daniel and Kandula, Srikanth and Padhye, Jitendra and Bahl, Paramvir and Wetherall, David",Augmenting Data Center Networks with Multi-Gigabit Wireless Links,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018442,10.1145/2043164.2018442,"The 60 GHz wireless technology that is now emerging has the potential to provide dense and extremely fast connectivity at low cost. In this paper, we explore its use to relieve hotspots in oversubscribed data center (DC) networks. By experimenting with prototype equipment, we show that the DC environment is well suited to a deployment of 60GHz links contrary to concerns about interference and link reliability. Using directional antennas, many wireless links can run concurrently at multi-Gbps rates on top-of-rack (ToR) switches. The wired DC network can be used to sidestep several common wireless problems. By analyzing production traces of DC traffic for four real applications, we show that adding a small amount of network capacity in the form of wireless flyways to the wired DC network can improve performance. However, to be of significant value, we find that one hop indirect routing is needed. Informed by our 60GHz experiments and DC traffic analysis, we present a design that uses DC traffic levels to select and adds flyways to the wired DC network. Trace-driven evaluations show that network-limited DC applications with predictable traffic workloads running on a 1:2 oversubscribed network can be sped up by 45% in 95% of the cases, with just one wireless device per ToR switch. With two devices, in 40% of the cases, the performance is identical to that of a non-oversubscribed network.",,38–49,12,"60 ghz, datacenter, network, 802.11ad, data center, flyways",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
997,inproceedings,"Halperin, Daniel and Kandula, Srikanth and Padhye, Jitendra and Bahl, Paramvir and Wetherall, David",Augmenting Data Center Networks with Multi-Gigabit Wireless Links,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018442,10.1145/2018436.2018442,"The 60 GHz wireless technology that is now emerging has the potential to provide dense and extremely fast connectivity at low cost. In this paper, we explore its use to relieve hotspots in oversubscribed data center (DC) networks. By experimenting with prototype equipment, we show that the DC environment is well suited to a deployment of 60GHz links contrary to concerns about interference and link reliability. Using directional antennas, many wireless links can run concurrently at multi-Gbps rates on top-of-rack (ToR) switches. The wired DC network can be used to sidestep several common wireless problems. By analyzing production traces of DC traffic for four real applications, we show that adding a small amount of network capacity in the form of wireless flyways to the wired DC network can improve performance. However, to be of significant value, we find that one hop indirect routing is needed. Informed by our 60GHz experiments and DC traffic analysis, we present a design that uses DC traffic levels to select and adds flyways to the wired DC network. Trace-driven evaluations show that network-limited DC applications with predictable traffic workloads running on a 1:2 oversubscribed network can be sped up by 45% in 95% of the cases, with just one wireless device per ToR switch. With two devices, in 40% of the cases, the performance is identical to that of a non-oversubscribed network.",Proceedings of the ACM SIGCOMM 2011 Conference,38–49,12,"datacenter, data center, network, flyways, 802.11ad, 60 ghz","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
998,article,"Wilson, Christo and Ballani, Hitesh and Karagiannis, Thomas and Rowtron, Ant",Better Never than Late: Meeting Deadlines in Datacenter Networks,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018443,10.1145/2043164.2018443,"The soft real-time nature of large scale web applications in today's datacenters, combined with their distributed workflow, leads to deadlines being associated with the datacenter application traffic. A network flow is useful, and contributes to application throughput and operator revenue if, and only if, it completes within its deadline. Today's transport pro- tocols (TCP included), given their Internet origins, are agnostic to such flow deadlines. Instead, they strive to share network resources fairly. We show that this can hurt application performance.Motivated by these observations, and other (previously known) deficiencies of TCP in the datacenter environment, this paper presents the design and implementation of D3, a deadline-aware control protocol that is customized for the datacenter environment. D3 uses explicit rate control to apportion bandwidth according to flow deadlines. Evaluation from a 19-node, two-tier datacenter testbed shows that D3, even without any deadline information, easily outper- forms TCP in terms of short flow latency and burst tolerance. Further, by utilizing deadline information, D3 effectively doubles the peak load that the datacenter network cansupport.",,50–61,12,"sla, datacenter, online services, rate control, deadline",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
999,inproceedings,"Wilson, Christo and Ballani, Hitesh and Karagiannis, Thomas and Rowtron, Ant",Better Never than Late: Meeting Deadlines in Datacenter Networks,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018443,10.1145/2018436.2018443,"The soft real-time nature of large scale web applications in today's datacenters, combined with their distributed workflow, leads to deadlines being associated with the datacenter application traffic. A network flow is useful, and contributes to application throughput and operator revenue if, and only if, it completes within its deadline. Today's transport pro- tocols (TCP included), given their Internet origins, are agnostic to such flow deadlines. Instead, they strive to share network resources fairly. We show that this can hurt application performance.Motivated by these observations, and other (previously known) deficiencies of TCP in the datacenter environment, this paper presents the design and implementation of D3, a deadline-aware control protocol that is customized for the datacenter environment. D3 uses explicit rate control to apportion bandwidth according to flow deadlines. Evaluation from a 19-node, two-tier datacenter testbed shows that D3, even without any deadline information, easily outper- forms TCP in terms of short flow latency and burst tolerance. Further, by utilizing deadline information, D3 effectively doubles the peak load that the datacenter network cansupport.",Proceedings of the ACM SIGCOMM 2011 Conference,50–61,12,"online services, deadline, sla, datacenter, rate control","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1000,article,"Mudigonda, Jayaram and Yalagandula, Praveen and Mogul, Jeff and Stiekes, Bryan and Pouffary, Yanick",NetLord: A Scalable Multi-Tenant Network Architecture for Virtualized Datacenters,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018444,10.1145/2043164.2018444,"Providers of ""Infrastructure-as-a-Service"" need datacenter networks that support multi-tenancy, scale, and ease of operation, at low cost. Most existing network architectures cannot meet all of these needs simultaneously.In this paper we present NetLord, a novel multi-tenant network architecture. NetLord provides tenants with simple and flexible network abstractions, by fully and efficiently virtualizing the address space at both L2 and L3. NetLord can exploit inexpensive commodity equipment to scale the network to several thousands of tenants and millions of virtual machines. NetLord requires only a small amount of offline, one-time configuration. We implemented NetLord on a testbed, and demonstrated its scalability, while achieving order-of-magnitude goodput improvements over previous approaches.",,62–73,12,"multi-tenant, datacenter network, scalable ethernet, network virtualization, multi-pathing",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1001,inproceedings,"Mudigonda, Jayaram and Yalagandula, Praveen and Mogul, Jeff and Stiekes, Bryan and Pouffary, Yanick",NetLord: A Scalable Multi-Tenant Network Architecture for Virtualized Datacenters,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018444,10.1145/2018436.2018444,"Providers of ""Infrastructure-as-a-Service"" need datacenter networks that support multi-tenancy, scale, and ease of operation, at low cost. Most existing network architectures cannot meet all of these needs simultaneously.In this paper we present NetLord, a novel multi-tenant network architecture. NetLord provides tenants with simple and flexible network abstractions, by fully and efficiently virtualizing the address space at both L2 and L3. NetLord can exploit inexpensive commodity equipment to scale the network to several thousands of tenants and millions of virtual machines. NetLord requires only a small amount of offline, one-time configuration. We implemented NetLord on a testbed, and demonstrated its scalability, while achieving order-of-magnitude goodput improvements over previous approaches.",Proceedings of the ACM SIGCOMM 2011 Conference,62–73,12,"scalable ethernet, network virtualization, multi-tenant, multi-pathing, datacenter network","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1002,inproceedings,"Clark, David",Session Details: Bulk Data Transfers,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256190,10.1145/3256190,,Proceedings of the ACM SIGCOMM 2011 Conference,,,,"Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1003,article,"Laoutaris, Nikolaos and Sirivianos, Michael and Yang, Xiaoyuan and Rodriguez, Pablo",Inter-Datacenter Bulk Transfers with Netstitcher,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018446,10.1145/2043164.2018446,"Large datacenter operators with sites at multiple locations dimension their key resources according to the peak demand of the geographic area that each site covers. The demand of specific areas follows strong diurnal patterns with high peak to valley ratios that result in poor average utilization across a day. In this paper, we show how to rescue unutilized bandwidth across multiple datacenters and backbone networks and use it for non-real-time applications, such as backups, propagation of bulky updates, and migration of data. Achieving the above is non-trivial since leftover bandwidth appears at different times, for different durations, and at different places in the world.For this purpose, we have designed, implemented, and validated NetStitcher, a system that employs a network of storage nodes to stitch together unutilized bandwidth, whenever and wherever it exists. It gathers information about leftover resources, uses a store-and-forward algorithm to schedule data transfers, and adapts to resource fluctuations.We have compared NetStitcher with other bulk transfer mechanisms using both a testbed and a live deployment on a real CDN. Our testbed evaluation shows that NetStitcher outperforms all other mechanisms and can rescue up to five times additional datacenter bandwidth thus making it a valuable tool for datacenter providers. Our live CDN deployment demonstrates that our solution can perform large data transfers at a much lower cost than naive end-to-end or store-and-forward schemes.",,74–85,12,"inter-datacenter traffic, bulk transfers, store-and-forward",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1004,inproceedings,"Laoutaris, Nikolaos and Sirivianos, Michael and Yang, Xiaoyuan and Rodriguez, Pablo",Inter-Datacenter Bulk Transfers with Netstitcher,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018446,10.1145/2018436.2018446,"Large datacenter operators with sites at multiple locations dimension their key resources according to the peak demand of the geographic area that each site covers. The demand of specific areas follows strong diurnal patterns with high peak to valley ratios that result in poor average utilization across a day. In this paper, we show how to rescue unutilized bandwidth across multiple datacenters and backbone networks and use it for non-real-time applications, such as backups, propagation of bulky updates, and migration of data. Achieving the above is non-trivial since leftover bandwidth appears at different times, for different durations, and at different places in the world.For this purpose, we have designed, implemented, and validated NetStitcher, a system that employs a network of storage nodes to stitch together unutilized bandwidth, whenever and wherever it exists. It gathers information about leftover resources, uses a store-and-forward algorithm to schedule data transfers, and adapts to resource fluctuations.We have compared NetStitcher with other bulk transfer mechanisms using both a testbed and a live deployment on a real CDN. Our testbed evaluation shows that NetStitcher outperforms all other mechanisms and can rescue up to five times additional datacenter bandwidth thus making it a valuable tool for datacenter providers. Our live CDN deployment demonstrates that our solution can perform large data transfers at a much lower cost than naive end-to-end or store-and-forward schemes.",Proceedings of the ACM SIGCOMM 2011 Conference,74–85,12,"bulk transfers, inter-datacenter traffic, store-and-forward","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1005,article,"Zohar, Eyal and Cidon, Israel and Mokryn, Osnat (Ossi)",The Power of Prediction: Cloud Bandwidth and Cost Reduction,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018447,10.1145/2043164.2018447,"In this paper we present PACK (Predictive ACKs), a novel end-to-end Traffic Redundancy Elimination (TRE) system, designed for cloud computing customers.Cloud-based TRE needs to apply a judicious use of cloud resources so that the bandwidth cost reduction combined with the additional cost of TRE computation and storage would be optimized. PACK's main advantage is its capability of offloading the cloud-server TRE effort to end-clients, thus minimizing the processing costs induced by the TRE algorithm.Unlike previous solutions, PACK does not require the server to continuously maintain clients' status. This makes PACK very suitable for pervasive computation environments that combine client mobility and server migration to maintain cloud elasticity.PACK is based on a novel TRE technique, which allows the client to use newly received chunks to identify previously received chunk chains, which in turn can be used as reliable predictors to future transmitted chunks.We present a fully functional PACK implementation, transparent to all TCP-based applications and network devices. Finally, we analyze PACK benefits for cloud users, using traffic traces from various sources.",,86–97,12,"caching, cloud computing, network optimization, traffic redundancy elimination",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1006,inproceedings,"Zohar, Eyal and Cidon, Israel and Mokryn, Osnat (Ossi)",The Power of Prediction: Cloud Bandwidth and Cost Reduction,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018447,10.1145/2018436.2018447,"In this paper we present PACK (Predictive ACKs), a novel end-to-end Traffic Redundancy Elimination (TRE) system, designed for cloud computing customers.Cloud-based TRE needs to apply a judicious use of cloud resources so that the bandwidth cost reduction combined with the additional cost of TRE computation and storage would be optimized. PACK's main advantage is its capability of offloading the cloud-server TRE effort to end-clients, thus minimizing the processing costs induced by the TRE algorithm.Unlike previous solutions, PACK does not require the server to continuously maintain clients' status. This makes PACK very suitable for pervasive computation environments that combine client mobility and server migration to maintain cloud elasticity.PACK is based on a novel TRE technique, which allows the client to use newly received chunks to identify previously received chunk chains, which in turn can be used as reliable predictors to future transmitted chunks.We present a fully functional PACK implementation, transparent to all TCP-based applications and network devices. Finally, we analyze PACK benefits for cloud users, using traffic traces from various sources.",Proceedings of the ACM SIGCOMM 2011 Conference,86–97,12,"caching, cloud computing, network optimization, traffic redundancy elimination","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1007,article,"Chowdhury, Mosharaf and Zaharia, Matei and Ma, Justin and Jordan, Michael I. and Stoica, Ion",Managing Data Transfers in Computer Clusters with Orchestra,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018448,10.1145/2043164.2018448,"Cluster computing applications like MapReduce and Dryad transfer massive amounts of data between their computation stages. These transfers can have a significant impact on job performance, accounting for more than 50% of job completion times. Despite this impact, there has been relatively little work on optimizing the performance of these data transfers, with networking researchers traditionally focusing on per-flow traffic management. We address this limitation by proposing a global management architecture and a set of algorithms that (1) improve the transfer times of common communication patterns, such as broadcast and shuffle, and (2) allow scheduling policies at the transfer level, such as prioritizing a transfer over other transfers. Using a prototype implementation, we show that our solution improves broadcast completion times by up to 4.5X compared to the status quo in Hadoop. We also show that transfer-level scheduling can reduce the completion time of high-priority transfers by 1.7X.",,98–109,12,"data-intensive applications, data transfer, datacenter networks",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1008,inproceedings,"Chowdhury, Mosharaf and Zaharia, Matei and Ma, Justin and Jordan, Michael I. and Stoica, Ion",Managing Data Transfers in Computer Clusters with Orchestra,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018448,10.1145/2018436.2018448,"Cluster computing applications like MapReduce and Dryad transfer massive amounts of data between their computation stages. These transfers can have a significant impact on job performance, accounting for more than 50% of job completion times. Despite this impact, there has been relatively little work on optimizing the performance of these data transfers, with networking researchers traditionally focusing on per-flow traffic management. We address this limitation by proposing a global management architecture and a set of algorithms that (1) improve the transfer times of common communication patterns, such as broadcast and shuffle, and (2) allow scheduling policies at the transfer level, such as prioritizing a transfer over other transfers. Using a prototype implementation, we show that our solution improves broadcast completion times by up to 4.5X compared to the status quo in Hadoop. We also show that transfer-level scheduling can reduce the completion time of high-priority transfers by 1.7X.",Proceedings of the ACM SIGCOMM 2011 Conference,98–109,12,"datacenter networks, data-intensive applications, data transfer","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1009,inproceedings,"Goldberg, Sharon",Session Details: Network Measurement I -- Wide-Area Measurement,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256191,10.1145/3256191,,Proceedings of the ACM SIGCOMM 2011 Conference,,,,"Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1010,article,"Otto, John S. and S\'{a",On Blind Mice and the Elephant: Understanding the Network Impact of a Large Distributed System,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018450,10.1145/2043164.2018450,"A thorough understanding of the network impact of emerging large-scale distributed systems -- where traffic flows and what it costs -- must encompass users' behavior, the traffic they generate and the topology over which that traffic flows. In the case of BitTorrent, however, previous studies have been limited by narrow perspectives that restrict such analysis.This paper presents a comprehensive view of BitTorrent, using data from a representative set of 500,000 users sampled over a two year period, located in 169 countries and 3,150 networks. This unique perspective captures unseen trends and reveals several unexpected features of the largest peer-to-peer system. For instance, over the past year total BitTorrent traffic has increased by 12%, driven by 25% increases in per-peer hourly download volume despite a 10% decrease in the average number of online peers. We also observe stronger diurnal usage patterns and, surprisingly given the bandwidth-intensive nature of the application, a close alignment between these patterns and overall traffic. Considering the aggregated traffic across access links, this has potential implications on BitTorrent-associated costs for Internet Service Providers (ISPs). Using data from a transit ISP, we find a disproportionately large impact on a commonly used burstable (95th-percentile) billing model. Last, when examining BitTorrent traffic's paths, we find that for over half its users, most network traffic never reaches large transit networks, but is instead carried by small transit ISPs. This raises questions on the effectiveness of most in-network monitoring systems to capture trends on peer-to-peer traffic and further motivates our approach.",,110–121,12,"peer-to-peer, internet-scale systems, evaluation",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1011,inproceedings,"Otto, John S. and S\'{a",On Blind Mice and the Elephant: Understanding the Network Impact of a Large Distributed System,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018450,10.1145/2018436.2018450,"A thorough understanding of the network impact of emerging large-scale distributed systems -- where traffic flows and what it costs -- must encompass users' behavior, the traffic they generate and the topology over which that traffic flows. In the case of BitTorrent, however, previous studies have been limited by narrow perspectives that restrict such analysis.This paper presents a comprehensive view of BitTorrent, using data from a representative set of 500,000 users sampled over a two year period, located in 169 countries and 3,150 networks. This unique perspective captures unseen trends and reveals several unexpected features of the largest peer-to-peer system. For instance, over the past year total BitTorrent traffic has increased by 12%, driven by 25% increases in per-peer hourly download volume despite a 10% decrease in the average number of online peers. We also observe stronger diurnal usage patterns and, surprisingly given the bandwidth-intensive nature of the application, a close alignment between these patterns and overall traffic. Considering the aggregated traffic across access links, this has potential implications on BitTorrent-associated costs for Internet Service Providers (ISPs). Using data from a transit ISP, we find a disproportionately large impact on a commonly used burstable (95th-percentile) billing model. Last, when examining BitTorrent traffic's paths, we find that for over half its users, most network traffic never reaches large transit networks, but is instead carried by small transit ISPs. This raises questions on the effectiveness of most in-network monitoring systems to capture trends on peer-to-peer traffic and further motivates our approach.",Proceedings of the ACM SIGCOMM 2011 Conference,110–121,12,"internet-scale systems, peer-to-peer, evaluation","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1012,article,"Cunha, Italo and Teixeira, Renata and Veitch, Darryl and Diot, Christophe",Predicting and Tracking Internet Path Changes,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018451,10.1145/2043164.2018451,"This paper investigates to what extent it is possible to use traceroute-style probing for accurately tracking Internet path changes. When the number of paths is large, the usual traceroute based approach misses many path changes because it probes all paths equally. Based on empirical observations, we argue that monitors can optimize probing according to the likelihood of path changes. We design a simple predictor of path changes using a nearest neighbor model. Although predicting path changes is not very accurate, we show that it can be used to improve probe targeting. Our path tracking method, called DTrack, detects up to two times more path changes than traditional probing, with lower detection delay, as well as providing complete load-balancer information.",,122–133,12,"prediction, tracking, path changes, topology mapping",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1013,inproceedings,"Cunha, Italo and Teixeira, Renata and Veitch, Darryl and Diot, Christophe",Predicting and Tracking Internet Path Changes,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018451,10.1145/2018436.2018451,"This paper investigates to what extent it is possible to use traceroute-style probing for accurately tracking Internet path changes. When the number of paths is large, the usual traceroute based approach misses many path changes because it probes all paths equally. Based on empirical observations, we argue that monitors can optimize probing according to the likelihood of path changes. We design a simple predictor of path changes using a nearest neighbor model. Although predicting path changes is not very accurate, we show that it can be used to improve probe targeting. Our path tracking method, called DTrack, detects up to two times more path changes than traditional probing, with lower detection delay, as well as providing complete load-balancer information.",Proceedings of the ACM SIGCOMM 2011 Conference,122–133,12,"path changes, tracking, topology mapping, prediction","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1014,article,"Sundaresan, Srikanth and de Donato, Walter and Feamster, Nick and Teixeira, Renata and Crawford, Sam and Pescap\`{e",Broadband Internet Performance: A View from the Gateway,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018452,10.1145/2043164.2018452,"We present the first study of network access link performance measured directly from home gateway devices. Policymakers, ISPs, and users are increasingly interested in studying the performance of Internet access links. Because of many confounding factors in a home network or on end hosts, however, thoroughly understanding access network performance requires deploying measurement infrastructure in users' homes as gateway devices. In conjunction with the Federal Communication Commission's study of broadband Internet access in the United States, we study the throughput and latency of network access links using longitudinal measurements from nearly 4,000 gateway devices across 8 ISPs from a deployment of over 4,200 devices. We study the performance users achieve and how various factors ranging from the user's choice of modem to the ISP's traffic shaping policies can affect performance. Our study yields many important findings about the characteristics of existing access networks. Our findings also provide insights into the ways that access network performance should be measured and presented to users, which can help inform ongoing broader efforts to benchmark the performance of access networks.",,134–145,12,"benchmarking, access networks, bismark, broadband networks",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1015,inproceedings,"Sundaresan, Srikanth and de Donato, Walter and Feamster, Nick and Teixeira, Renata and Crawford, Sam and Pescap\`{e",Broadband Internet Performance: A View from the Gateway,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018452,10.1145/2018436.2018452,"We present the first study of network access link performance measured directly from home gateway devices. Policymakers, ISPs, and users are increasingly interested in studying the performance of Internet access links. Because of many confounding factors in a home network or on end hosts, however, thoroughly understanding access network performance requires deploying measurement infrastructure in users' homes as gateway devices. In conjunction with the Federal Communication Commission's study of broadband Internet access in the United States, we study the throughput and latency of network access links using longitudinal measurements from nearly 4,000 gateway devices across 8 ISPs from a deployment of over 4,200 devices. We study the performance users achieve and how various factors ranging from the user's choice of modem to the ISP's traffic shaping policies can affect performance. Our study yields many important findings about the characteristics of existing access networks. Our findings also provide insights into the ways that access network performance should be measured and presented to users, which can help inform ongoing broader efforts to benchmark the performance of access networks.",Proceedings of the ACM SIGCOMM 2011 Conference,134–145,12,"access networks, benchmarking, broadband networks, bismark","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1016,inproceedings,"Karp, Brad",Session Details: Wireless,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256192,10.1145/3256192,,Proceedings of the ACM SIGCOMM 2011 Conference,,,,"Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1017,article,"Lin, Kate Ching-Ju and Gollakota, Shyamnath and Katabi, Dina",Random Access Heterogeneous MIMO Networks,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018454,10.1145/2043164.2018454,"This paper presents the design and implementation of 802.11n+, a fully distributed random access protocol for MIMO networks. 802.11n+ allows nodes that differ in the number of antennas to contend not just for time, but also for the degrees of freedom provided by multiple antennas. We show that even when the medium is already occupied by some nodes, nodes with more antennas can transmit concurrently without harming the ongoing transmissions. Furthermore, such nodes can contend for the medium in a fully distributed way. Our testbed evaluation shows that even for a small network with three competing node pairs, the resulting system about doubles the average network throughput. It also maintains the random access nature of today's 802.11n networks.",,146–157,12,"interference alignment, mimo, interference nulling",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1018,inproceedings,"Lin, Kate Ching-Ju and Gollakota, Shyamnath and Katabi, Dina",Random Access Heterogeneous MIMO Networks,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018454,10.1145/2018436.2018454,"This paper presents the design and implementation of 802.11n+, a fully distributed random access protocol for MIMO networks. 802.11n+ allows nodes that differ in the number of antennas to contend not just for time, but also for the degrees of freedom provided by multiple antennas. We show that even when the medium is already occupied by some nodes, nodes with more antennas can transmit concurrently without harming the ongoing transmissions. Furthermore, such nodes can contend for the medium in a fully distributed way. Our testbed evaluation shows that even for a small network with three competing node pairs, the resulting system about doubles the average network throughput. It also maintains the random access nature of today's 802.11n networks.",Proceedings of the ACM SIGCOMM 2011 Conference,146–157,12,"mimo, interference nulling, interference alignment","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1019,article,"Gudipati, Aditya and Katti, Sachin",Strider: Automatic Rate Adaptation and Collision Handling,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018455,10.1145/2043164.2018455,"This paper presents the design, implementation and evaluation of Strider, a system that automatically achieves almost the optimal rate adaptation without incurring any overhead. The key component in Strider is a novel code that has two important properties: it is rateless and collision-resilient. First, in time-varying wireless channels, Strider's rateless code allows a sender to effectively achieve almost the optimal bitrate, without knowing how the channel state varies. Second, Strider's collision-resilient code allows a receiver to decode both packets from collisions, and achieves the same throughput as the collision-free scheduler. We show via theoretical analysis that Strider achieves Shannon capacity for Gaussian channels, and our empirical evaluation shows that Strider outperforms SoftRate, a state of the art rate adaptation technique by 70% in mobile scenarios and by upto 2.8\texttimes{",,158–169,12,"collision decoding, rateless coding, rate adaptation, hidden terminals",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1020,inproceedings,"Gudipati, Aditya and Katti, Sachin",Strider: Automatic Rate Adaptation and Collision Handling,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018455,10.1145/2018436.2018455,"This paper presents the design, implementation and evaluation of Strider, a system that automatically achieves almost the optimal rate adaptation without incurring any overhead. The key component in Strider is a novel code that has two important properties: it is rateless and collision-resilient. First, in time-varying wireless channels, Strider's rateless code allows a sender to effectively achieve almost the optimal bitrate, without knowing how the channel state varies. Second, Strider's collision-resilient code allows a receiver to decode both packets from collisions, and achieves the same throughput as the collision-free scheduler. We show via theoretical analysis that Strider achieves Shannon capacity for Gaussian channels, and our empirical evaluation shows that Strider outperforms SoftRate, a state of the art rate adaptation technique by 70% in mobile scenarios and by upto 2.8\texttimes{",Proceedings of the ACM SIGCOMM 2011 Conference,158–169,12,"rateless coding, rate adaptation, collision decoding, hidden terminals","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1021,article,"Gollakota, Shyamnath and Adib, Fadel and Katabi, Dina and Seshan, Srinivasan",Clearing the RF Smog: Making 802.11n Robust to Cross-Technology Interference,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018456,10.1145/2043164.2018456,"Recent studies show that high-power cross-technology interference is becoming a major problem in today's 802.11 networks. Devices like baby monitors and cordless phones can cause a wireless LAN to lose connectivity. The existing approach for dealing with such high-power interferers makes the 802.11 network switch to a different channel; yet the ISM band is becoming increasingly crowded with diverse technologies, and hence many 802.11 access points may not find an interference-free channel.This paper presents TIMO, a MIMO design that enables 802.11n to communicate in the presence of high-power cross-technology interference. Unlike existing MIMO designs, however, which require all concurrent transmissions to belong to the same technology, TIMO can exploit MIMO capabilities to decode in the presence of a signal from a different technology, hence enabling diverse technologies to share the same frequency band. We implement a prototype of TIMO in GNURadio-USRP2 and show that it enables 802.11n to communicate in the presence of interference from baby monitors, cordless phones, and microwave ovens, transforming scenarios with a complete loss of connectivity to operational networks.",,170–181,12,"cross-technology interference, cognitive mimo",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1022,inproceedings,"Gollakota, Shyamnath and Adib, Fadel and Katabi, Dina and Seshan, Srinivasan",Clearing the RF Smog: Making 802.11n Robust to Cross-Technology Interference,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018456,10.1145/2018436.2018456,"Recent studies show that high-power cross-technology interference is becoming a major problem in today's 802.11 networks. Devices like baby monitors and cordless phones can cause a wireless LAN to lose connectivity. The existing approach for dealing with such high-power interferers makes the 802.11 network switch to a different channel; yet the ISM band is becoming increasingly crowded with diverse technologies, and hence many 802.11 access points may not find an interference-free channel.This paper presents TIMO, a MIMO design that enables 802.11n to communicate in the presence of high-power cross-technology interference. Unlike existing MIMO designs, however, which require all concurrent transmissions to belong to the same technology, TIMO can exploit MIMO capabilities to decode in the presence of a signal from a different technology, hence enabling diverse technologies to share the same frequency band. We implement a prototype of TIMO in GNURadio-USRP2 and show that it enables 802.11n to communicate in the presence of interference from baby monitors, cordless phones, and microwave ovens, transforming scenarios with a complete loss of connectivity to operational networks.",Proceedings of the ACM SIGCOMM 2011 Conference,170–181,12,"cross-technology interference, cognitive mimo","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1023,inproceedings,"Mahajan, Ratul",Session Details: Network Modeling,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256193,10.1145/3256193,,Proceedings of the ACM SIGCOMM 2011 Conference,,,,"Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1024,article,"Rahman, Rameez and Vink\'{o",Design Space Analysis for Modeling Incentives in Distributed Systems,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018458,10.1145/2043164.2018458,"Distributed systems without a central authority, such as peer-to-peer (P2P) systems, employ incentives to encourage nodes to follow the prescribed protocol. Game theoretic analysis is often used to evaluate incentives in such systems. However, most game-theoretic analyses of distributed systems do not adequately model the repeated interactions of nodes inherent in such systems. We present a game-theoretic analysis of a popular P2P protocol, Bit-Torrent, that models the repeated interactions in such protocols. We also note that an analytical approach for modeling incentives is often infeasible given the complicated nature of most deployed protocols. In order to comprehensively model incentives in complex protocols, we propose a simulation-based method, which we call Design Space Analysis (DSA). DSA provides a tractable analysis of competing protocol variants within a detailed design space. We apply DSA to P2P file swarming systems. With extensive simulations we analyze a wide-range of protocol variants and gain insights into their robustness and performance. To validate these results and to demonstrate the efficacy of DSA, we modify an instrumented BitTorrent client and evaluate protocols discovered using DSA. We show that they yield higher system performance and robustness relative to the reference implementation.",,182–193,12,"robustness, design space analysis, incentive systems, game theory",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1025,inproceedings,"Rahman, Rameez and Vink\'{o",Design Space Analysis for Modeling Incentives in Distributed Systems,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018458,10.1145/2018436.2018458,"Distributed systems without a central authority, such as peer-to-peer (P2P) systems, employ incentives to encourage nodes to follow the prescribed protocol. Game theoretic analysis is often used to evaluate incentives in such systems. However, most game-theoretic analyses of distributed systems do not adequately model the repeated interactions of nodes inherent in such systems. We present a game-theoretic analysis of a popular P2P protocol, Bit-Torrent, that models the repeated interactions in such protocols. We also note that an analytical approach for modeling incentives is often infeasible given the complicated nature of most deployed protocols. In order to comprehensively model incentives in complex protocols, we propose a simulation-based method, which we call Design Space Analysis (DSA). DSA provides a tractable analysis of competing protocol variants within a detailed design space. We apply DSA to P2P file swarming systems. With extensive simulations we analyze a wide-range of protocol variants and gain insights into their robustness and performance. To validate these results and to demonstrate the efficacy of DSA, we modify an instrumented BitTorrent client and evaluate protocols discovered using DSA. We show that they yield higher system performance and robustness relative to the reference implementation.",Proceedings of the ACM SIGCOMM 2011 Conference,182–193,12,"design space analysis, game theory, robustness, incentive systems","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1026,article,"Valancius, Vytautas and Lumezanu, Cristian and Feamster, Nick and Johari, Ramesh and Vazirani, Vijay V.",How Many Tiers? Pricing in the Internet Transit Market,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018459,10.1145/2043164.2018459,"ISPs are increasingly selling ""tiered"" contracts, which offer Internet connectivity to wholesale customers in bundles, at rates based on the cost of the links that the traffic in the bundle is traversing. Although providers have already begun to implement and deploy tiered pricing contracts, little is known about how to structure them. While contracts that sell connectivity on finer granularities improve market efficiency, they are also more costly for ISPs to implement and more difficult for customers to understand. Our goal is to analyze whether current tiered pricing practices in the wholesale transit market yield optimal profits for ISPs and whether better bundling strategies might exist. In the process, we deliver two contributions: 1) we develop a novel way of mapping traffic and topology data to a demand and cost model, and 2) we fit this model on three large real-world networks: an European transit ISP, a content distribution network, and an academic research network, and run counterfactuals to evaluate the effects of different bundling strategies. Our results show that the common ISP practice of structuring tiered contracts according to the cost of carrying the traffic flows (e.g., offering a discount for traffic that is local) can be suboptimal and that dividing contracts based on both traffic demand and the cost of carrying it into only three or four tiers yields near-optimal profit for the ISP.",,194–205,12,"network economics, bandwidth pricing",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1027,inproceedings,"Valancius, Vytautas and Lumezanu, Cristian and Feamster, Nick and Johari, Ramesh and Vazirani, Vijay V.",How Many Tiers? Pricing in the Internet Transit Market,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018459,10.1145/2018436.2018459,"ISPs are increasingly selling ""tiered"" contracts, which offer Internet connectivity to wholesale customers in bundles, at rates based on the cost of the links that the traffic in the bundle is traversing. Although providers have already begun to implement and deploy tiered pricing contracts, little is known about how to structure them. While contracts that sell connectivity on finer granularities improve market efficiency, they are also more costly for ISPs to implement and more difficult for customers to understand. Our goal is to analyze whether current tiered pricing practices in the wholesale transit market yield optimal profits for ISPs and whether better bundling strategies might exist. In the process, we deliver two contributions: 1) we develop a novel way of mapping traffic and topology data to a demand and cost model, and 2) we fit this model on three large real-world networks: an European transit ISP, a content distribution network, and an academic research network, and run counterfactuals to evaluate the effects of different bundling strategies. Our results show that the common ISP practice of structuring tiered contracts according to the cost of carrying the traffic flows (e.g., offering a discount for traffic that is local) can be suboptimal and that dividing contracts based on both traffic demand and the cost of carrying it into only three or four tiers yields near-optimal profit for the ISP.",Proceedings of the ACM SIGCOMM 2011 Conference,194–205,12,"network economics, bandwidth pricing","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1028,article,"Akhshabi, Saamer and Dovrolis, Constantine",The Evolution of Layered Protocol Stacks Leads to an Hourglass-Shaped Architecture,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018460,10.1145/2043164.2018460,"The Internet protocol stack has a layered architecture that resembles an hourglass. The lower and higher layers tend to see frequent innovations, while the protocols at the waist of the hourglass appear to be ""ossified"". We propose EvoArch, an abstract model for studying protocol stacks and their evolution. EvoArch is based on a few principles about layered network architectures and their evolution in a competitive environment where protocols acquire value based on their higher layer applications and compete with other protocols at the same layer. EvoArch produces an hourglass structure that is similar to the Internet architecture from general initial conditions and in a robust manner. It also suggests a plausible explanation why some protocols, such as TCP or IP, managed to survive much longer than most other protocols at the same layers. Furthermore, it suggests ways to design more competitive new protocols and more evolvable future Internet architectures.",,206–217,12,"future internet, network science, internet architecture, evolution, evolutionary kernels, layering",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1029,inproceedings,"Akhshabi, Saamer and Dovrolis, Constantine",The Evolution of Layered Protocol Stacks Leads to an Hourglass-Shaped Architecture,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018460,10.1145/2018436.2018460,"The Internet protocol stack has a layered architecture that resembles an hourglass. The lower and higher layers tend to see frequent innovations, while the protocols at the waist of the hourglass appear to be ""ossified"". We propose EvoArch, an abstract model for studying protocol stacks and their evolution. EvoArch is based on a few principles about layered network architectures and their evolution in a competitive environment where protocols acquire value based on their higher layer applications and compete with other protocols at the same layer. EvoArch produces an hourglass structure that is similar to the Internet architecture from general initial conditions and in a robust manner. It also suggests a plausible explanation why some protocols, such as TCP or IP, managed to survive much longer than most other protocols at the same layers. Furthermore, it suggests ways to design more competitive new protocols and more evolvable future Internet architectures.",Proceedings of the ACM SIGCOMM 2011 Conference,206–217,12,"layering, evolution, evolutionary kernels, network science, future internet, internet architecture","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1030,inproceedings,"Seshan, Srinivasan",Session Details: Neat Tricks,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256194,10.1145/3256194,,Proceedings of the ACM SIGCOMM 2011 Conference,,,,"Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1031,article,"Eppstein, David and Goodrich, Michael T. and Uyeda, Frank and Varghese, George",What's the Difference? Efficient Set Reconciliation without Prior Context,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018462,10.1145/2043164.2018462,"We describe a synopsis structure, the Difference Digest, that allows two nodes to compute the elements belonging to the set difference in a single round with communication overhead proportional to the size of the difference times the logarithm of the keyspace. While set reconciliation can be done efficiently using logs, logs require overhead for every update and scale poorly when multiple users are to be reconciled. By contrast, our abstraction assumes no prior context and is useful in networking and distributed systems applications such as trading blocks in a peer-to-peer network, and synchronizing link-state databases after a partition.Our basic set-reconciliation method has a similarity with the peeling algorithm used in Tornado codes [6], which is not surprising, as there is an intimate connection between set difference and coding. Beyond set reconciliation, an essential component in our Difference Digest is a new estimator for the size of the set difference that outperforms min-wise sketches [3] for small set differences.Our experiments show that the Difference Digest is more efficient than prior approaches such as Approximate Reconciliation Trees [5] and Characteristic Polynomial Interpolation [17]. We use Difference Digests to implement a generic KeyDiff service in Linux that runs over TCP and returns the sets of keys that differ between machines.",,218–229,12,"difference digest, set difference, invertible bloom filter",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1032,inproceedings,"Eppstein, David and Goodrich, Michael T. and Uyeda, Frank and Varghese, George",What's the Difference? Efficient Set Reconciliation without Prior Context,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018462,10.1145/2018436.2018462,"We describe a synopsis structure, the Difference Digest, that allows two nodes to compute the elements belonging to the set difference in a single round with communication overhead proportional to the size of the difference times the logarithm of the keyspace. While set reconciliation can be done efficiently using logs, logs require overhead for every update and scale poorly when multiple users are to be reconciled. By contrast, our abstraction assumes no prior context and is useful in networking and distributed systems applications such as trading blocks in a peer-to-peer network, and synchronizing link-state databases after a partition.Our basic set-reconciliation method has a similarity with the peeling algorithm used in Tornado codes [6], which is not surprising, as there is an intimate connection between set difference and coding. Beyond set reconciliation, an essential component in our Difference Digest is a new estimator for the size of the set difference that outperforms min-wise sketches [3] for small set differences.Our experiments show that the Difference Digest is more efficient than prior approaches such as Approximate Reconciliation Trees [5] and Characteristic Polynomial Interpolation [17]. We use Difference Digests to implement a generic KeyDiff service in Linux that runs over TCP and returns the sets of keys that differ between machines.",Proceedings of the ACM SIGCOMM 2011 Conference,218–229,12,"difference digest, invertible bloom filter, set difference","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1033,article,"Hong, Steven Siying and Katti, Sachin Rajsekhar",DOF: A Local Wireless Information Plane,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018463,10.1145/2043164.2018463,"The ability to detect what unlicensed radios are operating in a neigh borhood, their spectrum occupancies and the spatial directions their signals are traversing is a fundamental primitive needed by many applications, ranging from smart radios to coexistence to network management to security. In this paper we present DOF, a detector that in a single framework accurately estimates all three parameters. DOF builds on the insight that in most wireless protocols, there are hidden repeating patterns in the signals that can be used to construct unique signatures, and accurately estimate signal types and their spectral and spatial parameters. We show via experimental evaluation in an indoor testbed that DOF is robust and accurate, it achieves greater than 85% accuracy even when the SNRs of the detected signals are as low as 0 dB, and even when there are multiple interfering signals present. To demonstrate the benefits of DOF, we design and implement a preliminary prototype of a smart radio that operates on top of DOF, and show experimentally that it provides a 80% increase in throughput over Jello, the best known prior implementation, while causing less than 10% performance drop for co-existing WiFi and Zigbee radios.",,230–241,12,"wireless degrees of freedom, smart radios",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1034,inproceedings,"Hong, Steven Siying and Katti, Sachin Rajsekhar",DOF: A Local Wireless Information Plane,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018463,10.1145/2018436.2018463,"The ability to detect what unlicensed radios are operating in a neigh borhood, their spectrum occupancies and the spatial directions their signals are traversing is a fundamental primitive needed by many applications, ranging from smart radios to coexistence to network management to security. In this paper we present DOF, a detector that in a single framework accurately estimates all three parameters. DOF builds on the insight that in most wireless protocols, there are hidden repeating patterns in the signals that can be used to construct unique signatures, and accurately estimate signal types and their spectral and spatial parameters. We show via experimental evaluation in an indoor testbed that DOF is robust and accurate, it achieves greater than 85% accuracy even when the SNRs of the detected signals are as low as 0 dB, and even when there are multiple interfering signals present. To demonstrate the benefits of DOF, we design and implement a preliminary prototype of a smart radio that operates on top of DOF, and show experimentally that it provides a 80% increase in throughput over Jello, the best known prior implementation, while causing less than 10% performance drop for co-existing WiFi and Zigbee radios.",Proceedings of the ACM SIGCOMM 2011 Conference,230–241,12,"smart radios, wireless degrees of freedom","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1035,inproceedings,"Oran, David",Session Details: Data Center Network Performance,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256195,10.1145/3256195,,Proceedings of the ACM SIGCOMM 2011 Conference,,,,"Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1036,article,"Ballani, Hitesh and Costa, Paolo and Karagiannis, Thomas and Rowstron, Ant",Towards Predictable Datacenter Networks,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018465,10.1145/2043164.2018465,"The shared nature of the network in today's multi-tenant datacenters implies that network performance for tenants can vary significantly. This applies to both production datacenters and cloud environments. Network performance variability hurts application performance which makes tenant costs unpredictable and causes provider revenue loss. Motivated by these factors, this paper makes the case for extending the tenant-provider interface to explicitly account for the network. We argue this can be achieved by providing tenants with a virtual network connecting their compute instances. To this effect, the key contribution of this paper is the design of virtual network abstractions that capture the trade-off between the performance guarantees offered to tenants, their costs and the provider revenue.To illustrate the feasibility of virtual networks, we develop Oktopus, a system that implements the proposed abstractions. Using realistic, large-scale simulations and an Oktopus deployment on a 25-node two-tier testbed, we demonstrate that the use of virtual networks yields significantly better and more predictable tenant performance. Further, using a simple pricing model, we find that the our abstractions can reduce tenant costs by up to 74% while maintaining provider revenue neutrality.",,242–253,12,"virtual network, bandwidth, datacenter, allocation",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1037,inproceedings,"Ballani, Hitesh and Costa, Paolo and Karagiannis, Thomas and Rowstron, Ant",Towards Predictable Datacenter Networks,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018465,10.1145/2018436.2018465,"The shared nature of the network in today's multi-tenant datacenters implies that network performance for tenants can vary significantly. This applies to both production datacenters and cloud environments. Network performance variability hurts application performance which makes tenant costs unpredictable and causes provider revenue loss. Motivated by these factors, this paper makes the case for extending the tenant-provider interface to explicitly account for the network. We argue this can be achieved by providing tenants with a virtual network connecting their compute instances. To this effect, the key contribution of this paper is the design of virtual network abstractions that capture the trade-off between the performance guarantees offered to tenants, their costs and the provider revenue.To illustrate the feasibility of virtual networks, we develop Oktopus, a system that implements the proposed abstractions. Using realistic, large-scale simulations and an Oktopus deployment on a 25-node two-tier testbed, we demonstrate that the use of virtual networks yields significantly better and more predictable tenant performance. Further, using a simple pricing model, we find that the our abstractions can reduce tenant costs by up to 74% while maintaining provider revenue neutrality.",Proceedings of the ACM SIGCOMM 2011 Conference,242–253,12,"virtual network, allocation, bandwidth, datacenter","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1038,article,"Curtis, Andrew R. and Mogul, Jeffrey C. and Tourrilhes, Jean and Yalagandula, Praveen and Sharma, Puneet and Banerjee, Sujata",DevoFlow: Scaling Flow Management for High-Performance Networks,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018466,10.1145/2043164.2018466,"OpenFlow is a great concept, but its original design imposes excessive overheads. It can simplify network and traffic management in enterprise and data center environments, because it enables flow-level control over Ethernet switching and provides global visibility of the flows in the network. However, such fine-grained control and visibility comes with costs: the switch-implementation costs of involving the switch's control-plane too often and the distributed-system costs of involving the OpenFlow controller too frequently, both on flow setups and especially for statistics-gathering.In this paper, we analyze these overheads, and show that OpenFlow's current design cannot meet the needs of high-performance networks. We design and evaluate DevoFlow, a modification of the OpenFlow model which gently breaks the coupling between control and global visibility, in a way that maintains a useful amount of visibility without imposing unnecessary costs. We evaluate DevoFlow through simulations, and find that it can load-balance data center traffic as well as fine-grained solutions, without as much overhead: DevoFlow uses 10--53 times fewer flow table entries at an average switch, and uses 10--42 times fewer control messages.",,254–265,12,"data center, flow-based networking, switch design",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1039,inproceedings,"Curtis, Andrew R. and Mogul, Jeffrey C. and Tourrilhes, Jean and Yalagandula, Praveen and Sharma, Puneet and Banerjee, Sujata",DevoFlow: Scaling Flow Management for High-Performance Networks,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018466,10.1145/2018436.2018466,"OpenFlow is a great concept, but its original design imposes excessive overheads. It can simplify network and traffic management in enterprise and data center environments, because it enables flow-level control over Ethernet switching and provides global visibility of the flows in the network. However, such fine-grained control and visibility comes with costs: the switch-implementation costs of involving the switch's control-plane too often and the distributed-system costs of involving the OpenFlow controller too frequently, both on flow setups and especially for statistics-gathering.In this paper, we analyze these overheads, and show that OpenFlow's current design cannot meet the needs of high-performance networks. We design and evaluate DevoFlow, a modification of the OpenFlow model which gently breaks the coupling between control and global visibility, in a way that maintains a useful amount of visibility without imposing unnecessary costs. We evaluate DevoFlow through simulations, and find that it can load-balance data center traffic as well as fine-grained solutions, without as much overhead: DevoFlow uses 10--53 times fewer flow table entries at an average switch, and uses 10--42 times fewer control messages.",Proceedings of the ACM SIGCOMM 2011 Conference,254–265,12,"switch design, data center, flow-based networking","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1040,article,"Raiciu, Costin and Barre, Sebastien and Pluntke, Christopher and Greenhalgh, Adam and Wischik, Damon and Handley, Mark",Improving Datacenter Performance and Robustness with Multipath TCP,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018467,10.1145/2043164.2018467,"The latest large-scale data centers offer higher aggregate bandwidth and robustness by creating multiple paths in the core of the net- work. To utilize this bandwidth requires different flows take different paths, which poses a challenge. In short, a single-path transport seems ill-suited to such networks.We propose using Multipath TCP as a replacement for TCP in such data centers, as it can effectively and seamlessly use available bandwidth, giving improved throughput and better fairness on many topologies. We investigate what causes these benefits, teasing apart the contribution of each of the mechanisms used by MPTCP.Using MPTCP lets us rethink data center networks, with a different mindset as to the relationship between transport protocols, rout- ing and topology. MPTCP enables topologies that single path TCP cannot utilize. As a proof-of-concept, we present a dual-homed variant of the FatTree topology. With MPTCP, this outperforms FatTree for a wide range of workloads, but costs the same.In existing data centers, MPTCP is readily deployable leveraging widely deployed technologies such as ECMP. We have run MPTCP on Amazon EC2 and found that it outperforms TCP by a factor of three when there is path diversity. But the biggest benefits will come when data centers are designed for multipath transports.",,266–277,12,"mptcp, data center",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1041,inbook,"Raiciu, Costin and Barre, Sebastien and Pluntke, Christopher and Greenhalgh, Adam and Wischik, Damon and Handley, Mark",Improving Datacenter Performance and Robustness with Multipath TCP,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018467,,"The latest large-scale data centers offer higher aggregate bandwidth and robustness by creating multiple paths in the core of the net- work. To utilize this bandwidth requires different flows take different paths, which poses a challenge. In short, a single-path transport seems ill-suited to such networks.We propose using Multipath TCP as a replacement for TCP in such data centers, as it can effectively and seamlessly use available bandwidth, giving improved throughput and better fairness on many topologies. We investigate what causes these benefits, teasing apart the contribution of each of the mechanisms used by MPTCP.Using MPTCP lets us rethink data center networks, with a different mindset as to the relationship between transport protocols, rout- ing and topology. MPTCP enables topologies that single path TCP cannot utilize. As a proof-of-concept, we present a dual-homed variant of the FatTree topology. With MPTCP, this outperforms FatTree for a wide range of workloads, but costs the same.In existing data centers, MPTCP is readily deployable leveraging widely deployed technologies such as ECMP. We have run MPTCP on Amazon EC2 and found that it outperforms TCP by a factor of three when there is path diversity. But the biggest benefits will come when data centers are designed for multipath transports.",Proceedings of the ACM SIGCOMM 2011 Conference,266–277,12,,,,,,,,,
1042,inproceedings,"Katti, Sachin",Session Details: Network Management -- Reasoning and Debugging,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256196,10.1145/3256196,,Proceedings of the ACM SIGCOMM 2011 Conference,,,,"Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1043,article,"Shieh, Alan and Sirer, Emin G\""{u",NetQuery: A Knowledge Plane for Reasoning about Network Properties,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018469,10.1145/2043164.2018469,"This paper presents the design and implementation of NetQuery, a knowledge plane for federated networks such as the Internet. In such networks, not all administrative domains will generate information that an application can trust and many administrative domains may have restrictive policies on disclosing network information. Thus, both the trustworthiness and accessibility of network information pose obstacles to effective reasoning. NetQuery employs trustworthy computing techniques to facilitate reasoning about the trustworthiness of information contained in the knowledge plane while preserving confidentiality guarantees for operator data. By characterizing information disclosure between operators, NetQuery enables remote verification of advertised claims and contractual stipulations; this enables new applications because network guarantees can span administrative boundaries. We have implemented NetQuery, built several NetQuery-enabled devices, and deployed applications for cloud datacenters, enterprise networks, and the Internet. Simulations, testbed experiments, and a deployment on a departmental network indicate NetQuery can support hundreds of thousands of operations per second and can thus scale to large ISPs.",,278–289,12,"trustworthy computing, knowledge plane, tpm",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1044,inproceedings,"Shieh, Alan and Sirer, Emin G\""{u",NetQuery: A Knowledge Plane for Reasoning about Network Properties,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018469,10.1145/2018436.2018469,"This paper presents the design and implementation of NetQuery, a knowledge plane for federated networks such as the Internet. In such networks, not all administrative domains will generate information that an application can trust and many administrative domains may have restrictive policies on disclosing network information. Thus, both the trustworthiness and accessibility of network information pose obstacles to effective reasoning. NetQuery employs trustworthy computing techniques to facilitate reasoning about the trustworthiness of information contained in the knowledge plane while preserving confidentiality guarantees for operator data. By characterizing information disclosure between operators, NetQuery enables remote verification of advertised claims and contractual stipulations; this enables new applications because network guarantees can span administrative boundaries. We have implemented NetQuery, built several NetQuery-enabled devices, and deployed applications for cloud datacenters, enterprise networks, and the Internet. Simulations, testbed experiments, and a deployment on a departmental network indicate NetQuery can support hundreds of thousands of operations per second and can thus scale to large ISPs.",Proceedings of the ACM SIGCOMM 2011 Conference,278–289,12,"knowledge plane, trustworthy computing, tpm","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1045,article,"Mai, Haohui and Khurshid, Ahmed and Agarwal, Rachit and Caesar, Matthew and Godfrey, P. Brighten and King, Samuel Talmadge",Debugging the Data Plane with Anteater,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018470,10.1145/2043164.2018470,"Diagnosing problems in networks is a time-consuming and error-prone process. Existing tools to assist operators primarily focus on analyzing control plane configuration. Configuration analysis is limited in that it cannot find bugs in router software, and is harder to generalize across protocols since it must model complex configuration languages and dynamic protocol behavior.This paper studies an alternate approach: diagnosing problems through static analysis of the data plane. This approach can catch bugs that are invisible at the level of configuration files, and simplifies unified analysis of a network across many protocols and implementations. We present Anteater, a tool for checking invariants in the data plane. Anteater translates high-level network invariants into boolean satisfiability problems (SAT), checks them against network state using a SAT solver, and reports counterexamples if violations have been found. Applied to a large university network, Anteater revealed 23 bugs, including forwarding loops and stale ACL rules, with only five false positives. Nine of these faults are being fixed by campus network operators.",,290–301,12,"network troubleshooting, boolean satisfiability, data plane analysis",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1046,inproceedings,"Mai, Haohui and Khurshid, Ahmed and Agarwal, Rachit and Caesar, Matthew and Godfrey, P. Brighten and King, Samuel Talmadge",Debugging the Data Plane with Anteater,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018470,10.1145/2018436.2018470,"Diagnosing problems in networks is a time-consuming and error-prone process. Existing tools to assist operators primarily focus on analyzing control plane configuration. Configuration analysis is limited in that it cannot find bugs in router software, and is harder to generalize across protocols since it must model complex configuration languages and dynamic protocol behavior.This paper studies an alternate approach: diagnosing problems through static analysis of the data plane. This approach can catch bugs that are invisible at the level of configuration files, and simplifies unified analysis of a network across many protocols and implementations. We present Anteater, a tool for checking invariants in the data plane. Anteater translates high-level network invariants into boolean satisfiability problems (SAT), checks them against network state using a SAT solver, and reports counterexamples if violations have been found. Applied to a large university network, Anteater revealed 23 bugs, including forwarding loops and stale ACL rules, with only five false positives. Nine of these faults are being fixed by campus network operators.",Proceedings of the ACM SIGCOMM 2011 Conference,290–301,12,"boolean satisfiability, network troubleshooting, data plane analysis","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1047,article,"Benson, Theophilus and Akella, Aditya and Shaikh, Aman",Demystifying Configuration Challenges and Trade-Offs in Network-Based ISP Services,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018471,10.1145/2043164.2018471,"ISPs are increasingly offering a variety of network-based services such as VPN, VPLS, VoIP, Virtual-Wire and DDoS protection. Although both enterprise and residential networks are rapidly adopting these services, there is little systematic work on the design challenges and trade-offs ISPs face in providing them. The goal of our paper is to understand the complexity underlying the layer-3 design of services and to highlight potential factors that hinder their introduction, evolution and management. Using daily snapshots of configuration and device metadata collected from a tier-1 ISP, we examine the logical dependencies and special cases in device configurations for five different network-based services. We find: (1) the design of the core data-plane is usually service-agnostic and simple, but the control-planes for different services become more complex as services evolve; (2) more crucially, the configuration at the service edge inevitably becomes more complex over time, potentially hindering key management issues such as service upgrades and troubleshooting; and (3) there are key service-specific issues that also contribute significantly to the overall design complexity. Thus, the high prevalent complexity could impede the adoption and growth of network-based services. We show initial evidence that some of the complexity can be mitigated systematically.",,302–313,12,"network modeling, configuration analysis, network services",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1048,inproceedings,"Benson, Theophilus and Akella, Aditya and Shaikh, Aman",Demystifying Configuration Challenges and Trade-Offs in Network-Based ISP Services,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018471,10.1145/2018436.2018471,"ISPs are increasingly offering a variety of network-based services such as VPN, VPLS, VoIP, Virtual-Wire and DDoS protection. Although both enterprise and residential networks are rapidly adopting these services, there is little systematic work on the design challenges and trade-offs ISPs face in providing them. The goal of our paper is to understand the complexity underlying the layer-3 design of services and to highlight potential factors that hinder their introduction, evolution and management. Using daily snapshots of configuration and device metadata collected from a tier-1 ISP, we examine the logical dependencies and special cases in device configurations for five different network-based services. We find: (1) the design of the core data-plane is usually service-agnostic and simple, but the control-planes for different services become more complex as services evolve; (2) more crucially, the configuration at the service edge inevitably becomes more complex over time, potentially hindering key management issues such as service upgrades and troubleshooting; and (3) there are key service-specific issues that also contribute significantly to the overall design complexity. Thus, the high prevalent complexity could impede the adoption and growth of network-based services. We show initial evidence that some of the complexity can be mitigated systematically.",Proceedings of the ACM SIGCOMM 2011 Conference,302–313,12,"network services, network modeling, configuration analysis","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1049,inproceedings,"Willinger, Walter",Session Details: ISPs and Wide-Area Networking,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256197,10.1145/3256197,,Proceedings of the ACM SIGCOMM 2011 Conference,,,,"Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1050,article,"Vanbever, Laurent and Vissicchio, Stefano and Pelsser, Cristel and Francois, Pierre and Bonaventure, Olivier",Seamless Network-Wide IGP Migrations,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018473,10.1145/2043164.2018473,"Network-wide migrations of a running network, such as the replacement of a routing protocol or the modification of its configuration, can improve the performance, scalability, manageability, and security of the entire network. However, such migrations are an important source of concerns for network operators as the reconfiguration campaign can lead to long and service-affecting outages.In this paper, we propose a methodology which addresses the problem of seamlessly modifying the configuration of commonly used link-state Interior Gateway Protocols (IGP). We illustrate the benefits of our methodology by considering several migration scenarios, including the addition or the removal of routing hierarchy in an existing IGP and the replacement of one IGP with another. We prove that a strict operational ordering can guarantee that the migration will not create IP transit service outages. Although finding a safe ordering is NP complete, we describe techniques which efficiently find such an ordering and evaluate them using both real-world and inferred ISP topologies. Finally, we describe the implementation of a provisioning system which automatically performs the migration by pushing the configurations on the routers in the appropriate order, while monitoring the entire migration process.",,314–325,12,"design guidelines, configuration, summarization, interior gateway protocol (igp), migration",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1051,inproceedings,"Vanbever, Laurent and Vissicchio, Stefano and Pelsser, Cristel and Francois, Pierre and Bonaventure, Olivier",Seamless Network-Wide IGP Migrations,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018473,10.1145/2018436.2018473,"Network-wide migrations of a running network, such as the replacement of a routing protocol or the modification of its configuration, can improve the performance, scalability, manageability, and security of the entire network. However, such migrations are an important source of concerns for network operators as the reconfiguration campaign can lead to long and service-affecting outages.In this paper, we propose a methodology which addresses the problem of seamlessly modifying the configuration of commonly used link-state Interior Gateway Protocols (IGP). We illustrate the benefits of our methodology by considering several migration scenarios, including the addition or the removal of routing hierarchy in an existing IGP and the replacement of one IGP with another. We prove that a strict operational ordering can guarantee that the migration will not create IP transit service outages. Although finding a safe ordering is NP complete, we describe techniques which efficiently find such an ordering and evaluate them using both real-world and inferred ISP topologies. Finally, we describe the implementation of a provisioning system which automatically performs the migration by pushing the configurations on the routers in the appropriate order, while monitoring the entire migration process.",Proceedings of the ACM SIGCOMM 2011 Conference,314–325,12,"interior gateway protocol (igp), migration, design guidelines, summarization, configuration","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1052,article,"Peterson, Ryan S. and Wong, Bernard and Sirer, Emin Gun",A Content Propagation Metric for Efficient Content Distribution,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018474,10.1145/2043164.2018474,"Efficient content distribution in large networks comprising datacenters, end hosts, and distributed in-network caches is a difficult problem. Existing systems rely on mechanisms and metrics that fail to effectively utilize all available sources of bandwidth in the network. This paper presents a novel metric, called the Content Propagation Metric (CPM), for quantitatively evaluating the marginal benefit of available bandwidth to competing consumers, enabling efficient utilization of the bandwidth resource. The metric is simple to implement, imposes only a modest overhead, and can be retrofitted easily into existing content distribution systems. We have designed and implemented a high-performance content distribution system, called V-Formation, based on the CPM. The CPM guides V-Formation toward a global allocation of bandwidth that maximizes the aggregate download bandwidth of consumers. Results from a PlanetLab deployment and extensive simulations show that V-Formation achieves high aggregate bandwidth and that the CPM enables hosts to converge quickly on a stable allocation of resources in a wide range of deployment scenarios.",,326–337,12,"peer-to-peer, hybrid, content distribution",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1053,inproceedings,"Peterson, Ryan S. and Wong, Bernard and Sirer, Emin Gun",A Content Propagation Metric for Efficient Content Distribution,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018474,10.1145/2018436.2018474,"Efficient content distribution in large networks comprising datacenters, end hosts, and distributed in-network caches is a difficult problem. Existing systems rely on mechanisms and metrics that fail to effectively utilize all available sources of bandwidth in the network. This paper presents a novel metric, called the Content Propagation Metric (CPM), for quantitatively evaluating the marginal benefit of available bandwidth to competing consumers, enabling efficient utilization of the bandwidth resource. The metric is simple to implement, imposes only a modest overhead, and can be retrofitted easily into existing content distribution systems. We have designed and implemented a high-performance content distribution system, called V-Formation, based on the CPM. The CPM guides V-Formation toward a global allocation of bandwidth that maximizes the aggregate download bandwidth of consumers. Results from a PlanetLab deployment and extensive simulations show that V-Formation achieves high aggregate bandwidth and that the CPM enables hosts to converge quickly on a stable allocation of resources in a wide range of deployment scenarios.",Proceedings of the ACM SIGCOMM 2011 Conference,326–337,12,"content distribution, peer-to-peer, hybrid","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1054,article,"Goma, Eduard and Canini, Marco and Lopez Toledo, Alberto and Laoutaris, Nikolaos and Kosti\'{c",Insomnia in the Access: Or How to Curb Access Network Related Energy Consumption,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018475,10.1145/2043164.2018475,"Access networks include modems, home gateways, and DSL Access Multiplexers (DSLAMs), and are responsible for 70-80% of total network-based energy consumption. In this paper, we take an in-depth look at the problem of greening access networks, identify root problems, and propose practical solutions for their user- and ISP-parts. On the user side, the combination of continuous light traffic and lack of alternative paths condemns gateways to being powered most of the time despite having Sleep-on-Idle (SoI) capabilities. To address this, we introduce Broadband Hitch-Hiking (BH2), that takes advantage of the overlap of wireless networks to aggregate user traffic in as few gateways as possible. In current urban settings BH2 can power off 65-90% of gateways. Powering off gateways permits the remaining ones to synchronize at higher speeds due to reduced crosstalk from having fewer active lines. Our tests reveal speedup up to 25%. On the ISP side, we propose introducing simple inexpensive switches at the distribution frame for batching active lines to a subset of cards letting the remaining ones sleep. Overall, our results show an 80% energy savings margin in access networks. The combination of B2 and switching gets close to this margin, saving 66% on average.",,338–349,12,"broadband access networks, energy",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1055,inproceedings,"Goma, Eduard and Canini, Marco and Lopez Toledo, Alberto and Laoutaris, Nikolaos and Kosti\'{c",Insomnia in the Access: Or How to Curb Access Network Related Energy Consumption,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018475,10.1145/2018436.2018475,"Access networks include modems, home gateways, and DSL Access Multiplexers (DSLAMs), and are responsible for 70-80% of total network-based energy consumption. In this paper, we take an in-depth look at the problem of greening access networks, identify root problems, and propose practical solutions for their user- and ISP-parts. On the user side, the combination of continuous light traffic and lack of alternative paths condemns gateways to being powered most of the time despite having Sleep-on-Idle (SoI) capabilities. To address this, we introduce Broadband Hitch-Hiking (BH2), that takes advantage of the overlap of wireless networks to aggregate user traffic in as few gateways as possible. In current urban settings BH2 can power off 65-90% of gateways. Powering off gateways permits the remaining ones to synchronize at higher speeds due to reduced crosstalk from having fewer active lines. Our tests reveal speedup up to 25%. On the ISP side, we propose introducing simple inexpensive switches at the distribution frame for batching active lines to a subset of cards letting the remaining ones sleep. Overall, our results show an 80% energy savings margin in access networks. The combination of B2 and switching gets close to this margin, saving 66% on average.",Proceedings of the ACM SIGCOMM 2011 Conference,338–349,12,"broadband access networks, energy","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1056,inproceedings,"Sirer, Emin G\""{u",Session Details: Network Measurement II -- What's Going On?,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256198,10.1145/3256198,,Proceedings of the ACM SIGCOMM 2011 Conference,,,,"Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1057,article,"Gill, Phillipa and Jain, Navendu and Nagappan, Nachiappan","Understanding Network Failures in Data Centers: Measurement, Analysis, and Implications",2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018477,10.1145/2043164.2018477,"We present the first large-scale analysis of failures in a data center network. Through our analysis, we seek to answer several fundamental questions: which devices/links are most unreliable, what causes failures, how do failures impact network traffic and how effective is network redundancy? We answer these questions using multiple data sources commonly collected by network operators. The key findings of our study are that (1) data center networks show high reliability, (2) commodity switches such as ToRs and AggS are highly reliable, (3) load balancers dominate in terms of failure occurrences with many short-lived software related faults,(4) failures have potential to cause loss of many small packets such as keep alive messages and ACKs, and (5) network redundancy is only 40% effective in reducing the median impact of failure.",,350–361,12,"data centers, network reliability",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1058,inproceedings,"Gill, Phillipa and Jain, Navendu and Nagappan, Nachiappan","Understanding Network Failures in Data Centers: Measurement, Analysis, and Implications",2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018477,10.1145/2018436.2018477,"We present the first large-scale analysis of failures in a data center network. Through our analysis, we seek to answer several fundamental questions: which devices/links are most unreliable, what causes failures, how do failures impact network traffic and how effective is network redundancy? We answer these questions using multiple data sources commonly collected by network operators. The key findings of our study are that (1) data center networks show high reliability, (2) commodity switches such as ToRs and AggS are highly reliable, (3) load balancers dominate in terms of failure occurrences with many short-lived software related faults,(4) failures have potential to cause loss of many small packets such as keep alive messages and ACKs, and (5) network redundancy is only 40% effective in reducing the median impact of failure.",Proceedings of the ACM SIGCOMM 2011 Conference,350–361,12,"network reliability, data centers","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1059,article,"Dobrian, Florin and Sekar, Vyas and Awan, Asad and Stoica, Ion and Joseph, Dilip and Ganjam, Aditya and Zhan, Jibin and Zhang, Hui",Understanding the Impact of Video Quality on User Engagement,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018478,10.1145/2043164.2018478,"As the distribution of the video over the Internet becomes main- stream and its consumption moves from the computer to the TV screen, user expectation for high quality is constantly increasing. In this context, it is crucial for content providers to understand if and how video quality affects user engagement and how to best invest their resources to optimize video quality. This paper is a first step towards addressing these questions. We use a unique dataset that spans different content types, including short video on demand (VoD), long VoD, and live content from popular video con- tent providers. Using client-side instrumentation, we measure quality metrics such as the join time, buffering ratio, average bitrate, rendering quality, and rate of buffering events.We quantify user engagement both at a per-video (or view) level and a per-user (or viewer) level. In particular, we find that the percentage of time spent in buffering (buffering ratio) has the largest impact on the user engagement across all types of content. However, the magnitude of this impact depends on the content type, with live content being the most impacted. For example, a 1% increase in buffering ratio can reduce user engagement by more than three minutes for a 90-minute live video event. We also see that the average bitrate plays a significantly more important role in the case of live content than VoD content.",,362–373,12,"measurement, engagement, video quality",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1060,inproceedings,"Dobrian, Florin and Sekar, Vyas and Awan, Asad and Stoica, Ion and Joseph, Dilip and Ganjam, Aditya and Zhan, Jibin and Zhang, Hui",Understanding the Impact of Video Quality on User Engagement,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018478,10.1145/2018436.2018478,"As the distribution of the video over the Internet becomes main- stream and its consumption moves from the computer to the TV screen, user expectation for high quality is constantly increasing. In this context, it is crucial for content providers to understand if and how video quality affects user engagement and how to best invest their resources to optimize video quality. This paper is a first step towards addressing these questions. We use a unique dataset that spans different content types, including short video on demand (VoD), long VoD, and live content from popular video con- tent providers. Using client-side instrumentation, we measure quality metrics such as the join time, buffering ratio, average bitrate, rendering quality, and rate of buffering events.We quantify user engagement both at a per-video (or view) level and a per-user (or viewer) level. In particular, we find that the percentage of time spent in buffering (buffering ratio) has the largest impact on the user engagement across all types of content. However, the magnitude of this impact depends on the content type, with live content being the most impacted. For example, a 1% increase in buffering ratio can reduce user engagement by more than three minutes for a 90-minute live video event. We also see that the average bitrate plays a significantly more important role in the case of live content than VoD content.",Proceedings of the ACM SIGCOMM 2011 Conference,362–373,12,"engagement, measurement, video quality","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1061,article,"Wang, Zhaoguang and Qian, Zhiyun and Xu, Qiang and Mao, Zhuoqing and Zhang, Ming",An Untold Story of Middleboxes in Cellular Networks,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018479,10.1145/2043164.2018479,"The use of cellular data networks is increasingly popular as network coverage becomes more ubiquitous and many diverse user-contributed mobile applications become available. The growing cellular traffic demand means that cellular network carriers are facing greater challenges to provide users with good network performance and energy efficiency, while protecting networks from potential attacks. To better utilize their limited network resources while securing the network and protecting client devices the carriers have already deployed various network policies that influence traffic behavior. Today, these policies are mostly opaque, though they directly impact application designs and may even introduce network vulnerabilities.We present NetPiculet, the first tool that unveils carriers' NAT and firewall policies by conducting intelligent measurement. By running NetPiculet on the major U.S. cellular providers as well as deploying it as a smartphone application in the wild covering more than 100 cellular ISPs, we identified the key NAT and firewall policies which have direct implications on performance, energy, and security. For example, NAT boxes and firewalls set timeouts for idle TCP connections, which sometimes cause significant energy waste on mobile devices. Although most carriers today deploy sophisticated firewalls, they are still vulnerable to various attacks such as battery draining and denial of service. These findings can inform developers in optimizing the interaction between mobile applications and cellular networks and also guide carriers in improving their network configurations.",,374–385,12,"cellular data network, nat, tcp performance, firewall, middlebox",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1062,inproceedings,"Wang, Zhaoguang and Qian, Zhiyun and Xu, Qiang and Mao, Zhuoqing and Zhang, Ming",An Untold Story of Middleboxes in Cellular Networks,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018479,10.1145/2018436.2018479,"The use of cellular data networks is increasingly popular as network coverage becomes more ubiquitous and many diverse user-contributed mobile applications become available. The growing cellular traffic demand means that cellular network carriers are facing greater challenges to provide users with good network performance and energy efficiency, while protecting networks from potential attacks. To better utilize their limited network resources while securing the network and protecting client devices the carriers have already deployed various network policies that influence traffic behavior. Today, these policies are mostly opaque, though they directly impact application designs and may even introduce network vulnerabilities.We present NetPiculet, the first tool that unveils carriers' NAT and firewall policies by conducting intelligent measurement. By running NetPiculet on the major U.S. cellular providers as well as deploying it as a smartphone application in the wild covering more than 100 cellular ISPs, we identified the key NAT and firewall policies which have direct implications on performance, energy, and security. For example, NAT boxes and firewalls set timeouts for idle TCP connections, which sometimes cause significant energy waste on mobile devices. Although most carriers today deploy sophisticated firewalls, they are still vulnerable to various attacks such as battery draining and denial of service. These findings can inform developers in optimizing the interaction between mobile applications and cellular networks and also guide carriers in improving their network configurations.",Proceedings of the ACM SIGCOMM 2011 Conference,374–385,12,"tcp performance, cellular data network, nat, firewall, middlebox","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1063,article,"Wang, Pengcheng and Gao, Zhaoyu and Xu, Xinhui and Zhou, Yujiao and Zhu, Haojin and Zhu, Kenny Q.",Automatic Inference of Movements from Contact Histories,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018481,10.1145/2043164.2018481,"This paper introduces a new security problem in which individuals movement traces (in terms of accurate routes) can be inferred from just a series of mutual contact records and the map of the area in which they roam around. Such contact records may be obtained through the bluetooth communication on mobile phones.We present an approach that solve the trace inference problem in reasonable time, and analyze some properties of the inference algorithm.",,386–387,2,"traces, location privacy, inference, contacts",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1064,inproceedings,"Wang, Pengcheng and Gao, Zhaoyu and Xu, Xinhui and Zhou, Yujiao and Zhu, Haojin and Zhu, Kenny Q.",Automatic Inference of Movements from Contact Histories,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018481,10.1145/2018436.2018481,"This paper introduces a new security problem in which individuals movement traces (in terms of accurate routes) can be inferred from just a series of mutual contact records and the map of the area in which they roam around. Such contact records may be obtained through the bluetooth communication on mobile phones.We present an approach that solve the trace inference problem in reasonable time, and analyze some properties of the inference algorithm.",Proceedings of the ACM SIGCOMM 2011 Conference,386–387,2,"contacts, traces, inference, location privacy","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1065,article,"Shvartzshnaider, Yan and Ott, Maximilian",Towards a Fully Distributed N-Tuple Store,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018482,10.1145/2043164.2018482,"We present our work towards building a novel distributed n-tuple store by extending the Kademlia DHT [1] algorithm to support n dimensional keys as well as an multi get operator, where some of the dimensions of the ""query"" key can be left unspecified.",,388–389,2,"distributed pattern matching, kademlia",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1066,inproceedings,"Shvartzshnaider, Yan and Ott, Maximilian",Towards a Fully Distributed N-Tuple Store,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018482,10.1145/2018436.2018482,"We present our work towards building a novel distributed n-tuple store by extending the Kademlia DHT [1] algorithm to support n dimensional keys as well as an multi get operator, where some of the dimensions of the ""query"" key can be left unspecified.",Proceedings of the ACM SIGCOMM 2011 Conference,388–389,2,"kademlia, distributed pattern matching","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1067,article,"Li, Qingxi and Zhou, Wenxuan and Caesar, Matthew and Godfrey, P. Brighten",ASAP: A Low-Latency Transport Layer,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018483,10.1145/2043164.2018483,"For interactive networked applications like web browsing, every round-trip time (RTT) matters. We introduce ASAP, a new naming and transport protocol that reduces latency by shortcutting DNS requests and eliminating TCP's three-way handshake, while ensuring the key security property of verifiable provenance of client requests. ASAP eliminates between one and two RTTs, cutting the delay of small requests by up to two-thirds.",,390–391,2,"security, latency, dns, tcp",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1068,inproceedings,"Li, Qingxi and Zhou, Wenxuan and Caesar, Matthew and Godfrey, P. Brighten",ASAP: A Low-Latency Transport Layer,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018483,10.1145/2018436.2018483,"For interactive networked applications like web browsing, every round-trip time (RTT) matters. We introduce ASAP, a new naming and transport protocol that reduces latency by shortcutting DNS requests and eliminating TCP's three-way handshake, while ensuring the key security property of verifiable provenance of client requests. ASAP eliminates between one and two RTTs, cutting the delay of small requests by up to two-thirds.",Proceedings of the ACM SIGCOMM 2011 Conference,390–391,2,"dns, latency, security, tcp","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1069,article,"Woo, Kyungmoon and Kwon, Hyukmin and Kim, Hyun-chul and Kim, Chong-kwon and Kim, Huy Kang",What Can Free Money Tell Us on the Virtual Black Market?,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018484,10.1145/2043164.2018484,"""Real money trading"" or ""Gold farming"" refers to a set of illicit practices for gathering and distributing virtual goods in online games for real money. Unlike previous work, we use network-wide economic interactions among in-game characters as a lens to monitor, detect and identify gold farming networks. Our work is based on a set of real in-game trade activity logs collected for one month in year 2010 from the world's second largest MMORPG called AION (with 3.4 million subscribers). This is the first work that empirically (i) shows that ""free money network"" is a promising measure/approximation for detecting and characterizing gold farming networks, and (ii) measures the size of the free money net and in-game virtual economy in a large-scale MMORPG in terms of the cash flow.",,392–393,2,"onlinr game security, gold farming, real money trading",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1070,inproceedings,"Woo, Kyungmoon and Kwon, Hyukmin and Kim, Hyun-chul and Kim, Chong-kwon and Kim, Huy Kang",What Can Free Money Tell Us on the Virtual Black Market?,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018484,10.1145/2018436.2018484,"""Real money trading"" or ""Gold farming"" refers to a set of illicit practices for gathering and distributing virtual goods in online games for real money. Unlike previous work, we use network-wide economic interactions among in-game characters as a lens to monitor, detect and identify gold farming networks. Our work is based on a set of real in-game trade activity logs collected for one month in year 2010 from the world's second largest MMORPG called AION (with 3.4 million subscribers). This is the first work that empirically (i) shows that ""free money network"" is a promising measure/approximation for detecting and characterizing gold farming networks, and (ii) measures the size of the free money net and in-game virtual economy in a large-scale MMORPG in terms of the cash flow.",Proceedings of the ACM SIGCOMM 2011 Conference,392–393,2,"real money trading, gold farming, onlinr game security","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1071,article,"Baden, Randy",LoKI: Location-Based PKI for Social Networks,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018485,10.1145/2043164.2018485,"Decentralized online social networks (OSNs) typically rely on the existence of a public key infrastructure (PKI), but certificate authorities (CAs) cannot scalably identify all of the members of an OSN. Our system, LoKI, uses the ubiquity of mobile devices to exchange secrets during real-world meetings that can be used for the purposes of identification in-band, allowing each user to easily discover the keys of many one-hop relationships in the OSN.We measure the frequency of such real-world meetings among OSN users with data sets crawled from Facebook, Twitter, and Foursquare. We quantify the resources consumed on the mobile devices in terms of storage and battery based on traces that reveal the number of mobile devices expected to be seen under normal activity. Lastly, we describe a rendezvous service that enables background peer-to-peer (P2P) communication on non-rooted Android phones, which we believe to be a practical and necessary service for many mobile peer-to-peer systems.",,394–395,2,"public key infrastructure, mobility, online social networks, location",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1072,inproceedings,"Baden, Randy",LoKI: Location-Based PKI for Social Networks,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018485,10.1145/2018436.2018485,"Decentralized online social networks (OSNs) typically rely on the existence of a public key infrastructure (PKI), but certificate authorities (CAs) cannot scalably identify all of the members of an OSN. Our system, LoKI, uses the ubiquity of mobile devices to exchange secrets during real-world meetings that can be used for the purposes of identification in-band, allowing each user to easily discover the keys of many one-hop relationships in the OSN.We measure the frequency of such real-world meetings among OSN users with data sets crawled from Facebook, Twitter, and Foursquare. We quantify the resources consumed on the mobile devices in terms of storage and battery based on traces that reveal the number of mobile devices expected to be seen under normal activity. Lastly, we describe a rendezvous service that enables background peer-to-peer (P2P) communication on non-rooted Android phones, which we believe to be a practical and necessary service for many mobile peer-to-peer systems.",Proceedings of the ACM SIGCOMM 2011 Conference,394–395,2,"mobility, public key infrastructure, online social networks, location","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1073,article,"Chasaki, Danai","""Roto-Rooting"" Your Router: Solution against New Potential DoS Attacks on Modern Routers",2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018486,10.1145/2043164.2018486,"Our work presents the first practical example of an entirely new class of network attacks - attacks that target the network infrastructure. Modern routers use general purpose programmable processors, and the software used for packet processing on these systems is potentially vulnerable to remote exploits. We describe a specific attack that can launch a devastating denial-of-service attack by sending just a single packet. We also show that there are effective defense techniques, based on processor monitoring, that can help in detecting and avoiding such attacks.",,396–397,2,"next-generation internet, embedded processors",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1074,inproceedings,"Chasaki, Danai","""Roto-Rooting"" Your Router: Solution against New Potential DoS Attacks on Modern Routers",2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018486,10.1145/2018436.2018486,"Our work presents the first practical example of an entirely new class of network attacks - attacks that target the network infrastructure. Modern routers use general purpose programmable processors, and the software used for packet processing on these systems is potentially vulnerable to remote exploits. We describe a specific attack that can launch a devastating denial-of-service attack by sending just a single packet. We also show that there are effective defense techniques, based on processor monitoring, that can help in detecting and avoiding such attacks.",Proceedings of the ACM SIGCOMM 2011 Conference,396–397,2,"embedded processors, next-generation internet","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1075,article,"Mondal, Mainack and Viswanath, Bimal and Clement, Allen and Druschel, Peter and Gummadi, Krishna P. and Mislove, Alan and Post, Ansley",Limiting Large-Scale Crawls of Social Networking Sites,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018487,10.1145/2043164.2018487,"Online social networking sites (OSNs) like Facebook and Orkut contain personal data of millions of users. Many OSNs view this data as a valuable asset that is at the core of their business model. Both OSN users and OSNs have strong incentives to restrict large scale crawls of this data. OSN users want to protect their privacy and OSNs their business interest. Traditional defenses against crawlers involve rate- limiting browsing activity per user account. These defense schemes, however, are vulnerable to Sybil attacks, where a crawler creates a large number of fake user accounts. In this paper, we propose Genie, a system that can be deployed by OSN operators to defend against Sybil crawlers. Genie is based on a simple yet powerful insight: the social network itself can be leveraged to defend against Sybil crawlers. We first present Genie's design and then discuss how Genie can limit crawlers while allowing browsing of user profiles by normal users.",,398–399,2,"social network-based sybil defense, sybil attacks, social networks",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1076,inproceedings,"Mondal, Mainack and Viswanath, Bimal and Clement, Allen and Druschel, Peter and Gummadi, Krishna P. and Mislove, Alan and Post, Ansley",Limiting Large-Scale Crawls of Social Networking Sites,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018487,10.1145/2018436.2018487,"Online social networking sites (OSNs) like Facebook and Orkut contain personal data of millions of users. Many OSNs view this data as a valuable asset that is at the core of their business model. Both OSN users and OSNs have strong incentives to restrict large scale crawls of this data. OSN users want to protect their privacy and OSNs their business interest. Traditional defenses against crawlers involve rate- limiting browsing activity per user account. These defense schemes, however, are vulnerable to Sybil attacks, where a crawler creates a large number of fake user accounts. In this paper, we propose Genie, a system that can be deployed by OSN operators to defend against Sybil crawlers. Genie is based on a simple yet powerful insight: the social network itself can be leveraged to defend against Sybil crawlers. We first present Genie's design and then discuss how Genie can limit crawlers while allowing browsing of user profiles by normal users.",Proceedings of the ACM SIGCOMM 2011 Conference,398–399,2,"social networks, social network-based sybil defense, sybil attacks","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1077,article,"Leng, Christof and Lehn, Max and Rehner, Robert and Buchmann, Alejandro",Designing a Testbed for Large-Scale Distributed Systems,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018488,10.1145/2043164.2018488,"Different evaluation methods for distributed systems like prototyping, simulation and emulation have different tradeoffs. We present a testbed for Internet applications that supports real-network prototypes and multiple simulators with unchanged application code. To ensure maximum portability between runtimes, a compact but flexible system interface is defined.",,400–401,2,"event-based simulator, peer-to-peer, network simulator",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1078,inproceedings,"Leng, Christof and Lehn, Max and Rehner, Robert and Buchmann, Alejandro",Designing a Testbed for Large-Scale Distributed Systems,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018488,10.1145/2018436.2018488,"Different evaluation methods for distributed systems like prototyping, simulation and emulation have different tradeoffs. We present a testbed for Internet applications that supports real-network prototypes and multiple simulators with unchanged application code. To ensure maximum portability between runtimes, a compact but flexible system interface is defined.",Proceedings of the ACM SIGCOMM 2011 Conference,400–401,2,"network simulator, peer-to-peer, event-based simulator","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1079,article,"Soroush, Hamed and Gilbert, Peter and Banerjee, Nilanjan and Corner, Mark D. and Levine, Brian N. and Cox, Landon",Spider: Improving Mobile Networking with Concurrent Wi-Fi Connections,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018489,10.1145/2043164.2018489,"We investigate attempting concurrent connections to multiple Wi-Fi access points (APs) from highly mobile clients. Previous multi-AP solutions are limited to stationary wireless clients and do not take into account a myriad of mobile factors. We show that connection duration, AP response times, channel scheduling, available and offered bandwidth, node speed, and dhcp joins all affect performance. Building on these results, we present a system, Spider, that establishes and maintains concurrent connections to 802.11 APs in a mobile environment. While Spider can manage multiple channels, we demonstrate that it achieves maximum throughput when using multiple APs on a single channel.",,402–403,2,"mobile networks, concurrent wi-fi",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1080,inproceedings,"Soroush, Hamed and Gilbert, Peter and Banerjee, Nilanjan and Corner, Mark D. and Levine, Brian N. and Cox, Landon",Spider: Improving Mobile Networking with Concurrent Wi-Fi Connections,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018489,10.1145/2018436.2018489,"We investigate attempting concurrent connections to multiple Wi-Fi access points (APs) from highly mobile clients. Previous multi-AP solutions are limited to stationary wireless clients and do not take into account a myriad of mobile factors. We show that connection duration, AP response times, channel scheduling, available and offered bandwidth, node speed, and dhcp joins all affect performance. Building on these results, we present a system, Spider, that establishes and maintains concurrent connections to 802.11 APs in a mobile environment. While Spider can manage multiple channels, we demonstrate that it achieves maximum throughput when using multiple APs on a single channel.",Proceedings of the ACM SIGCOMM 2011 Conference,402–403,2,"concurrent wi-fi, mobile networks","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1081,article,"Ahmadzadeh, Seyed Ali and Agnew, Gordon",Covert Channels in Multiple Access Protocols,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018490,10.1145/2043164.2018490,"In this paper, the use of structural behavior of communication protocols in designing new covert channels is investigated. In this way, a new covert transmitter is designed based on a modified CSMA protocol that enables the transmitter to embed a covert message in its overt traffic. The proposed scheme provides high covert rate without compromising the stealthiness of the channel.",,404–405,2,"information hiding, wireless networks, covert channel",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1082,inproceedings,"Ahmadzadeh, Seyed Ali and Agnew, Gordon",Covert Channels in Multiple Access Protocols,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018490,10.1145/2018436.2018490,"In this paper, the use of structural behavior of communication protocols in designing new covert channels is investigated. In this way, a new covert transmitter is designed based on a modified CSMA protocol that enables the transmitter to embed a covert message in its overt traffic. The proposed scheme provides high covert rate without compromising the stealthiness of the channel.",Proceedings of the ACM SIGCOMM 2011 Conference,404–405,2,"covert channel, wireless networks, information hiding","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1083,article,"Chen, Zhuo and Chen, Yang and Ding, Cong and Deng, Beixing and Li, Xing",Pomelo: Accurate and Decentralized Shortest-Path Distance Estimation in Social Graphs,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018491,10.1145/2043164.2018491,"Computing the shortest-path distances between nodes is a key problem in analyzing social graphs. Traditional methods like breadth-first search (BFS) do not scale well with graph size. Recently, a Graph Coordinate System, called Orion, has been proposed to estimate shortest-path distances in a scalable way. Orion uses a landmark-based approach, which does not take account of the shortest-path distances between non-landmark nodes in coordinate calculation. Such biased input for the coordinate system cannot characterize the graph structure well. In this paper, we propose Pomelo, which calculates the graph coordinates in a decentralized manner. Every node in Pomelo computes its shortest-path distances to both nearby neighbors and some random distant neighbors. By introducing the novel partial BFS, the computational overhead of Pomelo is tunable. Our experimental results from different representative social graphs show that Pomelo greatly outperforms Orion in estimation accuracy while maintaining the same computational overhead.",,406–407,2,"graph coordinate system, online social network",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1084,inproceedings,"Chen, Zhuo and Chen, Yang and Ding, Cong and Deng, Beixing and Li, Xing",Pomelo: Accurate and Decentralized Shortest-Path Distance Estimation in Social Graphs,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018491,10.1145/2018436.2018491,"Computing the shortest-path distances between nodes is a key problem in analyzing social graphs. Traditional methods like breadth-first search (BFS) do not scale well with graph size. Recently, a Graph Coordinate System, called Orion, has been proposed to estimate shortest-path distances in a scalable way. Orion uses a landmark-based approach, which does not take account of the shortest-path distances between non-landmark nodes in coordinate calculation. Such biased input for the coordinate system cannot characterize the graph structure well. In this paper, we propose Pomelo, which calculates the graph coordinates in a decentralized manner. Every node in Pomelo computes its shortest-path distances to both nearby neighbors and some random distant neighbors. By introducing the novel partial BFS, the computational overhead of Pomelo is tunable. Our experimental results from different representative social graphs show that Pomelo greatly outperforms Orion in estimation accuracy while maintaining the same computational overhead.",Proceedings of the ACM SIGCOMM 2011 Conference,406–407,2,"online social network, graph coordinate system","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1085,article,"Mishra, Abhishek and Venkitasubramaniam, Parv",Dummy Rate Analysis of Buffer Constrained Chaum Mix,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018492,10.1145/2043164.2018492,,,408–409,2,"buffer constraint, chaum mix, dummy rate",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1086,inproceedings,"Mishra, Abhishek and Venkitasubramaniam, Parv",Dummy Rate Analysis of Buffer Constrained Chaum Mix,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018492,10.1145/2018436.2018492,,Proceedings of the ACM SIGCOMM 2011 Conference,408–409,2,"buffer constraint, dummy rate, chaum mix","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1087,article,L\'{o,Minimising Cell Transmit Power: Towards Self-Organized Resource Allocation in OFDMA Femtocells,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018494,10.1145/2043164.2018494,"With the introduction of femtocells, cellular networks are moving from the conventional centralised architecture to a distributed one, where each network cell should make its own radio resource management decisions, while providing inter-cell interference mitigation. However, realising this distributed cellular network architecture is not a trivial task. In this paper, we first introduce a simple self-organisation rule under which a distributed cellular network is able to converge into an efficient resource allocation pattern, then propose a novel resource allocation model taking realistic resource allocation constraints into account, and finally evaluate the performance of the proposed self-organisation rule and resource allocation model using system-level simulations.",,410–411,2,"resource allocation, femtocell, interference",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1088,inproceedings,L\'{o,Minimising Cell Transmit Power: Towards Self-Organized Resource Allocation in OFDMA Femtocells,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018494,10.1145/2018436.2018494,"With the introduction of femtocells, cellular networks are moving from the conventional centralised architecture to a distributed one, where each network cell should make its own radio resource management decisions, while providing inter-cell interference mitigation. However, realising this distributed cellular network architecture is not a trivial task. In this paper, we first introduce a simple self-organisation rule under which a distributed cellular network is able to converge into an efficient resource allocation pattern, then propose a novel resource allocation model taking realistic resource allocation constraints into account, and finally evaluate the performance of the proposed self-organisation rule and resource allocation model using system-level simulations.",Proceedings of the ACM SIGCOMM 2011 Conference,410–411,2,"interference, femtocell, resource allocation","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1089,article,"Werle, Christoph and Papadimitriou, Panagiotis and Houidi, Ines and Louati, Wajdi and Zeghlache, Djamal and Bless, Roland and Mathy, Laurent",Building Virtual Networks across Multiple Domains,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018495,10.1145/2043164.2018495,"This paper presents a platform for virtual network (VN) provisioning across multiple domains. The platform decomposes VN provisioning into multiple steps to address the implications of limited information disclosure on resource discovery and allocation. A new VN embedding algorithm with simultaneous node and link mapping allows to assign resources within each domain. For inter-domain virtual link setup, we design and realize a signaling protocol that also integrates resource reservations for providing virtual links with Quality-of-Service guarantees. Experimental results show that small VNs can be provisioned within a few seconds.",,412–413,2,"network virtualization, platform design, resource provisioning",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1090,inproceedings,"Werle, Christoph and Papadimitriou, Panagiotis and Houidi, Ines and Louati, Wajdi and Zeghlache, Djamal and Bless, Roland and Mathy, Laurent",Building Virtual Networks across Multiple Domains,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018495,10.1145/2018436.2018495,"This paper presents a platform for virtual network (VN) provisioning across multiple domains. The platform decomposes VN provisioning into multiple steps to address the implications of limited information disclosure on resource discovery and allocation. A new VN embedding algorithm with simultaneous node and link mapping allows to assign resources within each domain. For inter-domain virtual link setup, we design and realize a signaling protocol that also integrates resource reservations for providing virtual links with Quality-of-Service guarantees. Experimental results show that small VNs can be provisioned within a few seconds.",Proceedings of the ACM SIGCOMM 2011 Conference,412–413,2,"resource provisioning, network virtualization, platform design","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1091,article,"Son, Seokshin and Kang, Ah Reum and Kim, Hyun-chul and Kwon, Ted Taekyoung and Park, Juyong and Kim, Huy Kang",Multi-Relational Social Networks in a Large-Scale MMORPG,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018496,10.1145/2043164.2018496,"We analyze multi-relational social interaction networks in a large-scale commercial Massively Multiplayer Online Role-Playing Game(MMORPG). Our work is based on data from AION, currently the world's second most-played MMORPG with 3.4 million subscribers as of mid 2010, created and serviced by NCSoft, Inc. We construct and characterize six distinct interactivity networks (Friend, Private Messaging, Party invitation, Trade, Mail, and Shop), each representing diverse player interaction types.",,414–415,2,"social network analysis, massively multiplayer online game, quantitative social science",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1092,inproceedings,"Son, Seokshin and Kang, Ah Reum and Kim, Hyun-chul and Kwon, Ted Taekyoung and Park, Juyong and Kim, Huy Kang",Multi-Relational Social Networks in a Large-Scale MMORPG,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018496,10.1145/2018436.2018496,"We analyze multi-relational social interaction networks in a large-scale commercial Massively Multiplayer Online Role-Playing Game(MMORPG). Our work is based on data from AION, currently the world's second most-played MMORPG with 3.4 million subscribers as of mid 2010, created and serviced by NCSoft, Inc. We construct and characterize six distinct interactivity networks (Friend, Private Messaging, Party invitation, Trade, Mail, and Shop), each representing diverse player interaction types.",Proceedings of the ACM SIGCOMM 2011 Conference,414–415,2,"social network analysis, massively multiplayer online game, quantitative social science","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1093,article,"Zhu, Nan and Rao, Lei and Liu, Xue and Liu, Jie and Guan, Haibin",Taming Power Peaks in Mapreduce Clusters,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018497,10.1145/2043164.2018497,"Along with the surging service demands on the cloud, the energy cost of Internet Data Centers (IDCs) is dramatically increasing. Energy management for IDCs is becoming ever more important. A large portion of applications running on data centers are data-intensive applications. MapReduce (and Hadoop) has been one of the mostly deployed frameworks for data-intensive applications. Both academia and industry have been greatly concerned with the problem of how to reduce the energy consumption of IDCs. However the critical power peak problem for MapReduce clusters has been overlooked, which is a new challenge brought by the usage of MapReduce. We elaborate the power peak problem and investigate the cause of the problem in details. Then we design an adaptive approach to regulate power peaks.",,416–417,2,"mapreduce, power peak, regulation",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1094,inproceedings,"Zhu, Nan and Rao, Lei and Liu, Xue and Liu, Jie and Guan, Haibin",Taming Power Peaks in Mapreduce Clusters,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018497,10.1145/2018436.2018497,"Along with the surging service demands on the cloud, the energy cost of Internet Data Centers (IDCs) is dramatically increasing. Energy management for IDCs is becoming ever more important. A large portion of applications running on data centers are data-intensive applications. MapReduce (and Hadoop) has been one of the mostly deployed frameworks for data-intensive applications. Both academia and industry have been greatly concerned with the problem of how to reduce the energy consumption of IDCs. However the critical power peak problem for MapReduce clusters has been overlooked, which is a new challenge brought by the usage of MapReduce. We elaborate the power peak problem and investigate the cause of the problem in details. Then we design an adaptive approach to regulate power peaks.",Proceedings of the ACM SIGCOMM 2011 Conference,416–417,2,"mapreduce, regulation, power peak","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1095,article,"Kristiansen, Stein and Plagemann, Thomas and Goebel, Vera",Towards Scalable and Realistic Node Models for Network Simulators,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018498,10.1145/2043164.2018498,"Network simulators typically do not include node models. Our studies show that in networks such as mobile networks, the impact of nodes on performance can be significant. Existing techniques to simulate nodes' are not scalable for network simulations, and require a too large modelling effort to be feasible for network research. In this paper, we propose to capture flexible per-protocol performance profiles from real, running systems using instrumentation and traffic benchmarking techniques. By using the obtained profiles as input into an extended scheduler simulator, the behaviour of the node can be accurately reproduced. Since the processing overhead is represented statistically, we preserve scalability and a low modelling overhead.",,418–419,2,"operating system, ns-3, node model, network simulation, multi-threading, linux, manet, resource constraints",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1096,inproceedings,"Kristiansen, Stein and Plagemann, Thomas and Goebel, Vera",Towards Scalable and Realistic Node Models for Network Simulators,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018498,10.1145/2018436.2018498,"Network simulators typically do not include node models. Our studies show that in networks such as mobile networks, the impact of nodes on performance can be significant. Existing techniques to simulate nodes' are not scalable for network simulations, and require a too large modelling effort to be feasible for network research. In this paper, we propose to capture flexible per-protocol performance profiles from real, running systems using instrumentation and traffic benchmarking techniques. By using the obtained profiles as input into an extended scheduler simulator, the behaviour of the node can be accurately reproduced. Since the processing overhead is represented statistically, we preserve scalability and a low modelling overhead.",Proceedings of the ACM SIGCOMM 2011 Conference,418–419,2,"operating system, network simulation, ns-3, resource constraints, linux, multi-threading, manet, node model","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1097,article,"van der Linden, Simon and Detal, Gregory and Bonaventure, Olivier",Revisiting Next-Hop Selection in Multipath Networks,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018499,10.1145/2043164.2018499,"Multipath routing strategies such as Equal-Cost MultiPath (ECMP) are widely used in IP and data-center networks. Most current methods to balance packets over the multiple next hops toward the destination base their decision on a hash computed over selected fields of the packet headers. Because of the non-invertible nature of hash functions, it is hard to determine the values of those fields so as to make the packet follow a specific path in the network. However, several applications might benefit from being able to choose such a path. Therefore, we propose a novel next-hop selection method based on an invertible function. By encoding the selection of successive routers into common fields of packet headers, the proposed method enables end hosts to force their packets to follow a specific path.",,420–421,2,"load balancing, multipath, path selection",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1098,inproceedings,"van der Linden, Simon and Detal, Gregory and Bonaventure, Olivier",Revisiting Next-Hop Selection in Multipath Networks,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018499,10.1145/2018436.2018499,"Multipath routing strategies such as Equal-Cost MultiPath (ECMP) are widely used in IP and data-center networks. Most current methods to balance packets over the multiple next hops toward the destination base their decision on a hash computed over selected fields of the packet headers. Because of the non-invertible nature of hash functions, it is hard to determine the values of those fields so as to make the packet follow a specific path in the network. However, several applications might benefit from being able to choose such a path. Therefore, we propose a novel next-hop selection method based on an invertible function. By encoding the selection of successive routers into common fields of packet headers, the proposed method enables end hosts to force their packets to follow a specific path.",Proceedings of the ACM SIGCOMM 2011 Conference,420–421,2,"load balancing, multipath, path selection","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1099,article,"Rizzo, Luigi and Landi, Matteo",Netmap: Memory Mapped Access to Network Devices,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018500,10.1145/2043164.2018500,"Recent papers have shown that wire-speed packet processing is feasible in software even at 10~Gbit/s, but the result has been achieved taking direct control of the network controllers to cut down OS and device driver overheads.In this paper we show how to achieve similar performance in safer conditions on standard operating systems. As in some other proposals, our framework, called netmap, maps packet buffers into the process' memory space; but unlike other proposals, any operation that may affect the state of the hardware is filtered by the OS. This protects the system from crashes induced by misbehaving programs, and simplifies the use of the API.Our tests show that netmap takes as little as 90 clock cycles to move one packet between the wire and the application, almost one order of magnitude less than using the standard OS path. A single core at 1.33~GHz can send or receive packets at wire speed on 10~Gbit/s links (14.8~Mpps), with very good scalability in the number of cores and clock speed.At least three factors contribute to this performance: i) no overhead for encapsulation and metadata management; ii) no per-packet system calls and data copying (ioctl()s are still required, but involve no copying and their cost is amortized over a batch of packets); iii) much simpler device driver operation, because buffers have a plain and simple format that requires",,422–423,2,"device drivers, packet forwarding, monitoring",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1100,inproceedings,"Rizzo, Luigi and Landi, Matteo",Netmap: Memory Mapped Access to Network Devices,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018500,10.1145/2018436.2018500,"Recent papers have shown that wire-speed packet processing is feasible in software even at 10~Gbit/s, but the result has been achieved taking direct control of the network controllers to cut down OS and device driver overheads.In this paper we show how to achieve similar performance in safer conditions on standard operating systems. As in some other proposals, our framework, called netmap, maps packet buffers into the process' memory space; but unlike other proposals, any operation that may affect the state of the hardware is filtered by the OS. This protects the system from crashes induced by misbehaving programs, and simplifies the use of the API.Our tests show that netmap takes as little as 90 clock cycles to move one packet between the wire and the application, almost one order of magnitude less than using the standard OS path. A single core at 1.33~GHz can send or receive packets at wire speed on 10~Gbit/s links (14.8~Mpps), with very good scalability in the number of cores and clock speed.At least three factors contribute to this performance: i) no overhead for encapsulation and metadata management; ii) no per-packet system calls and data copying (ioctl()s are still required, but involve no copying and their cost is amortized over a batch of packets); iii) much simpler device driver operation, because buffers have a plain and simple format that requires",Proceedings of the ACM SIGCOMM 2011 Conference,422–423,2,"device drivers, monitoring, packet forwarding","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1101,article,"Giotsas, Vasileios and Zhou, Shi",Detecting and Assessing the Hybrid IPv4/IPv6 as Relationships,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018501,10.1145/2043164.2018501,"The business relationships between the Autonomous Systems (ASes) play a central role in the BGP routing. The existing relationship inference algorithms are profoundly based on the valley-free rule and generalize their inference heuristics for both the IPv4 and IPv6 planes, introducing unavoidable inference artifacts. To discover and analyze the Type-of-Relationship (ToR) properties of the IPv6 topology we mine the BGP Communities attribute which provides an unexploited wealth of reliable relationship information. We obtain the actual relationships for 72% of the IPv6 AS links that are visible in the RouteViews and RIPE RIS repositories. Our results show that as many as 13% of AS links that serve both IPv4 and IPv6 traffic have different relationships depending on the IP version. Such relationships are characterized as hybrid. We observe that links with hybrid relationships are present in a large number of IPv6 AS paths. Furthermore, an unusually large portion of IPv6 AS paths violate the valley-free rule, indicating that the global reachability in the IPv6 Internet requires the relaxation of the valley-free rule. Our work highlights the importance of correctly inferring the AS relationships and the need to appreciate the distinct characteristics of IPv6 routing policies.",,424–425,2,"autonomous systems, topology, internet, bgp, as relationship, inference algorithms, ipv6, inter-domain routing",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1102,inproceedings,"Giotsas, Vasileios and Zhou, Shi",Detecting and Assessing the Hybrid IPv4/IPv6 as Relationships,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018501,10.1145/2018436.2018501,"The business relationships between the Autonomous Systems (ASes) play a central role in the BGP routing. The existing relationship inference algorithms are profoundly based on the valley-free rule and generalize their inference heuristics for both the IPv4 and IPv6 planes, introducing unavoidable inference artifacts. To discover and analyze the Type-of-Relationship (ToR) properties of the IPv6 topology we mine the BGP Communities attribute which provides an unexploited wealth of reliable relationship information. We obtain the actual relationships for 72% of the IPv6 AS links that are visible in the RouteViews and RIPE RIS repositories. Our results show that as many as 13% of AS links that serve both IPv4 and IPv6 traffic have different relationships depending on the IP version. Such relationships are characterized as hybrid. We observe that links with hybrid relationships are present in a large number of IPv6 AS paths. Furthermore, an unusually large portion of IPv6 AS paths violate the valley-free rule, indicating that the global reachability in the IPv6 Internet requires the relaxation of the valley-free rule. Our work highlights the importance of correctly inferring the AS relationships and the need to appreciate the distinct characteristics of IPv6 routing policies.",Proceedings of the ACM SIGCOMM 2011 Conference,424–425,2,"inference algorithms, as relationship, ipv6, internet, autonomous systems, bgp, topology, inter-domain routing","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1103,article,"Li, Ang and Zong, Xuanran and Kandula, Srikanth and Yang, Xiaowei and Zhang, Ming",CloudProphet: Towards Application Performance Prediction in Cloud,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018502,10.1145/2043164.2018502,"Choosing the best-performing cloud for one's application is a critical problem for potential cloud customers. We propose CloudProphet, a trace-and-replay tool to predict a legacy application's performance if migrated to a cloud infrastructure. CloudProphet traces the workload of the application when running locally, and replays the same workload in the cloud for prediction. We discuss two key technical challenges in designing CloudProphet, and some preliminary results using a prototype implementation.",,426–427,2,"prediction, cloud computing, performance",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1104,inproceedings,"Li, Ang and Zong, Xuanran and Kandula, Srikanth and Yang, Xiaowei and Zhang, Ming",CloudProphet: Towards Application Performance Prediction in Cloud,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018502,10.1145/2018436.2018502,"Choosing the best-performing cloud for one's application is a critical problem for potential cloud customers. We propose CloudProphet, a trace-and-replay tool to predict a legacy application's performance if migrated to a cloud infrastructure. CloudProphet traces the workload of the application when running locally, and replays the same workload in the cloud for prediction. We discuss two key technical challenges in designing CloudProphet, and some preliminary results using a prototype implementation.",Proceedings of the ACM SIGCOMM 2011 Conference,426–427,2,"prediction, cloud computing, performance","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1105,article,"Newaz, S.H. Shah and Cuevas, \'{A",A Novel Approach for Making Energy Efficient PON,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018503,10.1145/2043164.2018503,"Nowadays Passive Optical Network (PON) requires that Optical Network Units (ONUs) wake up periodically to check if the Optical Line Terminal (OLT) has any message directed to them. This implies that ONUs change from sleeping mode in which they just consume 1 W to active mode in which the consumption goes up to 10 W. In many cases, the OLT does not have any packets for the ONU and it goes to sleep again, what supposes a waste of energy. In this paper, we propose a novel Hybrid ONU that relies on a low-cost and low-energy technology, IEEE 802.15.4, to wake up those ONUs that are going to receive a packet. Our first estimations demonstrates that our solution would save around 25000$ per year and OLT.",,428–429,2,"energy saving, pon, converged., sleep mode",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1106,inproceedings,"Newaz, S.H. Shah and Cuevas, \'{A",A Novel Approach for Making Energy Efficient PON,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018503,10.1145/2018436.2018503,"Nowadays Passive Optical Network (PON) requires that Optical Network Units (ONUs) wake up periodically to check if the Optical Line Terminal (OLT) has any message directed to them. This implies that ONUs change from sleeping mode in which they just consume 1 W to active mode in which the consumption goes up to 10 W. In many cases, the OLT does not have any packets for the ONU and it goes to sleep again, what supposes a waste of energy. In this paper, we propose a novel Hybrid ONU that relies on a low-cost and low-energy technology, IEEE 802.15.4, to wake up those ONUs that are going to receive a packet. Our first estimations demonstrates that our solution would save around 25000$ per year and OLT.",Proceedings of the ACM SIGCOMM 2011 Conference,428–429,2,"converged., energy saving, pon, sleep mode","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1107,article,"Dixit, Advait and Prakash, Pawan and Kompella, Ramana Rao",On the Efficacy of Fine-Grained Traffic Splitting Protocolsin Data Center Networks,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018504,10.1145/2043164.2018504,"Multi-rooted tree topologies are commonly used to construct high-bandwidth data center network fabrics. In these networks, switches typically rely on equal-cost multipath (ECMP) routing techniques to split traffic across multiple paths, such that packets within a flow traverse the same end-to-end path. Unfortunately, since ECMP splits traffic based on flow-granularity, it can cause load imbalance across paths resulting in poor utilization of network resources. More fine-grained traffic splitting techniques are typically not preferred because they can cause packet reordering that can, according to conventional wisdom, lead to severe TCP throughput degradation. In this work, we revisit this fact in the context of regular data center topologies such as fat-tree architectures. We argue that packet-level traffic splitting, where packets of a flow are sprayed through all available paths, would lead to a better load-balanced network, which in turn leads to significantly more balanced queues and much higher throughput compared to ECMP.",,430–431,2,"data centers, traffic splitting",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1108,inproceedings,"Dixit, Advait and Prakash, Pawan and Kompella, Ramana Rao",On the Efficacy of Fine-Grained Traffic Splitting Protocolsin Data Center Networks,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018504,10.1145/2018436.2018504,"Multi-rooted tree topologies are commonly used to construct high-bandwidth data center network fabrics. In these networks, switches typically rely on equal-cost multipath (ECMP) routing techniques to split traffic across multiple paths, such that packets within a flow traverse the same end-to-end path. Unfortunately, since ECMP splits traffic based on flow-granularity, it can cause load imbalance across paths resulting in poor utilization of network resources. More fine-grained traffic splitting techniques are typically not preferred because they can cause packet reordering that can, according to conventional wisdom, lead to severe TCP throughput degradation. In this work, we revisit this fact in the context of regular data center topologies such as fat-tree architectures. We argue that packet-level traffic splitting, where packets of a flow are sprayed through all available paths, would lead to a better load-balanced network, which in turn leads to significantly more balanced queues and much higher throughput compared to ECMP.",Proceedings of the ACM SIGCOMM 2011 Conference,430–431,2,"data centers, traffic splitting","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1109,article,"Konte, Maria and Feamster, Nick",Wide-Area Routing Dynamics of Malicious Networks,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018505,10.1145/2043164.2018505,"This paper studies the routing dynamics of malicious networks. We characterize the routing behavior of malicious networks on both short and long timescales. We find that malicious networks more consistently advertise prefixes with short durations and long inter- arrival times; over longer timescales, we find that malicious ASes connect with more upstream providers than legitimate ASes, and they also change upstream providers more frequently.",,432–433,2,"bgp, security, spam",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1110,inproceedings,"Konte, Maria and Feamster, Nick",Wide-Area Routing Dynamics of Malicious Networks,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018505,10.1145/2018436.2018505,"This paper studies the routing dynamics of malicious networks. We characterize the routing behavior of malicious networks on both short and long timescales. We find that malicious networks more consistently advertise prefixes with short durations and long inter- arrival times; over longer timescales, we find that malicious ASes connect with more upstream providers than legitimate ASes, and they also change upstream providers more frequently.",Proceedings of the ACM SIGCOMM 2011 Conference,432–433,2,"security, bgp, spam","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1111,article,"Canini, Marco and Jovanovi\'{c",Online Testing of Federated and Heterogeneous Distributed Systems,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018507,10.1145/2043164.2018507,"DiCE is a system for online testing of federated and heterogeneous distributed systems. We have built a prototype of DiCE and integrated it with an open-source BGP router. DiCE quickly detects three important classes of faults, resulting from configuration mistakes, policy conflicts and programming errors.The goal of this demo is to showcase our DiCE prototype while it executes an experiment that involves exploring BGP system behavior in a topology with 27 BGP routers and Internet-like conditions (Figure 1).",,434–435,2,"fault detection, federated and heterogeneous distributed systems, online testing",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1112,inproceedings,"Canini, Marco and Jovanovi\'{c",Online Testing of Federated and Heterogeneous Distributed Systems,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018507,10.1145/2018436.2018507,"DiCE is a system for online testing of federated and heterogeneous distributed systems. We have built a prototype of DiCE and integrated it with an open-source BGP router. DiCE quickly detects three important classes of faults, resulting from configuration mistakes, policy conflicts and programming errors.The goal of this demo is to showcase our DiCE prototype while it executes an experiment that involves exploring BGP system behavior in a topology with 27 BGP routers and Internet-like conditions (Figure 1).",Proceedings of the ACM SIGCOMM 2011 Conference,434–435,2,"online testing, federated and heterogeneous distributed systems, fault detection","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1113,article,"Saucez, Damien and Bonaventure, Olivier",Performance Based Traffic Control with IDIPS,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018508,10.1145/2043164.2018508,"Nowadays Internet is ubiquitous resulting in an increasing path diversity and content duplication. However, while content can be retrieved from many different places, the paths to those places are not equivalent. Indeed, some paths offer better bandwidth while others are less expensive or more stable. In addition, a new range of applications is sensitive to the performance of the paths that carry their traffic. To support this evolution of the Internet, we propose ISP-Driven Informed Path Selection (IDIPS). Any ISP can easily deploy IDIPS to help its customers to select the paths that best meet their requirements in order to reach their content. IDIPS helps in this selection through pro-active measurements and ISP-defined policies. IDIPS is scalable and can support thousands of clients. IDIPS is also flexible and can thus be used by the ISP to optimize its routing decisions to take the performance of its inter-domain links into account.",,436–437,2,"traffic engineering, xorp, route control",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1114,inproceedings,"Saucez, Damien and Bonaventure, Olivier",Performance Based Traffic Control with IDIPS,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018508,10.1145/2018436.2018508,"Nowadays Internet is ubiquitous resulting in an increasing path diversity and content duplication. However, while content can be retrieved from many different places, the paths to those places are not equivalent. Indeed, some paths offer better bandwidth while others are less expensive or more stable. In addition, a new range of applications is sensitive to the performance of the paths that carry their traffic. To support this evolution of the Internet, we propose ISP-Driven Informed Path Selection (IDIPS). Any ISP can easily deploy IDIPS to help its customers to select the paths that best meet their requirements in order to reach their content. IDIPS helps in this selection through pro-active measurements and ISP-defined policies. IDIPS is scalable and can support thousands of clients. IDIPS is also flexible and can thus be used by the ISP to optimize its routing decisions to take the performance of its inter-domain links into account.",Proceedings of the ACM SIGCOMM 2011 Conference,436–437,2,"xorp, route control, traffic engineering","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1115,article,"Chen, Lien-Wu and Sharma, Pranay and Tseng, Yu-Chee",Eco-Sign: A Load-Based Traffic Light Control System for Environmental Protection with Vehicular Communications,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018509,10.1145/2043164.2018509,"The Eco-Sign system is a traffic light control system for minimizing greenhouse gases emitted by idling vehicles at intersections. Eco-Sign provides the following features: (i) it can notify vehicles to turn on/off their engines based on expected waiting time for green lights at intersections, (ii) it can dynamically adjust traffic light timing to minimize the number of vehicles stopping at an intersection based on vehicle arrival and departure rates, and (iii) it is a fully distributed system in the sense that each intersection can learn its local traffic condition and optimize its traffic sign setting to prevent congestions and thus traffic jams. Eco-Sign thus demonstrates a new traffic light control system for environmental protection.",,438–439,2,"vehicular communications, environmental protection, ignition control, dynamic traffic light control",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1116,inproceedings,"Chen, Lien-Wu and Sharma, Pranay and Tseng, Yu-Chee",Eco-Sign: A Load-Based Traffic Light Control System for Environmental Protection with Vehicular Communications,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018509,10.1145/2018436.2018509,"The Eco-Sign system is a traffic light control system for minimizing greenhouse gases emitted by idling vehicles at intersections. Eco-Sign provides the following features: (i) it can notify vehicles to turn on/off their engines based on expected waiting time for green lights at intersections, (ii) it can dynamically adjust traffic light timing to minimize the number of vehicles stopping at an intersection based on vehicle arrival and departure rates, and (iii) it is a fully distributed system in the sense that each intersection can learn its local traffic condition and optimize its traffic sign setting to prevent congestions and thus traffic jams. Eco-Sign thus demonstrates a new traffic light control system for environmental protection.",Proceedings of the ACM SIGCOMM 2011 Conference,438–439,2,"dynamic traffic light control, environmental protection, ignition control, vehicular communications","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1117,article,"Ren, Yiqing and Zhou, Wenchao and Wang, Anduo and Jia, Limin and Gurney, Alexander J.T. and Loo, Boon Thau and Rexford, Jennifer",FSR: Formal Analysis and Implementation Toolkit for Safe Inter-Domain Routing,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018510,10.1145/2043164.2018510,"We present the demonstration of a comprehensive toolkit for analyzing and implementing routing policies, ranging from high-level guidelines to specific router configurations. Our Formally Safe Routing (FSR) toolkit performs all of these functions from the same algebraic representation of routing policy. We show that routing algebra has a very natural translation to both integer constraints (to perform safety analysis using SMT solvers) and declarative programs (to generate distributed implementations). Our demonstration with realistic topologies and policies shows how FSR can detect problems in an AS's iBGP configuration, prove sufficient conditions for BGP safety, and empirically evaluate convergence time.",,440–441,2,"safety analysis, formally safe routing, formal verification",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1118,inproceedings,"Ren, Yiqing and Zhou, Wenchao and Wang, Anduo and Jia, Limin and Gurney, Alexander J.T. and Loo, Boon Thau and Rexford, Jennifer",FSR: Formal Analysis and Implementation Toolkit for Safe Inter-Domain Routing,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018510,10.1145/2018436.2018510,"We present the demonstration of a comprehensive toolkit for analyzing and implementing routing policies, ranging from high-level guidelines to specific router configurations. Our Formally Safe Routing (FSR) toolkit performs all of these functions from the same algebraic representation of routing policy. We show that routing algebra has a very natural translation to both integer constraints (to perform safety analysis using SMT solvers) and declarative programs (to generate distributed implementations). Our demonstration with realistic topologies and policies shows how FSR can detect problems in an AS's iBGP configuration, prove sufficient conditions for BGP safety, and empirically evaluate convergence time.",Proceedings of the ACM SIGCOMM 2011 Conference,440–441,2,"formally safe routing, formal verification, safety analysis","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1119,article,"Sun, Zheng and Purohit, Aveek and De Wagter, Philippe and Brinster, Irina and Hamm, Chorom and Zhang, Pei",PANDAA: A Physical Arrangement Detection Technique for Networked Devices through Ambient-Sound Awareness,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018511,10.1145/2043164.2018511,"This demo presents PANDAA, a zero-configuration automatic spatial localization technique for networked devices based on ambient sound sensing. We will demonstrate that after initial placement of the devices, ambient sounds, such as human speech, music, footsteps, finger snaps, hand claps, or coughs and sneezes, can be used to autonomously resolve the spatial relative arrangement of devices, such as mobile phones, using trigonometric bounds and successive approximation.",,442–443,2,"localization, arrangement detection, networked devices",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1120,inproceedings,"Sun, Zheng and Purohit, Aveek and De Wagter, Philippe and Brinster, Irina and Hamm, Chorom and Zhang, Pei",PANDAA: A Physical Arrangement Detection Technique for Networked Devices through Ambient-Sound Awareness,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018511,10.1145/2018436.2018511,"This demo presents PANDAA, a zero-configuration automatic spatial localization technique for networked devices based on ambient sound sensing. We will demonstrate that after initial placement of the devices, ambient sounds, such as human speech, music, footsteps, finger snaps, hand claps, or coughs and sneezes, can be used to autonomously resolve the spatial relative arrangement of devices, such as mobile phones, using trigonometric bounds and successive approximation.",Proceedings of the ACM SIGCOMM 2011 Conference,442–443,2,"networked devices, arrangement detection, localization","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1121,article,"Rojas, Elisa and Naous, Jad and Iba\~{n",Implementing ARP-Path Low Latency Bridges in NetFPGA,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018512,10.1145/2043164.2018512,"The demo is focused on the implementation of ARP-Path (a.k.a. FastPath) bridges, a recently proposed concept for low latency bridges. ARP-Path Bridges rely on the race between broadcast ARP Request packets, to discover the minimum latency path to the destination host. Several implementations (in Omnet++, Linux, OpenFlow, NetFPGA) have shown that ARP-Path exhibits loop-freedom, does not block links, is fully transparent to hosts and neither needs a spanning tree protocol to prevent loops nor a link state protocol to obtain low latency paths. This demo compares our hardware implementation on NetFPGA to bridges running STP, showing that ARP-Path finds lower latency paths than STP.",,444–445,2,"shortest path bridges, ethernet, routing bridges, spanning tree, netfpga",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1122,inproceedings,"Rojas, Elisa and Naous, Jad and Iba\~{n",Implementing ARP-Path Low Latency Bridges in NetFPGA,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018512,10.1145/2018436.2018512,"The demo is focused on the implementation of ARP-Path (a.k.a. FastPath) bridges, a recently proposed concept for low latency bridges. ARP-Path Bridges rely on the race between broadcast ARP Request packets, to discover the minimum latency path to the destination host. Several implementations (in Omnet++, Linux, OpenFlow, NetFPGA) have shown that ARP-Path exhibits loop-freedom, does not block links, is fully transparent to hosts and neither needs a spanning tree protocol to prevent loops nor a link state protocol to obtain low latency paths. This demo compares our hardware implementation on NetFPGA to bridges running STP, showing that ARP-Path finds lower latency paths than STP.",Proceedings of the ACM SIGCOMM 2011 Conference,444–445,2,"spanning tree, netfpga, shortest path bridges, routing bridges, ethernet","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1123,article,"Mehendale, Hrushikesh and Paranjpe, Ashwin and Vempala, Santosh",LifeNet: A Flexible Ad Hoc Networking Solution for Transient Environments,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018513,10.1145/2043164.2018513,"We demonstrate a new ad hoc routing method that can handle transience such as node-mobility, obstructions and node failures. It has controlled management overhead, and is platform-independent (our demo includes phones, routers, and laptops running different operating systems). It achieves reliability and flexibility at the expense of throughput. It is ideal for scenarios where the reliability of connectivity is critical and bandwidth requirements are low. For e.g., disaster relief operations and sensor networks. Along with applications, we exhibit measurements to illustrate the advantages of our approach in dealing with transience.",,446–447,2,"minimum infrastructure, manets, reliable routing",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1124,inproceedings,"Mehendale, Hrushikesh and Paranjpe, Ashwin and Vempala, Santosh",LifeNet: A Flexible Ad Hoc Networking Solution for Transient Environments,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018513,10.1145/2018436.2018513,"We demonstrate a new ad hoc routing method that can handle transience such as node-mobility, obstructions and node failures. It has controlled management overhead, and is platform-independent (our demo includes phones, routers, and laptops running different operating systems). It achieves reliability and flexibility at the expense of throughput. It is ideal for scenarios where the reliability of connectivity is critical and bandwidth requirements are low. For e.g., disaster relief operations and sensor networks. Along with applications, we exhibit measurements to illustrate the advantages of our approach in dealing with transience.",Proceedings of the ACM SIGCOMM 2011 Conference,446–447,2,"reliable routing, manets, minimum infrastructure","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1125,article,"Rabl, Tilmann and Stegmaier, Florian and D\""{o",A Protocol for Disaster Data Evacuation,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018514,10.1145/2043164.2018514,"Data is the basis of the modern information society. However, recent natural catastrophes have shown that it is not possible to definitively secure a data storage location. Even if the storage location is not destroyed itself the access may quickly become impossible, due to the breakdown of connections or power supply. However, this rarely happens without any warning. While floods have hours or days of warning time, tsunamis usually leave only minutes for reaction and for earthquakes there are only seconds. In such situations, timely evacuation of important data is the key challenge. Consequently, the focus lies on minimizing the time to move away all data from the storage location whereas the actual time to arrival remains less (but still) important. This demonstration presents the dynamic fast send protocol (DFSP), a new bulk data transfer protocol. It employs striping to dynamic intermediate nodes in order to minimize sending time and to utilize the sender's resources to a high extent.",,448–449,2,"data evacuation, dynamic fast send protocol, dfsp, fsp",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1126,inproceedings,"Rabl, Tilmann and Stegmaier, Florian and D\""{o",A Protocol for Disaster Data Evacuation,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018514,10.1145/2018436.2018514,"Data is the basis of the modern information society. However, recent natural catastrophes have shown that it is not possible to definitively secure a data storage location. Even if the storage location is not destroyed itself the access may quickly become impossible, due to the breakdown of connections or power supply. However, this rarely happens without any warning. While floods have hours or days of warning time, tsunamis usually leave only minutes for reaction and for earthquakes there are only seconds. In such situations, timely evacuation of important data is the key challenge. Consequently, the focus lies on minimizing the time to move away all data from the storage location whereas the actual time to arrival remains less (but still) important. This demonstration presents the dynamic fast send protocol (DFSP), a new bulk data transfer protocol. It employs striping to dynamic intermediate nodes in order to minimize sending time and to utilize the sender's resources to a high extent.",Proceedings of the ACM SIGCOMM 2011 Conference,448–449,2,"fsp, dynamic fast send protocol, dfsp, data evacuation","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1127,article,"Chen, Ming-Hung and Yang, Chun-Yu and Chang, Chun-Yun and Hsu, Ming-Yuan and Lee, Ke-Han and Chou, Cheng-Fu",Towards Energy-Efficient Streaming System for Mobile Hotspots,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018515,10.1145/2043164.2018515,"Modern mobile devices have become an important part of our daily life but the performance of multimedia applications still suffers from the constrained energy supply and communication bandwidth of the mobile devices. In this work, we develop an energy-efficient streaming system for mobile hotspots to achieve better Quality-of-Experience. Our main idea is (a) to avoid redundant 3G transmissions as well as reduce the usage of 3G links for those low residual-energy users, and (b) to enable nearby mobile users cooperatively to share the downloaded data via short-range interfaces. The experiment results shows our scheme can improve the system lifetime by 27%, and provide better throughput as well as lower loss rate than conversional 3G systems do.",,450–451,2,"composite networks, cooperative networks, energy efficiency, mobile hotspots, wireless networks, streaming",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1128,inproceedings,"Chen, Ming-Hung and Yang, Chun-Yu and Chang, Chun-Yun and Hsu, Ming-Yuan and Lee, Ke-Han and Chou, Cheng-Fu",Towards Energy-Efficient Streaming System for Mobile Hotspots,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018515,10.1145/2018436.2018515,"Modern mobile devices have become an important part of our daily life but the performance of multimedia applications still suffers from the constrained energy supply and communication bandwidth of the mobile devices. In this work, we develop an energy-efficient streaming system for mobile hotspots to achieve better Quality-of-Experience. Our main idea is (a) to avoid redundant 3G transmissions as well as reduce the usage of 3G links for those low residual-energy users, and (b) to enable nearby mobile users cooperatively to share the downloaded data via short-range interfaces. The experiment results shows our scheme can improve the system lifetime by 27%, and provide better throughput as well as lower loss rate than conversional 3G systems do.",Proceedings of the ACM SIGCOMM 2011 Conference,450–451,2,"mobile hotspots, energy efficiency, wireless networks, streaming, cooperative networks, composite networks","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1129,article,"Sharafat, Ali Reza and Das, Saurav and Parulkar, Guru and McKeown, Nick",MPLS-TE and MPLS VPNS with Openflow,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018516,10.1145/2043164.2018516,"We demonstrate MPLS Traffic Engineering (MPLS-TE) and MPLS-based Virtual Private Networks (MPLS VPNs) using OpenFlow [1] and NOX [6]. The demonstration is the outcome of an engineering experiment to answer the following questions: How hard is it to implement a complex control plane on top of a network controller such as NOX? Does the global vantage point in NOX make the implementation easier than the traditional method of implementing it on every switch, embedded in the data plane? We implemented every major feature of MPLS-TE and MPLS-VPN in just 2,000 lines of code, compared to much larger lines of code in the more traditional approach, such as Quagga-MPLS. Because NOX maintains a consistent, up-to-date topology map, the MPLS control plane features are quite simple to implement. And its simplicity makes it easy to extend: We have easily added several new features; something a network operator could do to customize their network to meet their customers' needs.The demo consists of two parts: MPLS-TE services and then MPLS VPN driven by a GUI.",,452–453,2,"mpls, openflow, mpls-te, traffic engineering, vpn",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1130,inproceedings,"Sharafat, Ali Reza and Das, Saurav and Parulkar, Guru and McKeown, Nick",MPLS-TE and MPLS VPNS with Openflow,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018516,10.1145/2018436.2018516,"We demonstrate MPLS Traffic Engineering (MPLS-TE) and MPLS-based Virtual Private Networks (MPLS VPNs) using OpenFlow [1] and NOX [6]. The demonstration is the outcome of an engineering experiment to answer the following questions: How hard is it to implement a complex control plane on top of a network controller such as NOX? Does the global vantage point in NOX make the implementation easier than the traditional method of implementing it on every switch, embedded in the data plane? We implemented every major feature of MPLS-TE and MPLS-VPN in just 2,000 lines of code, compared to much larger lines of code in the more traditional approach, such as Quagga-MPLS. Because NOX maintains a consistent, up-to-date topology map, the MPLS control plane features are quite simple to implement. And its simplicity makes it easy to extend: We have easily added several new features; something a network operator could do to customize their network to meet their customers' needs.The demo consists of two parts: MPLS-TE services and then MPLS VPN driven by a GUI.",Proceedings of the ACM SIGCOMM 2011 Conference,452–453,2,"mpls, openflow, mpls-te, traffic engineering, vpn","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1131,article,S\'{a,Dasu - ISP Characterization from the Edge: A BitTorrent Implementation,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018517,10.1145/2043164.2018517,"Evaluating and characterizing access ISPs is critical to consumers shopping for alternative services and governments surveying the availability of broadband services to their citizens. We present Dasu, a service for crowdsourcing ISP characterization to the edge of the network. Dasu is implemented as an extension to a popular BitTorrent client and has been available since July 2010. While the prototype uses BitTorrent as its host application, its design is agnostic to the particular host application. The demo showcases our current implementation using both a prerecorded execution trace and a live run.",,454–455,2,"isp, characterization, broadband access networks",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1132,inproceedings,S\'{a,Dasu - ISP Characterization from the Edge: A BitTorrent Implementation,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018517,10.1145/2018436.2018517,"Evaluating and characterizing access ISPs is critical to consumers shopping for alternative services and governments surveying the availability of broadband services to their citizens. We present Dasu, a service for crowdsourcing ISP characterization to the edge of the network. Dasu is implemented as an extension to a popular BitTorrent client and has been available since July 2010. While the prototype uses BitTorrent as its host application, its design is agnostic to the particular host application. The demo showcases our current implementation using both a prerecorded execution trace and a live run.",Proceedings of the ACM SIGCOMM 2011 Conference,454–455,2,"broadband access networks, isp, characterization","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1133,article,"Wilhelm, Matthias and Martinovic, Ivan and Schmitt, Jens B. and Lenders, Vincent",WiFire: A Firewall for Wireless Networks,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018518,10.1145/2043164.2018518,"Firewalls are extremely effective at enforcing security policies in wired networks. Perhaps surprisingly, firewalls are entirely nonexistent in the wireless domain. Yet, the need to selectively control and block radio communication is particularly high in a broadcast environment since any node may receive and send packets. In this demo, we present WiFire, a system that brings the firewall concept to wireless networks. First, WiFire detects and analyzes packets during their transmission, checking their content against a set of rules. It then relies on reactive jamming techniques to selectively block undesired communication. We show the feasibility and performance of WiFire, which is implemented on the USRP2 software-defined radio platform, in several scenarios with IEEE 802.15.4 radios. WiFire is able to classify and effectively block undesired communication without interfering with desired communication.",,456–457,2,"reactive jamming, 802.15.4, software-defined jammer, wsn",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1134,inproceedings,"Wilhelm, Matthias and Martinovic, Ivan and Schmitt, Jens B. and Lenders, Vincent",WiFire: A Firewall for Wireless Networks,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018518,10.1145/2018436.2018518,"Firewalls are extremely effective at enforcing security policies in wired networks. Perhaps surprisingly, firewalls are entirely nonexistent in the wireless domain. Yet, the need to selectively control and block radio communication is particularly high in a broadcast environment since any node may receive and send packets. In this demo, we present WiFire, a system that brings the firewall concept to wireless networks. First, WiFire detects and analyzes packets during their transmission, checking their content against a set of rules. It then relies on reactive jamming techniques to selectively block undesired communication. We show the feasibility and performance of WiFire, which is implemented on the USRP2 software-defined radio platform, in several scenarios with IEEE 802.15.4 radios. WiFire is able to classify and effectively block undesired communication without interfering with desired communication.",Proceedings of the ACM SIGCOMM 2011 Conference,456–457,2,"802.15.4, software-defined jammer, reactive jamming, wsn","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1135,article,"Salvadori, Elio and Doriguzzi Corin, Roberto and Gerola, Matteo and Broglio, Attilio and De Pellegrini, Francesco",Demonstrating Generalized Virtual Topologies in an Openflow Network,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018520,10.1145/2043164.2018520,,,458–459,2,"openflow, network virtualization",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1136,inproceedings,"Salvadori, Elio and Doriguzzi Corin, Roberto and Gerola, Matteo and Broglio, Attilio and De Pellegrini, Francesco",Demonstrating Generalized Virtual Topologies in an Openflow Network,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018520,10.1145/2018436.2018520,,Proceedings of the ACM SIGCOMM 2011 Conference,458–459,2,"openflow, network virtualization","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1137,article,"Liao, Qi and Shi, Lei and He, Yuan and Li, Rui and Su, Zhong and Striegel, Aaron and Liu, Yunhao",Visualizing Anomalies in Sensor Networks,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018521,10.1145/2043164.2018521,"Diagnosing a large-scale sensor network is a crucial but challenging task due to the spatiotemporally dynamic network behaviors of sensor nodes. In this demo, we present Sensor Anomaly Visualization Engine (SAVE), an integrated system that tackles the sensor network diagnosis problem using both visualization and anomaly detection analytics to guide the user quickly and accurately diagnose sensor network failures. Temporal expansion model, correlation graphs and dynamic projection views are proposed to effectively interpret the topological, correlational and dimensional sensor data dynamics and their anomalies. Through a real-world large-scale wireless sensor network deployment (GreenOrbs), we demonstrate that SAVE is able to help better locate the problem and further identify the root cause of major sensor network failures.",,460–461,2,"anomaly detection and analysis, diagnosing, visualization, wireless sensor networks",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1138,inproceedings,"Liao, Qi and Shi, Lei and He, Yuan and Li, Rui and Su, Zhong and Striegel, Aaron and Liu, Yunhao",Visualizing Anomalies in Sensor Networks,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018521,10.1145/2018436.2018521,"Diagnosing a large-scale sensor network is a crucial but challenging task due to the spatiotemporally dynamic network behaviors of sensor nodes. In this demo, we present Sensor Anomaly Visualization Engine (SAVE), an integrated system that tackles the sensor network diagnosis problem using both visualization and anomaly detection analytics to guide the user quickly and accurately diagnose sensor network failures. Temporal expansion model, correlation graphs and dynamic projection views are proposed to effectively interpret the topological, correlational and dimensional sensor data dynamics and their anomalies. Through a real-world large-scale wireless sensor network deployment (GreenOrbs), we demonstrate that SAVE is able to help better locate the problem and further identify the root cause of major sensor network failures.",Proceedings of the ACM SIGCOMM 2011 Conference,460–461,2,"wireless sensor networks, visualization, diagnosing, anomaly detection and analysis","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1139,article,"Dhananjay, Aditya and Tierney, Matt and Li, Jinyang and Subramanian, Lakshminarayanan",WiRE: A New Rural Connectivity Paradigm,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018522,10.1145/2043164.2018522,"Many rural areas in developing regions remain largely disconnected from the rest of the world due to low purchasing power and the exorbitant cost of existing connectivity solutions. Wireless Rural Extensions (WiRE) is a low-power rural wireless network architecture that provides inexpensive, self-sustainable, and high-bandwidth connectivity. WiRE relies on a high-bandwidth directional wireless backbone with local distribution networks to provide focused IP coverage. WiRE also provides cellular connectivity using OpenBTS-based GSM microcells. It supports a naming and addressing framework that inter-operates with traditional telecom networks and enables a wide range of mobile services on a common IP framework. The entire name network can be built by integrating a range of off-the-shelf components and existing open source tools.",,462–463,2,cellular,,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1140,inproceedings,"Dhananjay, Aditya and Tierney, Matt and Li, Jinyang and Subramanian, Lakshminarayanan",WiRE: A New Rural Connectivity Paradigm,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018522,10.1145/2018436.2018522,"Many rural areas in developing regions remain largely disconnected from the rest of the world due to low purchasing power and the exorbitant cost of existing connectivity solutions. Wireless Rural Extensions (WiRE) is a low-power rural wireless network architecture that provides inexpensive, self-sustainable, and high-bandwidth connectivity. WiRE relies on a high-bandwidth directional wireless backbone with local distribution networks to provide focused IP coverage. WiRE also provides cellular connectivity using OpenBTS-based GSM microcells. It supports a naming and addressing framework that inter-operates with traditional telecom networks and enables a wide range of mobile services on a common IP framework. The entire name network can be built by integrating a range of off-the-shelf components and existing open source tools.",Proceedings of the ACM SIGCOMM 2011 Conference,462–463,2,cellular,"Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1141,article,"Mortier, Richard and Bedwell, Ben and Glover, Kevin and Lodge, Tom and Rodden, Tom and Rotsos, Charalampos and Moore, Andrew W. and Koliousis, Alexandros and Sventek, Joseph",Supporting Novel Home Network Management Interfaces with Openflow and NOX,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018523,10.1145/2043164.2018523,"The Homework project has examined redesign of existing home network infrastructures to better support the needs and requirements of actual home users. Integrating results from several ethnographic studies, we have designed and built a home networking platform providing detailed per-flow measurement and management capabilities supporting several novel management interfaces. This demo specifically shows these new visualization and control interfaces (1), and describes the broader benefits of taking an integrated view of the networking infrastructure, realised through our router's augmented measurement and control APIs (2).Aspects of this work have been published: the Homework Database in Internet Management (IM) 2011 [3] and implications of the ethnographic results are to appear at the SIGCOMM W-MUST workshop 2011 [2]. Separate, more detailed expositions of the interface elements and system performance and implications are currently under submission at other venues. A partial code release is already available and we anticipate fuller public beta release by Q4 2011.",,464–465,2,"home networks, nox, dhcp, openflow, network management",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1142,inproceedings,"Mortier, Richard and Bedwell, Ben and Glover, Kevin and Lodge, Tom and Rodden, Tom and Rotsos, Charalampos and Moore, Andrew W. and Koliousis, Alexandros and Sventek, Joseph",Supporting Novel Home Network Management Interfaces with Openflow and NOX,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018523,10.1145/2018436.2018523,"The Homework project has examined redesign of existing home network infrastructures to better support the needs and requirements of actual home users. Integrating results from several ethnographic studies, we have designed and built a home networking platform providing detailed per-flow measurement and management capabilities supporting several novel management interfaces. This demo specifically shows these new visualization and control interfaces (1), and describes the broader benefits of taking an integrated view of the networking infrastructure, realised through our router's augmented measurement and control APIs (2).Aspects of this work have been published: the Homework Database in Internet Management (IM) 2011 [3] and implications of the ethnographic results are to appear at the SIGCOMM W-MUST workshop 2011 [2]. Separate, more detailed expositions of the interface elements and system performance and implications are currently under submission at other venues. A partial code release is already available and we anticipate fuller public beta release by Q4 2011.",Proceedings of the ACM SIGCOMM 2011 Conference,464–465,2,"nox, openflow, network management, home networks, dhcp","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1143,article,"Li, Tao and Sun, Zhigang and Jia, Chunbo and Su, Qi and Lee, Myungjin",Using NetMagic to Observe Fine-Grained per-Flow Latency Measurements,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018524,10.1145/2043164.2018524,"We introduce NetMagic to demonstrate the efficacy of RLI architecture RLI for the fine-grained per-flow latency measurements. In this demo, the main function of RLI is implemented in NetMagic, which is the key component of our experimental network comprising several computers and switches. We are going to show how NetMagic can provide rapid implementation and evaluation of RLI architecture that is difficult with commercial switch or router platforms. In the demo, the estimated fine-grained per-flow latency by RLI is monitored and dynamically presented. Further, the true latency with a resolution of 8ns is also provided by NetMagic for the evaluation. The efficacy of RLI architecture can be observed in a real-time fashion by the difference between estimated latencies and true ones.",,466–467,2,"per-flow latency, measurement, netmagic platform, rli architecture",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1144,inproceedings,"Li, Tao and Sun, Zhigang and Jia, Chunbo and Su, Qi and Lee, Myungjin",Using NetMagic to Observe Fine-Grained per-Flow Latency Measurements,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018524,10.1145/2018436.2018524,"We introduce NetMagic to demonstrate the efficacy of RLI architecture RLI for the fine-grained per-flow latency measurements. In this demo, the main function of RLI is implemented in NetMagic, which is the key component of our experimental network comprising several computers and switches. We are going to show how NetMagic can provide rapid implementation and evaluation of RLI architecture that is difficult with commercial switch or router platforms. In the demo, the estimated fine-grained per-flow latency by RLI is monitored and dynamically presented. Further, the true latency with a resolution of 8ns is also provided by NetMagic for the evaluation. The efficacy of RLI architecture can be observed in a real-time fashion by the difference between estimated latencies and true ones.",Proceedings of the ACM SIGCOMM 2011 Conference,466–467,2,"per-flow latency, measurement, rli architecture, netmagic platform","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1145,article,"Puype, Bart and Papadimitriou, Dimitri and Das, Goutam and Colle, Didier and Pickavet, Mario and Demeester, Piet",OSPF Failure Reconvergence through SRG Inference and Prediction of Link State Advertisements,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018525,10.1145/2043164.2018525,"We demonstrate machine learning augmented Open Shortest Path First (OSPF) routing which infers Shared Risk Groups (SRG) from link failure history. For an initial link failure matching an SRG, it predicts subsequent link state advertisements corresponding with that SRG, improving convergence and recovery times during multiple network failures.",,468–469,2,"machine learning, shared risk group, ospf, cognitive routing, network recovery",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1146,inproceedings,"Puype, Bart and Papadimitriou, Dimitri and Das, Goutam and Colle, Didier and Pickavet, Mario and Demeester, Piet",OSPF Failure Reconvergence through SRG Inference and Prediction of Link State Advertisements,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018525,10.1145/2018436.2018525,"We demonstrate machine learning augmented Open Shortest Path First (OSPF) routing which infers Shared Risk Groups (SRG) from link failure history. For an initial link failure matching an SRG, it predicts subsequent link state advertisements corresponding with that SRG, improving convergence and recovery times during multiple network failures.",Proceedings of the ACM SIGCOMM 2011 Conference,468–469,2,"shared risk group, network recovery, ospf, cognitive routing, machine learning","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1147,article,"Kim, Hyojoon and Sundaresan, Srikanth and Chetty, Marshini and Feamster, Nick and Edwards, W. Keith",Communicating with Caps: Managing Usage Caps in Home Networks,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018526,10.1145/2043164.2018526,"As Internet service providers increasingly implement and impose ""usage caps"", consumers need better ways to help them understand and control how devices in the home use up the available network resources or available capacity. Towards this goal, we will demonstrate a system that allows users to monitor and manage their usage caps. The system uses the BISMark firmware running on network gateways to collect usage statistics and report them to a logically centralized controller, which displays usage information. The controller allows users to specify policies about how different people, devices, and applications should consume the usage cap; it implements and enforces these policies via a secure OpenFlow control channel to each gateway device. The demonstration will show various use cases, such as limiting the usage of a particular application, visualizing usage statistics, and allowing users within a single household to ""trade"" caps with one another.",,470–471,2,"home network, usage cap, openflow",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1148,inproceedings,"Kim, Hyojoon and Sundaresan, Srikanth and Chetty, Marshini and Feamster, Nick and Edwards, W. Keith",Communicating with Caps: Managing Usage Caps in Home Networks,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018526,10.1145/2018436.2018526,"As Internet service providers increasingly implement and impose ""usage caps"", consumers need better ways to help them understand and control how devices in the home use up the available network resources or available capacity. Towards this goal, we will demonstrate a system that allows users to monitor and manage their usage caps. The system uses the BISMark firmware running on network gateways to collect usage statistics and report them to a logically centralized controller, which displays usage information. The controller allows users to specify policies about how different people, devices, and applications should consume the usage cap; it implements and enforces these policies via a secure OpenFlow control channel to each gateway device. The demonstration will show various use cases, such as limiting the usage of a particular application, visualizing usage statistics, and allowing users within a single household to ""trade"" caps with one another.",Proceedings of the ACM SIGCOMM 2011 Conference,470–471,2,"home network, openflow, usage cap","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1149,article,"Wu, Fang-Jing and Chu, Feng-I and Tseng, Yu-Chee",Cyber-Physical Handshake,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018527,10.1145/2043164.2018527,"While sensor-enabled devices have greatly enriched human interactions in our daily life, discovering the essential knowledge behind sensing data is a critical issue to connect the cyber world and the physical world. This motivates us to design an innovative sensor-aided social network system, termed cyber-physical handshake. It allows two users to naturally exchange personal information with each other after detecting and authenticating the handshaking patterns between them. This work describes our design of detection and authentication mechanisms to achieve this purpose and our prototype system to facilitate handshake social behavior.",,472–473,2,"participatory sensing, social network, cyber-physical system, pervasive computing, wireless sensor network",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1150,inproceedings,"Wu, Fang-Jing and Chu, Feng-I and Tseng, Yu-Chee",Cyber-Physical Handshake,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018527,10.1145/2018436.2018527,"While sensor-enabled devices have greatly enriched human interactions in our daily life, discovering the essential knowledge behind sensing data is a critical issue to connect the cyber world and the physical world. This motivates us to design an innovative sensor-aided social network system, termed cyber-physical handshake. It allows two users to naturally exchange personal information with each other after detecting and authenticating the handshaking patterns between them. This work describes our design of detection and authentication mechanisms to achieve this purpose and our prototype system to facilitate handshake social behavior.",Proceedings of the ACM SIGCOMM 2011 Conference,472–473,2,"pervasive computing, wireless sensor network, social network, cyber-physical system, participatory sensing","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1151,article,"Lehn, Max and Leng, Christof and Rehner, Robert and Triebel, Tonio and Buchmann, Alejandro",An Online Gaming Testbed for Peer-to-Peer Architectures,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018528,10.1145/2043164.2018528,"In this demo we present a testbed environment for Peer-to-Peer (P2P) game architectures. It is based on Planet PI4, an online multiplayer game whose gameplay provides a standard workload for a set of gaming-specific network interfaces. Its pluggable architecture allows for the evaluation and comparison of existing and new P2P networking approaches. Planet PI4 can run on a real network for prototypical evaluation as well as in a discrete-event simulator providing a reproducible environment.",,474–475,2,"online gaming, benchmarking, peer-to-peer",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1152,inproceedings,"Lehn, Max and Leng, Christof and Rehner, Robert and Triebel, Tonio and Buchmann, Alejandro",An Online Gaming Testbed for Peer-to-Peer Architectures,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018528,10.1145/2018436.2018528,"In this demo we present a testbed environment for Peer-to-Peer (P2P) game architectures. It is based on Planet PI4, an online multiplayer game whose gameplay provides a standard workload for a set of gaming-specific network interfaces. Its pluggable architecture allows for the evaluation and comparison of existing and new P2P networking approaches. Planet PI4 can run on a real network for prototypical evaluation as well as in a discrete-event simulator providing a reproducible environment.",Proceedings of the ACM SIGCOMM 2011 Conference,474–475,2,"peer-to-peer, benchmarking, online gaming","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1153,article,"May, Martin and Diot, Christophe and Le Guyadec, Pascal and Picconi, Fabio and Roussel, Joris and Soule, Augustin",Service Hosting Gateways: A Platform for Distributed Service Deployment in End User Homes,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018529,10.1145/2043164.2018529,"The success of broadband residential Internet access is changing the way home users consume digital content and services. Currently, each home service requires the installation of a separate physical box (for instance, the NetFlix box or IPTV set-top-boxes). Instead, we argue for deploying a single box in the home that is powerful and flexible enough to host a variety of home services. In addition, this box is managed by the Internet Service provider and is able to provide service guarantees. We call such a box a service-hosting gateway (SHG), as it combines the functionalities of the home gateway managed by the network service provider with the capability of hosting services. Isolation between such services is ensured by virtualization.We demonstrate a prototype of our (SHG). It is based on the hardware platform that will be used for future home gateways. We illustrate the features of the SHG with multiple use cases ranging from simple service deployment scenarios to complex media distribution services and home automation features.",,476–477,2,nano datacenters,,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1154,inproceedings,"May, Martin and Diot, Christophe and Le Guyadec, Pascal and Picconi, Fabio and Roussel, Joris and Soule, Augustin",Service Hosting Gateways: A Platform for Distributed Service Deployment in End User Homes,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018529,10.1145/2018436.2018529,"The success of broadband residential Internet access is changing the way home users consume digital content and services. Currently, each home service requires the installation of a separate physical box (for instance, the NetFlix box or IPTV set-top-boxes). Instead, we argue for deploying a single box in the home that is powerful and flexible enough to host a variety of home services. In addition, this box is managed by the Internet Service provider and is able to provide service guarantees. We call such a box a service-hosting gateway (SHG), as it combines the functionalities of the home gateway managed by the network service provider with the capability of hosting services. Isolation between such services is ensured by virtualization.We demonstrate a prototype of our (SHG). It is based on the hardware platform that will be used for future home gateways. We illustrate the features of the SHG with multiple use cases ranging from simple service deployment scenarios to complex media distribution services and home automation features.",Proceedings of the ACM SIGCOMM 2011 Conference,476–477,2,nano datacenters,"Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1155,article,"Erickson, David and Heller, Brandon and Yang, Shuang and Chu, Jonathan and Ellithorpe, Jonathan and Whyte, Scott and Stuart, Stephen and McKeown, Nick and Parulkar, Guru and Rosenblum, Mendel",Optimizing a Virtualized Data Center,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018530,10.1145/2043164.2018530,"Many data centers extensively use virtual machines (VMs), which provide the flexibility to move workload among physical servers. VMs can be placed to maximize application performance, power efficiency, or even fault tolerance. However, VMs are typically repositioned without considering network topology, congestion, or traffic routes.In this demo, we show a system, Virtue, which enables the comparison of different algorithms for VM placement and network routing at the scale of an entire data center. Our goal is to understand how placement and routing affect overall application performance by varying the types and mix of workloads, network topologies, and compute resources; these parameters will be available for demo attendees to explore.",,478–479,2,"virtue, openflow, virtualization, data center network",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1156,inproceedings,"Erickson, David and Heller, Brandon and Yang, Shuang and Chu, Jonathan and Ellithorpe, Jonathan and Whyte, Scott and Stuart, Stephen and McKeown, Nick and Parulkar, Guru and Rosenblum, Mendel",Optimizing a Virtualized Data Center,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018530,10.1145/2018436.2018530,"Many data centers extensively use virtual machines (VMs), which provide the flexibility to move workload among physical servers. VMs can be placed to maximize application performance, power efficiency, or even fault tolerance. However, VMs are typically repositioned without considering network topology, congestion, or traffic routes.In this demo, we show a system, Virtue, which enables the comparison of different algorithms for VM placement and network routing at the scale of an entire data center. Our goal is to understand how placement and routing affect overall application performance by varying the types and mix of workloads, network topologies, and compute resources; these parameters will be available for demo attendees to explore.",Proceedings of the ACM SIGCOMM 2011 Conference,478–479,2,"virtualization, virtue, openflow, data center network","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1157,article,"Krifa, Amir and Mendonca, Marc and Bin Rais, Rao Naveed and Barakat, Chadi and Turletti, Thierry and Obraczka, Katia",Efficient Content Dissemination in Heterogeneous Networks Prone to Episodic Connectivity,2011,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2043164.2018531,10.1145/2043164.2018531,"Ubiquity of portable computing devices coupled with wide availability of wireless communication present new impor- tant opportunities for applications involving media-rich content dissemination. However, as access networks become increasingly more heterogeneous, seamless data delivery across internets consisting of a variety of network technology becomes a real challenge. In this demonstration, we showcase a system that enables content dissemination over heterogeneous internets consisting of wired, infrastructure-based and infrastructure-less wireless networks that may be prone to intermittent connectivity. Using an efficient, yet flexible buffer management scheme, we are able to address application-specific performance requirements such as average delay, delivery probability, energy efficiency, etc. Our system uses the Message Delivery in Heterogeneous, Disruption-prone Networks (MeDeHa) [2]) framework to deliver messages across a heterogeneous internet coupled with History-Based Scheduling and Drop (HBSD) buffer management [1] as a way to optimize resources provided by opportunistic networks. MeDeHa, which is described in detail in [2], provides seamless data delivery over interconnecting networks of different types, i.e., infrastructure-based and infrastructure-less networks. MeDeHa's comprehensive approach to bridging infrastructure-based and infrastructureless networks also copes with intermittent connectivity. For this demonstration, we showcase a ""complete stack"" solution featuring, from to top to bottom, the DTN2 ""bundle"" layer, HBSD as an ""external router"" to DTN2, and MeDeHa, which handles message delivery. We have implemented, on a Linux-based testbed, (i) the MeDeHa framework, (ii) the HBSD [3] external router for the DTN2 [4] architecture.",,480–481,2,"heterogeneous networks, buffer management, disruption tolerance, episodic connectivity",,,August 2011,41,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1158,inproceedings,"Krifa, Amir and Mendonca, Marc and Bin Rais, Rao Naveed and Barakat, Chadi and Turletti, Thierry and Obraczka, Katia",Efficient Content Dissemination in Heterogeneous Networks Prone to Episodic Connectivity,2011,9781450307970,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/2018436.2018531,10.1145/2018436.2018531,"Ubiquity of portable computing devices coupled with wide availability of wireless communication present new impor- tant opportunities for applications involving media-rich content dissemination. However, as access networks become increasingly more heterogeneous, seamless data delivery across internets consisting of a variety of network technology becomes a real challenge. In this demonstration, we showcase a system that enables content dissemination over heterogeneous internets consisting of wired, infrastructure-based and infrastructure-less wireless networks that may be prone to intermittent connectivity. Using an efficient, yet flexible buffer management scheme, we are able to address application-specific performance requirements such as average delay, delivery probability, energy efficiency, etc. Our system uses the Message Delivery in Heterogeneous, Disruption-prone Networks (MeDeHa) [2]) framework to deliver messages across a heterogeneous internet coupled with History-Based Scheduling and Drop (HBSD) buffer management [1] as a way to optimize resources provided by opportunistic networks. MeDeHa, which is described in detail in [2], provides seamless data delivery over interconnecting networks of different types, i.e., infrastructure-based and infrastructure-less networks. MeDeHa's comprehensive approach to bridging infrastructure-based and infrastructureless networks also copes with intermittent connectivity. For this demonstration, we showcase a ""complete stack"" solution featuring, from to top to bottom, the DTN2 ""bundle"" layer, HBSD as an ""external router"" to DTN2, and MeDeHa, which handles message delivery. We have implemented, on a Linux-based testbed, (i) the MeDeHa framework, (ii) the HBSD [3] external router for the DTN2 [4] architecture.",Proceedings of the ACM SIGCOMM 2011 Conference,480–481,2,"heterogeneous networks, buffer management, disruption tolerance, episodic connectivity","Toronto, Ontario, Canada",SIGCOMM '11,,,,,,
1159,inproceedings,"Ramakrishnan, K. K.",Session Details: Keynote Address,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3248803,10.1145/3248803,,Proceedings of the ACM SIGCOMM 2010 Conference,,,,"New Delhi, India",SIGCOMM '10,,,,,,
1160,article,"Perlman, Radia",Protocol Design for Effective Communication among Silicon or Carbon-Based Nodes,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851184,10.1145/1851275.1851184,"In this talk I will discuss some of the lessons I've discovered about network protocol design; how to make protocols self-stabilizing, how to make networks self-configuring, how to provide optional configuration in such a way that misconfiguration does no harm, and how to design within constraints such as backward compatibility and politics. I'll also talk about some of my recent work. It is confusing enough that forwarding is done at both layers 2 and 3. Why am I designing a layer 2 1/2? And having spent most of my life in the world of computer network communication protocol design, I will share my observations about ways in which communication protocols in other areas could be improved.",,1–2,2,protocol design,,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1161,inproceedings,"Perlman, Radia",Protocol Design for Effective Communication among Silicon or Carbon-Based Nodes,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851184,10.1145/1851182.1851184,"In this talk I will discuss some of the lessons I've discovered about network protocol design; how to make protocols self-stabilizing, how to make networks self-configuring, how to provide optional configuration in such a way that misconfiguration does no harm, and how to design within constraints such as backward compatibility and politics. I'll also talk about some of my recent work. It is confusing enough that forwarding is done at both layers 2 and 3. Why am I designing a layer 2 1/2? And having spent most of my life in the world of computer network communication protocol design, I will share my observations about ways in which communication protocols in other areas could be improved.",Proceedings of the ACM SIGCOMM 2010 Conference,1–2,2,protocol design,"New Delhi, India",SIGCOMM '10,,,,,,
1162,inproceedings,"Davie, Bruce",Session Details: Wireless and Measurement,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3248804,10.1145/3248804,,Proceedings of the ACM SIGCOMM 2010 Conference,,,,"New Delhi, India",SIGCOMM '10,,,,,,
1163,article,"Chen, Binbin and Zhou, Ziling and Zhao, Yuda and Yu, Haifeng",Efficient Error Estimating Coding: Feasibility and Applications,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851186,10.1145/1851275.1851186,"Motivated by recent emerging systems that can leverage partially correct packets in wireless networks, this paper investigates the novel concept of error estimating codes (EEC). Without correcting the errors in the packet, EEC enables the receiver of the packet to estimate the packet's bit error rate, which is perhaps the most important meta-information of a partially correct packet. Our EEC algorithm provides provable estimation quality, with rather low redundancy and computational overhead. To demonstrate the utility of EEC, we exploit and implement EEC in two wireless network applications, Wi-Fi rate adaptation and real-time video streaming. Our real-world experiments show that these applications can significantly benefit from EEC.",,3–14,12,"bit error rate, partial packet, error correcting coding, partially correct packet, error estimating coding",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1164,inproceedings,"Chen, Binbin and Zhou, Ziling and Zhao, Yuda and Yu, Haifeng",Efficient Error Estimating Coding: Feasibility and Applications,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851186,10.1145/1851182.1851186,"Motivated by recent emerging systems that can leverage partially correct packets in wireless networks, this paper investigates the novel concept of error estimating codes (EEC). Without correcting the errors in the packet, EEC enables the receiver of the packet to estimate the packet's bit error rate, which is perhaps the most important meta-information of a partially correct packet. Our EEC algorithm provides provable estimation quality, with rather low redundancy and computational overhead. To demonstrate the utility of EEC, we exploit and implement EEC in two wireless network applications, Wi-Fi rate adaptation and real-time video streaming. Our real-world experiments show that these applications can significantly benefit from EEC.",Proceedings of the ACM SIGCOMM 2010 Conference,3–14,12,"error correcting coding, bit error rate, partial packet, error estimating coding, partially correct packet","New Delhi, India",SIGCOMM '10,,,,,,
1165,article,"Sen, Sayandeep and Gilani, Syed and Srinath, Shreesha and Schmitt, Stephen and Banerjee, Suman","Design and Implementation of an ""Approximate"" Communication System for Wireless Media Applications",2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851187,10.1145/1851275.1851187,"All practical wireless communication systems are prone to errors. At the symbol level such wireless errors have a well-defined structure: when a receiver decodes a symbol erroneously, it is more likely that the decoded symbol is a good ""approximation"" of the transmitted symbol than a randomly chosen symbol among all possible transmitted symbols. Based on this property, we define approximate communication, a method that exploits this error structure to natively provide unequal error protection to data bits. Unlike traditional (FEC-based) mechanisms of unequal error protection that consumes additional network and spectrum resources to encode redundant data, the approximate communication technique achieves this property at the PHY layer without consuming any additional network or spectrum resources (apart from a minimal signaling overhead). Approximate communication is particularly useful to media delivery applications that can benefit significantly from unequal error protection of data bits. We show the usefulness of this method to such applications by designing and implementing an end-to-end media delivery system, called Apex. Our Software Defined Radio (SDR)-based experiments reveal that Apex can improve video quality by 5 to 20 dB (PSNR) across a diverse set of wireless conditions, when compared to traditional approaches. We believe that mechanisms such as Apex can be a cornerstone in designing future wireless media delivery systems under any error-prone channel condition.",,15–26,12,"media delivery, wireless PHY, cross layer",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1166,inproceedings,"Sen, Sayandeep and Gilani, Syed and Srinath, Shreesha and Schmitt, Stephen and Banerjee, Suman","Design and Implementation of an ""Approximate"" Communication System for Wireless Media Applications",2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851187,10.1145/1851182.1851187,"All practical wireless communication systems are prone to errors. At the symbol level such wireless errors have a well-defined structure: when a receiver decodes a symbol erroneously, it is more likely that the decoded symbol is a good ""approximation"" of the transmitted symbol than a randomly chosen symbol among all possible transmitted symbols. Based on this property, we define approximate communication, a method that exploits this error structure to natively provide unequal error protection to data bits. Unlike traditional (FEC-based) mechanisms of unequal error protection that consumes additional network and spectrum resources to encode redundant data, the approximate communication technique achieves this property at the PHY layer without consuming any additional network or spectrum resources (apart from a minimal signaling overhead). Approximate communication is particularly useful to media delivery applications that can benefit significantly from unequal error protection of data bits. We show the usefulness of this method to such applications by designing and implementing an end-to-end media delivery system, called Apex. Our Software Defined Radio (SDR)-based experiments reveal that Apex can improve video quality by 5 to 20 dB (PSNR) across a diverse set of wireless conditions, when compared to traditional approaches. We believe that mechanisms such as Apex can be a cornerstone in designing future wireless media delivery systems under any error-prone channel condition.",Proceedings of the ACM SIGCOMM 2010 Conference,15–26,12,"wireless PHY, cross layer, media delivery","New Delhi, India",SIGCOMM '10,,,,,,
1167,article,"Lee, Myungjin and Duffield, Nick and Kompella, Ramana Rao",Not All Microseconds Are Equal: Fine-Grained per-Flow Measurements with Reference Latency Interpolation,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851188,10.1145/1851275.1851188,"New applications such as algorithmic trading and high-performance computing require extremely low latency (in microseconds). Network operators today lack sufficient fine-grain measurement tools to detect, localize and repair performance anomalies and delay spikes that cause application SLA violations. A recently proposed solution called LDA provides a scalable way to obtain latency, but only provides aggregate measurements. However, debugging application-specific problems requires per-flow measurements, since different flows may exhibit significantly different characteristics even when they are traversing the same link. To enable fine-grained per-flow measurements in routers, we propose a new scalable architecture called reference latency interpolation (RLI) that is based on our observation that packets potentially belonging to different flows that are closely spaced to each other exhibit similar delay properties. In our evaluation using simulations over real traces, we show that RLI achieves a median relative error of 12% and one to two orders of magnitude higher accuracy than previous per-flow measurement solutions with small overhead.",,27–38,12,"active measurement, approximation",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1168,inproceedings,"Lee, Myungjin and Duffield, Nick and Kompella, Ramana Rao",Not All Microseconds Are Equal: Fine-Grained per-Flow Measurements with Reference Latency Interpolation,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851188,10.1145/1851182.1851188,"New applications such as algorithmic trading and high-performance computing require extremely low latency (in microseconds). Network operators today lack sufficient fine-grain measurement tools to detect, localize and repair performance anomalies and delay spikes that cause application SLA violations. A recently proposed solution called LDA provides a scalable way to obtain latency, but only provides aggregate measurements. However, debugging application-specific problems requires per-flow measurements, since different flows may exhibit significantly different characteristics even when they are traversing the same link. To enable fine-grained per-flow measurements in routers, we propose a new scalable architecture called reference latency interpolation (RLI) that is based on our observation that packets potentially belonging to different flows that are closely spaced to each other exhibit similar delay properties. In our evaluation using simulations over real traces, we show that RLI achieves a median relative error of 12% and one to two orders of magnitude higher accuracy than previous per-flow measurement solutions with small overhead.",Proceedings of the ACM SIGCOMM 2010 Conference,27–38,12,"approximation, active measurement","New Delhi, India",SIGCOMM '10,,,,,,
1169,inproceedings,"Padhye, Jitu",Session Details: Data Center Networks,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3248805,10.1145/3248805,,Proceedings of the ACM SIGCOMM 2010 Conference,,,,"New Delhi, India",SIGCOMM '10,,,,,,
1170,article,"Chen, Kai and Guo, Chuanxiong and Wu, Haitao and Yuan, Jing and Feng, Zhenqian and Chen, Yan and Lu, Songwu and Wu, Wenfei",Generic and Automatic Address Configuration for Data Center Networks,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851190,10.1145/1851275.1851190,"Data center networks encode locality and topology information into their server and switch addresses for performance and routing purposes. For this reason, the traditional address configuration protocols such as DHCP require huge amount of manual input, leaving them error-prone.In this paper, we present DAC, a generic and automatic Data center Address Configuration system. With an automatically generated blueprint which defines the connections of servers and switches labeled by logical IDs, e.g., IP addresses, DAC first learns the physical topology labeled by device IDs, e.g., MAC addresses. Then at the core of DAC is its device-to-logical ID mapping and malfunction detection. DAC makes an innovation in abstracting the device-to-logical ID mapping to the graph isomorphism problem, and solves it with low time-complexity by leveraging the attributes of data center network topologies. Its malfunction detection scheme detects errors such as device and link failures and miswirings, including the most difficult case where miswirings do not cause any node degree change.We have evaluated DAC via simulation, implementation and experiments. Our simulation results show that DAC can accurately find all the hardest-to-detect malfunctions and can autoconfigure a large data center with 3.8 million devices in 46 seconds. In our implementation, we successfully autoconfigure a small 64-server BCube network within 300 milliseconds and show that DAC is a viable solution for data center autoconfiguration.",,39–50,12,"graph isomorphism, address configuration, data center networks",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1171,inproceedings,"Chen, Kai and Guo, Chuanxiong and Wu, Haitao and Yuan, Jing and Feng, Zhenqian and Chen, Yan and Lu, Songwu and Wu, Wenfei",Generic and Automatic Address Configuration for Data Center Networks,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851190,10.1145/1851182.1851190,"Data center networks encode locality and topology information into their server and switch addresses for performance and routing purposes. For this reason, the traditional address configuration protocols such as DHCP require huge amount of manual input, leaving them error-prone.In this paper, we present DAC, a generic and automatic Data center Address Configuration system. With an automatically generated blueprint which defines the connections of servers and switches labeled by logical IDs, e.g., IP addresses, DAC first learns the physical topology labeled by device IDs, e.g., MAC addresses. Then at the core of DAC is its device-to-logical ID mapping and malfunction detection. DAC makes an innovation in abstracting the device-to-logical ID mapping to the graph isomorphism problem, and solves it with low time-complexity by leveraging the attributes of data center network topologies. Its malfunction detection scheme detects errors such as device and link failures and miswirings, including the most difficult case where miswirings do not cause any node degree change.We have evaluated DAC via simulation, implementation and experiments. Our simulation results show that DAC can accurately find all the hardest-to-detect malfunctions and can autoconfigure a large data center with 3.8 million devices in 46 seconds. In our implementation, we successfully autoconfigure a small 64-server BCube network within 300 milliseconds and show that DAC is a viable solution for data center autoconfiguration.",Proceedings of the ACM SIGCOMM 2010 Conference,39–50,12,"address configuration, data center networks, graph isomorphism","New Delhi, India",SIGCOMM '10,,,,,,
1172,article,"Abu-Libdeh, Hussam and Costa, Paolo and Rowstron, Antony and O'Shea, Greg and Donnelly, Austin",Symbiotic Routing in Future Data Centers,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851191,10.1145/1851275.1851191,"Building distributed applications that run in data centers is hard. The CamCube project explores the design of a shipping container sized data center with the goal of building an easier platform on which to build these applications. CamCube replaces the traditional switch-based network with a 3D torus topology, with each server directly connected to six other servers. As in other proposals, e.g. DCell and BCube, multi-hop routing in CamCube requires servers to participate in packet forwarding. To date, as in existing data centers, these approaches have all provided a single routing protocol for the applications.In this paper we explore if allowing applications to implement their own routing services is advantageous, and if we can support it efficiently. This is based on the observation that, due to the flexibility offered by the CamCube API, many applications implemented their own routing protocol in order to achieve specific application-level characteristics, such as trading off higher-latency for better path convergence. Using large-scale simulations we demonstrate the benefits and network-level impact of running multiple routing protocols. We demonstrate that applications are more efficient and do not generate additional control traffic overhead. This motivates us to design an extended routing service allowing easy implementation of application-specific routing protocols on CamCube. Finally, we demonstrate that the additional performance overhead incurred when using the extended routing service on a prototype CamCube is very low.",,51–62,12,"key-value stores, routing protocols, data centers",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1173,inproceedings,"Abu-Libdeh, Hussam and Costa, Paolo and Rowstron, Antony and O'Shea, Greg and Donnelly, Austin",Symbiotic Routing in Future Data Centers,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851191,10.1145/1851182.1851191,"Building distributed applications that run in data centers is hard. The CamCube project explores the design of a shipping container sized data center with the goal of building an easier platform on which to build these applications. CamCube replaces the traditional switch-based network with a 3D torus topology, with each server directly connected to six other servers. As in other proposals, e.g. DCell and BCube, multi-hop routing in CamCube requires servers to participate in packet forwarding. To date, as in existing data centers, these approaches have all provided a single routing protocol for the applications.In this paper we explore if allowing applications to implement their own routing services is advantageous, and if we can support it efficiently. This is based on the observation that, due to the flexibility offered by the CamCube API, many applications implemented their own routing protocol in order to achieve specific application-level characteristics, such as trading off higher-latency for better path convergence. Using large-scale simulations we demonstrate the benefits and network-level impact of running multiple routing protocols. We demonstrate that applications are more efficient and do not generate additional control traffic overhead. This motivates us to design an extended routing service allowing easy implementation of application-specific routing protocols on CamCube. Finally, we demonstrate that the additional performance overhead incurred when using the extended routing service on a prototype CamCube is very low.",Proceedings of the ACM SIGCOMM 2010 Conference,51–62,12,"data centers, routing protocols, key-value stores","New Delhi, India",SIGCOMM '10,,,,,,
1174,article,"Alizadeh, Mohammad and Greenberg, Albert and Maltz, David A. and Padhye, Jitendra and Patel, Parveen and Prabhakar, Balaji and Sengupta, Sudipta and Sridharan, Murari",Data Center TCP (DCTCP),2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851192,10.1145/1851275.1851192,"Cloud data centers host diverse applications, mixing workloads that require small predictable latency with others requiring large sustained throughput. In this environment, today's state-of-the-art TCP protocol falls short. We present measurements of a 6000 server production cluster and reveal impairments that lead to high application latencies, rooted in TCP's demands on the limited buffer space available in data center switches. For example, bandwidth hungry ""background"" flows build up queues at the switches, and thus impact the performance of latency sensitive ""foreground"" traffic.To address these problems, we propose DCTCP, a TCP-like protocol for data center networks. DCTCP leverages Explicit Congestion Notification (ECN) in the network to provide multi-bit feedback to the end hosts. We evaluate DCTCP at 1 and 10Gbps speeds using commodity, shallow buffered switches. We find DCTCP delivers the same or better throughput than TCP, while using 90% less buffer space. Unlike TCP, DCTCP also provides high burst tolerance and low latency for short flows. In handling workloads derived from operational measurements, we found DCTCP enables the applications to handle 10X the current background traffic, without impacting foreground traffic. Further, a 10X increase in foreground traffic does not cause any timeouts, thus largely eliminating incast problems.",,63–74,12,"data center network, TCP, ECN",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1175,inproceedings,"Alizadeh, Mohammad and Greenberg, Albert and Maltz, David A. and Padhye, Jitendra and Patel, Parveen and Prabhakar, Balaji and Sengupta, Sudipta and Sridharan, Murari",Data Center TCP (DCTCP),2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851192,10.1145/1851182.1851192,"Cloud data centers host diverse applications, mixing workloads that require small predictable latency with others requiring large sustained throughput. In this environment, today's state-of-the-art TCP protocol falls short. We present measurements of a 6000 server production cluster and reveal impairments that lead to high application latencies, rooted in TCP's demands on the limited buffer space available in data center switches. For example, bandwidth hungry ""background"" flows build up queues at the switches, and thus impact the performance of latency sensitive ""foreground"" traffic.To address these problems, we propose DCTCP, a TCP-like protocol for data center networks. DCTCP leverages Explicit Congestion Notification (ECN) in the network to provide multi-bit feedback to the end hosts. We evaluate DCTCP at 1 and 10Gbps speeds using commodity, shallow buffered switches. We find DCTCP delivers the same or better throughput than TCP, while using 90% less buffer space. Unlike TCP, DCTCP also provides high burst tolerance and low latency for short flows. In handling workloads derived from operational measurements, we found DCTCP enables the applications to handle 10X the current background traffic, without impacting foreground traffic. Further, a 10X increase in foreground traffic does not cause any timeouts, thus largely eliminating incast problems.",Proceedings of the ACM SIGCOMM 2010 Conference,63–74,12,"data center network, TCP, ECN","New Delhi, India",SIGCOMM '10,,,,,,
1176,inproceedings,"Willinger, Walter",Session Details: Inter-Domain Routing and Addressing,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3248806,10.1145/3248806,,Proceedings of the ACM SIGCOMM 2010 Conference,,,,"New Delhi, India",SIGCOMM '10,,,,,,
1177,article,"Labovitz, Craig and Iekel-Johnson, Scott and McPherson, Danny and Oberheide, Jon and Jahanian, Farnam",Internet Inter-Domain Traffic,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851194,10.1145/1851275.1851194,"In this paper, we examine changes in Internet inter-domain traffic demands and interconnection policies. We analyze more than 200 Exabytes of commercial Internet traffic over a two year period through the instrumentation of 110 large and geographically diverse cable operators, international transit backbones, regional networks and content providers. Our analysis shows significant changes in inter-AS traffic patterns and an evolution of provider peering strategies. Specifically, we find the majority of inter-domain traffic by volume now flows directly between large content providers, data center / CDNs and consumer networks. We also show significant changes in Internet application usage, including a global decline of P2P and a significant rise in video traffic. We conclude with estimates of the current size of the Internet by inter-domain traffic volume and rate of annualized inter-domain traffic growth.",,75–86,12,"peering policies, inter-domain traffic, architecture, internet",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1178,inproceedings,"Labovitz, Craig and Iekel-Johnson, Scott and McPherson, Danny and Oberheide, Jon and Jahanian, Farnam",Internet Inter-Domain Traffic,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851194,10.1145/1851182.1851194,"In this paper, we examine changes in Internet inter-domain traffic demands and interconnection policies. We analyze more than 200 Exabytes of commercial Internet traffic over a two year period through the instrumentation of 110 large and geographically diverse cable operators, international transit backbones, regional networks and content providers. Our analysis shows significant changes in inter-AS traffic patterns and an evolution of provider peering strategies. Specifically, we find the majority of inter-domain traffic by volume now flows directly between large content providers, data center / CDNs and consumer networks. We also show significant changes in Internet application usage, including a global decline of P2P and a significant rise in video traffic. We conclude with estimates of the current size of the Internet by inter-domain traffic volume and rate of annualized inter-domain traffic growth.",Proceedings of the ACM SIGCOMM 2010 Conference,75–86,12,"peering policies, internet, inter-domain traffic, architecture","New Delhi, India",SIGCOMM '10,,,,,,
1179,article,"Goldberg, Sharon and Schapira, Michael and Hummon, Peter and Rexford, Jennifer",How Secure Are Secure Interdomain Routing Protocols,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851195,10.1145/1851275.1851195,"In response to high-profile Internet outages, BGP security variants have been proposed to prevent the propagation of bogus routing information. To inform discussions of which variant should be deployed in the Internet, we quantify the ability of the main protocols (origin authentication, soBGP, S-BGP, and data-plane verification) to blunt traffic-attraction attacks; i.e., an attacker that deliberately attracts traffic to drop, tamper, or eavesdrop on packets.Intuition suggests that an attacker can maximize the traffic he attracts by widely announcing a short path that is not flagged as bogus by the secure protocol. Through simulations on an empirically-determined AS-level topology, we show that this strategy is surprisingly effective, even when the network uses an advanced security solution like S-BGP or data-plane verification. Worse yet, we show that these results underestimate the severity of attacks. We prove that finding the most damaging strategy is NP-hard, and show how counterintuitive strategies, like announcing longer paths, announcing to fewer neighbors, or triggering BGP loop-detection, can be used to attract even more traffic than the strategy above. These counterintuitive examples are not merely hypothetical; we searched the empirical AS topology to identify specific ASes that can launch them. Finally, we find that a clever export policy can often attract almost as much traffic as a bogus path announcement. Thus, our work implies that mechanisms that police export policies (e.g., defensive filtering) are crucial, even if S-BGP is fully deployed.",,87–98,12,"simulations, BGP, security, traffic attraction attacks, as-level topology",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1180,inproceedings,"Goldberg, Sharon and Schapira, Michael and Hummon, Peter and Rexford, Jennifer",How Secure Are Secure Interdomain Routing Protocols,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851195,10.1145/1851182.1851195,"In response to high-profile Internet outages, BGP security variants have been proposed to prevent the propagation of bogus routing information. To inform discussions of which variant should be deployed in the Internet, we quantify the ability of the main protocols (origin authentication, soBGP, S-BGP, and data-plane verification) to blunt traffic-attraction attacks; i.e., an attacker that deliberately attracts traffic to drop, tamper, or eavesdrop on packets.Intuition suggests that an attacker can maximize the traffic he attracts by widely announcing a short path that is not flagged as bogus by the secure protocol. Through simulations on an empirically-determined AS-level topology, we show that this strategy is surprisingly effective, even when the network uses an advanced security solution like S-BGP or data-plane verification. Worse yet, we show that these results underestimate the severity of attacks. We prove that finding the most damaging strategy is NP-hard, and show how counterintuitive strategies, like announcing longer paths, announcing to fewer neighbors, or triggering BGP loop-detection, can be used to attract even more traffic than the strategy above. These counterintuitive examples are not merely hypothetical; we searched the empirical AS topology to identify specific ASes that can launch them. Finally, we find that a clever export policy can often attract almost as much traffic as a bogus path announcement. Thus, our work implies that mechanisms that police export policies (e.g., defensive filtering) are crucial, even if S-BGP is fully deployed.",Proceedings of the ACM SIGCOMM 2010 Conference,87–98,12,"traffic attraction attacks, simulations, security, as-level topology, BGP","New Delhi, India",SIGCOMM '10,,,,,,
1181,article,"Cai, Xue and Heidemann, John",Understanding Block-Level Address Usage in the Visible Internet,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851196,10.1145/1851275.1851196,"Although the Internet is widely used today, we have little information about the edge of the network. Decentralized management, firewalls, and sensitivity to probing prevent easy answers and make measurement difficult. Building on frequent ICMP probing of 1% of the Internet address space, we develop clustering and analysis methods to estimate how Internet addresses are used. We show that adjacent addresses often have similar characteristics and are used for similar purposes (61% of addresses we probe are consistent blocks of 64 neighbours or more). We then apply this block-level clustering to provide data to explore several open questions in how networks are managed. First, we provide information about how effectively network address blocks appear to be used, finding that a significant number of blocks are only lightly used (most addresses in about one-fifth of 24 blocks are in use less than 10% of the time), an important issue as the IPv4 address space nears full allocation. Second, we provide new measurements about dynamically managed address space, showing nearly 40% of 24 blocks appear to be dynamically allocated, and dynamic addressing is most widely used in countries more recent to the Internet (more than 80% in China, while less than 30% in the U.S.). Third, we distinguish blocks with low-bitrate last-hops and show that such blocks are often underutilized.",,99–110,12,"RTT, availability, low-bitrate, median-up, volatility, pattern analysis, internet address usage, survey, clustering, classification",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1182,inproceedings,"Cai, Xue and Heidemann, John",Understanding Block-Level Address Usage in the Visible Internet,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851196,10.1145/1851182.1851196,"Although the Internet is widely used today, we have little information about the edge of the network. Decentralized management, firewalls, and sensitivity to probing prevent easy answers and make measurement difficult. Building on frequent ICMP probing of 1% of the Internet address space, we develop clustering and analysis methods to estimate how Internet addresses are used. We show that adjacent addresses often have similar characteristics and are used for similar purposes (61% of addresses we probe are consistent blocks of 64 neighbours or more). We then apply this block-level clustering to provide data to explore several open questions in how networks are managed. First, we provide information about how effectively network address blocks appear to be used, finding that a significant number of blocks are only lightly used (most addresses in about one-fifth of 24 blocks are in use less than 10% of the time), an important issue as the IPv4 address space nears full allocation. Second, we provide new measurements about dynamically managed address space, showing nearly 40% of 24 blocks appear to be dynamically allocated, and dynamic addressing is most widely used in countries more recent to the Internet (more than 80% in China, while less than 30% in the U.S.). Third, we distinguish blocks with low-bitrate last-hops and show that such blocks are often underutilized.",Proceedings of the ACM SIGCOMM 2010 Conference,99–110,12,"median-up, clustering, volatility, classification, survey, low-bitrate, availability, pattern analysis, internet address usage, RTT","New Delhi, India",SIGCOMM '10,,,,,,
1183,inproceedings,"Spring, Neil",Session Details: Privacy,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3248807,10.1145/3248807,,Proceedings of the ACM SIGCOMM 2010 Conference,,,,"New Delhi, India",SIGCOMM '10,,,,,,
1184,article,"Isdal, Tomas and Piatek, Michael and Krishnamurthy, Arvind and Anderson, Thomas",Privacy-Preserving P2P Data Sharing with OneSwarm,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851198,10.1145/1851275.1851198,"Privacy -- the protection of information from unauthorized disclosure -- is increasingly scarce on the Internet. The lack of privacy is particularly true for popular peer-to-peer data sharing applications such as BitTorrent where user behavior is easily monitored by third parties. Anonymizing overlays such as Tor and Freenet can improve user privacy, but only at a cost of substantially reduced performance. Most users are caught in the middle, unwilling to sacrifice either privacy or performance.In this paper, we explore a new design point in this tradeoff between privacy and performance. We describe the design and implementation of a new P2P data sharing protocol, called OneSwarm, that provides users much better privacy than BitTorrent and much better performance than Tor or Freenet. A key aspect of the OneSwarm design is that users have explicit configurable control over the amount of trust they place in peers and in the sharing model for their data: the same data can be shared publicly, anonymously, or with access control, with both trusted and untrusted peers. OneSwarm's novel lookup and transfer techniques yield a median factor of 3.4 improvement in download times relative to Tor and a factor of 6.9 improvement relative to Freenet. OneSwarm is publicly available and has been downloaded by hundreds of thousands of users since its release.",,111–122,12,oneswarm,,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1185,inproceedings,"Isdal, Tomas and Piatek, Michael and Krishnamurthy, Arvind and Anderson, Thomas",Privacy-Preserving P2P Data Sharing with OneSwarm,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851198,10.1145/1851182.1851198,"Privacy -- the protection of information from unauthorized disclosure -- is increasingly scarce on the Internet. The lack of privacy is particularly true for popular peer-to-peer data sharing applications such as BitTorrent where user behavior is easily monitored by third parties. Anonymizing overlays such as Tor and Freenet can improve user privacy, but only at a cost of substantially reduced performance. Most users are caught in the middle, unwilling to sacrifice either privacy or performance.In this paper, we explore a new design point in this tradeoff between privacy and performance. We describe the design and implementation of a new P2P data sharing protocol, called OneSwarm, that provides users much better privacy than BitTorrent and much better performance than Tor or Freenet. A key aspect of the OneSwarm design is that users have explicit configurable control over the amount of trust they place in peers and in the sharing model for their data: the same data can be shared publicly, anonymously, or with access control, with both trusted and untrusted peers. OneSwarm's novel lookup and transfer techniques yield a median factor of 3.4 improvement in download times relative to Tor and a factor of 6.9 improvement relative to Freenet. OneSwarm is publicly available and has been downloaded by hundreds of thousands of users since its release.",Proceedings of the ACM SIGCOMM 2010 Conference,111–122,12,oneswarm,"New Delhi, India",SIGCOMM '10,,,,,,
1186,article,"McSherry, Frank and Mahajan, Ratul",Differentially-Private Network Trace Analysis,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851199,10.1145/1851275.1851199,"We consider the potential for network trace analysis while providing the guarantees of ""differential privacy."" While differential privacy provably obscures the presence or absence of individual records in a dataset, it has two major limitations: analyses must (presently) be expressed in a higher level declarative language; and the analysis results are randomized before returning to the analyst.We report on our experiences conducting a diverse set of analyses in a differentially private manner. We are able to express all of our target analyses, though for some of them an approximate expression is required to keep the error-level low. By running these analyses on real datasets, we find that the error introduced for the sake of privacy is often (but not always) low even at high levels of privacy. We factor our learning into a toolkit that will be likely useful for other analyses. Overall, we conclude that differential privacy shows promise for a broad class of network analyses.",,123–134,12,"differential privacy, trace analysis",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1187,inbook,"McSherry, Frank and Mahajan, Ratul",Differentially-Private Network Trace Analysis,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851199,,"We consider the potential for network trace analysis while providing the guarantees of ""differential privacy."" While differential privacy provably obscures the presence or absence of individual records in a dataset, it has two major limitations: analyses must (presently) be expressed in a higher level declarative language; and the analysis results are randomized before returning to the analyst.We report on our experiences conducting a diverse set of analyses in a differentially private manner. We are able to express all of our target analyses, though for some of them an approximate expression is required to keep the error-level low. By running these analyses on real datasets, we find that the error introduced for the sake of privacy is often (but not always) low even at high levels of privacy. We factor our learning into a toolkit that will be likely useful for other analyses. Overall, we conclude that differential privacy shows promise for a broad class of network analyses.",Proceedings of the ACM SIGCOMM 2010 Conference,123–134,12,,,,,,,,,
1188,article,"Kounavis, Michael E. and Kang, Xiaozhu and Grewal, Ken and Eszenyi, Mathew and Gueron, Shay and Durham, David",Encrypting the Internet,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851200,10.1145/1851275.1851200,"End-to-end communication encryption is considered necessary for protecting the privacy of user data in the Internet. Only a small fraction of all Internet traffic, however, is protected today. The primary reason for this neglect is economic, mainly security protocol speed and cost. In this paper we argue that recent advances in the implementation of cryptographic algorithms can make general purpose processors capable of encrypting packets at line rates. This implies that the Internet can be gradually transformed to an information delivery infrastructure where all traffic is encrypted and authenticated. We justify our claim by presenting technologies that accelerate end-to-end encryption and authentication by a factor of 6 and a high performance TLS 1.2 protocol implementation that takes advantage of these innovations. Our implementation is available in the public domain for experimentation.",,135–146,12,"SSL, cryptographic algorithm acceleration, HTTPS, secure communications, AES, TLS, RSA, GCM",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1189,inproceedings,"Kounavis, Michael E. and Kang, Xiaozhu and Grewal, Ken and Eszenyi, Mathew and Gueron, Shay and Durham, David",Encrypting the Internet,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851200,10.1145/1851182.1851200,"End-to-end communication encryption is considered necessary for protecting the privacy of user data in the Internet. Only a small fraction of all Internet traffic, however, is protected today. The primary reason for this neglect is economic, mainly security protocol speed and cost. In this paper we argue that recent advances in the implementation of cryptographic algorithms can make general purpose processors capable of encrypting packets at line rates. This implies that the Internet can be gradually transformed to an information delivery infrastructure where all traffic is encrypted and authenticated. We justify our claim by presenting technologies that accelerate end-to-end encryption and authentication by a factor of 6 and a high performance TLS 1.2 protocol implementation that takes advantage of these innovations. Our implementation is available in the public domain for experimentation.",Proceedings of the ACM SIGCOMM 2010 Conference,135–146,12,"cryptographic algorithm acceleration, RSA, TLS, AES, HTTPS, secure communications, SSL, GCM","New Delhi, India",SIGCOMM '10,,,,,,
1190,inproceedings,"Jamieson, Kyle",Session Details: Wireless LANs,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3248808,10.1145/3248808,,Proceedings of the ACM SIGCOMM 2010 Conference,,,,"New Delhi, India",SIGCOMM '10,,,,,,
1191,article,"Tan, Kun and Fang, Ji and Zhang, Yuanyang and Chen, Shouyuan and Shi, Lixin and Zhang, Jiansong and Zhang, Yongguang",Fine-Grained Channel Access in Wireless LAN,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851202,10.1145/1851275.1851202,"Modern communication technologies are steadily advancing the physical layer (PHY) data rate in wireless LANs, from hundreds of Mbps in current 802.11n to over Gbps in the near future. As PHY data rates increase, however, the overhead of media access control (MAC) progressively degrades data throughput efficiency. This trend reflects a fundamental aspect of the current MAC protocol, which allocates the channel as a single resource at a time.This paper argues that, in a high data rate WLAN, the channel should be divided into separate subchannels whose width is commensurate with PHY data rate and typical frame size. Multiple stations can then contend for and use subchannels simultaneously according to their traffic demands, thereby increasing overall efficiency. We introduce FICA, a fine-grained channel access method that embodies this approach to media access using two novel techniques. First, it proposes a new PHY architecture based on OFDM that retains orthogonality among subchannels while relying solely on the coordination mechanisms in existing WLAN, carrier-sensing and broadcasting. Second, FICA employs a frequency-domain contention method that uses physical layer RTS/CTS signaling and frequency domain backoff to efficiently coordinate subchannel access. We have implemented FICA, both MAC and PHY layers, using a software radio platform, and our experiments demonstrate the feasibility of the FICA design. Further, our simulation results suggest FICA can improve the efficiency ratio of WLANs by up to 400% compared to existing 802.11.",,147–158,12,"OFDMA, MAC, fine-grained channel access, cross-layer",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1192,inproceedings,"Tan, Kun and Fang, Ji and Zhang, Yuanyang and Chen, Shouyuan and Shi, Lixin and Zhang, Jiansong and Zhang, Yongguang",Fine-Grained Channel Access in Wireless LAN,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851202,10.1145/1851182.1851202,"Modern communication technologies are steadily advancing the physical layer (PHY) data rate in wireless LANs, from hundreds of Mbps in current 802.11n to over Gbps in the near future. As PHY data rates increase, however, the overhead of media access control (MAC) progressively degrades data throughput efficiency. This trend reflects a fundamental aspect of the current MAC protocol, which allocates the channel as a single resource at a time.This paper argues that, in a high data rate WLAN, the channel should be divided into separate subchannels whose width is commensurate with PHY data rate and typical frame size. Multiple stations can then contend for and use subchannels simultaneously according to their traffic demands, thereby increasing overall efficiency. We introduce FICA, a fine-grained channel access method that embodies this approach to media access using two novel techniques. First, it proposes a new PHY architecture based on OFDM that retains orthogonality among subchannels while relying solely on the coordination mechanisms in existing WLAN, carrier-sensing and broadcasting. Second, FICA employs a frequency-domain contention method that uses physical layer RTS/CTS signaling and frequency domain backoff to efficiently coordinate subchannel access. We have implemented FICA, both MAC and PHY layers, using a software radio platform, and our experiments demonstrate the feasibility of the FICA design. Further, our simulation results suggest FICA can improve the efficiency ratio of WLANs by up to 400% compared to existing 802.11.",Proceedings of the ACM SIGCOMM 2010 Conference,147–158,12,"MAC, OFDMA, fine-grained channel access, cross-layer","New Delhi, India",SIGCOMM '10,,,,,,
1193,article,"Halperin, Daniel and Hu, Wenjun and Sheth, Anmol and Wetherall, David",Predictable 802.11 Packet Delivery from Wireless Channel Measurements,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851203,10.1145/1851275.1851203,"RSSI is known to be a fickle indicator of whether a wireless link will work, for many reasons. This greatly complicates operation because it requires testing and adaptation to find the best rate, transmit power or other parameter that is tuned to boost performance. We show that, for the first time, wireless packet delivery can be accurately predicted for commodity 802.11 NICs from only the channel measurements that they provide. Our model uses 802.11n Channel State Information measurements as input to an OFDM receiver model we develop by using the concept of effective SNR. It is simple, easy to deploy, broadly useful, and accurate. It makes packet delivery predictions for 802.11a/g SISO rates and 802.11n MIMO rates, plus choices of transmit power and antennas. We report testbed experiments that show narrow transition regions (<2 dB for most links) similar to the near-ideal case of narrowband, frequency-flat channels. Unlike RSSI, this lets us predict the highest rate that will work for a link, trim transmit power, and more. We use trace-driven simulation to show that our rate prediction is as good as the best rate adaptation algorithms for 802.11a/g, even over dynamic channels, and extends this good performance to 802.11n.",,159–170,12,"rate adaptation, wireless, effective snr, mimo, fading, 802.11n, link adaptation, rssi, ofdm, power control, snr",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1194,inproceedings,"Halperin, Daniel and Hu, Wenjun and Sheth, Anmol and Wetherall, David",Predictable 802.11 Packet Delivery from Wireless Channel Measurements,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851203,10.1145/1851182.1851203,"RSSI is known to be a fickle indicator of whether a wireless link will work, for many reasons. This greatly complicates operation because it requires testing and adaptation to find the best rate, transmit power or other parameter that is tuned to boost performance. We show that, for the first time, wireless packet delivery can be accurately predicted for commodity 802.11 NICs from only the channel measurements that they provide. Our model uses 802.11n Channel State Information measurements as input to an OFDM receiver model we develop by using the concept of effective SNR. It is simple, easy to deploy, broadly useful, and accurate. It makes packet delivery predictions for 802.11a/g SISO rates and 802.11n MIMO rates, plus choices of transmit power and antennas. We report testbed experiments that show narrow transition regions (<2 dB for most links) similar to the near-ideal case of narrowband, frequency-flat channels. Unlike RSSI, this lets us predict the highest rate that will work for a link, trim transmit power, and more. We use trace-driven simulation to show that our rate prediction is as good as the best rate adaptation algorithms for 802.11a/g, even over dynamic channels, and extends this good performance to 802.11n.",Proceedings of the ACM SIGCOMM 2010 Conference,159–170,12,"effective snr, ofdm, link adaptation, snr, rssi, 802.11n, mimo, fading, power control, wireless, rate adaptation","New Delhi, India",SIGCOMM '10,,,,,,
1195,article,"Rahul, Hariharan and Hassanieh, Haitham and Katabi, Dina",SourceSync: A Distributed Wireless Architecture for Exploiting Sender Diversity,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851204,10.1145/1851275.1851204,"Diversity is an intrinsic property of wireless networks. Recent years have witnessed the emergence of many distributed protocols like ExOR, MORE, SOAR, SOFT, and MIXIT that exploit receiver diversity in 802.11-like networks. In contrast, the dual of receiver diversity, sender diversity, has remained largely elusive to such networks.This paper presents SourceSync, a distributed architecture for harnessing sender diversity. SourceSync enables concurrent senders to synchronize their transmissions to symbol boundaries, and cooperate to forward packets at higher data rates than they could have achieved by transmitting separately. The paper shows that SourceSync improves the performance of opportunistic routing protocols. Specifically, SourceSync allows all nodes that overhear a packet in a wireless mesh to simultaneously transmit it to their nexthops, in contrast to existing opportunistic routing protocols that are forced to pick a single forwarder from among the overhearing nodes. Such simultaneous transmission reduces bit errors and improves throughput. The paper also shows that SourceSync increases the throughput of 802.11 last hop diversity protocols by allowing multiple APs to transmit simultaneously to a client, thereby harnessing sender diversity. We have implemented SourceSync on the FPGA of an 802.11-like radio platform. We have also evaluated our system in an indoor wireless testbed, empirically showing its benefits.",,171–182,12,"symbol-level synchronization, cooperative diversity, wireless, sender diversity",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1196,inproceedings,"Rahul, Hariharan and Hassanieh, Haitham and Katabi, Dina",SourceSync: A Distributed Wireless Architecture for Exploiting Sender Diversity,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851204,10.1145/1851182.1851204,"Diversity is an intrinsic property of wireless networks. Recent years have witnessed the emergence of many distributed protocols like ExOR, MORE, SOAR, SOFT, and MIXIT that exploit receiver diversity in 802.11-like networks. In contrast, the dual of receiver diversity, sender diversity, has remained largely elusive to such networks.This paper presents SourceSync, a distributed architecture for harnessing sender diversity. SourceSync enables concurrent senders to synchronize their transmissions to symbol boundaries, and cooperate to forward packets at higher data rates than they could have achieved by transmitting separately. The paper shows that SourceSync improves the performance of opportunistic routing protocols. Specifically, SourceSync allows all nodes that overhear a packet in a wireless mesh to simultaneously transmit it to their nexthops, in contrast to existing opportunistic routing protocols that are forced to pick a single forwarder from among the overhearing nodes. Such simultaneous transmission reduces bit errors and improves throughput. The paper also shows that SourceSync increases the throughput of 802.11 last hop diversity protocols by allowing multiple APs to transmit simultaneously to a client, thereby harnessing sender diversity. We have implemented SourceSync on the FPGA of an 802.11-like radio platform. We have also evaluated our system in an indoor wireless testbed, empirically showing its benefits.",Proceedings of the ACM SIGCOMM 2010 Conference,171–182,12,"wireless, sender diversity, cooperative diversity, symbol-level synchronization","New Delhi, India",SIGCOMM '10,,,,,,
1197,inproceedings,"Prabhakar, Balaji",Session Details: Novel Implementations of Network Components,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3248809,10.1145/3248809,,Proceedings of the ACM SIGCOMM 2010 Conference,,,,"New Delhi, India",SIGCOMM '10,,,,,,
1198,article,"Anwer, Muhammad Bilal and Motiwala, Murtaza and Tariq, Mukarram bin and Feamster, Nick",SwitchBlade: A Platform for Rapid Deployment of Network Protocols on Programmable Hardware,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851206,10.1145/1851275.1851206,"We present SwitchBlade, a platform for rapidly deploying custom protocols on programmable hardware. SwitchBlade uses a pipeline-based design that allows individual hardware modules to be enabled or disabled on the fly, integrates software exception handling, and provides support for forwarding based on custom header fields. SwitchBlade's ease of programmability and wire-speed performance enables rapid prototyping of custom data-plane functions that can be directly deployed in a production network. SwitchBlade integrates common packet-processing functions as hardware modules, enabling different protocols to use these functions without having to resynthesize hardware. SwitchBlade's customizable forwarding engine supports both longest-prefix matching in the packet header and exact matching on a hash value. SwitchBlade's software exceptions can be invoked based on either packet or flow-based rules and updated quickly at runtime, thus making it easy to integrate more flexible forwarding function into the pipeline. SwitchBlade also allows multiple custom data planes to operate in parallel on the same physical hardware, while providing complete isolation for protocols running in parallel. We implemented SwitchBlade using NetFPGA board, but SwitchBlade can be implemented with any FPGA. To demonstrate SwitchBlade's flexibility, we use SwitchBlade to implement and evaluate a variety of custom network protocols: we present instances of IPv4, IPv6, Path Splicing, and an OpenFlow switch, all running in parallel while forwarding packets at line rate.",,183–194,12,"network virtualization, NetFPGA",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1199,inproceedings,"Anwer, Muhammad Bilal and Motiwala, Murtaza and Tariq, Mukarram bin and Feamster, Nick",SwitchBlade: A Platform for Rapid Deployment of Network Protocols on Programmable Hardware,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851206,10.1145/1851182.1851206,"We present SwitchBlade, a platform for rapidly deploying custom protocols on programmable hardware. SwitchBlade uses a pipeline-based design that allows individual hardware modules to be enabled or disabled on the fly, integrates software exception handling, and provides support for forwarding based on custom header fields. SwitchBlade's ease of programmability and wire-speed performance enables rapid prototyping of custom data-plane functions that can be directly deployed in a production network. SwitchBlade integrates common packet-processing functions as hardware modules, enabling different protocols to use these functions without having to resynthesize hardware. SwitchBlade's customizable forwarding engine supports both longest-prefix matching in the packet header and exact matching on a hash value. SwitchBlade's software exceptions can be invoked based on either packet or flow-based rules and updated quickly at runtime, thus making it easy to integrate more flexible forwarding function into the pipeline. SwitchBlade also allows multiple custom data planes to operate in parallel on the same physical hardware, while providing complete isolation for protocols running in parallel. We implemented SwitchBlade using NetFPGA board, but SwitchBlade can be implemented with any FPGA. To demonstrate SwitchBlade's flexibility, we use SwitchBlade to implement and evaluate a variety of custom network protocols: we present instances of IPv4, IPv6, Path Splicing, and an OpenFlow switch, all running in parallel while forwarding packets at line rate.",Proceedings of the ACM SIGCOMM 2010 Conference,183–194,12,"network virtualization, NetFPGA","New Delhi, India",SIGCOMM '10,,,,,,
1200,article,"Han, Sangjin and Jang, Keon and Park, KyoungSoo and Moon, Sue",PacketShader: A GPU-Accelerated Software Router,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851207,10.1145/1851275.1851207,"We present PacketShader, a high-performance software router framework for general packet processing with Graphics Processing Unit (GPU) acceleration. PacketShader exploits the massively-parallel processing power of GPU to address the CPU bottleneck in current software routers. Combined with our high-performance packet I/O engine, PacketShader outperforms existing software routers by more than a factor of four, forwarding 64B IPv4 packets at 39 Gbps on a single commodity PC. We have implemented IPv4 and IPv6 forwarding, OpenFlow switching, and IPsec tunneling to demonstrate the flexibility and performance advantage of PacketShader. The evaluation results show that GPU brings significantly higher throughput over the CPU-only implementation, confirming the effectiveness of GPU for computation and memory-intensive operations in packet processing.",,195–206,12,"GPU, software router, CUDA",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1201,inproceedings,"Han, Sangjin and Jang, Keon and Park, KyoungSoo and Moon, Sue",PacketShader: A GPU-Accelerated Software Router,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851207,10.1145/1851182.1851207,"We present PacketShader, a high-performance software router framework for general packet processing with Graphics Processing Unit (GPU) acceleration. PacketShader exploits the massively-parallel processing power of GPU to address the CPU bottleneck in current software routers. Combined with our high-performance packet I/O engine, PacketShader outperforms existing software routers by more than a factor of four, forwarding 64B IPv4 packets at 39 Gbps on a single commodity PC. We have implemented IPv4 and IPv6 forwarding, OpenFlow switching, and IPsec tunneling to demonstrate the flexibility and performance advantage of PacketShader. The evaluation results show that GPU brings significantly higher throughput over the CPU-only implementation, confirming the effectiveness of GPU for computation and memory-intensive operations in packet processing.",Proceedings of the ACM SIGCOMM 2010 Conference,195–206,12,"GPU, CUDA, software router","New Delhi, India",SIGCOMM '10,,,,,,
1202,article,"Vamanan, Balajee and Voskuilen, Gwendolyn and Vijaykumar, T. N.",EffiCuts: Optimizing Packet Classification for Memory and Throughput,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851208,10.1145/1851275.1851208,"Packet Classification is a key functionality provided by modern routers. Previous decision-tree algorithms, HiCuts and HyperCuts, cut the multi-dimensional rule space to separate a classifier's rules. Despite their optimizations, the algorithms incur considerable memory overhead due to two issues: (1) Many rules in a classifier overlap and the overlapping rules vary vastly in size, causing the algorithms' fine cuts for separating the small rules to replicate the large rules. (2) Because a classifier's rule-space density varies significantly, the algorithms' equi-sized cuts for separating the dense parts needlessly partition the sparse parts, resulting in many ineffectual nodes that hold only a few rules. We propose EffiCuts which employs four novel ideas: (1) Separable trees: To eliminate overlap among small and large rules, we separate all small and large rules. We define a subset of rules to be separable if all the rules are either small or large in each dimension. We build a distinct tree for each such subset where each dimension can be cut coarsely to separate the large rules, or finely to separate the small rules without incurring replication. (2) Selective tree merging: To reduce the multiple trees' extra accesses which degrade throughput, we selectively merge separable trees mixing rules that may be small or large in at most one dimension. (3) Equi-dense cuts: We employ unequal cuts which distribute a node's rules evenly among the children, avoiding ineffectual nodes at the cost of a small processing overhead in the tree traversal. (4) Node Co-location: To achieve fewer accesses per node than HiCuts and HyperCuts, we co-locate parts of a node and its children. Using ClassBench, we show that for similar throughput EffiCuts needs factors of 57 less memory than HyperCuts and of 4-8 less power than TCAM.",,207–218,12,"decision-tree algorithm, rule replication, packet classification",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1203,inproceedings,"Vamanan, Balajee and Voskuilen, Gwendolyn and Vijaykumar, T. N.",EffiCuts: Optimizing Packet Classification for Memory and Throughput,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851208,10.1145/1851182.1851208,"Packet Classification is a key functionality provided by modern routers. Previous decision-tree algorithms, HiCuts and HyperCuts, cut the multi-dimensional rule space to separate a classifier's rules. Despite their optimizations, the algorithms incur considerable memory overhead due to two issues: (1) Many rules in a classifier overlap and the overlapping rules vary vastly in size, causing the algorithms' fine cuts for separating the small rules to replicate the large rules. (2) Because a classifier's rule-space density varies significantly, the algorithms' equi-sized cuts for separating the dense parts needlessly partition the sparse parts, resulting in many ineffectual nodes that hold only a few rules. We propose EffiCuts which employs four novel ideas: (1) Separable trees: To eliminate overlap among small and large rules, we separate all small and large rules. We define a subset of rules to be separable if all the rules are either small or large in each dimension. We build a distinct tree for each such subset where each dimension can be cut coarsely to separate the large rules, or finely to separate the small rules without incurring replication. (2) Selective tree merging: To reduce the multiple trees' extra accesses which degrade throughput, we selectively merge separable trees mixing rules that may be small or large in at most one dimension. (3) Equi-dense cuts: We employ unequal cuts which distribute a node's rules evenly among the children, avoiding ineffectual nodes at the cost of a small processing overhead in the tree traversal. (4) Node Co-location: To achieve fewer accesses per node than HiCuts and HyperCuts, we co-locate parts of a node and its children. Using ClassBench, we show that for similar throughput EffiCuts needs factors of 57 less memory than HyperCuts and of 4-8 less power than TCAM.",Proceedings of the ACM SIGCOMM 2010 Conference,207–218,12,"rule replication, decision-tree algorithm, packet classification","New Delhi, India",SIGCOMM '10,,,,,,
1204,inproceedings,"Ramjee, Ram",Session Details: Cloud and Routing,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3248810,10.1145/3248810,,Proceedings of the ACM SIGCOMM 2010 Conference,,,,"New Delhi, India",SIGCOMM '10,,,,,,
1205,article,"Le, Franck and Xie, Geoffrey G. and Zhang, Hui",Theory and New Primitives for Safely Connecting Routing Protocol Instances,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851210,10.1145/1851275.1851210,"Recent studies have shown that the current primitives for connecting multiple routing protocol instances (OSPF 1, OSPF 2, EIGRP 10, etc.) are pervasively deployed in enterprise networks and the Internet. Furthermore, these primitives are extremely vulnerable to routing anomalies (route oscillations, forwarding loops, etc.) and at the same time too rigid to support some of today's operational objectives. In this paper, we propose a new theory to reason about routing properties across multiple routing instances. The theory directly applies to both link-state and vector routing protocols. Each routing protocol still makes independent routing decisions and may consider a combination of routing metrics, including bandwidth, delay, cost, and reliability. While the theory permits a range of solutions, we focus on a design that requires no changes to existing routing protocols. Guided by the theory, we derive a new set of connecting primitives, which are not only provably safe but also more expressive than the current version. We have implemented and validated the new primitives using XORP. The results confirm that our design can support a large range of desirable operational goals, including those not achievable today, safely and with little manual configuration.",,219–230,12,"connecting primitives, route selection, route redistribution",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1206,inproceedings,"Le, Franck and Xie, Geoffrey G. and Zhang, Hui",Theory and New Primitives for Safely Connecting Routing Protocol Instances,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851210,10.1145/1851182.1851210,"Recent studies have shown that the current primitives for connecting multiple routing protocol instances (OSPF 1, OSPF 2, EIGRP 10, etc.) are pervasively deployed in enterprise networks and the Internet. Furthermore, these primitives are extremely vulnerable to routing anomalies (route oscillations, forwarding loops, etc.) and at the same time too rigid to support some of today's operational objectives. In this paper, we propose a new theory to reason about routing properties across multiple routing instances. The theory directly applies to both link-state and vector routing protocols. Each routing protocol still makes independent routing decisions and may consider a combination of routing metrics, including bandwidth, delay, cost, and reliability. While the theory permits a range of solutions, we focus on a design that requires no changes to existing routing protocols. Guided by the theory, we derive a new set of connecting primitives, which are not only provably safe but also more expressive than the current version. We have implemented and validated the new primitives using XORP. The results confirm that our design can support a large range of desirable operational goals, including those not achievable today, safely and with little manual configuration.",Proceedings of the ACM SIGCOMM 2010 Conference,219–230,12,"connecting primitives, route selection, route redistribution","New Delhi, India",SIGCOMM '10,,,,,,
1207,article,"Wendell, Patrick and Jiang, Joe Wenjie and Freedman, Michael J. and Rexford, Jennifer",DONAR: Decentralized Server Selection for Cloud Services,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851211,10.1145/1851275.1851211,"Geo-replicated services need an effective way to direct client requests to a particular location, based on performance, load, and cost. This paper presents DONAR, a distributed system that can offload the burden of replica selection, while providing these services with a sufficiently expressive interface for specifying mapping policies. Most existing approaches for replica selection rely on either central coordination (which has reliability, security, and scalability limitations) or distributed heuristics (which lead to suboptimal request distributions, or even instability). In contrast, the distributed mapping nodes in DONAR run a simple, efficient algorithm to coordinate their replica-selection decisions for clients. The protocol solves an optimization problem that jointly considers both client performance and server load, allowing us to show that the distributed algorithm is stable and effective. Experiments with our DONAR prototype--providing replica selection for CoralCDN and the Measurement Lab--demonstrate that our algorithm performs well ""in the wild."" Our prototype supports DNS- and HTTP-based redirection, IP anycast, and a secure update protocol, and can handle many customer services with diverse policy objectives.",,231–242,12,"distributed optimization, load balancing, replica selection, geo-locality, DNS",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1208,inproceedings,"Wendell, Patrick and Jiang, Joe Wenjie and Freedman, Michael J. and Rexford, Jennifer",DONAR: Decentralized Server Selection for Cloud Services,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851211,10.1145/1851182.1851211,"Geo-replicated services need an effective way to direct client requests to a particular location, based on performance, load, and cost. This paper presents DONAR, a distributed system that can offload the burden of replica selection, while providing these services with a sufficiently expressive interface for specifying mapping policies. Most existing approaches for replica selection rely on either central coordination (which has reliability, security, and scalability limitations) or distributed heuristics (which lead to suboptimal request distributions, or even instability). In contrast, the distributed mapping nodes in DONAR run a simple, efficient algorithm to coordinate their replica-selection decisions for clients. The protocol solves an optimization problem that jointly considers both client performance and server load, allowing us to show that the distributed algorithm is stable and effective. Experiments with our DONAR prototype--providing replica selection for CoralCDN and the Measurement Lab--demonstrate that our algorithm performs well ""in the wild."" Our prototype supports DNS- and HTTP-based redirection, IP anycast, and a secure update protocol, and can handle many customer services with diverse policy objectives.",Proceedings of the ACM SIGCOMM 2010 Conference,231–242,12,"load balancing, distributed optimization, DNS, replica selection, geo-locality","New Delhi, India",SIGCOMM '10,,,,,,
1209,article,"Hajjat, Mohammad and Sun, Xin and Sung, Yu-Wei Eric and Maltz, David and Rao, Sanjay and Sripanidkulchai, Kunwadee and Tawarmalani, Mohit",Cloudward Bound: Planning for Beneficial Migration of Enterprise Applications to the Cloud,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851212,10.1145/1851275.1851212,"In this paper, we tackle challenges in migrating enterprise services into hybrid cloud-based deployments, where enterprise operations are partly hosted on-premise and partly in the cloud. Such hybrid architectures enable enterprises to benefit from cloud-based architectures, while honoring application performance requirements, and privacy restrictions on what services may be migrated to the cloud. We make several contributions. First, we highlight the complexity inherent in enterprise applications today in terms of their multi-tiered nature, large number of application components, and interdependencies. Second, we have developed a model to explore the benefits of a hybrid migration approach. Our model takes into account enterprise-specific constraints, cost savings, and increased transaction delays and wide-area communication costs that may result from the migration. Evaluations based on real enterprise applications and Azure-based cloud deployments show the benefits of a hybrid migration approach, and the importance of planning which components to migrate. Third, we shed insight on security policies associated with enterprise applications in data centers. We articulate the importance of ensuring assurable reconfiguration of security policies as enterprise applications are migrated to the cloud. We present algorithms to achieve this goal, and demonstrate their efficacy on realistic migration scenarios.",,243–254,12,"security policies, enterprise applications, cloud computing, performance modeling, network configurations",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1210,inproceedings,"Hajjat, Mohammad and Sun, Xin and Sung, Yu-Wei Eric and Maltz, David and Rao, Sanjay and Sripanidkulchai, Kunwadee and Tawarmalani, Mohit",Cloudward Bound: Planning for Beneficial Migration of Enterprise Applications to the Cloud,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851212,10.1145/1851182.1851212,"In this paper, we tackle challenges in migrating enterprise services into hybrid cloud-based deployments, where enterprise operations are partly hosted on-premise and partly in the cloud. Such hybrid architectures enable enterprises to benefit from cloud-based architectures, while honoring application performance requirements, and privacy restrictions on what services may be migrated to the cloud. We make several contributions. First, we highlight the complexity inherent in enterprise applications today in terms of their multi-tiered nature, large number of application components, and interdependencies. Second, we have developed a model to explore the benefits of a hybrid migration approach. Our model takes into account enterprise-specific constraints, cost savings, and increased transaction delays and wide-area communication costs that may result from the migration. Evaluations based on real enterprise applications and Azure-based cloud deployments show the benefits of a hybrid migration approach, and the importance of planning which components to migrate. Third, we shed insight on security policies associated with enterprise applications in data centers. We articulate the importance of ensuring assurable reconfiguration of security policies as enterprise applications are migrated to the cloud. We present algorithms to achieve this goal, and demonstrate their efficacy on realistic migration scenarios.",Proceedings of the ACM SIGCOMM 2010 Conference,243–254,12,"network configurations, performance modeling, cloud computing, security policies, enterprise applications","New Delhi, India",SIGCOMM '10,,,,,,
1211,inproceedings,"Zhang, Yin",Session Details: Network IDS,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3248811,10.1145/3248811,,Proceedings of the ACM SIGCOMM 2010 Conference,,,,"New Delhi, India",SIGCOMM '10,,,,,,
1212,article,"Liu, Xin and Yang, Xiaowei and Xia, Yong",NetFence: Preventing Internet Denial of Service from inside Out,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851214,10.1145/1851275.1851214,"Denial of Service (DoS) attacks frequently happen on the Internet, paralyzing Internet services and causing millions of dollars of financial loss. This work presents NetFence, a scalable DoS-resistant network architecture. NetFence uses a novel mechanism, secure congestion policing feedback, to enable robust congestion policing inside the network. Bottleneck routers update the feedback in packet headers to signal congestion, and access routers use it to police senders' traffic. Targeted DoS victims can use the secure congestion policing feedback as capability tokens to suppress unwanted traffic. When compromised senders and receivers organize into pairs to congest a network link, NetFence provably guarantees a legitimate sender its fair share of network resources without keeping per-host state at the congested link. We use a Linux implementation, ns-2 simulations, and theoretical analysis to show that NetFence is an effective and scalable DoS solution: it reduces the amount of state maintained by a congested router from per-host to at most per-(Autonomous System).",,255–266,12,"congestion policing, internet, denial-of-service, capability",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1213,inproceedings,"Liu, Xin and Yang, Xiaowei and Xia, Yong",NetFence: Preventing Internet Denial of Service from inside Out,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851214,10.1145/1851182.1851214,"Denial of Service (DoS) attacks frequently happen on the Internet, paralyzing Internet services and causing millions of dollars of financial loss. This work presents NetFence, a scalable DoS-resistant network architecture. NetFence uses a novel mechanism, secure congestion policing feedback, to enable robust congestion policing inside the network. Bottleneck routers update the feedback in packet headers to signal congestion, and access routers use it to police senders' traffic. Targeted DoS victims can use the secure congestion policing feedback as capability tokens to suppress unwanted traffic. When compromised senders and receivers organize into pairs to congest a network link, NetFence provably guarantees a legitimate sender its fair share of network resources without keeping per-host state at the congested link. We use a Linux implementation, ns-2 simulations, and theoretical analysis to show that NetFence is an effective and scalable DoS solution: it reduces the amount of state maintained by a congested router from per-host to at most per-(Autonomous System).",Proceedings of the ACM SIGCOMM 2010 Conference,255–266,12,"capability, congestion policing, internet, denial-of-service","New Delhi, India",SIGCOMM '10,,,,,,
1214,article,"Silveira, Fernando and Diot, Christophe and Taft, Nina and Govindan, Ramesh",ASTUTE: Detecting a Different Class of Traffic Anomalies,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851215,10.1145/1851275.1851215,"When many flows are multiplexed on a non-saturated link, their volume changes over short timescales tend to cancel each other out, making the average change across flows close to zero. This equilibrium property holds if the flows are nearly independent, and it is violated by traffic changes caused by several, potentially small, correlated flows. Many traffic anomalies (both malicious and benign) fit this description. Based on this observation, we exploit equilibrium to design a computationally simple detection method for correlated anomalous flows. We compare our new method to two well known techniques on three network links. We manually classify the anomalies detected by the three methods, and discover that our method uncovers a different class of anomalies than previous techniques do.",,267–278,12,"statistical test, anomaly detection",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1215,inproceedings,"Silveira, Fernando and Diot, Christophe and Taft, Nina and Govindan, Ramesh",ASTUTE: Detecting a Different Class of Traffic Anomalies,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851215,10.1145/1851182.1851215,"When many flows are multiplexed on a non-saturated link, their volume changes over short timescales tend to cancel each other out, making the average change across flows close to zero. This equilibrium property holds if the flows are nearly independent, and it is violated by traffic changes caused by several, potentially small, correlated flows. Many traffic anomalies (both malicious and benign) fit this description. Based on this observation, we exploit equilibrium to design a computationally simple detection method for correlated anomalous flows. We compare our new method to two well known techniques on three network links. We manually classify the anomalies detected by the three methods, and discover that our method uncovers a different class of anomalies than previous techniques do.",Proceedings of the ACM SIGCOMM 2010 Conference,267–278,12,"statistical test, anomaly detection","New Delhi, India",SIGCOMM '10,,,,,,
1216,article,"Li, Zhichun and Xia, Gao and Gao, Hongyu and Tang, Yi and Chen, Yan and Liu, Bin and Jiang, Junchen and Lv, Yuezhou",NetShield: Massive Semantics-Based Vulnerability Signature Matching for High-Speed Networks,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851216,10.1145/1851275.1851216,"Accuracy and speed are the two most important metrics for Network Intrusion Detection/Prevention Systems (NIDS/NIPSes). Due to emerging polymorphic attacks and the fact that in many cases regular expressions (regexes) cannot capture the vulnerability conditions accurately, the accuracy of existing regex-based NIDS/NIPS systems has become a serious problem. In contrast, the recently-proposed vulnerability signatures (a.k.a data patches) can exactly describe the vulnerability conditions and achieve better accuracy. However, how to efficiently apply vulnerability signatures to high speed NIDS/NIPS with a large ruleset remains an untouched but challenging issue.This paper presents the first systematic design of vulnerability signature based parsing and matching engine, NetShield, which achieves multi-gigabit throughput while offering much better accuracy. Particularly, we made the following contributions: (i) we proposed a candidate selection algorithm which efficiently matches thousands of vulnerability signatures simultaneously requiring a small amount of memory; (ii) we proposed an automatic lightweight parsing state machine achieving fast protocol parsing. Experimental results show that the core engine of NetShield achieves at least 1.9+Gbps signature matching throughput on a 3.8GHz single-core PC, and can scale-up to at least 11+Gbps under a 8-core machine for 794 HTTP vulnerability signatures.",,279–290,12,"deep packet inspection, vulnerability signature, signature matching, intrusion detection",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1217,inproceedings,"Li, Zhichun and Xia, Gao and Gao, Hongyu and Tang, Yi and Chen, Yan and Liu, Bin and Jiang, Junchen and Lv, Yuezhou",NetShield: Massive Semantics-Based Vulnerability Signature Matching for High-Speed Networks,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851216,10.1145/1851182.1851216,"Accuracy and speed are the two most important metrics for Network Intrusion Detection/Prevention Systems (NIDS/NIPSes). Due to emerging polymorphic attacks and the fact that in many cases regular expressions (regexes) cannot capture the vulnerability conditions accurately, the accuracy of existing regex-based NIDS/NIPS systems has become a serious problem. In contrast, the recently-proposed vulnerability signatures (a.k.a data patches) can exactly describe the vulnerability conditions and achieve better accuracy. However, how to efficiently apply vulnerability signatures to high speed NIDS/NIPS with a large ruleset remains an untouched but challenging issue.This paper presents the first systematic design of vulnerability signature based parsing and matching engine, NetShield, which achieves multi-gigabit throughput while offering much better accuracy. Particularly, we made the following contributions: (i) we proposed a candidate selection algorithm which efficiently matches thousands of vulnerability signatures simultaneously requiring a small amount of memory; (ii) we proposed an automatic lightweight parsing state machine achieving fast protocol parsing. Experimental results show that the core engine of NetShield achieves at least 1.9+Gbps signature matching throughput on a 3.8GHz single-core PC, and can scale-up to at least 11+Gbps under a 8-core machine for 794 HTTP vulnerability signatures.",Proceedings of the ACM SIGCOMM 2010 Conference,279–290,12,"intrusion detection, deep packet inspection, signature matching, vulnerability signature","New Delhi, India",SIGCOMM '10,,,,,,
1218,inproceedings,"Teixeira, Renata",Session Details: Network Architecture and Operations,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3248812,10.1145/3248812,,Proceedings of the ACM SIGCOMM 2010 Conference,,,,"New Delhi, India",SIGCOMM '10,,,,,,
1219,article,"Wang, Ye and Wang, Hao and Mahimkar, Ajay and Alimi, Richard and Zhang, Yin and Qiu, Lili and Yang, Yang Richard",R3: Resilient Routing Reconfiguration,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851218,10.1145/1851275.1851218,"Network resiliency is crucial to IP network operations. Existing techniques to recover from one or a series of failures do not offer performance predictability and may cause serious congestion. In this paper, we propose Resilient Routing Reconfiguration (R3), a novel routing protection scheme that is (i) provably congestion-free under a large number of failure scenarios; (ii) efficient by having low router processing overhead and memory requirements; (iii) flexible in accommodating different performance requirements (e.g., handling realistic failure scenarios, prioritized traffic, and the trade-off between performance and resilience); and (iv) robust to both topology failures and traffic variations. We implement R3 on Linux using a simple extension of MPLS, called MPLS-ff. We then conduct extensive Emulab experiments and simulations using realistic network topologies and traffic demands. Our results show that R3 achieves near-optimal performance and is at least 50% better than the existing schemes under a wide range of failure scenarios.",,291–302,12,"network resiliency, routing, routing protection",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1220,inproceedings,"Wang, Ye and Wang, Hao and Mahimkar, Ajay and Alimi, Richard and Zhang, Yin and Qiu, Lili and Yang, Yang Richard",R3: Resilient Routing Reconfiguration,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851218,10.1145/1851182.1851218,"Network resiliency is crucial to IP network operations. Existing techniques to recover from one or a series of failures do not offer performance predictability and may cause serious congestion. In this paper, we propose Resilient Routing Reconfiguration (R3), a novel routing protection scheme that is (i) provably congestion-free under a large number of failure scenarios; (ii) efficient by having low router processing overhead and memory requirements; (iii) flexible in accommodating different performance requirements (e.g., handling realistic failure scenarios, prioritized traffic, and the trade-off between performance and resilience); and (iv) robust to both topology failures and traffic variations. We implement R3 on Linux using a simple extension of MPLS, called MPLS-ff. We then conduct extensive Emulab experiments and simulations using realistic network topologies and traffic demands. Our results show that R3 achieves near-optimal performance and is at least 50% better than the existing schemes under a wide range of failure scenarios.",Proceedings of the ACM SIGCOMM 2010 Conference,291–302,12,"network resiliency, routing protection, routing","New Delhi, India",SIGCOMM '10,,,,,,
1221,article,"Mahimkar, Ajay Anil and Song, Han Hee and Ge, Zihui and Shaikh, Aman and Wang, Jia and Yates, Jennifer and Zhang, Yin and Emmons, Joanne",Detecting the Performance Impact of Upgrades in Large Operational Networks,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851219,10.1145/1851275.1851219,"Networks continue to change to support new applications, improve reliability and performance and reduce the operational cost. The changes are made to the network in the form of upgrades such as software or hardware upgrades, new network or service features and network configuration changes. It is crucial to monitor the network when upgrades are made because they can have a significant impact on network performance and if not monitored may lead to unexpected consequences in operational networks. This can be achieved manually for a small number of devices, but does not scale to large networks with hundreds or thousands of routers and extremely large number of different upgrades made on a regular basis.In this paper, we design and implement a novel infrastructure MERCURY for detecting the impact of network upgrades (or triggers) on performance. MERCURY extracts interesting triggers from a large number of network maintenance activities. It then identifies behavior changes in network performance caused by the triggers. It uses statistical rule mining and network configuration to identify commonality across the behavior changes. We systematically evaluate MERCURY using data collected at a large tier-1 ISP network. By comparing to operational practice, we show that MERCURY is able to capture the interesting triggers and behavior changes induced by the triggers. In some cases, MERCURY also discovers previously unknown network behaviors demonstrating the effectiveness in identifying network conditions flying under the radar.",,303–314,12,"change detection, performance impact, network upgrades, statistical data mining",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1222,inproceedings,"Mahimkar, Ajay Anil and Song, Han Hee and Ge, Zihui and Shaikh, Aman and Wang, Jia and Yates, Jennifer and Zhang, Yin and Emmons, Joanne",Detecting the Performance Impact of Upgrades in Large Operational Networks,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851219,10.1145/1851182.1851219,"Networks continue to change to support new applications, improve reliability and performance and reduce the operational cost. The changes are made to the network in the form of upgrades such as software or hardware upgrades, new network or service features and network configuration changes. It is crucial to monitor the network when upgrades are made because they can have a significant impact on network performance and if not monitored may lead to unexpected consequences in operational networks. This can be achieved manually for a small number of devices, but does not scale to large networks with hundreds or thousands of routers and extremely large number of different upgrades made on a regular basis.In this paper, we design and implement a novel infrastructure MERCURY for detecting the impact of network upgrades (or triggers) on performance. MERCURY extracts interesting triggers from a large number of network maintenance activities. It then identifies behavior changes in network performance caused by the triggers. It uses statistical rule mining and network configuration to identify commonality across the behavior changes. We systematically evaluate MERCURY using data collected at a large tier-1 ISP network. By comparing to operational practice, we show that MERCURY is able to capture the interesting triggers and behavior changes induced by the triggers. In some cases, MERCURY also discovers previously unknown network behaviors demonstrating the effectiveness in identifying network conditions flying under the radar.",Proceedings of the ACM SIGCOMM 2010 Conference,303–314,12,"statistical data mining, change detection, network upgrades, performance impact","New Delhi, India",SIGCOMM '10,,,,,,
1223,article,"Turner, Daniel and Levchenko, Kirill and Snoeren, Alex C. and Savage, Stefan",California Fault Lines: Understanding the Causes and Impact of Network Failures,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851220,10.1145/1851275.1851220,"Of the major factors affecting end-to-end service availability, network component failure is perhaps the least well understood. How often do failures occur, how long do they last, what are their causes, and how do they impact customers? Traditionally, answering questions such as these has required dedicated (and often expensive) instrumentation broadly deployed across a network.We propose an alternative approach: opportunistically mining ""low-quality"" data sources that are already available in modern network environments. We describe a methodology for recreating a succinct history of failure events in an IP network using a combination of structured data (router configurations and syslogs) and semi-structured data (email logs). Using this technique we analyze over five years of failure events in a large regional network consisting of over 200 routers; to our knowledge, this is the largest study of its kind.",,315–326,12,failure,,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1224,inproceedings,"Turner, Daniel and Levchenko, Kirill and Snoeren, Alex C. and Savage, Stefan",California Fault Lines: Understanding the Causes and Impact of Network Failures,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851220,10.1145/1851182.1851220,"Of the major factors affecting end-to-end service availability, network component failure is perhaps the least well understood. How often do failures occur, how long do they last, what are their causes, and how do they impact customers? Traditionally, answering questions such as these has required dedicated (and often expensive) instrumentation broadly deployed across a network.We propose an alternative approach: opportunistically mining ""low-quality"" data sources that are already available in modern network environments. We describe a methodology for recreating a succinct history of failure events in an IP network using a combination of structured data (router configurations and syslogs) and semi-structured data (email logs). Using this technique we analyze over five years of failure events in a large regional network consisting of over 200 routers; to our knowledge, this is the largest study of its kind.",Proceedings of the ACM SIGCOMM 2010 Conference,315–326,12,failure,"New Delhi, India",SIGCOMM '10,,,,,,
1225,inproceedings,"Guo, Chuanxiong",Session Details: Novel Technologies for Data Center Networks,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3248813,10.1145/3248813,,Proceedings of the ACM SIGCOMM 2010 Conference,,,,"New Delhi, India",SIGCOMM '10,,,,,,
1226,article,"Wang, Guohui and Andersen, David G. and Kaminsky, Michael and Papagiannaki, Konstantina and Ng, T.S. Eugene and Kozuch, Michael and Ryan, Michael",C-Through: Part-Time Optics in Data Centers,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851222,10.1145/1851275.1851222,"Data-intensive applications that operate on large volumes of data have motivated a fresh look at the design of data center networks. The first wave of proposals focused on designing pure packet-switched networks that provide full bisection bandwidth. However, these proposals significantly increase network complexity in terms of the number of links and switches required and the restricted rules to wire them up. On the other hand, optical circuit switching technology holds a very large bandwidth advantage over packet switching technology. This fact motivates us to explore how optical circuit switching technology could benefit a data center network. In particular, we propose a hybrid packet and circuit switched data center network architecture (or HyPaC for short) which augments the traditional hierarchy of packet switches with a high speed, low complexity, rack-to-rack optical circuit-switched network to supply high bandwidth to applications. We discuss the fundamental requirements of this hybrid architecture and their design options. To demonstrate the potential benefits of the hybrid architecture, we have built a prototype system called c-Through. c-Through represents a design point where the responsibility for traffic demand estimation and traffic demultiplexing resides in end hosts, making it compatible with existing packet switches. Our emulation experiments show that the hybrid architecture can provide large benefits to unmodified popular data center applications at a modest scale. Furthermore, our experimental experience provides useful insights on the applicability of the hybrid architecture across a range of deployment scenarios.",,327–338,12,"hybrid network, data center networking, optical circuit switching",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1227,inproceedings,"Wang, Guohui and Andersen, David G. and Kaminsky, Michael and Papagiannaki, Konstantina and Ng, T.S. Eugene and Kozuch, Michael and Ryan, Michael",C-Through: Part-Time Optics in Data Centers,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851222,10.1145/1851182.1851222,"Data-intensive applications that operate on large volumes of data have motivated a fresh look at the design of data center networks. The first wave of proposals focused on designing pure packet-switched networks that provide full bisection bandwidth. However, these proposals significantly increase network complexity in terms of the number of links and switches required and the restricted rules to wire them up. On the other hand, optical circuit switching technology holds a very large bandwidth advantage over packet switching technology. This fact motivates us to explore how optical circuit switching technology could benefit a data center network. In particular, we propose a hybrid packet and circuit switched data center network architecture (or HyPaC for short) which augments the traditional hierarchy of packet switches with a high speed, low complexity, rack-to-rack optical circuit-switched network to supply high bandwidth to applications. We discuss the fundamental requirements of this hybrid architecture and their design options. To demonstrate the potential benefits of the hybrid architecture, we have built a prototype system called c-Through. c-Through represents a design point where the responsibility for traffic demand estimation and traffic demultiplexing resides in end hosts, making it compatible with existing packet switches. Our emulation experiments show that the hybrid architecture can provide large benefits to unmodified popular data center applications at a modest scale. Furthermore, our experimental experience provides useful insights on the applicability of the hybrid architecture across a range of deployment scenarios.",Proceedings of the ACM SIGCOMM 2010 Conference,327–338,12,"hybrid network, data center networking, optical circuit switching","New Delhi, India",SIGCOMM '10,,,,,,
1228,article,"Farrington, Nathan and Porter, George and Radhakrishnan, Sivasankar and Bazzaz, Hamid Hajabdolali and Subramanya, Vikram and Fainman, Yeshaiahu and Papen, George and Vahdat, Amin",Helios: A Hybrid Electrical/Optical Switch Architecture for Modular Data Centers,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851223,10.1145/1851275.1851223,"The basic building block of ever larger data centers has shifted from a rack to a modular container with hundreds or even thousands of servers. Delivering scalable bandwidth among such containers is a challenge. A number of recent efforts promise full bisection bandwidth between all servers, though with significant cost, complexity, and power consumption. We present Helios, a hybrid electrical/optical switch architecture that can deliver significant reductions in the number of switching elements, cabling, cost, and power consumption relative to recently proposed data center network architectures. We explore architectural trade offs and challenges associated with realizing these benefits through the evaluation of a fully functional Helios prototype.",,339–350,12,"data center networks, optical networks",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1229,inproceedings,"Farrington, Nathan and Porter, George and Radhakrishnan, Sivasankar and Bazzaz, Hamid Hajabdolali and Subramanya, Vikram and Fainman, Yeshaiahu and Papen, George and Vahdat, Amin",Helios: A Hybrid Electrical/Optical Switch Architecture for Modular Data Centers,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851223,10.1145/1851182.1851223,"The basic building block of ever larger data centers has shifted from a rack to a modular container with hundreds or even thousands of servers. Delivering scalable bandwidth among such containers is a challenge. A number of recent efforts promise full bisection bandwidth between all servers, though with significant cost, complexity, and power consumption. We present Helios, a hybrid electrical/optical switch architecture that can deliver significant reductions in the number of switching elements, cabling, cost, and power consumption relative to recently proposed data center network architectures. We explore architectural trade offs and challenges associated with realizing these benefits through the evaluation of a fully functional Helios prototype.",Proceedings of the ACM SIGCOMM 2010 Conference,339–350,12,"optical networks, data center networks","New Delhi, India",SIGCOMM '10,,,,,,
1230,article,"Yu, Minlan and Rexford, Jennifer and Freedman, Michael J. and Wang, Jia",Scalable Flow-Based Networking with DIFANE,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851224,10.1145/1851275.1851224,"Ideally, enterprise administrators could specify fine-grain policies that drive how the underlying switches forward, drop, and measure traffic. However, existing techniques for flow-based networking rely too heavily on centralized controller software that installs rules reactively, based on the first packet of each flow. In this paper, we propose DIFANE, a scalable and efficient solution that keeps all traffic in the data plane by selectively directing packets through intermediate switches that store the necessary rules. DIFANE relegates the controller to the simpler task of partitioning these rules over the switches. DIFANE can be readily implemented with commodity switch hardware, since all data-plane functions can be expressed in terms of wildcard rules that perform simple actions on matching packets. Experiments with our prototype on Click-based OpenFlow switches show that DIFANE scales to larger networks with richer policies.",,351–362,12,"openflow, access control, scalability, network architecture",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1231,inproceedings,"Yu, Minlan and Rexford, Jennifer and Freedman, Michael J. and Wang, Jia",Scalable Flow-Based Networking with DIFANE,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851224,10.1145/1851182.1851224,"Ideally, enterprise administrators could specify fine-grain policies that drive how the underlying switches forward, drop, and measure traffic. However, existing techniques for flow-based networking rely too heavily on centralized controller software that installs rules reactively, based on the first packet of each flow. In this paper, we propose DIFANE, a scalable and efficient solution that keeps all traffic in the data plane by selectively directing packets through intermediate switches that store the necessary rules. DIFANE relegates the controller to the simpler task of partitioning these rules over the switches. DIFANE can be readily implemented with commodity switch hardware, since all data-plane functions can be expressed in terms of wildcard rules that perform simple actions on matching packets. Experiments with our prototype on Click-based OpenFlow switches show that DIFANE scales to larger networks with richer policies.",Proceedings of the ACM SIGCOMM 2010 Conference,351–362,12,"access control, scalability, openflow, network architecture","New Delhi, India",SIGCOMM '10,,,,,,
1232,inproceedings,"Moon, Sue",Session Details: Social Networks,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3248814,10.1145/3248814,,Proceedings of the ACM SIGCOMM 2010 Conference,,,,"New Delhi, India",SIGCOMM '10,,,,,,
1233,article,"Viswanath, Bimal and Post, Ansley and Gummadi, Krishna P. and Mislove, Alan",An Analysis of Social Network-Based Sybil Defenses,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851226,10.1145/1851275.1851226,"Recently, there has been much excitement in the research community over using social networks to mitigate multiple identity, or Sybil, attacks. A number of schemes have been proposed, but they differ greatly in the algorithms they use and in the networks upon which they are evaluated. As a result, the research community lacks a clear understanding of how these schemes compare against each other, how well they would work on real-world social networks with different structural properties, or whether there exist other (potentially better) ways of Sybil defense.In this paper, we show that, despite their considerable differences, existing Sybil defense schemes work by detecting local communities (i.e., clusters of nodes more tightly knit than the rest of the graph) around a trusted node. Our finding has important implications for both existing and future designs of Sybil defense schemes. First, we show that there is an opportunity to leverage the substantial amount of prior work on general community detection algorithms in order to defend against Sybils. Second, our analysis reveals the fundamental limits of current social network-based Sybil defenses: We demonstrate that networks with well-defined community structure are inherently more vulnerable to Sybil attacks, and that, in such networks, Sybils can carefully target their links in order make their attacks more effective.",,363–374,12,"sybil attacks, social networks, social network-based Sybil defense, communities",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1234,inproceedings,"Viswanath, Bimal and Post, Ansley and Gummadi, Krishna P. and Mislove, Alan",An Analysis of Social Network-Based Sybil Defenses,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851226,10.1145/1851182.1851226,"Recently, there has been much excitement in the research community over using social networks to mitigate multiple identity, or Sybil, attacks. A number of schemes have been proposed, but they differ greatly in the algorithms they use and in the networks upon which they are evaluated. As a result, the research community lacks a clear understanding of how these schemes compare against each other, how well they would work on real-world social networks with different structural properties, or whether there exist other (potentially better) ways of Sybil defense.In this paper, we show that, despite their considerable differences, existing Sybil defense schemes work by detecting local communities (i.e., clusters of nodes more tightly knit than the rest of the graph) around a trusted node. Our finding has important implications for both existing and future designs of Sybil defense schemes. First, we show that there is an opportunity to leverage the substantial amount of prior work on general community detection algorithms in order to defend against Sybils. Second, our analysis reveals the fundamental limits of current social network-based Sybil defenses: We demonstrate that networks with well-defined community structure are inherently more vulnerable to Sybil attacks, and that, in such networks, Sybils can carefully target their links in order make their attacks more effective.",Proceedings of the ACM SIGCOMM 2010 Conference,363–374,12,"communities, social networks, sybil attacks, social network-based Sybil defense","New Delhi, India",SIGCOMM '10,,,,,,
1235,article,"Pujol, Josep M. and Erramilli, Vijay and Siganos, Georgos and Yang, Xiaoyuan and Laoutaris, Nikos and Chhabra, Parminder and Rodriguez, Pablo",The Little Engine(s) That Could: Scaling Online Social Networks,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851227,10.1145/1851275.1851227,"The difficulty of scaling Online Social Networks (OSNs) has introduced new system design challenges that has often caused costly re-architecting for services like Twitter and Facebook. The complexity of interconnection of users in social networks has introduced new scalability challenges. Conventional vertical scaling by resorting to full replication can be a costly proposition. Horizontal scaling by partitioning and distributing data among multiples servers - e.g. using DHTs - can lead to costly inter-server communication.We design, implement, and evaluate SPAR, a social partitioning and replication middle-ware that transparently leverages the social graph structure to achieve data locality while minimizing replication. SPAR guarantees that for all users in an OSN, their direct neighbor's data is co-located in the same server. The gains from this approach are multi-fold: application developers can assume local semantics, i.e., develop as they would for a single server; scalability is achieved by adding commodity servers with low memory and network I/O requirements; and redundancy is achieved at a fraction of the cost.We detail our system design and an evaluation based on datasets from Twitter, Orkut, and Facebook, with a working implementation. We show that SPAR incurs minimum overhead, and can help a well-known open-source Twitter clone reach Twitter's scale without changing a line of its application logic and achieves higher throughput than Cassandra, Facebook's DHT based key-value store database.",,375–386,12,"scalability, social networks, replication, partition",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1236,inproceedings,"Pujol, Josep M. and Erramilli, Vijay and Siganos, Georgos and Yang, Xiaoyuan and Laoutaris, Nikos and Chhabra, Parminder and Rodriguez, Pablo",The Little Engine(s) That Could: Scaling Online Social Networks,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851227,10.1145/1851182.1851227,"The difficulty of scaling Online Social Networks (OSNs) has introduced new system design challenges that has often caused costly re-architecting for services like Twitter and Facebook. The complexity of interconnection of users in social networks has introduced new scalability challenges. Conventional vertical scaling by resorting to full replication can be a costly proposition. Horizontal scaling by partitioning and distributing data among multiples servers - e.g. using DHTs - can lead to costly inter-server communication.We design, implement, and evaluate SPAR, a social partitioning and replication middle-ware that transparently leverages the social graph structure to achieve data locality while minimizing replication. SPAR guarantees that for all users in an OSN, their direct neighbor's data is co-located in the same server. The gains from this approach are multi-fold: application developers can assume local semantics, i.e., develop as they would for a single server; scalability is achieved by adding commodity servers with low memory and network I/O requirements; and redundancy is achieved at a fraction of the cost.We detail our system design and an evaluation based on datasets from Twitter, Orkut, and Facebook, with a working implementation. We show that SPAR incurs minimum overhead, and can help a well-known open-source Twitter clone reach Twitter's scale without changing a line of its application logic and achieves higher throughput than Cassandra, Facebook's DHT based key-value store database.",Proceedings of the ACM SIGCOMM 2010 Conference,375–386,12,"replication, scalability, social networks, partition","New Delhi, India",SIGCOMM '10,,,,,,
1237,article,"Choffnes, David R. and Bustamante, Fabi\'{a",Crowdsourcing Service-Level Network Event Monitoring,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851228,10.1145/1851275.1851228,"The user experience for networked applications is becoming a key benchmark for customers and network providers. Perceived user experience is largely determined by the frequency, duration and severity of network events that impact a service. While today's networks implement sophisticated infrastructure that issues alarms for most failures, there remains a class of silent outages (e.g., caused by configuration errors) that are not detected. Further, existing alarms provide little information to help operators understand the impact of network events on services. Attempts to address this through infrastructure that monitors end-to-end performance for customers have been hampered by the cost of deployment and by the volume of data generated by these solutions.We present an alternative approach that pushes monitoring to applications on end systems and uses their collective view to detect network events and their impact on services - an approach we call Crowdsourcing Event Monitoring (CEM). This paper presents a general framework for CEM systems and demonstrates its effectiveness for a P2P application using a large dataset gathered from BitTorrent users and confirmed network events from two ISPs. We discuss how we designed and deployed a prototype CEM implementation as an extension to BitTorrent. This system performs online service-level network event detection through passive monitoring and correlation of performance in end-users' applications.",,387–398,12,"crowdsourcing, service-level network events, anomaly detection, P2P",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1238,inproceedings,"Choffnes, David R. and Bustamante, Fabi\'{a",Crowdsourcing Service-Level Network Event Monitoring,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851228,10.1145/1851182.1851228,"The user experience for networked applications is becoming a key benchmark for customers and network providers. Perceived user experience is largely determined by the frequency, duration and severity of network events that impact a service. While today's networks implement sophisticated infrastructure that issues alarms for most failures, there remains a class of silent outages (e.g., caused by configuration errors) that are not detected. Further, existing alarms provide little information to help operators understand the impact of network events on services. Attempts to address this through infrastructure that monitors end-to-end performance for customers have been hampered by the cost of deployment and by the volume of data generated by these solutions.We present an alternative approach that pushes monitoring to applications on end systems and uses their collective view to detect network events and their impact on services - an approach we call Crowdsourcing Event Monitoring (CEM). This paper presents a general framework for CEM systems and demonstrates its effectiveness for a P2P application using a large dataset gathered from BitTorrent users and confirmed network events from two ISPs. We discuss how we designed and deployed a prototype CEM implementation as an extension to BitTorrent. This system performs online service-level network event detection through passive monitoring and correlation of performance in end-users' applications.",Proceedings of the ACM SIGCOMM 2010 Conference,387–398,12,"crowdsourcing, service-level network events, P2P, anomaly detection","New Delhi, India",SIGCOMM '10,,,,,,
1239,article,"Wang, Yaogong",SIP Overload Control: A Backpressure-Based Approach,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851230,10.1145/1851275.1851230,"Overload happens in Session Initiation Protocol (SIP) networks when SIP servers have insufficient resources to handle all messages they receive. Under overload, SIP networks suffer from congestion collapse due to its ineffective overload control mechanism. In this paper we propose a backpressure-based SIP overload control mechanism called Bassoon. It consists of two parts: the first part is a provably optimal load balancing algorithm that ensures full utilization of the available network resources. The second part is an end-to-end load control algorithm that intelligently throttles excessive traffic at the edge of the network. We show that, by combining optimal load balancing and end-to-end load control, Bassoon effectively controls overload in SIP networks and outperforms existing schemes in terms of goodput, fairness and responsiveness.",,399–400,2,"SIP, load control, load balancing, goodput",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1240,inproceedings,"Wang, Yaogong",SIP Overload Control: A Backpressure-Based Approach,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851230,10.1145/1851182.1851230,"Overload happens in Session Initiation Protocol (SIP) networks when SIP servers have insufficient resources to handle all messages they receive. Under overload, SIP networks suffer from congestion collapse due to its ineffective overload control mechanism. In this paper we propose a backpressure-based SIP overload control mechanism called Bassoon. It consists of two parts: the first part is a provably optimal load balancing algorithm that ensures full utilization of the available network resources. The second part is an end-to-end load control algorithm that intelligently throttles excessive traffic at the edge of the network. We show that, by combining optimal load balancing and end-to-end load control, Bassoon effectively controls overload in SIP networks and outperforms existing schemes in terms of goodput, fairness and responsiveness.",Proceedings of the ACM SIGCOMM 2010 Conference,399–400,2,"load balancing, load control, goodput, SIP","New Delhi, India",SIGCOMM '10,,,,,,
1241,article,"Wang, Tianyi and Chen, Yang and Zhang, Zengbin and Sun, Peng and Deng, Beixing and Li, Xing",Unbiased Sampling in Directed Social Graph,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851231,10.1145/1851275.1851231,"Microblogging services, such as Twitter, are among the most important online social networks(OSNs). Different from OSNs such as Facebook, the topology of microblogging service is a directed graph instead of an undirected graph. Recently, due to the explosive increase of population size, graph sampling has started to play a critical role in measurement and characterization studies of such OSNs. However, previous studies have only focused on the unbiased sampling of undirected social graphs. In this paper, we study the unbiased sampling algorithm for directed social graphs. Based on the traditional Metropolis-Hasting Random Walk (MHRW) algorithm, we propose an unbiased sampling method for directed social graphs(USDSG). Using this method, we get the first, to the best of our knowledge, unbiased sample of directed social graphs. Through extensive experiments comparing with the ""ground truth"" (UNI, obtained through uniform sampling of directed graph nodes), we show that our method can achieve excellent performance in directed graph sampling and the error to UNI is less than 10%.",,401–402,2,"unbias, graph sampling, online social network",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1242,inproceedings,"Wang, Tianyi and Chen, Yang and Zhang, Zengbin and Sun, Peng and Deng, Beixing and Li, Xing",Unbiased Sampling in Directed Social Graph,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851231,10.1145/1851182.1851231,"Microblogging services, such as Twitter, are among the most important online social networks(OSNs). Different from OSNs such as Facebook, the topology of microblogging service is a directed graph instead of an undirected graph. Recently, due to the explosive increase of population size, graph sampling has started to play a critical role in measurement and characterization studies of such OSNs. However, previous studies have only focused on the unbiased sampling of undirected social graphs. In this paper, we study the unbiased sampling algorithm for directed social graphs. Based on the traditional Metropolis-Hasting Random Walk (MHRW) algorithm, we propose an unbiased sampling method for directed social graphs(USDSG). Using this method, we get the first, to the best of our knowledge, unbiased sample of directed social graphs. Through extensive experiments comparing with the ""ground truth"" (UNI, obtained through uniform sampling of directed graph nodes), we show that our method can achieve excellent performance in directed graph sampling and the error to UNI is less than 10%.",Proceedings of the ACM SIGCOMM 2010 Conference,401–402,2,"graph sampling, online social network, unbias","New Delhi, India",SIGCOMM '10,,,,,,
1243,article,"Yoon, Sungro",Contrabass: Concurrent Transmissions without Coordination,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851232,10.1145/1851275.1851232,"A PHY and MAC protocol for MIMO concurrent transmissions, called Contrabass, is presented. Concurrent transmissions, also referred to as multi-user MIMO, are simultaneous transmissions by multiple interfering nodes over the same carrier frequency. Concurrent transmissions technique has the potential of mitigating the overhead of MAC protocols by amortizing protocol overhead among multiple packets. However, existing proposals for concurrent transmissions could not achieve this as MIMO channel training and collision avoidance typically involve an expensive process of coordination and control message exchanges. This overhead has made MIMO concurrent transmission Impractical and thus unused in real applications. Contrabass implements simultaneous channel training and optimal transmission control without any coordination. As a result, Contrabass achieves very high aggregate throughput, low delays and scalability even under dynamic environments and outperforms the existing MIMO protocols. This is the first practical implementation of MIMO-based concurrent transmissions. We implemented Contrabass in GNU radios and also in NS-2.",,403–404,2,"spatial multiplexing, multi-user transmissions, IEEE 802.11n, interference cancellation, MIMO",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1244,inproceedings,"Yoon, Sungro",Contrabass: Concurrent Transmissions without Coordination,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851232,10.1145/1851182.1851232,"A PHY and MAC protocol for MIMO concurrent transmissions, called Contrabass, is presented. Concurrent transmissions, also referred to as multi-user MIMO, are simultaneous transmissions by multiple interfering nodes over the same carrier frequency. Concurrent transmissions technique has the potential of mitigating the overhead of MAC protocols by amortizing protocol overhead among multiple packets. However, existing proposals for concurrent transmissions could not achieve this as MIMO channel training and collision avoidance typically involve an expensive process of coordination and control message exchanges. This overhead has made MIMO concurrent transmission Impractical and thus unused in real applications. Contrabass implements simultaneous channel training and optimal transmission control without any coordination. As a result, Contrabass achieves very high aggregate throughput, low delays and scalability even under dynamic environments and outperforms the existing MIMO protocols. This is the first practical implementation of MIMO-based concurrent transmissions. We implemented Contrabass in GNU radios and also in NS-2.",Proceedings of the ACM SIGCOMM 2010 Conference,403–404,2,"MIMO, interference cancellation, spatial multiplexing, IEEE 802.11n, multi-user transmissions","New Delhi, India",SIGCOMM '10,,,,,,
1245,article,"Dvir, Amit and Vasilakos, Athanasios V.",Backpressure-Based Routing Protocol for DTNs,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851233,10.1145/1851275.1851233,"In this paper we consider an alternative, highly agile In this paper we consider an alternative, highly agile approach called backpressure routing for Delay Tolerant Networks (DTN), in which routing and forwarding decisions are made on a per-packet basis. Using information about queue backlogs, random walk and data packet scheduling nodes can make packet routing and forwarding decisions without the notion of end-to-end routes. To the best of our knowledge, this is the first ever implementation of dynamic backpressure routing in DTNs. Simulation results show that the proposed approach has advantages in terms of DTN networks.",,405–406,2,"delay tolerant network, backpressure approach",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1246,inproceedings,"Dvir, Amit and Vasilakos, Athanasios V.",Backpressure-Based Routing Protocol for DTNs,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851233,10.1145/1851182.1851233,"In this paper we consider an alternative, highly agile In this paper we consider an alternative, highly agile approach called backpressure routing for Delay Tolerant Networks (DTN), in which routing and forwarding decisions are made on a per-packet basis. Using information about queue backlogs, random walk and data packet scheduling nodes can make packet routing and forwarding decisions without the notion of end-to-end routes. To the best of our knowledge, this is the first ever implementation of dynamic backpressure routing in DTNs. Simulation results show that the proposed approach has advantages in terms of DTN networks.",Proceedings of the ACM SIGCOMM 2010 Conference,405–406,2,"backpressure approach, delay tolerant network","New Delhi, India",SIGCOMM '10,,,,,,
1247,article,"Dandapat, Sourav Kumar and Mitra, Bivas and Ganguly, Niloy and Choudhury, Romit Roy",Fair Bandwidth Allocation in Wireless Network Using Max-Flow,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851234,10.1145/1851275.1851234,"This paper proposes a fair association scheme between clients and APs in WiFi network, exploiting the hybrid nature of the recent WLAN architecture. We show that such an association outperforms RSSI based schemes in several scenarios, while remaining practical and scalable for wide-scale deployment.",,407–408,2,"fairness, association control, max-flow",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1248,inproceedings,"Dandapat, Sourav Kumar and Mitra, Bivas and Ganguly, Niloy and Choudhury, Romit Roy",Fair Bandwidth Allocation in Wireless Network Using Max-Flow,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851234,10.1145/1851182.1851234,"This paper proposes a fair association scheme between clients and APs in WiFi network, exploiting the hybrid nature of the recent WLAN architecture. We show that such an association outperforms RSSI based schemes in several scenarios, while remaining practical and scalable for wide-scale deployment.",Proceedings of the ACM SIGCOMM 2010 Conference,407–408,2,"fairness, association control, max-flow","New Delhi, India",SIGCOMM '10,,,,,,
1249,article,"Krishnan, Sundaresan and Chaporkar, Prasanna",Stochastic Approximation Algorithm for Optimal Throughput Performance of Wireless LANs,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851235,10.1145/1851275.1851235,"In this paper, we consider the problem of throughput maximization in an infrastructure based WLAN. We demonstrate that most of the proposed protocols though perform optimally for connected network (no hidden terminals), their performance is worse than even that of standard IEEE 802.11 in presence of hidden terminals. Here we present a stochastic approximation based algorithm that not only provide optimum throughput in a fully connected network but also when hidden nodes are present.",,409–410,2,"IEEE 802.11, weighted fairness, stochastic approximation, hidden nodes",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1250,inproceedings,"Krishnan, Sundaresan and Chaporkar, Prasanna",Stochastic Approximation Algorithm for Optimal Throughput Performance of Wireless LANs,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851235,10.1145/1851182.1851235,"In this paper, we consider the problem of throughput maximization in an infrastructure based WLAN. We demonstrate that most of the proposed protocols though perform optimally for connected network (no hidden terminals), their performance is worse than even that of standard IEEE 802.11 in presence of hidden terminals. Here we present a stochastic approximation based algorithm that not only provide optimum throughput in a fully connected network but also when hidden nodes are present.",Proceedings of the ACM SIGCOMM 2010 Conference,409–410,2,"stochastic approximation, weighted fairness, IEEE 802.11, hidden nodes","New Delhi, India",SIGCOMM '10,,,,,,
1251,article,"Oprescu, Iuniana M. and Meulle, Mickael and Uhlig, Steve and Pelsser, Cristel and Maennel, Olaf and Owezarski, Philippe",Rethinking IBGP Routing,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851236,10.1145/1851275.1851236,"The Internet is organized as a collection of administrative domains, known as Autonomous Systems (ASes). These ASes interact through the Border Gateway Protocol (BGP) that allows them to share reachability information. Adjacent routers in distinct ASes use external BGP (eBGP), whereas in a given AS routes are propagated over internal BGP (iBGP) sessions between any pair of routers. In large ASes where a logical full-mesh is not possible, confederations or route reflectors (RRs) are used. However, these somewhat scalable alternatives have introduced their own set of unpredictable effects (persistent routing oscillations and forwarding loops causing an increase of the convergence time) addressed in the literature [1].The solution we propose to these issues consists of a structured routing overlay holding a comprehensive view of the routes. We describe the design of a distributed entity that performs BGP route pre-computation for its clients inside a large backbone network and propagates the paths to the routers. Compared to the current iBGP routing, the advantage of the overlay approach is the separation between the responsibility of the control plane (route storage and best path computation) and the forwarding of the packets.One of the major improvements we bring is the divided routing table tackling the scalability concerns and allowing for parallel computation of paths.",,411–412,2,"bgp, routing",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1252,inproceedings,"Oprescu, Iuniana M. and Meulle, Mickael and Uhlig, Steve and Pelsser, Cristel and Maennel, Olaf and Owezarski, Philippe",Rethinking IBGP Routing,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851236,10.1145/1851182.1851236,"The Internet is organized as a collection of administrative domains, known as Autonomous Systems (ASes). These ASes interact through the Border Gateway Protocol (BGP) that allows them to share reachability information. Adjacent routers in distinct ASes use external BGP (eBGP), whereas in a given AS routes are propagated over internal BGP (iBGP) sessions between any pair of routers. In large ASes where a logical full-mesh is not possible, confederations or route reflectors (RRs) are used. However, these somewhat scalable alternatives have introduced their own set of unpredictable effects (persistent routing oscillations and forwarding loops causing an increase of the convergence time) addressed in the literature [1].The solution we propose to these issues consists of a structured routing overlay holding a comprehensive view of the routes. We describe the design of a distributed entity that performs BGP route pre-computation for its clients inside a large backbone network and propagates the paths to the routers. Compared to the current iBGP routing, the advantage of the overlay approach is the separation between the responsibility of the control plane (route storage and best path computation) and the forwarding of the packets.One of the major improvements we bring is the divided routing table tackling the scalability concerns and allowing for parallel computation of paths.",Proceedings of the ACM SIGCOMM 2010 Conference,411–412,2,"bgp, routing","New Delhi, India",SIGCOMM '10,,,,,,
1253,article,"Yao, Guang and Bi, Jun and Zhou, Zijian",Passive IP Traceback: Capturing the Origin of Anonymous Traffic through Network Telescopes,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851237,10.1145/1851275.1851237,"IP traceback can be used to find the origin of anonymous traffic; however, Internet-scale IP traceback systems have not been deployed due to a need for cooperation between Internet Service Providers (ISPs). This article presents an Internet-scale Passive IP Trackback (PIT) mechanism that does not require ISP deployment. PIT analyzes the ICMP messages that may scattered to a network telescope as spoofed packets travel from attacker to victim. An Internet route model is then used to help re-construct the attack path. Applying this mechanism to data collected by Cooperative Association for Internet Data Analysis (CAIDA), we found PIT can construct a trace tree from at least one intermediate router in 55.4% the fiercest packet spoofing attacks, and can construct a tree from at least 10 routers in 23.4% of attacks. This initial result shows PIT is a promising mechanism.",,413–414,2,"IP traceback, network telescope",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1254,inproceedings,"Yao, Guang and Bi, Jun and Zhou, Zijian",Passive IP Traceback: Capturing the Origin of Anonymous Traffic through Network Telescopes,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851237,10.1145/1851182.1851237,"IP traceback can be used to find the origin of anonymous traffic; however, Internet-scale IP traceback systems have not been deployed due to a need for cooperation between Internet Service Providers (ISPs). This article presents an Internet-scale Passive IP Trackback (PIT) mechanism that does not require ISP deployment. PIT analyzes the ICMP messages that may scattered to a network telescope as spoofed packets travel from attacker to victim. An Internet route model is then used to help re-construct the attack path. Applying this mechanism to data collected by Cooperative Association for Internet Data Analysis (CAIDA), we found PIT can construct a trace tree from at least one intermediate router in 55.4% the fiercest packet spoofing attacks, and can construct a tree from at least 10 routers in 23.4% of attacks. This initial result shows PIT is a promising mechanism.",Proceedings of the ACM SIGCOMM 2010 Conference,413–414,2,"IP traceback, network telescope","New Delhi, India",SIGCOMM '10,,,,,,
1255,article,"Xiong, Jie and Jamieson, Kyle",SecureAngle: Improving Wireless Security Using Angle-of-Arrival Information (Poster Abstract),2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851238,10.1145/1851275.1851238,"Wireless local area networks play an important role in our everyday lives, at the workplace and at home. However, wireless networks are also relatively vulnerable: physically located off-premises, attackers can circumvent wireless security protocols such as WEP, WPA, and even to some extent WPA2, presenting a security risk to the entire network. To address this problem, we propose SecureAngle, a system designed to operate alongside existing wireless security protocols, adding defense in depth. SecureAngle employs multiantenna APs to profile the directions at which a client's signal arrives, using this angle-of-arrival information to construct unique signatures that identify each client. With these signatures, we are currently investigating how a SecureAngle enabled AP can enable a ""virtual fence"" that drops frames injected into the network from a client physically located outside a building, and how a SecureAngle-enabled AP can prevent malicious parties from spoofing the link-layer address of legitimate clients.",,415–416,2,"SecureAngle, wireless, angle of arrival, 802.11",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1256,inproceedings,"Xiong, Jie and Jamieson, Kyle",SecureAngle: Improving Wireless Security Using Angle-of-Arrival Information (Poster Abstract),2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851238,10.1145/1851182.1851238,"Wireless local area networks play an important role in our everyday lives, at the workplace and at home. However, wireless networks are also relatively vulnerable: physically located off-premises, attackers can circumvent wireless security protocols such as WEP, WPA, and even to some extent WPA2, presenting a security risk to the entire network. To address this problem, we propose SecureAngle, a system designed to operate alongside existing wireless security protocols, adding defense in depth. SecureAngle employs multiantenna APs to profile the directions at which a client's signal arrives, using this angle-of-arrival information to construct unique signatures that identify each client. With these signatures, we are currently investigating how a SecureAngle enabled AP can enable a ""virtual fence"" that drops frames injected into the network from a client physically located outside a building, and how a SecureAngle-enabled AP can prevent malicious parties from spoofing the link-layer address of legitimate clients.",Proceedings of the ACM SIGCOMM 2010 Conference,415–416,2,"angle of arrival, SecureAngle, wireless, 802.11","New Delhi, India",SIGCOMM '10,,,,,,
1257,article,"Sundaresan, Srikanth and Lumezanu, Cristian and Feamster, Nick and Francois, Pierre",Autonomous Traffic Engineering with Self-Configuring Topologies,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851239,10.1145/1851275.1851239,"Network operators use traffic engineering (TE) to control the flow of traffic across their networks. Existing TE methods require manual configuration of link weights or tunnels, which is difficult to get right, or prior knowledge of traffic demands and hence may not be robust to link failures or traffic fluctuations. We present a self-configuring TE scheme, SculpTE, which automatically adapts the network-layer topology to changing traffic demands. SculpTE is responsive, stable, and achieves excellent load balancing.",,417–418,2,"sculpte, self-configuring, multi-path routing, traffic engineering, online",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1258,inproceedings,"Sundaresan, Srikanth and Lumezanu, Cristian and Feamster, Nick and Francois, Pierre",Autonomous Traffic Engineering with Self-Configuring Topologies,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851239,10.1145/1851182.1851239,"Network operators use traffic engineering (TE) to control the flow of traffic across their networks. Existing TE methods require manual configuration of link weights or tunnels, which is difficult to get right, or prior knowledge of traffic demands and hence may not be robust to link failures or traffic fluctuations. We present a self-configuring TE scheme, SculpTE, which automatically adapts the network-layer topology to changing traffic demands. SculpTE is responsive, stable, and achieves excellent load balancing.",Proceedings of the ACM SIGCOMM 2010 Conference,417–418,2,"multi-path routing, traffic engineering, online, sculpte, self-configuring","New Delhi, India",SIGCOMM '10,,,,,,
1259,article,"Capelis, D. J. and Long, Darrell D.E.",Fived: A Service-Based Architecture Implementation to Innovate at the Endpoints,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851240,10.1145/1851275.1851240,"Security functions such as access control, encryption and authentication are typically left up to applications on the modern Internet. There is no unified system to implement these critical features. The access control that does exist on the network doesn't integrate well with user authentication systems, so access control decisions are based on the network location of a computer rather than the privilege level of its user. Just about every layer of the Internet provides optional encryption, yet most data on the Internet continues to be sent in the clear. Application developers routinely make mistakes in security critical code leading to bugs that manifest in worms, malware or provide a doorway for actively malicious attackers. We propose a unified session layer that integrates trustworthiness features into the core of the network. This would reverse the fortunes of security on the Internet and lead us toward a safer, more secure global network.",,419–420,2,"network design, session layer, sessions, network architecture, fived",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1260,inproceedings,"Capelis, D. J. and Long, Darrell D.E.",Fived: A Service-Based Architecture Implementation to Innovate at the Endpoints,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851240,10.1145/1851182.1851240,"Security functions such as access control, encryption and authentication are typically left up to applications on the modern Internet. There is no unified system to implement these critical features. The access control that does exist on the network doesn't integrate well with user authentication systems, so access control decisions are based on the network location of a computer rather than the privilege level of its user. Just about every layer of the Internet provides optional encryption, yet most data on the Internet continues to be sent in the clear. Application developers routinely make mistakes in security critical code leading to bugs that manifest in worms, malware or provide a doorway for actively malicious attackers. We propose a unified session layer that integrates trustworthiness features into the core of the network. This would reverse the fortunes of security on the Internet and lead us toward a safer, more secure global network.",Proceedings of the ACM SIGCOMM 2010 Conference,419–420,2,"network architecture, session layer, fived, network design, sessions","New Delhi, India",SIGCOMM '10,,,,,,
1261,article,"Lai, Yu-Jen and Kuo, Wei-Hao and Chiu, Wan-Ting and Chang, Shao-Ting and Wei, Hung-Yu",Accelerometer-Assisted 802.11 Rate Adaptation on Mass Rapid Transit System,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851241,10.1145/1851275.1851241,"In-station Wi-Fi AP deployment provides opportunistic Wi-Fi access in underground Mass Rapid Transit (MRT) system. But such vehicular network faces the obstacle of limited connection time from the MS on the train to the BS at the station. Therefore, maximizing the throughput during the tens of second intervals becomes crucial to overcome such hindrance. To achieve the goal, we propose Accelerometer-Assisted Rate Adaptation (AARA) to divide the motion of the train into four phases; each adopts a specific rate adaptation mechanism. The experiments show that the average throughput of AARA outperforms that of the conventional scheme.",,421–422,2,"accelerometer, vehicular networks, IEEE 802.11, rate-adaptation",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1262,inproceedings,"Lai, Yu-Jen and Kuo, Wei-Hao and Chiu, Wan-Ting and Chang, Shao-Ting and Wei, Hung-Yu",Accelerometer-Assisted 802.11 Rate Adaptation on Mass Rapid Transit System,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851241,10.1145/1851182.1851241,"In-station Wi-Fi AP deployment provides opportunistic Wi-Fi access in underground Mass Rapid Transit (MRT) system. But such vehicular network faces the obstacle of limited connection time from the MS on the train to the BS at the station. Therefore, maximizing the throughput during the tens of second intervals becomes crucial to overcome such hindrance. To achieve the goal, we propose Accelerometer-Assisted Rate Adaptation (AARA) to divide the motion of the train into four phases; each adopts a specific rate adaptation mechanism. The experiments show that the average throughput of AARA outperforms that of the conventional scheme.",Proceedings of the ACM SIGCOMM 2010 Conference,421–422,2,"accelerometer, rate-adaptation, IEEE 802.11, vehicular networks","New Delhi, India",SIGCOMM '10,,,,,,
1263,article,"Joshi, Ajinkya Uday and Kulkarni, Purushottam",Vehicular Wifi Access and Rate Adaptation,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851243,10.1145/1851275.1851243,"Vehicular WiFi access is distinct in two respects, (i) continuous mobility of clients and (ii) possibility of predictable link quality. As part of this study, we aim to comprehensively evaluate existing rate adaptation algorithms in real environments. Further, if required, we aim to develop a simple, low-overhead rate adaptation algorithm suited for vehicular WiFi access.",,423–424,2,"mobility, rate adaptation, wifi",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1264,inproceedings,"Joshi, Ajinkya Uday and Kulkarni, Purushottam",Vehicular Wifi Access and Rate Adaptation,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851243,10.1145/1851182.1851243,"Vehicular WiFi access is distinct in two respects, (i) continuous mobility of clients and (ii) possibility of predictable link quality. As part of this study, we aim to comprehensively evaluate existing rate adaptation algorithms in real environments. Further, if required, we aim to develop a simple, low-overhead rate adaptation algorithm suited for vehicular WiFi access.",Proceedings of the ACM SIGCOMM 2010 Conference,423–424,2,"rate adaptation, mobility, wifi","New Delhi, India",SIGCOMM '10,,,,,,
1265,article,"Lee, Kyunghan and Lee, Joohyun and Yi, Yung and Rhee, Injong and Chong, Song",Mobile Data Offloading: How Much Can WiFi Deliver?,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851244,10.1145/1851275.1851244,"This is a quantitative study on the performance of 3G mobile data offloading through WiFi networks. We recruited about 100 iPhone users from a metropolitan area and collected statistics on their WiFi connectivity during about a two and half week period in February 2010. We find that a user is in WiFi coverage for 70% of the time on average and the distributions of WiFi connection and disconnection times have a strong heavy-tail tendency with means around 2 hours and 40 minutes, respectively. Using the acquired traces, we run trace-driven simulation to measure offloading efficiency under diverse conditions e.g. traffic types, deadlines and WiFi deployment scenarios. The results indicate that if users can tolerate a two hour delay in data transfer (e.g, video and image up-loads), the network can offload 70% of the total 3G data traffic on average. We also develop a theoretical framework that permits an analytical study of the average performance of offloading. This tool is useful for network providers to obtain a rough estimate on the average performance of offloading for a given inputWiFi deployment condition.",,425–426,2,"experimental networks, mobile data offloading, delayed transmission",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1266,inproceedings,"Lee, Kyunghan and Lee, Joohyun and Yi, Yung and Rhee, Injong and Chong, Song",Mobile Data Offloading: How Much Can WiFi Deliver?,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851244,10.1145/1851182.1851244,"This is a quantitative study on the performance of 3G mobile data offloading through WiFi networks. We recruited about 100 iPhone users from a metropolitan area and collected statistics on their WiFi connectivity during about a two and half week period in February 2010. We find that a user is in WiFi coverage for 70% of the time on average and the distributions of WiFi connection and disconnection times have a strong heavy-tail tendency with means around 2 hours and 40 minutes, respectively. Using the acquired traces, we run trace-driven simulation to measure offloading efficiency under diverse conditions e.g. traffic types, deadlines and WiFi deployment scenarios. The results indicate that if users can tolerate a two hour delay in data transfer (e.g, video and image up-loads), the network can offload 70% of the total 3G data traffic on average. We also develop a theoretical framework that permits an analytical study of the average performance of offloading. This tool is useful for network providers to obtain a rough estimate on the average performance of offloading for a given inputWiFi deployment condition.",Proceedings of the ACM SIGCOMM 2010 Conference,425–426,2,"mobile data offloading, experimental networks, delayed transmission","New Delhi, India",SIGCOMM '10,,,,,,
1267,article,"Sharma, Manuj and Sahoo, Anirudha",Residual White Space Distribution-Based Opportunistic Channel Access for Cognitive Radio Enabled Devices,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851245,10.1145/1851275.1851245,"We describe an opportunistic channel access scheme for cognitive radio-enabled secondary nodes (SNs). The proposed scheme uses the residual channel idle time distribution to estimate the transmission duration in the remaining idle time, subject to an acceptable Primary User (PU) interference constraint. The SN then transmits the frames within the estimated duration without further sensing the channel, which reduces sensing overhead. The scheme does not require the SN to continuously sense the channel to keep track of the start of the idle period, thereby conserving energy.",,427–428,2,"cognitive radio network, opportunistic spectrum access",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1268,inproceedings,"Sharma, Manuj and Sahoo, Anirudha",Residual White Space Distribution-Based Opportunistic Channel Access for Cognitive Radio Enabled Devices,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851245,10.1145/1851182.1851245,"We describe an opportunistic channel access scheme for cognitive radio-enabled secondary nodes (SNs). The proposed scheme uses the residual channel idle time distribution to estimate the transmission duration in the remaining idle time, subject to an acceptable Primary User (PU) interference constraint. The SN then transmits the frames within the estimated duration without further sensing the channel, which reduces sensing overhead. The scheme does not require the SN to continuously sense the channel to keep track of the start of the idle period, thereby conserving energy.",Proceedings of the ACM SIGCOMM 2010 Conference,427–428,2,"cognitive radio network, opportunistic spectrum access","New Delhi, India",SIGCOMM '10,,,,,,
1269,article,"Zhao, Jin and Zhang, Xinya and Wang, Xin and Xue, Xiangyang",Achieving O(1) IP Lookup on GPU-Based Software Routers,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851246,10.1145/1851275.1851246,"IP address lookup is a challenging problem due to the increasing routing table size, and higher line rate. This paper investigates a new way to build an efficient IP lookup scheme using graphics processor units(GPU). Our contribution here is to design a basic architecture for high-performance IP lookup engine with GPU, and to develop efficient algorithms for routing prefix operations such as lookup, deletion, insertion, and modification. In particular, the IP lookup scheme can achieve O(1) time complexity. Our experimental results on real-world route traces show promising 6x gains in IP lookup throughput.",,429–430,2,"IP lookup, GPU, Software router",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1270,inproceedings,"Zhao, Jin and Zhang, Xinya and Wang, Xin and Xue, Xiangyang",Achieving O(1) IP Lookup on GPU-Based Software Routers,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851246,10.1145/1851182.1851246,"IP address lookup is a challenging problem due to the increasing routing table size, and higher line rate. This paper investigates a new way to build an efficient IP lookup scheme using graphics processor units(GPU). Our contribution here is to design a basic architecture for high-performance IP lookup engine with GPU, and to develop efficient algorithms for routing prefix operations such as lookup, deletion, insertion, and modification. In particular, the IP lookup scheme can achieve O(1) time complexity. Our experimental results on real-world route traces show promising 6x gains in IP lookup throughput.",Proceedings of the ACM SIGCOMM 2010 Conference,429–430,2,"GPU, Software router, IP lookup","New Delhi, India",SIGCOMM '10,,,,,,
1271,article,"Rajendra, C. Viven and Kulkarni, Purushottam",Road Traffic Estimation Using In-Situ Acoustic Sensing,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851247,10.1145/1851275.1851247,"In this paper, we explore the efficacy of curb-side acoustic sensing to estimate road traffic conditions. We formulated a set of hypotheses which attempted to correlate traffic conditions with the ambient traffic noise. We present the evaluation of our hypotheses under various traffic conditions. Our threshold-based-classification yields 70-90% accuracy in distinguishing congested from free-flowing traffic.",,431–432,2,"sensor network, its, acoustic signal processing",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1272,inproceedings,"Rajendra, C. Viven and Kulkarni, Purushottam",Road Traffic Estimation Using In-Situ Acoustic Sensing,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851247,10.1145/1851182.1851247,"In this paper, we explore the efficacy of curb-side acoustic sensing to estimate road traffic conditions. We formulated a set of hypotheses which attempted to correlate traffic conditions with the ambient traffic noise. We present the evaluation of our hypotheses under various traffic conditions. Our threshold-based-classification yields 70-90% accuracy in distinguishing congested from free-flowing traffic.",Proceedings of the ACM SIGCOMM 2010 Conference,431–432,2,"its, sensor network, acoustic signal processing","New Delhi, India",SIGCOMM '10,,,,,,
1273,article,"Nikolaidis, Georgios and Zhushi, Astrit and Jamieson, Kyle and Karp, Brad",Cone of Silence: Adaptively Nulling Interferers in Wireless Networks,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851248,10.1145/1851275.1851248,"Dense 802.11 wireless networks present a pressing capacity challenge: users in proximity contend for limited unlicensed spectrum. Directional antennas promise increased capacity by improving the signal-to-interference-plus-noise ratio (SINR) at the receiver, potentially allowing successful decoding of packets at higher bit-rates. Many uses of directional antennas to date have directed high gain between two peers, thus maximizing the strength of the sender's signal reaching the receiver. But in an interference-rich environment, as in dense 802.11 deployments, directional antennas only truly come into their own when they explicitly null interference from competing concurrent senders. In this paper, we present Cone of Silence (CoS), a technique that leverages software-steerable directional antennas to improve the capacity of indoor 802.11 wireless networks by adaptively nulling interference. Using in situ signal strength measurements that account for the complex propagation environment, CoS derives custom antenna radiation patterns that maximize the strength of the signal arriving at an access point from a sender while nulling inteference from one or more concurrent interferers. CoS leverages multiple antennas, but requires only a single commodity 802.11 radio, thus avoiding the significant processing requirements of decoding multiple concurrent packets. Experiments in an indoor 802.11 deployment demonstrate that CoS improves throughput under interference.",,433–434,2,"beam forming, directional, beam steering, wireless, phased array, nulling, interference",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1274,inproceedings,"Nikolaidis, Georgios and Zhushi, Astrit and Jamieson, Kyle and Karp, Brad",Cone of Silence: Adaptively Nulling Interferers in Wireless Networks,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851248,10.1145/1851182.1851248,"Dense 802.11 wireless networks present a pressing capacity challenge: users in proximity contend for limited unlicensed spectrum. Directional antennas promise increased capacity by improving the signal-to-interference-plus-noise ratio (SINR) at the receiver, potentially allowing successful decoding of packets at higher bit-rates. Many uses of directional antennas to date have directed high gain between two peers, thus maximizing the strength of the sender's signal reaching the receiver. But in an interference-rich environment, as in dense 802.11 deployments, directional antennas only truly come into their own when they explicitly null interference from competing concurrent senders. In this paper, we present Cone of Silence (CoS), a technique that leverages software-steerable directional antennas to improve the capacity of indoor 802.11 wireless networks by adaptively nulling interference. Using in situ signal strength measurements that account for the complex propagation environment, CoS derives custom antenna radiation patterns that maximize the strength of the signal arriving at an access point from a sender while nulling inteference from one or more concurrent interferers. CoS leverages multiple antennas, but requires only a single commodity 802.11 radio, thus avoiding the significant processing requirements of decoding multiple concurrent packets. Experiments in an indoor 802.11 deployment demonstrate that CoS improves throughput under interference.",Proceedings of the ACM SIGCOMM 2010 Conference,433–434,2,"beam steering, phased array, directional, beam forming, wireless, interference, nulling","New Delhi, India",SIGCOMM '10,,,,,,
1275,article,"Riley, Ryan D. and Ali, Nada Mohammed and Al-Senaidi, Kholoud Saleh and Al-Kuwari, Aisha Lahdan",Empowering Users against Sidejacking Attacks,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851249,10.1145/1851275.1851249,"SideJacking occurs when an attacker intercepts a session cookie and uses it to impersonate a user and gain unauthorized access to a web-based service. To prevent SideJacking, a server should enable HTTPS and configure all session cookies to only be transmitted over a secure link. Many websites do not do this, however, and the user may be unaware. In this work we present a Firefox extension that will allow users to quickly and easily determine whether the server they are visiting is susceptible to SideJacking attacks.",,435–436,2,"https, web browsers, sidejacking",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1276,inproceedings,"Riley, Ryan D. and Ali, Nada Mohammed and Al-Senaidi, Kholoud Saleh and Al-Kuwari, Aisha Lahdan",Empowering Users against Sidejacking Attacks,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851249,10.1145/1851182.1851249,"SideJacking occurs when an attacker intercepts a session cookie and uses it to impersonate a user and gain unauthorized access to a web-based service. To prevent SideJacking, a server should enable HTTPS and configure all session cookies to only be transmitted over a secure link. Many websites do not do this, however, and the user may be unaware. In this work we present a Firefox extension that will allow users to quickly and easily determine whether the server they are visiting is susceptible to SideJacking attacks.",Proceedings of the ACM SIGCOMM 2010 Conference,435–436,2,"web browsers, sidejacking, https","New Delhi, India",SIGCOMM '10,,,,,,
1277,article,"Jang, Keon and Han, Sangjin and Han, Seungyeop and Moon, Sue and Park, KyoungSoo",Accelerating SSL with GPUs,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851250,10.1145/1851275.1851250,"SSL/TLS is a standard protocol for secure Internet communication. Despite its great success, today's SSL deployment is largely limited to security-critical domains. The low adoption rate of SSL is mainly due to high computation overhead on the server side.In this paper, we propose Graphics Processing Units (GPUs) as a new source of computing power to reduce the server-side overhead. We have designed and implemented an SSL proxy that opportunistically offloads cryptographic operations to GPUs. The evaluation results show that our GPU implementation of cryptographic operations, RSA, AES, and HMAC-SHA1, achieves high throughput while keeping the latency low. The SSL proxy significantly boosts the throughput of SSL transactions, handling 25.8K SSL transactions per second, and has comparable response time even when overloaded.",,437–438,2,"CUDA, SSL, GPU",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1278,inproceedings,"Jang, Keon and Han, Sangjin and Han, Seungyeop and Moon, Sue and Park, KyoungSoo",Accelerating SSL with GPUs,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851250,10.1145/1851182.1851250,"SSL/TLS is a standard protocol for secure Internet communication. Despite its great success, today's SSL deployment is largely limited to security-critical domains. The low adoption rate of SSL is mainly due to high computation overhead on the server side.In this paper, we propose Graphics Processing Units (GPUs) as a new source of computing power to reduce the server-side overhead. We have designed and implemented an SSL proxy that opportunistically offloads cryptographic operations to GPUs. The evaluation results show that our GPU implementation of cryptographic operations, RSA, AES, and HMAC-SHA1, achieves high throughput while keeping the latency low. The SSL proxy significantly boosts the throughput of SSL transactions, handling 25.8K SSL transactions per second, and has comparable response time even when overloaded.",Proceedings of the ACM SIGCOMM 2010 Conference,437–438,2,"SSL, GPU, CUDA","New Delhi, India",SIGCOMM '10,,,,,,
1279,article,"Bozakov, Zdravko",An Open Router Virtualization Framework Using a Programmable Forwarding Plane,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851251,10.1145/1851275.1851251,"Network virtualization promises to spur innovation and add flexibility to the Future Internet infrastructure. Routers supporting virtualization allow the deployment of concurrent virtual networks, and can be employed to consolidate resources and improve energy efficiency in data centers. The closed nature of commercial router systems poses a significant problem for research in the field of virtual network architectures. On the other hand, the performance of software-based, open routing solutions is typically limited. In this work we outline an open router virtualization framework utilizing OpenFlow enabled hardware as a fast, programmable forwarding plane.",,439–440,2,"virtualization, openflow, routers, commodity hardware",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1280,inproceedings,"Bozakov, Zdravko",An Open Router Virtualization Framework Using a Programmable Forwarding Plane,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851251,10.1145/1851182.1851251,"Network virtualization promises to spur innovation and add flexibility to the Future Internet infrastructure. Routers supporting virtualization allow the deployment of concurrent virtual networks, and can be employed to consolidate resources and improve energy efficiency in data centers. The closed nature of commercial router systems poses a significant problem for research in the field of virtual network architectures. On the other hand, the performance of software-based, open routing solutions is typically limited. In this work we outline an open router virtualization framework utilizing OpenFlow enabled hardware as a fast, programmable forwarding plane.",Proceedings of the ACM SIGCOMM 2010 Conference,439–440,2,"openflow, virtualization, commodity hardware, routers","New Delhi, India",SIGCOMM '10,,,,,,
1281,article,"Nascimento, Marcelo Ribeiro and Rothenberg, Christian Esteve and Salvador, Marcos Rog\'{e",QuagFlow: Partnering Quagga with OpenFlow,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851252,10.1145/1851275.1851252,"Computing history has shown that open, multi-layer hardware and software stacks encourage innovation and bring costs down. Only recently this trend is meeting the networking world with the availability of entire open source networking stacks being closer than ever. Towards this goal, we are working on QuagFlow, a transparent interplay between the popular Quagga open source routing suite and the low level vendor-independent OpenFlow interface. QuagFlow is a distributed system implemented as a NOX controller application and a series of slave daemons running along the virtual machines hosting the Quagga routing instances.",,441–442,2,"open-source, routing, virtualization",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1282,inproceedings,"Nascimento, Marcelo Ribeiro and Rothenberg, Christian Esteve and Salvador, Marcos Rog\'{e",QuagFlow: Partnering Quagga with OpenFlow,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851252,10.1145/1851182.1851252,"Computing history has shown that open, multi-layer hardware and software stacks encourage innovation and bring costs down. Only recently this trend is meeting the networking world with the availability of entire open source networking stacks being closer than ever. Towards this goal, we are working on QuagFlow, a transparent interplay between the popular Quagga open source routing suite and the low level vendor-independent OpenFlow interface. QuagFlow is a distributed system implemented as a NOX controller application and a series of slave daemons running along the virtual machines hosting the Quagga routing instances.",Proceedings of the ACM SIGCOMM 2010 Conference,441–442,2,"open-source, virtualization, routing","New Delhi, India",SIGCOMM '10,,,,,,
1283,article,Barr\'{e,Experimenting with Multipath TCP,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851254,10.1145/1851275.1851254,"It is becoming the norm for small mobile devices to have access to multiple technologies for connecting to the Internet. This gives researchers an increasing interest for solutions allowing to use efficiently several communication mediums. We propose a demonstration of our Multipath TCP implementation for Linux, that allows spreading a single TCP flow across multiple Internet paths, without requiring any change to applications. The demonstration will involve a real Internet communication with MPTCP, with simultaneous use of several paths, as well as a demonstration of MPTCP failover capability.",,443–444,2,"multipath transport, MPTCP",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1284,inproceedings,Barr\'{e,Experimenting with Multipath TCP,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851254,10.1145/1851182.1851254,"It is becoming the norm for small mobile devices to have access to multiple technologies for connecting to the Internet. This gives researchers an increasing interest for solutions allowing to use efficiently several communication mediums. We propose a demonstration of our Multipath TCP implementation for Linux, that allows spreading a single TCP flow across multiple Internet paths, without requiring any change to applications. The demonstration will involve a real Internet communication with MPTCP, with simultaneous use of several paths, as well as a demonstration of MPTCP failover capability.",Proceedings of the ACM SIGCOMM 2010 Conference,443–444,2,"multipath transport, MPTCP","New Delhi, India",SIGCOMM '10,,,,,,
1285,article,"Ghobadi, Monia and Labrecque, Martin and Salmon, Geoffrey and Aasaraai, Kaveh and Hassas Yeganeh, Soheil and Ganjali, Yashar and Steffan, J. Gregory",Caliper: A Tool to Generate Precise and Closed-Loop Traffic,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851255,10.1145/1851275.1851255,"Generating realistic and responsive traffic that reflects different network conditions is a challenging problem associated with performing valid experiments in network testbeds. In this work, we preset Caliper, a highly precise traffic generation tool, built on NetThreads, a flexible platform that we have created for developing packet processing applications on FPGA-based devices and the NetFPGA in particular. We will demonstrate the effect of ad-hoc inter-departure times on a commodity NIC compared to precisely timed inter-departures with Caliper. Both NetThreads and Caliper are available as free software to download.",,445–446,2,"netfpga, soft processors, traffic generation",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1286,inproceedings,"Ghobadi, Monia and Labrecque, Martin and Salmon, Geoffrey and Aasaraai, Kaveh and Hassas Yeganeh, Soheil and Ganjali, Yashar and Steffan, J. Gregory",Caliper: A Tool to Generate Precise and Closed-Loop Traffic,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851255,10.1145/1851182.1851255,"Generating realistic and responsive traffic that reflects different network conditions is a challenging problem associated with performing valid experiments in network testbeds. In this work, we preset Caliper, a highly precise traffic generation tool, built on NetThreads, a flexible platform that we have created for developing packet processing applications on FPGA-based devices and the NetFPGA in particular. We will demonstrate the effect of ad-hoc inter-departure times on a commodity NIC compared to precisely timed inter-departures with Caliper. Both NetThreads and Caliper are available as free software to download.",Proceedings of the ACM SIGCOMM 2010 Conference,445–446,2,"soft processors, traffic generation, netfpga","New Delhi, India",SIGCOMM '10,,,,,,
1287,article,"Santos, Tacio and Henke, Christian and Schmoll, Carsten and Zseby, Tanja",Multi-Hop Packet Tracking for Experimental Facilities,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851256,10.1145/1851275.1851256,"The Internet has become a complex system with increasing numbers of end-systems, applications, protocols and types of networks. Although we have a good understanding of how data is transferred over the network we cannot observe what happens with our data after sending and before receiving it - how packets traverse through the network and with which QoS characteristics remains unknown. Towards this objective we have developed a multi-hop packet tracking system intended to be used in experimental facilities, such as PlanetLab, where we have made our first tests. This paper describes our packet tracking realization and the results from our prototype implementation.",,447–448,2,"multipoint measurement, IPFIX, hash-based packet selection",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1288,inproceedings,"Santos, Tacio and Henke, Christian and Schmoll, Carsten and Zseby, Tanja",Multi-Hop Packet Tracking for Experimental Facilities,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851256,10.1145/1851182.1851256,"The Internet has become a complex system with increasing numbers of end-systems, applications, protocols and types of networks. Although we have a good understanding of how data is transferred over the network we cannot observe what happens with our data after sending and before receiving it - how packets traverse through the network and with which QoS characteristics remains unknown. Towards this objective we have developed a multi-hop packet tracking system intended to be used in experimental facilities, such as PlanetLab, where we have made our first tests. This paper describes our packet tracking realization and the results from our prototype implementation.",Proceedings of the ACM SIGCOMM 2010 Conference,447–448,2,"hash-based packet selection, multipoint measurement, IPFIX","New Delhi, India",SIGCOMM '10,,,,,,
1289,article,"Jakubczak, Szymon and Katabi, Dina",SoftCast: One-Size-Fits-All Wireless Video,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851257,10.1145/1851275.1851257,"The focus of this demonstration is the performance of streaming video over the mobile wireless channel. We compare two schemes: the standard approach to video which transmits H.264/AVC-encoded stream over 802.11-like PHY, and SoftCast -- a clean-slate design for wireless video where the source transmits one video stream that each receiver decodes to a video quality commensurate with its specific instantaneous channel quality.",,449–450,2,"scalable video communications, joint source-channel coding, wireless networks",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1290,inproceedings,"Jakubczak, Szymon and Katabi, Dina",SoftCast: One-Size-Fits-All Wireless Video,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851257,10.1145/1851182.1851257,"The focus of this demonstration is the performance of streaming video over the mobile wireless channel. We compare two schemes: the standard approach to video which transmits H.264/AVC-encoded stream over 802.11-like PHY, and SoftCast -- a clean-slate design for wireless video where the source transmits one video stream that each receiver decodes to a video quality commensurate with its specific instantaneous channel quality.",Proceedings of the ACM SIGCOMM 2010 Conference,449–450,2,"joint source-channel coding, scalable video communications, wireless networks","New Delhi, India",SIGCOMM '10,,,,,,
1291,article,"Perli, Samuel David and Ahmed, Nabeel and Katabi, Dina",PixNet: LCD-Camera Pairs as Communication Links,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851258,10.1145/1851275.1851258,"Given the abundance of cameras and LCDs in today's environment, there exists an untapped opportunity for using these devices for communication. Specifically, cameras can tune to nearby LCDs and use them for network access. The key feature of these LCD-camera links is that they are highly directional and hence enable a form of interference-free wireless communication. This makes them an attractive technology for dense, high contention scenarios. The main challenge, however, to enable such LCD-camera links is to maximize coverage, that is to deliver multiple Mb/s over multi-meter distances, independent of the view angle. To do so, these links need to address unique types of channel distortions, such as perspective distortion and blur.In this demo, we show how these LCD-camera links can be used to wirelessly transmit information. We present PixNet, an LCD-camera communication system. PixNet generalizes the popular OFDM transmission algorithms to address the unique properties of the LCD-camera link, including perspective distortion and blur. We have built a prototype of PixNet using off-the-shelf LCDs and cameras. In our demo, we will show our prototype communicating data from an LCD to a camera-equipped PC, over multi-meter distances and wide viewing angles.",,451–452,2,"perspective distortion, camera, optical links, OFDM",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1292,inproceedings,"Perli, Samuel David and Ahmed, Nabeel and Katabi, Dina",PixNet: LCD-Camera Pairs as Communication Links,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851258,10.1145/1851182.1851258,"Given the abundance of cameras and LCDs in today's environment, there exists an untapped opportunity for using these devices for communication. Specifically, cameras can tune to nearby LCDs and use them for network access. The key feature of these LCD-camera links is that they are highly directional and hence enable a form of interference-free wireless communication. This makes them an attractive technology for dense, high contention scenarios. The main challenge, however, to enable such LCD-camera links is to maximize coverage, that is to deliver multiple Mb/s over multi-meter distances, independent of the view angle. To do so, these links need to address unique types of channel distortions, such as perspective distortion and blur.In this demo, we show how these LCD-camera links can be used to wirelessly transmit information. We present PixNet, an LCD-camera communication system. PixNet generalizes the popular OFDM transmission algorithms to address the unique properties of the LCD-camera link, including perspective distortion and blur. We have built a prototype of PixNet using off-the-shelf LCDs and cameras. In our demo, we will show our prototype communicating data from an LCD to a camera-equipped PC, over multi-meter distances and wide viewing angles.",Proceedings of the ACM SIGCOMM 2010 Conference,451–452,2,"perspective distortion, camera, optical links, OFDM","New Delhi, India",SIGCOMM '10,,,,,,
1293,article,"Reich, Joshua and Laadan, Oren and Brosh, Eli and Sherman, Alex and Misra, Vishal and Nieh, Jason and Rubenstein, Dan",VMtorrent: Virtual Appliances on-Demand,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851259,10.1145/1851275.1851259,"Virtual Appliances (VAs) are Virtual Machines (VMs) geared towards a specific set of tasks. They require little or no configuration, working out-of-the-box. VAs fit neatly into the Cloud Computing paradigm - many copies of an identical machine can be launched in a data center, or home/business users can grab the appliance they need from the cloud to run locally just for so long as required. Companies and projects whose sole offerings are VAs ready for either desktop or data center use [3, 11] attest to the growing popularity of VAs. VMware's Appliance directory alone currently lists over 1400 VAs available for the VMware family of Virtual Machine Monitors (VMMs) [13].Current VA distribution generally requires download of the complete virtual disk image, only after which the VA can be run. Given that compressed VA sizes run anywhere from several hundred MB to a few GB, there can be significant delays from the time a user decides he/she wants to run a particular VA until the time that VA can be used. These problems are only exacerbated when demand for particular VAs spikes and server bandwidth resources become the distribution bottleneck.",,453–454,2,"on-demand delivery, p2p, virtual appliances, virtual machines, bittorrent, swarming, cloud computing, file systems",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1294,inproceedings,"Reich, Joshua and Laadan, Oren and Brosh, Eli and Sherman, Alex and Misra, Vishal and Nieh, Jason and Rubenstein, Dan",VMtorrent: Virtual Appliances on-Demand,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851259,10.1145/1851182.1851259,"Virtual Appliances (VAs) are Virtual Machines (VMs) geared towards a specific set of tasks. They require little or no configuration, working out-of-the-box. VAs fit neatly into the Cloud Computing paradigm - many copies of an identical machine can be launched in a data center, or home/business users can grab the appliance they need from the cloud to run locally just for so long as required. Companies and projects whose sole offerings are VAs ready for either desktop or data center use [3, 11] attest to the growing popularity of VAs. VMware's Appliance directory alone currently lists over 1400 VAs available for the VMware family of Virtual Machine Monitors (VMMs) [13].Current VA distribution generally requires download of the complete virtual disk image, only after which the VA can be run. Given that compressed VA sizes run anywhere from several hundred MB to a few GB, there can be significant delays from the time a user decides he/she wants to run a particular VA until the time that VA can be used. These problems are only exacerbated when demand for particular VAs spikes and server bandwidth resources become the distribution bottleneck.",Proceedings of the ACM SIGCOMM 2010 Conference,453–454,2,"p2p, virtual appliances, file systems, on-demand delivery, bittorrent, swarming, virtual machines, cloud computing","New Delhi, India",SIGCOMM '10,,,,,,
1295,article,"Kumar, Arun and Anand, Ashok and Akella, Aditya and Balachandran, Athula and Sekar, Vyas and Seshan, Srinivasan",Flexible Multimedia Content Retrieval Using InfoNames,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851260,10.1145/1851275.1851260,"Multimedia content is a dominant fraction of Internet usage today. At the same time, there is significant heterogeneity in video presentation modes and operating conditions of Internet-enabled devices that access such content. Users are often interested in the content, rather than the specific sources or the formats. The host-centric format of the current Internet does not support these requirements naturally. Neither do the recent data-centric naming proposals, since they rely on naming content based on raw byte-level hashing schemes. We argue that to meet these requirements, enabling content retrieval mechanisms to name and query directly for the underlying information is a good way forward. In addition to decoupling content from available sources and transfer protocols, these ""information-aware names"" or InfoNames explicitly decouple the information from content presentation factors as well. We envision an InfoName Resolution System (IRS) to resolve location based on InfoNames, while taking into account the operating conditions of devices. In this demo, we present an application to show how InfoNames can serve as presentation-invariant and portable names to fetch video content independent of device capabilities and resource constraints.",,455–456,2,"information based architecture, naming, InfoName, content retrieval, multimedia",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1296,inproceedings,"Kumar, Arun and Anand, Ashok and Akella, Aditya and Balachandran, Athula and Sekar, Vyas and Seshan, Srinivasan",Flexible Multimedia Content Retrieval Using InfoNames,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851260,10.1145/1851182.1851260,"Multimedia content is a dominant fraction of Internet usage today. At the same time, there is significant heterogeneity in video presentation modes and operating conditions of Internet-enabled devices that access such content. Users are often interested in the content, rather than the specific sources or the formats. The host-centric format of the current Internet does not support these requirements naturally. Neither do the recent data-centric naming proposals, since they rely on naming content based on raw byte-level hashing schemes. We argue that to meet these requirements, enabling content retrieval mechanisms to name and query directly for the underlying information is a good way forward. In addition to decoupling content from available sources and transfer protocols, these ""information-aware names"" or InfoNames explicitly decouple the information from content presentation factors as well. We envision an InfoName Resolution System (IRS) to resolve location based on InfoNames, while taking into account the operating conditions of devices. In this demo, we present an application to show how InfoNames can serve as presentation-invariant and portable names to fetch video content independent of device capabilities and resource constraints.",Proceedings of the ACM SIGCOMM 2010 Conference,455–456,2,"content retrieval, multimedia, information based architecture, naming, InfoName","New Delhi, India",SIGCOMM '10,,,,,,
1297,article,"Heller, Brandon and Erickson, David and McKeown, Nick and Griffith, Rean and Ganichev, Igor and Whyte, Scott and Zarifis, Kyriakos and Moon, Daekyeong and Shenker, Scott and Stuart, Stephen",Ripcord: A Modular Platform for Data Center Networking,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851261,10.1145/1851275.1851261,"In this demo, we present Ripcord, a modular platform for rapidly prototyping scale-out data center networks. Ripcord enables researchers to build and evaluate new network features and topologies, using only commercially available hardware and open-source software. The Ripcord demo will show three examples of custom network functions, operating together, on top of a 160-node cluster. The first is a routing engine that isolates classes of traffic. The second is a dynamic network manager than adjusts links and switch power states to reduce energy. The third is a statistics aggregator that supports network health monitoring and automatic alerts. The demo will be interactive, with a visualization of live parameters for each link and switch, such as bandwidth, drops, and power status, as well a control panel to modify the traffic load. We feel that an interactive demo is the best way to introduce the research community to Ripcord and get their feedback.",,457–458,2,"openflow, ripcord, data center network",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1298,inproceedings,"Heller, Brandon and Erickson, David and McKeown, Nick and Griffith, Rean and Ganichev, Igor and Whyte, Scott and Zarifis, Kyriakos and Moon, Daekyeong and Shenker, Scott and Stuart, Stephen",Ripcord: A Modular Platform for Data Center Networking,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851261,10.1145/1851182.1851261,"In this demo, we present Ripcord, a modular platform for rapidly prototyping scale-out data center networks. Ripcord enables researchers to build and evaluate new network features and topologies, using only commercially available hardware and open-source software. The Ripcord demo will show three examples of custom network functions, operating together, on top of a 160-node cluster. The first is a routing engine that isolates classes of traffic. The second is a dynamic network manager than adjusts links and switch power states to reduce energy. The third is a statistics aggregator that supports network health monitoring and automatic alerts. The demo will be interactive, with a visualization of live parameters for each link and switch, such as bandwidth, drops, and power status, as well a control panel to modify the traffic load. We feel that an interactive demo is the best way to introduce the research community to Ripcord and get their feedback.",Proceedings of the ACM SIGCOMM 2010 Conference,457–458,2,"ripcord, openflow, data center network","New Delhi, India",SIGCOMM '10,,,,,,
1299,article,"Lee, Jeongkeun and Tourrilhes, Jean and Sharma, Puneet and Banerjee, Sujata",No More Middlebox: Integrate Processing into Network,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851262,10.1145/1851275.1851262,"Traditionally, in-network services like firewall, proxy, cache, and transcoders have been provided by dedicated hardware middleboxes. A recent trend has been to remove the middleboxes by deploying the network services into switch/router-integrated computing modules or separate server/blade machines. In this abstract, by using a web Ad-insertion application as an example, we demonstrate our in-network processing (INP) framework that orchestrates various computing resources and network devices and enables seamless and efficient deployments of network services.",,459–460,2,"middlebox, controller, in-network processing",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1300,inproceedings,"Lee, Jeongkeun and Tourrilhes, Jean and Sharma, Puneet and Banerjee, Sujata",No More Middlebox: Integrate Processing into Network,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851262,10.1145/1851182.1851262,"Traditionally, in-network services like firewall, proxy, cache, and transcoders have been provided by dedicated hardware middleboxes. A recent trend has been to remove the middleboxes by deploying the network services into switch/router-integrated computing modules or separate server/blade machines. In this abstract, by using a web Ad-insertion application as an example, we demonstrate our in-network processing (INP) framework that orchestrates various computing resources and network devices and enables seamless and efficient deployments of network services.",Proceedings of the ACM SIGCOMM 2010 Conference,459–460,2,"middlebox, controller, in-network processing","New Delhi, India",SIGCOMM '10,,,,,,
1301,article,"Gabale, Vijay and Raman, Bhaskaran and Chebrolu, Kameswari and Kulkarni, Purushottam",LokVaani: Demonstrating Interactive Voice in Lo3,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851263,10.1145/1851275.1851263,"In this work, we consider the goal of enabling effective voice communication in a TDMA, multi-hop mesh network, using low cost and low power platforms. We consider two primary usage scenarios: (1) enabling a local voice communication within a village-like setting, in developing regions (2) supporting an on-site local communication among a team of users e.g. during emergency response systems.While there is plentiful literature on the use of TDMA for multi-hop wireless mesh networks, a practical multi-hop TDMA system remains elusive. Our contributions in this regard are three-fold. (1) We demonstrate the working of an 802.15.4-based low-cost, low-power, local communication system (referred as Lo3) using custom made handsets and off-the-shelf platforms. (2) We show the practicability of LiT: a full-fledged TDMA-based multi-hop, multi-channel MAC protocol for real-time applications; especially on resource constrained platforms, (3) We present implementation-based evaluations results of LiT and show that our protocol achieves practical synchronization, and robust operation in the face of wireless packet errors. As the part of the demo, we showcase LokVaani: an interactive voice application for local communication with the help of Lo3 prototype.",,461–462,2,"voice applications, 802.15.4, TDMA-based multi-hop mac",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1302,inproceedings,"Gabale, Vijay and Raman, Bhaskaran and Chebrolu, Kameswari and Kulkarni, Purushottam",LokVaani: Demonstrating Interactive Voice in Lo3,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851263,10.1145/1851182.1851263,"In this work, we consider the goal of enabling effective voice communication in a TDMA, multi-hop mesh network, using low cost and low power platforms. We consider two primary usage scenarios: (1) enabling a local voice communication within a village-like setting, in developing regions (2) supporting an on-site local communication among a team of users e.g. during emergency response systems.While there is plentiful literature on the use of TDMA for multi-hop wireless mesh networks, a practical multi-hop TDMA system remains elusive. Our contributions in this regard are three-fold. (1) We demonstrate the working of an 802.15.4-based low-cost, low-power, local communication system (referred as Lo3) using custom made handsets and off-the-shelf platforms. (2) We show the practicability of LiT: a full-fledged TDMA-based multi-hop, multi-channel MAC protocol for real-time applications; especially on resource constrained platforms, (3) We present implementation-based evaluations results of LiT and show that our protocol achieves practical synchronization, and robust operation in the face of wireless packet errors. As the part of the demo, we showcase LokVaani: an interactive voice application for local communication with the help of Lo3 prototype.",Proceedings of the ACM SIGCOMM 2010 Conference,461–462,2,"802.15.4, TDMA-based multi-hop mac, voice applications","New Delhi, India",SIGCOMM '10,,,,,,
1303,article,"Valancius, Vytautas and Kim, Hyojoon and Feamster, Nick",Transit Portal: BGP Connectivity as a Service,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851265,10.1145/1851275.1851265,,,463–464,2,"transit portal, bgp-mux",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1304,inproceedings,"Valancius, Vytautas and Kim, Hyojoon and Feamster, Nick",Transit Portal: BGP Connectivity as a Service,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851265,10.1145/1851182.1851265,,Proceedings of the ACM SIGCOMM 2010 Conference,463–464,2,"bgp-mux, transit portal","New Delhi, India",SIGCOMM '10,,,,,,
1305,article,"Chiba, Yasunobu and Shinohara, Yusuke and Shimonishi, Hideyuki",Source Flow: Handling Millions of Flows on Flow-Based Nodes,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851266,10.1145/1851275.1851266,"Flow-based networks such as OpenFlow-based networks have difficulty handling a large number of flows in a node due to the capacity limitation of search engine devices such as ternary content-addressable memory (TCAM). One typical solution of this problem would be to use MPLS-like tunneling, but this approach spoils the advantage of flow-by-flow path selection for load-balancing or QoS. We demonstrate a method named ""Source Flow"" that allows us to handle a huge amount of flows without changing the granularity of flows. By using our method, expensive and power consuming search engine devices can be removed from the core nodes, and the network can grow pretty scalable. In our demo, we construct a small network that consists of small number of OpenFlow switches, a single OpenFlow controller, and end-hosts. The hosts generate more than one million flows simultaneously and the flows are controlled on a per-flow-basis. All active flows are monitored and visualized on a user interface and the user interface allows audiences to confirm if our method is feasible and deployable.",,465–466,2,"future internet, flow-based network, openflow",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1306,inproceedings,"Chiba, Yasunobu and Shinohara, Yusuke and Shimonishi, Hideyuki",Source Flow: Handling Millions of Flows on Flow-Based Nodes,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851266,10.1145/1851182.1851266,"Flow-based networks such as OpenFlow-based networks have difficulty handling a large number of flows in a node due to the capacity limitation of search engine devices such as ternary content-addressable memory (TCAM). One typical solution of this problem would be to use MPLS-like tunneling, but this approach spoils the advantage of flow-by-flow path selection for load-balancing or QoS. We demonstrate a method named ""Source Flow"" that allows us to handle a huge amount of flows without changing the granularity of flows. By using our method, expensive and power consuming search engine devices can be removed from the core nodes, and the network can grow pretty scalable. In our demo, we construct a small network that consists of small number of OpenFlow switches, a single OpenFlow controller, and end-hosts. The hosts generate more than one million flows simultaneously and the flows are controlled on a per-flow-basis. All active flows are monitored and visualized on a user interface and the user interface allows audiences to confirm if our method is feasible and deployable.",Proceedings of the ACM SIGCOMM 2010 Conference,465–466,2,"future internet, openflow, flow-based network","New Delhi, India",SIGCOMM '10,,,,,,
1307,article,"Paredes-Oliva, Ignasi and Dimitropoulos, Xenofontas and Molina, Maurizio and Barlet-Ros, Pere and Brauckhoff, Daniela",Automating Root-Cause Analysis of Network Anomalies Using Frequent Itemset Mining,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851267,10.1145/1851275.1851267,"Finding the root-cause of a network security anomaly is essential for network operators. In our recent work, we introduced a generic technique that uses frequent itemset mining to automatically extract and summarize the traffic flows causing an anomaly. Our evaluation using two different anomaly detectors (including a commercial one) showed that our approach works surprisingly well extracting the anomalous flows in most studied cases using sampled and unsampled NetFlow traces from two networks. In this demonstration, we will showcase an open-source anomaly-extraction system based on our technique, which we integrated with a commercial anomaly detector and use in the NOC of the G\'{E",,467–468,2,"anomaly validation, association rules, anomaly extraction",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1308,inproceedings,"Paredes-Oliva, Ignasi and Dimitropoulos, Xenofontas and Molina, Maurizio and Barlet-Ros, Pere and Brauckhoff, Daniela",Automating Root-Cause Analysis of Network Anomalies Using Frequent Itemset Mining,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851267,10.1145/1851182.1851267,"Finding the root-cause of a network security anomaly is essential for network operators. In our recent work, we introduced a generic technique that uses frequent itemset mining to automatically extract and summarize the traffic flows causing an anomaly. Our evaluation using two different anomaly detectors (including a commercial one) showed that our approach works surprisingly well extracting the anomalous flows in most studied cases using sampled and unsampled NetFlow traces from two networks. In this demonstration, we will showcase an open-source anomaly-extraction system based on our technique, which we integrated with a commercial anomaly detector and use in the NOC of the G\'{E",Proceedings of the ACM SIGCOMM 2010 Conference,467–468,2,"anomaly extraction, association rules, anomaly validation","New Delhi, India",SIGCOMM '10,,,,,,
1309,article,"Zhang, Jiansong and Tan, Kun and Xiang, Sen and Yin, Qiufeng and Luo, Qi and He, Yong and Fang, Ji and Zhang, Yongguang",Experimenting Software Radio with the Sora Platform,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851268,10.1145/1851275.1851268,"Sora is a fully programmable, high performance software radio platform based on commodity general-purpose PC. In this demonstration, we illustrate the main features of the Sora platform that provide researchers flexible and powerful means to conduct wireless experiments at different levels with various goals. Specifically, the demonstrator will show four useful applications for wireless research that are built based on the Sora platform: 1) A capture tool that allows one to take a snapshot on a wireless channel; 2) a signal generation tool that allows one to transmit arbitrary baseband wave-form over the air, from a monophonic tone to a complex modulated frame; 3) an on-line real-time receiving application that uses the Sora User-Mode Extension; and 4) a fully featured Software radio WiFi driver (SoftWiFi) that can seamlessly inter-operate with commercial WiFi cards.",,469–470,2,"software radio, wireless experiment, Sora",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1310,inproceedings,"Zhang, Jiansong and Tan, Kun and Xiang, Sen and Yin, Qiufeng and Luo, Qi and He, Yong and Fang, Ji and Zhang, Yongguang",Experimenting Software Radio with the Sora Platform,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851268,10.1145/1851182.1851268,"Sora is a fully programmable, high performance software radio platform based on commodity general-purpose PC. In this demonstration, we illustrate the main features of the Sora platform that provide researchers flexible and powerful means to conduct wireless experiments at different levels with various goals. Specifically, the demonstrator will show four useful applications for wireless research that are built based on the Sora platform: 1) A capture tool that allows one to take a snapshot on a wireless channel; 2) a signal generation tool that allows one to transmit arbitrary baseband wave-form over the air, from a monophonic tone to a complex modulated frame; 3) an on-line real-time receiving application that uses the Sora User-Mode Extension; and 4) a fully featured Software radio WiFi driver (SoftWiFi) that can seamlessly inter-operate with commercial WiFi cards.",Proceedings of the ACM SIGCOMM 2010 Conference,469–470,2,"Sora, software radio, wireless experiment","New Delhi, India",SIGCOMM '10,,,,,,
1311,article,"Burnett, Sam and Feamster, Nick and Vempala, Santosh",Circumventing Censorship with Collage,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851269,10.1145/1851275.1851269,"Oppressive regimes and even democratic governments restrict Internet access. Existing anti-censorship systems often require users to connect through proxies, but these systems are relatively easy for a censor to discover and block. We explore a possible next step in the censorship arms race: rather than relying on a single system or set of proxies to circumvent censorship firewalls, we use the vast deployment of sites that host user-generated content to breach these firewalls. We have developed Collage, which allows users to exchange messages through hidden channels in sites that host user-generated content. To send a message, a user embeds it into cover traffic and posts the content on some site, where receivers retrieve this content. Collage makes it difficult for a censor to monitor or block these messages by exploiting the sheer number of sites where users can exchange messages and the variety of ways that a message can be hidden.We have built a censorship-resistant news reader using Collage that can retrieve from behind a censorship firewall and show Collage's effectiveness with a live demonstration of its complete infrastructure.",,471–472,2,"availability, censorship",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1312,inproceedings,"Burnett, Sam and Feamster, Nick and Vempala, Santosh",Circumventing Censorship with Collage,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851269,10.1145/1851182.1851269,"Oppressive regimes and even democratic governments restrict Internet access. Existing anti-censorship systems often require users to connect through proxies, but these systems are relatively easy for a censor to discover and block. We explore a possible next step in the censorship arms race: rather than relying on a single system or set of proxies to circumvent censorship firewalls, we use the vast deployment of sites that host user-generated content to breach these firewalls. We have developed Collage, which allows users to exchange messages through hidden channels in sites that host user-generated content. To send a message, a user embeds it into cover traffic and posts the content on some site, where receivers retrieve this content. Collage makes it difficult for a censor to monitor or block these messages by exploiting the sheer number of sites where users can exchange messages and the variety of ways that a message can be hidden.We have built a censorship-resistant news reader using Collage that can retrieve from behind a censorship firewall and show Collage's effectiveness with a live demonstration of its complete infrastructure.",Proceedings of the ACM SIGCOMM 2010 Conference,471–472,2,"censorship, availability","New Delhi, India",SIGCOMM '10,,,,,,
1313,article,"Xu, Tianyin and Chen, Yang and Fu, Xiaoming and Hui, Pan",Twittering by Cuckoo: Decentralized and Socio-Aware Online Microblogging Services,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851270,10.1145/1851275.1851270,"Online microblogging services, as exemplified by Twitter, have become immensely popular during the latest years. However, current microblogging systems severely suffer from performance bottlenecks and malicious attacks due to the centralized architecture. As a result, centralized microblogging systems may threaten the scalability, reliability as well as availability of the offered services, not to mention the high operational and maintenance cost.This demo presents a decentralized, socio-aware microblogging system named Cuckoo. The key aspects of Cuckoo's design is to take advantage of the inherent social relations while leveraging peer-to-peer (P2P) techniques in order to provide scalable, reliable microblogging services. The demo will show these aspects of Cuckoo and provide insights on the performance gain that decentralization and socio-awareness can bring for microblogging systems.",,473–474,2,"microblogging services, online social networking, peer-to-peer systems",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1314,inproceedings,"Xu, Tianyin and Chen, Yang and Fu, Xiaoming and Hui, Pan",Twittering by Cuckoo: Decentralized and Socio-Aware Online Microblogging Services,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851270,10.1145/1851182.1851270,"Online microblogging services, as exemplified by Twitter, have become immensely popular during the latest years. However, current microblogging systems severely suffer from performance bottlenecks and malicious attacks due to the centralized architecture. As a result, centralized microblogging systems may threaten the scalability, reliability as well as availability of the offered services, not to mention the high operational and maintenance cost.This demo presents a decentralized, socio-aware microblogging system named Cuckoo. The key aspects of Cuckoo's design is to take advantage of the inherent social relations while leveraging peer-to-peer (P2P) techniques in order to provide scalable, reliable microblogging services. The demo will show these aspects of Cuckoo and provide insights on the performance gain that decentralization and socio-awareness can bring for microblogging systems.",Proceedings of the ACM SIGCOMM 2010 Conference,473–474,2,"peer-to-peer systems, microblogging services, online social networking","New Delhi, India",SIGCOMM '10,,,,,,
1315,article,"He, Yong and Fang, Ji and Zhang, Jiansong and Shen, Haichen and Tan, Kun and Zhang, Yongguang",MPAP: Virtualization Architecture for Heterogenous Wireless APs,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851271,10.1145/1851275.1851271,"This demonstration shows a novel virtualization architecture, called Multi-Purpose Access Point (MPAP), which can virtualize multiple heterogenous wireless standards based on software radio. The basic idea is to deploy a wide-band radio front-end to receive wireless signals from all wireless standards sharing the same spectrum band, and use separate software base-bands to demodulate information stream for each wireless standard. Based on software radio, MPAP consolidates multiple wireless devices into single hardware platform, and allows them to share the same general-purpose computing resource. Different software base-bands can easily communicate and coordinate with one another. Thus, it also provides better coexistence among heterogenous wireless standards. As an example, we demonstrate to use non-contiguous OFDM in 802.11g PHY to avoid the mutual interference with narrow-band ZigBee communication.",,475–476,2,"virtualization, software radio, wireless, Sora",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1316,inproceedings,"He, Yong and Fang, Ji and Zhang, Jiansong and Shen, Haichen and Tan, Kun and Zhang, Yongguang",MPAP: Virtualization Architecture for Heterogenous Wireless APs,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851271,10.1145/1851182.1851271,"This demonstration shows a novel virtualization architecture, called Multi-Purpose Access Point (MPAP), which can virtualize multiple heterogenous wireless standards based on software radio. The basic idea is to deploy a wide-band radio front-end to receive wireless signals from all wireless standards sharing the same spectrum band, and use separate software base-bands to demodulate information stream for each wireless standard. Based on software radio, MPAP consolidates multiple wireless devices into single hardware platform, and allows them to share the same general-purpose computing resource. Different software base-bands can easily communicate and coordinate with one another. Thus, it also provides better coexistence among heterogenous wireless standards. As an example, we demonstrate to use non-contiguous OFDM in 802.11g PHY to avoid the mutual interference with narrow-band ZigBee communication.",Proceedings of the ACM SIGCOMM 2010 Conference,475–476,2,"virtualization, software radio, Sora, wireless","New Delhi, India",SIGCOMM '10,,,,,,
1317,article,"Aggarwal, Bhavish and Chitnis, Pushkar and Dey, Amit and Jain, Kamal and Navda, Vishnu and Padmanabhan, Venkata N. and Ramjee, Ramachandran and Schulman, Aaron and Spring, Neil",Stratus: Energy-Efficient Mobile Communication Using Cloud Support,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851272,10.1145/1851275.1851272,"Cellular radio communication is a significant contributor to battery energy drain on smartphones, in some cases inflating the energy cost by a factor of 5 or more compared to the energy cost of the base device. Stratus is a system to reduce this energy consumption by leveraging cloud resources to make data communication on smartphones more efficient. Using a cloud-based proxy, Stratus employs optimizations that adapt an application's incoming and outgoing traffic to better match the energy characteristics of the radio interface. The optimizations include (a) aggregation to bunch up sporadic transmissions, (b) asymmetric dictionary-based compression to reduce the number of bits transmitted over the air, and (c) opportunistic scheduling to avoid communication during periods of poor signal reception. These optimizations can be used individually, or in combination, subject to an application's delay tolerance. For example, using our Stratus prototype, the aggregation and compression optimizations together achieve up to 50% energy savings for web browsing, while the aggregation and scheduling optimizations together achieve up to 35% energy savings for a media streaming application.",,477–478,2,"energy, cloud proxy, smartphone",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1318,inproceedings,"Aggarwal, Bhavish and Chitnis, Pushkar and Dey, Amit and Jain, Kamal and Navda, Vishnu and Padmanabhan, Venkata N. and Ramjee, Ramachandran and Schulman, Aaron and Spring, Neil",Stratus: Energy-Efficient Mobile Communication Using Cloud Support,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851272,10.1145/1851182.1851272,"Cellular radio communication is a significant contributor to battery energy drain on smartphones, in some cases inflating the energy cost by a factor of 5 or more compared to the energy cost of the base device. Stratus is a system to reduce this energy consumption by leveraging cloud resources to make data communication on smartphones more efficient. Using a cloud-based proxy, Stratus employs optimizations that adapt an application's incoming and outgoing traffic to better match the energy characteristics of the radio interface. The optimizations include (a) aggregation to bunch up sporadic transmissions, (b) asymmetric dictionary-based compression to reduce the number of bits transmitted over the air, and (c) opportunistic scheduling to avoid communication during periods of poor signal reception. These optimizations can be used individually, or in combination, subject to an application's delay tolerance. For example, using our Stratus prototype, the aggregation and compression optimizations together achieve up to 50% energy savings for web browsing, while the aggregation and scheduling optimizations together achieve up to 35% energy savings for a media streaming application.",Proceedings of the ACM SIGCOMM 2010 Conference,477–478,2,"energy, smartphone, cloud proxy","New Delhi, India",SIGCOMM '10,,,,,,
1319,article,"Tsiftes, Nicolas and Eriksson, Joakim and Finne, Niclas and \""{O","A Framework for Low-Power IPv6 Routing Simulation, Experimentation, and Evaluation",2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851273,10.1145/1851275.1851273,"Low-power networked devices, such as sensors and actuators, are becoming a vital part of our everyday infrastructure. Being networked, the continued development of these systems needs involvement from the networking community. We present a framework for simulation, experimentation, and evaluation of routing mechanisms for low-power IPv6 networking. The framework provides a detailed simulation environment for low-power routing mechanisms, and allows the system to be directly uploaded to a physical testbed for experimental measurements.",,479–480,2,"RPL, routing, wireless, IPv6, low-power",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1320,inproceedings,"Tsiftes, Nicolas and Eriksson, Joakim and Finne, Niclas and \""{O","A Framework for Low-Power IPv6 Routing Simulation, Experimentation, and Evaluation",2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851273,10.1145/1851182.1851273,"Low-power networked devices, such as sensors and actuators, are becoming a vital part of our everyday infrastructure. Being networked, the continued development of these systems needs involvement from the networking community. We present a framework for simulation, experimentation, and evaluation of routing mechanisms for low-power IPv6 networking. The framework provides a detailed simulation environment for low-power routing mechanisms, and allows the system to be directly uploaded to a physical testbed for experimental measurements.",Proceedings of the ACM SIGCOMM 2010 Conference,479–480,2,"wireless, low-power, routing, RPL, IPv6","New Delhi, India",SIGCOMM '10,,,,,,
1321,article,"Bayer, Nico and Loziak, Krzysztof and Garcia-Saavedra, Andres and Sengul, Cigdem and Serrano, Pablo",CARMEN: Resource Management and Abstraction in Wireless Heterogeneous Mesh Networks,2010,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851275.1851274,10.1145/1851275.1851274,"Even though current mesh networks are mostly WiFi-based, future networks are expected to be highly heterogeneous. Motivated by this expectation, CARMEN (CARrier grade MEsh Networks) project focuses on developing a heterogeneous mesh backhaul to provide carrier-grade (voice, video and data) services. This demo presents resource management and abstraction in CARMEN architecture, which allow meeting the challenges of heterogeneous radio access.",,481–482,2,"wireless mesh, heterogeneous, resource abstraction",,,October 2010,40,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1322,inproceedings,"Bayer, Nico and Loziak, Krzysztof and Garcia-Saavedra, Andres and Sengul, Cigdem and Serrano, Pablo",CARMEN: Resource Management and Abstraction in Wireless Heterogeneous Mesh Networks,2010,9781450302012,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1851182.1851274,10.1145/1851182.1851274,"Even though current mesh networks are mostly WiFi-based, future networks are expected to be highly heterogeneous. Motivated by this expectation, CARMEN (CARrier grade MEsh Networks) project focuses on developing a heterogeneous mesh backhaul to provide carrier-grade (voice, video and data) services. This demo presents resource management and abstraction in CARMEN architecture, which allow meeting the challenges of heterogeneous radio access.",Proceedings of the ACM SIGCOMM 2010 Conference,481–482,2,"heterogeneous, wireless mesh, resource abstraction","New Delhi, India",SIGCOMM '10,,,,,,
1323,inproceedings,"Crowcroft, Jon",The Internet of Ideas,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592569,10.1145/1592568.1592569,"As researchers we have a duty to communicate our ideas. As communications researchers, we are privileged to study the technology and phenomena that are so transforming human society. We seek to understand and enhance the very tools with which we can convey our contributions to human knowledge.In Japan there is a mode of presentation called, Pecha Kucha, where speakers present 20 images for 20 seconds each, as a way to convey new experimental thoughts effectively. I am going to present 12 ideas, each one in that spirit. Rather than a retrospective of my work, or a vision for a possible future Internet Architectures, I want to engage in what I do in my daily work with graduate students, thinking of problems that need solving in existing communications systems, or simply new ways to build communications systems.I do not expect more than one or two of these ideas to make it past first base, but there is value in learning how to apply one's critical faculties even when filtering out bad ideas. Metcalfe's law is that the value of the network is the square of the number of participants since each participant contributes new input for all other participants. In an Internet of ideas, then, this value is only achieved if we contribute our ideas freely.Researchers censor ideas for various reasons, including that a new communications system must have a business case. The Internet itself (the Web, Facebook and Twitter) was created without any business case. Many startups end up in a business that is nothing to do with the one-pager or elevator pitch they presented to the venture capitalists. We have no business filtering out ideas except if they are simply bad.So here are my twelve research elevator pitches.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,1–2,2,protocol experiments,"Barcelona, Spain",SIGCOMM '09,,,,,,
1324,article,"Crowcroft, Jon",The Internet of Ideas,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592569,10.1145/1594977.1592569,"As researchers we have a duty to communicate our ideas. As communications researchers, we are privileged to study the technology and phenomena that are so transforming human society. We seek to understand and enhance the very tools with which we can convey our contributions to human knowledge.In Japan there is a mode of presentation called, Pecha Kucha, where speakers present 20 images for 20 seconds each, as a way to convey new experimental thoughts effectively. I am going to present 12 ideas, each one in that spirit. Rather than a retrospective of my work, or a vision for a possible future Internet Architectures, I want to engage in what I do in my daily work with graduate students, thinking of problems that need solving in existing communications systems, or simply new ways to build communications systems.I do not expect more than one or two of these ideas to make it past first base, but there is value in learning how to apply one's critical faculties even when filtering out bad ideas. Metcalfe's law is that the value of the network is the square of the number of participants since each participant contributes new input for all other participants. In an Internet of ideas, then, this value is only achieved if we contribute our ideas freely.Researchers censor ideas for various reasons, including that a new communications system must have a business case. The Internet itself (the Web, Facebook and Twitter) was created without any business case. Many startups end up in a business that is nothing to do with the one-pager or elevator pitch they presented to the venture capitalists. We have no business filtering out ideas except if they are simply bad.So here are my twelve research elevator pitches.",,1–2,2,protocol experiments,,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1325,inproceedings,"Vutukuru, Mythili and Balakrishnan, Hari and Jamieson, Kyle",Cross-Layer Wireless Bit Rate Adaptation,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592571,10.1145/1592568.1592571,"This paper presents SoftRate, a wireless bit rate adaptation protocol that is responsive to rapidly varying channel conditions. Unlike previous work that uses either frame receptions or signal-to-noise ratio (SNR) estimates to select bit rates, SoftRate uses confidence information calculated by the physical layer and exported to higher layers via the SoftPHY interface to estimate the prevailing channel bit error rate (BER). Senders use this BER estimate, calculated over each received packet (even when the packet has no bit errors), to pick good bit rates. SoftRate's novel BER computation works across different wireless environments and hardware without requiring any retraining. SoftRate also uses abrupt changes in the BER estimate to identify interference, enabling it to reduce the bit rate only in response to channel errors caused by attenuation or fading. Our experiments conducted using a software radio prototype show that SoftRate achieves 2X higher throughput than popular frame-level protocols such as SampleRate and RRAA. It also achieves 20% more throughput than an SNR-based protocol trained on the operating environment, and up to 4X higher throughput than an untrained SNR-based protocol. The throughput gains using SoftRate stem from its ability to react to channel variations within a single packet-time and its robustness to collision losses.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,3–14,12,"cross-layer, bit rate adaptation, SoftPHY, wireless","Barcelona, Spain",SIGCOMM '09,,,,,,
1326,article,"Vutukuru, Mythili and Balakrishnan, Hari and Jamieson, Kyle",Cross-Layer Wireless Bit Rate Adaptation,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592571,10.1145/1594977.1592571,"This paper presents SoftRate, a wireless bit rate adaptation protocol that is responsive to rapidly varying channel conditions. Unlike previous work that uses either frame receptions or signal-to-noise ratio (SNR) estimates to select bit rates, SoftRate uses confidence information calculated by the physical layer and exported to higher layers via the SoftPHY interface to estimate the prevailing channel bit error rate (BER). Senders use this BER estimate, calculated over each received packet (even when the packet has no bit errors), to pick good bit rates. SoftRate's novel BER computation works across different wireless environments and hardware without requiring any retraining. SoftRate also uses abrupt changes in the BER estimate to identify interference, enabling it to reduce the bit rate only in response to channel errors caused by attenuation or fading. Our experiments conducted using a software radio prototype show that SoftRate achieves 2X higher throughput than popular frame-level protocols such as SampleRate and RRAA. It also achieves 20% more throughput than an SNR-based protocol trained on the operating environment, and up to 4X higher throughput than an untrained SNR-based protocol. The throughput gains using SoftRate stem from its ability to react to channel variations within a single packet-time and its robustness to collision losses.",,3–14,12,"cross-layer, bit rate adaptation, SoftPHY, wireless",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1327,inproceedings,"Dutta, Aveek and Saha, Dola and Grunwald, Dirk and Sicker, Douglas",SMACK: A SMart ACKnowledgment Scheme for Broadcast Messages in Wireless Networks,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592572,10.1145/1592568.1592572,"Network protocol designers, both at the physical and network level, have long considered interference and simultaneous transmission in wireless protocols as a problem to be avoided. This, coupled with a tendency to emulate wired network protocols in the wireless domain, has led to artificial limitations in wireless networks.In this paper, we argue that wireless protocols can exploit simultaneous transmission to reduce the cost of reliable multicast by orders of magnitude. With an appropriate application interface, simultaneous transmission can also greatly speed up common group communication primitives, such as anycast, broadcast, leader election and others.The proposed method precisely fits into the domain of directly reachable nodes where many group communication mechanisms are commonly used in routing protocols and other physical-layer mechanisms. We demonstrate how simultaneous transmission can be used to implement a reliable broadcast for an infrastructure and peer-to-peer network using a prototype reconfigurable hardware. We also validate the notion of using simple spectrum sensing techniques to distinguish multiple transmissions. We then describe how the mechanism can be extended to solve group communication problems and the challenges inherent to build innovative protocols which are faster and reliable at the same time.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,15–26,12,"software defined radio, orthogonal frequency division multiplexing","Barcelona, Spain",SIGCOMM '09,,,,,,
1328,article,"Dutta, Aveek and Saha, Dola and Grunwald, Dirk and Sicker, Douglas",SMACK: A SMart ACKnowledgment Scheme for Broadcast Messages in Wireless Networks,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592572,10.1145/1594977.1592572,"Network protocol designers, both at the physical and network level, have long considered interference and simultaneous transmission in wireless protocols as a problem to be avoided. This, coupled with a tendency to emulate wired network protocols in the wireless domain, has led to artificial limitations in wireless networks.In this paper, we argue that wireless protocols can exploit simultaneous transmission to reduce the cost of reliable multicast by orders of magnitude. With an appropriate application interface, simultaneous transmission can also greatly speed up common group communication primitives, such as anycast, broadcast, leader election and others.The proposed method precisely fits into the domain of directly reachable nodes where many group communication mechanisms are commonly used in routing protocols and other physical-layer mechanisms. We demonstrate how simultaneous transmission can be used to implement a reliable broadcast for an infrastructure and peer-to-peer network using a prototype reconfigurable hardware. We also validate the notion of using simple spectrum sensing techniques to distinguish multiple transmissions. We then describe how the mechanism can be extended to solve group communication problems and the challenges inherent to build innovative protocols which are faster and reliable at the same time.",,15–26,12,"software defined radio, orthogonal frequency division multiplexing",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1329,inproceedings,"Bahl, Paramvir and Chandra, Ranveer and Moscibroda, Thomas and Murty, Rohan and Welsh, Matt",White Space Networking with Wi-Fi like Connectivity,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592573,10.1145/1592568.1592573,"Networking over UHF white spaces is fundamentally different from conventional Wi-Fi along three axes: spatial variation, temporal variation, and fragmentation of the UHF spectrum. Each of these differences gives rise to new challenges for implementing a wireless network in this band. We present the design and implementation of Net7, the first Wi-Fi like system constructed on top of UHF white spaces. Net7 incorporates a new adaptive spectrum assignment algorithm to handle spectrum variation and fragmentation, and proposes a low overhead protocol to handle temporal variation. builds on a simple technique, called SIFT, that reduces the time to detect transmissions in variable channel width systems by analyzing raw signals in the time domain. We provide an extensive evaluation of the system in terms of a prototype implementation and detailed experimental and simulation results.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,27–38,12,"dynamic spectrum access, white spaces, wi-fi, cognitive radios, channel width","Barcelona, Spain",SIGCOMM '09,,,,,,
1330,article,"Bahl, Paramvir and Chandra, Ranveer and Moscibroda, Thomas and Murty, Rohan and Welsh, Matt",White Space Networking with Wi-Fi like Connectivity,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592573,10.1145/1594977.1592573,"Networking over UHF white spaces is fundamentally different from conventional Wi-Fi along three axes: spatial variation, temporal variation, and fragmentation of the UHF spectrum. Each of these differences gives rise to new challenges for implementing a wireless network in this band. We present the design and implementation of Net7, the first Wi-Fi like system constructed on top of UHF white spaces. Net7 incorporates a new adaptive spectrum assignment algorithm to handle spectrum variation and fragmentation, and proposes a low overhead protocol to handle temporal variation. builds on a simple technique, called SIFT, that reduces the time to detect transmissions in variable channel width systems by analyzing raw signals in the time domain. We provide an extensive evaluation of the system in terms of a prototype implementation and detailed experimental and simulation results.",,27–38,12,"dynamic spectrum access, wi-fi, channel width, white spaces, cognitive radios",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1331,inproceedings,"Niranjan Mysore, Radhika and Pamboris, Andreas and Farrington, Nathan and Huang, Nelson and Miri, Pardis and Radhakrishnan, Sivasankar and Subramanya, Vikram and Vahdat, Amin",PortLand: A Scalable Fault-Tolerant Layer 2 Data Center Network Fabric,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592575,10.1145/1592568.1592575,"This paper considers the requirements for a scalable, easily manageable, fault-tolerant, and efficient data center network fabric. Trends in multi-core processors, end-host virtualization, and commodities of scale are pointing to future single-site data centers with millions of virtual end points. Existing layer 2 and layer 3 network protocols face some combination of limitations in such a setting: lack of scalability, difficult management, inflexible communication, or limited support for virtual machine migration. To some extent, these limitations may be inherent for Ethernet/IP style protocols when trying to support arbitrary topologies. We observe that data center networks are often managed as a single logical network fabric with a known baseline topology and growth model. We leverage this observation in the design and implementation of PortLand, a scalable, fault tolerant layer 2 routing and forwarding protocol for data center environments. Through our implementation and evaluation, we show that PortLand holds promise for supporting a ``plug-and-play"" large-scale, data center network.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,39–50,12,"data center network fabric, layer 2 routing in data centers","Barcelona, Spain",SIGCOMM '09,,,,,,
1332,article,"Niranjan Mysore, Radhika and Pamboris, Andreas and Farrington, Nathan and Huang, Nelson and Miri, Pardis and Radhakrishnan, Sivasankar and Subramanya, Vikram and Vahdat, Amin",PortLand: A Scalable Fault-Tolerant Layer 2 Data Center Network Fabric,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592575,10.1145/1594977.1592575,"This paper considers the requirements for a scalable, easily manageable, fault-tolerant, and efficient data center network fabric. Trends in multi-core processors, end-host virtualization, and commodities of scale are pointing to future single-site data centers with millions of virtual end points. Existing layer 2 and layer 3 network protocols face some combination of limitations in such a setting: lack of scalability, difficult management, inflexible communication, or limited support for virtual machine migration. To some extent, these limitations may be inherent for Ethernet/IP style protocols when trying to support arbitrary topologies. We observe that data center networks are often managed as a single logical network fabric with a known baseline topology and growth model. We leverage this observation in the design and implementation of PortLand, a scalable, fault tolerant layer 2 routing and forwarding protocol for data center environments. Through our implementation and evaluation, we show that PortLand holds promise for supporting a ``plug-and-play"" large-scale, data center network.",,39–50,12,"layer 2 routing in data centers, data center network fabric",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1333,inproceedings,"Greenberg, Albert and Hamilton, James R. and Jain, Navendu and Kandula, Srikanth and Kim, Changhoon and Lahiri, Parantap and Maltz, David A. and Patel, Parveen and Sengupta, Sudipta",VL2: A Scalable and Flexible Data Center Network,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592576,10.1145/1592568.1592576,"To be agile and cost effective, data centers should allow dynamic resource allocation across large server pools. In particular, the data center network should enable any server to be assigned to any service. To meet these goals, we present VL2, a practical network architecture that scales to support huge data centers with uniform high capacity between servers, performance isolation between services, and Ethernet layer-2 semantics. VL2 uses (1) flat addressing to allow service instances to be placed anywhere in the network, (2) Valiant Load Balancing to spread traffic uniformly across network paths, and (3) end-system based address resolution to scale to large server pools, without introducing complexity to the network control plane. VL2's design is driven by detailed measurements of traffic and fault data from a large operational cloud service provider. VL2's implementation leverages proven network technologies, already available at low cost in high-speed hardware implementations, to build a scalable and reliable network architecture. As a result, VL2 networks can be deployed today, and we have built a working prototype. We evaluate the merits of the VL2 design using measurement, analysis, and experiments. Our VL2 prototype shuffles 2.7 TB of data among 75 servers in 395 seconds - sustaining a rate that is 94% of the maximum possible.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,51–62,12,"data center network, commoditization","Barcelona, Spain",SIGCOMM '09,,,,,,
1334,article,"Greenberg, Albert and Hamilton, James R. and Jain, Navendu and Kandula, Srikanth and Kim, Changhoon and Lahiri, Parantap and Maltz, David A. and Patel, Parveen and Sengupta, Sudipta",VL2: A Scalable and Flexible Data Center Network,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592576,10.1145/1594977.1592576,"To be agile and cost effective, data centers should allow dynamic resource allocation across large server pools. In particular, the data center network should enable any server to be assigned to any service. To meet these goals, we present VL2, a practical network architecture that scales to support huge data centers with uniform high capacity between servers, performance isolation between services, and Ethernet layer-2 semantics. VL2 uses (1) flat addressing to allow service instances to be placed anywhere in the network, (2) Valiant Load Balancing to spread traffic uniformly across network paths, and (3) end-system based address resolution to scale to large server pools, without introducing complexity to the network control plane. VL2's design is driven by detailed measurements of traffic and fault data from a large operational cloud service provider. VL2's implementation leverages proven network technologies, already available at low cost in high-speed hardware implementations, to build a scalable and reliable network architecture. As a result, VL2 networks can be deployed today, and we have built a working prototype. We evaluate the merits of the VL2 design using measurement, analysis, and experiments. Our VL2 prototype shuffles 2.7 TB of data among 75 servers in 395 seconds - sustaining a rate that is 94% of the maximum possible.",,51–62,12,"commoditization, data center network",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1335,inproceedings,"Guo, Chuanxiong and Lu, Guohan and Li, Dan and Wu, Haitao and Zhang, Xuan and Shi, Yunfeng and Tian, Chen and Zhang, Yongguang and Lu, Songwu","BCube: A High Performance, Server-Centric Network Architecture for Modular Data Centers",2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592577,10.1145/1592568.1592577,"This paper presents BCube, a new network architecture specifically designed for shipping-container based, modular data centers. At the core of the BCube architecture is its server-centric network structure, where servers with multiple network ports connect to multiple layers of COTS (commodity off-the-shelf) mini-switches. Servers act as not only end hosts, but also relay nodes for each other. BCube supports various bandwidth-intensive applications by speeding-up one-to-one, one-to-several, and one-to-all traffic patterns, and by providing high network capacity for all-to-all traffic.BCube exhibits graceful performance degradation as the server and/or switch failure rate increases. This property is of special importance for shipping-container data centers, since once the container is sealed and operational, it becomes very difficult to repair or replace its components.Our implementation experiences show that BCube can be seamlessly integrated with the TCP/IP protocol stack and BCube packet forwarding can be efficiently implemented in both hardware and software. Experiments in our testbed demonstrate that BCube is fault tolerant and load balancing and it significantly accelerates representative bandwidth-intensive applications.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,63–74,12,"multi-path, server-centric network, modular data center","Barcelona, Spain",SIGCOMM '09,,,,,,
1336,article,"Guo, Chuanxiong and Lu, Guohan and Li, Dan and Wu, Haitao and Zhang, Xuan and Shi, Yunfeng and Tian, Chen and Zhang, Yongguang and Lu, Songwu","BCube: A High Performance, Server-Centric Network Architecture for Modular Data Centers",2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592577,10.1145/1594977.1592577,"This paper presents BCube, a new network architecture specifically designed for shipping-container based, modular data centers. At the core of the BCube architecture is its server-centric network structure, where servers with multiple network ports connect to multiple layers of COTS (commodity off-the-shelf) mini-switches. Servers act as not only end hosts, but also relay nodes for each other. BCube supports various bandwidth-intensive applications by speeding-up one-to-one, one-to-several, and one-to-all traffic patterns, and by providing high network capacity for all-to-all traffic.BCube exhibits graceful performance degradation as the server and/or switch failure rate increases. This property is of special importance for shipping-container data centers, since once the container is sealed and operational, it becomes very difficult to repair or replace its components.Our implementation experiences show that BCube can be seamlessly integrated with the TCP/IP protocol stack and BCube packet forwarding can be efficiently implemented in both hardware and software. Experiments in our testbed demonstrate that BCube is fault tolerant and load balancing and it significantly accelerates representative bandwidth-intensive applications.",,63–74,12,"modular data center, server-centric network, multi-path",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1337,inproceedings,"Xie, Yinglian and Yu, Fang and Abadi, Martin",De-Anonymizing the Internet Using Unreliable IDs,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592579,10.1145/1592568.1592579,"Today's Internet is open and anonymous. While it permits free traffic from any host, attackers that generate malicious traffic cannot typically be held accountable. In this paper, we present a system called HostTracker that tracks dynamic bindings between hosts and IP addresses by leveraging application-level data with unreliable IDs. Using a month-long user login trace from a large email provider, we show that HostTracker can attribute most of the activities reliably to the responsible hosts, despite the existence of dynamic IP addresses, proxies, and NATs. With this information, we are able to analyze the host population, to conduct forensic analysis, and also to blacklist malicious hosts dynamically.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,75–86,12,"attack forensics, botnet, blacklist, accountability","Barcelona, Spain",SIGCOMM '09,,,,,,
1338,article,"Xie, Yinglian and Yu, Fang and Abadi, Martin",De-Anonymizing the Internet Using Unreliable IDs,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592579,10.1145/1594977.1592579,"Today's Internet is open and anonymous. While it permits free traffic from any host, attackers that generate malicious traffic cannot typically be held accountable. In this paper, we present a system called HostTracker that tracks dynamic bindings between hosts and IP addresses by leveraging application-level data with unreliable IDs. Using a month-long user login trace from a large email provider, we show that HostTracker can attribute most of the activities reliably to the responsible hosts, despite the existence of dynamic IP addresses, proxies, and NATs. With this information, we are able to analyze the host population, to conduct forensic analysis, and also to blacklist malicious hosts dynamically.",,75–86,12,"blacklist, attack forensics, botnet, accountability",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1339,inproceedings,"Anand, Ashok and Sekar, Vyas and Akella, Aditya",SmartRE: An Architecture for Coordinated Network-Wide Redundancy Elimination,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592580,10.1145/1592568.1592580,"Application-independent Redundancy Elimination (RE), or identifying and removing repeated content from network transfers, has been used with great success for improving network performance on enterprise access links. Recently, there is growing interest for supporting RE as a network-wide service. Such a network-wide RE service benefits ISPs by reducing link loads and increasing the effective network capacity to better accommodate the increasing number of bandwidth-intensive applications. Further, a networkwide RE service democratizes the benefits of RE to all end-to-end traffic and improves application performance by increasing throughput and reducing latencies.While the vision of a network-wide RE service is appealing, realizing it in practice is challenging. In particular, extending single vantage-point RE solutions designed for enterprise access links to the network-wide case is inefficient and/or requires modifying routing policies. We present SmartRE, a practical and efficient architecture for network-wide RE. We show that SmartRE can enable more effective utilization of the available resources at network devices, and thus can magnify the overall benefits of network-wide RE. We prototype our algorithms using Click and test our framework extensively using several real and synthetic traces.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,87–98,12,"redundancy elimination, caching","Barcelona, Spain",SIGCOMM '09,,,,,,
1340,article,"Anand, Ashok and Sekar, Vyas and Akella, Aditya",SmartRE: An Architecture for Coordinated Network-Wide Redundancy Elimination,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592580,10.1145/1594977.1592580,"Application-independent Redundancy Elimination (RE), or identifying and removing repeated content from network transfers, has been used with great success for improving network performance on enterprise access links. Recently, there is growing interest for supporting RE as a network-wide service. Such a network-wide RE service benefits ISPs by reducing link loads and increasing the effective network capacity to better accommodate the increasing number of bandwidth-intensive applications. Further, a networkwide RE service democratizes the benefits of RE to all end-to-end traffic and improves application performance by increasing throughput and reducing latencies.While the vision of a network-wide RE service is appealing, realizing it in practice is challenging. In particular, extending single vantage-point RE solutions designed for enterprise access links to the network-wide case is inefficient and/or requires modifying routing policies. We present SmartRE, a practical and efficient architecture for network-wide RE. We show that SmartRE can enable more effective utilization of the available resources at network devices, and thus can magnify the overall benefits of network-wide RE. We prototype our algorithms using Click and test our framework extensively using several real and synthetic traces.",,87–98,12,"caching, redundancy elimination",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1341,inproceedings,"Dhananjay, Aditya and Zhang, Hui and Li, Jinyang and Subramanian, Lakshminarayanan","Practical, Distributed Channel Assignment and Routing in Dual-Radio Mesh Networks",2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592581,10.1145/1592568.1592581,"Realizing the full potential of a multi-radio mesh network involves two main challenges: how to assign channels to radios at each node to minimize interference and how to choose high throughput routing paths in the face of lossy links, variable channel conditions and external load. This paper presents ROMA, a practical, distributed channel assignment and routing protocol that achieves good multi-hop path performance between every node and one or more designated gateway nodes in a dual-radio network. ROMA assigns non-overlapping channels to links along each gateway path to eliminate intra-path interference. ROMA reduces inter-path interference by assigning different channels to paths destined for different gateways whenever possible. Evaluations on a 24-node dual-radio testbed show that ROMA achieves high throughput in a variety of scenarios.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,99–110,12,"routing, wireless, channel assignment","Barcelona, Spain",SIGCOMM '09,,,,,,
1342,article,"Dhananjay, Aditya and Zhang, Hui and Li, Jinyang and Subramanian, Lakshminarayanan","Practical, Distributed Channel Assignment and Routing in Dual-Radio Mesh Networks",2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592581,10.1145/1594977.1592581,"Realizing the full potential of a multi-radio mesh network involves two main challenges: how to assign channels to radios at each node to minimize interference and how to choose high throughput routing paths in the face of lossy links, variable channel conditions and external load. This paper presents ROMA, a practical, distributed channel assignment and routing protocol that achieves good multi-hop path performance between every node and one or more designated gateway nodes in a dual-radio network. ROMA assigns non-overlapping channels to links along each gateway path to eliminate intra-path interference. ROMA reduces inter-path interference by assigning different channels to paths destined for different gateways whenever possible. Evaluations on a 24-node dual-radio testbed show that ROMA achieves high throughput in a variety of scenarios.",,99–110,12,"routing, channel assignment, wireless",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1343,inproceedings,"Godfrey, P. Brighten and Ganichev, Igor and Shenker, Scott and Stoica, Ion",Pathlet Routing,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592583,10.1145/1592568.1592583,"We present a new routing protocol, pathlet routing, in which networks advertise fragments of paths, called pathlets, that sources concatenate into end-to-end source routes. Intuitively, the pathlet is a highly flexible building block, capturing policy constraints as well as enabling an exponentially large number of path choices. In particular, we show that pathlet routing can emulate the policies of BGP, source routing, and several recent multipath proposals. This flexibility lets us address two major challenges for Internet routing: scalability and source-controlled routing. When a router's routing policy has only ""local"" constraints, it can be represented using a small number of pathlets, leading to very small forwarding tables and many choices of routes for senders. Crucially, pathlet routing does not impose a global requirement on what style of policy is used, but rather allows multiple styles to coexist. The protocol thus supports complex routing policies while enabling and incentivizing the adoption of policies that yield small forwarding plane state and a high degree of path choice.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,111–122,12,"forwarding table, pathlet, routing, scaling, source routing, multipath","Barcelona, Spain",SIGCOMM '09,,,,,,
1344,article,"Godfrey, P. Brighten and Ganichev, Igor and Shenker, Scott and Stoica, Ion",Pathlet Routing,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592583,10.1145/1594977.1592583,"We present a new routing protocol, pathlet routing, in which networks advertise fragments of paths, called pathlets, that sources concatenate into end-to-end source routes. Intuitively, the pathlet is a highly flexible building block, capturing policy constraints as well as enabling an exponentially large number of path choices. In particular, we show that pathlet routing can emulate the policies of BGP, source routing, and several recent multipath proposals. This flexibility lets us address two major challenges for Internet routing: scalability and source-controlled routing. When a router's routing policy has only ""local"" constraints, it can be represented using a small number of pathlets, leading to very small forwarding tables and many choices of routes for senders. Crucially, pathlet routing does not impose a global requirement on what style of policy is used, but rather allows multiple styles to coexist. The protocol thus supports complex routing policies while enabling and incentivizing the adoption of policies that yield small forwarding plane state and a high degree of path choice.",,111–122,12,"routing, multipath, scaling, source routing, pathlet, forwarding table",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1345,inproceedings,"Qureshi, Asfandyar and Weber, Rick and Balakrishnan, Hari and Guttag, John and Maggs, Bruce",Cutting the Electric Bill for Internet-Scale Systems,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592584,10.1145/1592568.1592584,"Energy expenses are becoming an increasingly important fraction of data center operating costs. At the same time, the energy expense per unit of computation can vary significantly between two different locations. In this paper, we characterize the variation due to fluctuating electricity prices and argue that existing distributed systems should be able to exploit this variation for significant economic gains. Electricity prices exhibit both temporal and geographic variation, due to regional demand differences, transmission inefficiencies, and generation diversity. Starting with historical electricity prices, for twenty nine locations in the US, and network traffic data collected on Akamai's CDN, we use simulation to quantify the possible economic gains for a realistic workload. Our results imply that existing systems may be able to save millions of dollars a year in electricity costs, by being cognizant of locational computation cost differences.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,123–134,12,"electricity markets, cloud computing, traffic engineering","Barcelona, Spain",SIGCOMM '09,,,,,,
1346,article,"Qureshi, Asfandyar and Weber, Rick and Balakrishnan, Hari and Guttag, John and Maggs, Bruce",Cutting the Electric Bill for Internet-Scale Systems,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592584,10.1145/1594977.1592584,"Energy expenses are becoming an increasingly important fraction of data center operating costs. At the same time, the energy expense per unit of computation can vary significantly between two different locations. In this paper, we characterize the variation due to fluctuating electricity prices and argue that existing distributed systems should be able to exploit this variation for significant economic gains. Electricity prices exhibit both temporal and geographic variation, due to regional demand differences, transmission inefficiencies, and generation diversity. Starting with historical electricity prices, for twenty nine locations in the US, and network traffic data collected on Akamai's CDN, we use simulation to quantify the possible economic gains for a realistic workload. Our results imply that existing systems may be able to save millions of dollars a year in electricity costs, by being cognizant of locational computation cost differences.",,123–134,12,"traffic engineering, electricity markets, cloud computing",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1347,inproceedings,"Baden, Randy and Bender, Adam and Spring, Neil and Bhattacharjee, Bobby and Starin, Daniel",Persona: An Online Social Network with User-Defined Privacy,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592585,10.1145/1592568.1592585,"Online social networks (OSNs) are immensely popular, with some claiming over 200 million users. Users share private content, such as personal information or photographs, using OSN applications. Users must trust the OSN service to protect personal information even as the OSN provider benefits from examining and sharing that information. We present Persona, an OSN where users dictate who may access their information. Persona hides user data with attribute-based encryption (ABE), allowing users to apply fine-grained policies over who may view their data. Persona provides an effective means of creating applications in which users, not the OSN, define policy over access to private data. We demonstrate new cryptographic mechanisms that enhance the general applicability of ABE. We show how Persona provides the functionality of existing online social networks with additional privacy benefits. We describe an implementation of Persona that replicates Facebook applications and show that Persona provides acceptable performance when browsing privacy-enhanced web pages, even on mobile devices.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,135–146,12,"social networks, OSN, persona, facebook, privacy, ABE","Barcelona, Spain",SIGCOMM '09,,,,,,
1348,article,"Baden, Randy and Bender, Adam and Spring, Neil and Bhattacharjee, Bobby and Starin, Daniel",Persona: An Online Social Network with User-Defined Privacy,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592585,10.1145/1594977.1592585,"Online social networks (OSNs) are immensely popular, with some claiming over 200 million users. Users share private content, such as personal information or photographs, using OSN applications. Users must trust the OSN service to protect personal information even as the OSN provider benefits from examining and sharing that information. We present Persona, an OSN where users dictate who may access their information. Persona hides user data with attribute-based encryption (ABE), allowing users to apply fine-grained policies over who may view their data. Persona provides an effective means of creating applications in which users, not the OSN, define policy over access to private data. We demonstrate new cryptographic mechanisms that enhance the general applicability of ABE. We show how Persona provides the functionality of existing online social networks with additional privacy benefits. We describe an implementation of Persona that replicates Facebook applications and show that Persona provides acceptable performance when browsing privacy-enhanced web pages, even on mobile devices.",,135–146,12,"persona, facebook, OSN, ABE, social networks, privacy",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1349,inproceedings,"Brodsky, Micah Z. and Morris, Robert T.",In Defense of Wireless Carrier Sense,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592587,10.1145/1592568.1592587,"Carrier sense is often used to regulate concurrency in wireless medium access control (MAC) protocols, balancing interference protection and spatial reuse. Carrier sense is known to be imperfect, and many improved techniques have been proposed. Is the search for a replacement justified? This paper presents a theoretical model for average case two-sender carrier sense based on radio propagation theory and Shannon capacity. Analysis using the model shows that carrier sense performance is surprisingly close to optimal for radios with adaptive bitrate. The model suggests that hidden and exposed terminals usually cause modest reductions in throughput rather than dramatic decreases. Finally, it is possible to choose a fixed sense threshold which performs well across a wide range of scenarios, in large part due to the role of the noise floor. Experimental results from an indoor 802.11 testbed support these claims.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,147–158,12,"analytical modeling, CSMA, medium access control, carrier sense","Barcelona, Spain",SIGCOMM '09,,,,,,
1350,article,"Brodsky, Micah Z. and Morris, Robert T.",In Defense of Wireless Carrier Sense,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592587,10.1145/1594977.1592587,"Carrier sense is often used to regulate concurrency in wireless medium access control (MAC) protocols, balancing interference protection and spatial reuse. Carrier sense is known to be imperfect, and many improved techniques have been proposed. Is the search for a replacement justified? This paper presents a theoretical model for average case two-sender carrier sense based on radio propagation theory and Shannon capacity. Analysis using the model shows that carrier sense performance is surprisingly close to optimal for radios with adaptive bitrate. The model suggests that hidden and exposed terminals usually cause modest reductions in throughput rather than dramatic decreases. Finally, it is possible to choose a fixed sense threshold which performs well across a wide range of scenarios, in large part due to the role of the noise floor. Experimental results from an indoor 802.11 testbed support these claims.",,147–158,12,"carrier sense, medium access control, CSMA, analytical modeling",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1351,inproceedings,"Gollakota, Shyamnath and Perli, Samuel David and Katabi, Dina",Interference Alignment and Cancellation,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592588,10.1145/1592568.1592588,"The throughput of existing MIMO LANs is limited by the number of antennas on the AP. This paper shows how to overcome this limit. It presents interference alignment and cancellation (IAC), a new approach for decoding concurrent sender-receiver pairs in MIMO networks. IAC synthesizes two signal processing techniques, interference alignment and interference cancellation, showing that the combination applies to scenarios where neither interference alignment nor cancellation applies alone. We show analytically that IAC almost doubles the throughput of MIMO LANs. We also implement IAC in GNU-Radio, and experimentally demonstrate that for 2x2 MIMO LANs, IAC increases the average throughput by 1.5x on the downlink and 2x on the uplink.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,159–170,12,"interference alignment, interference cancellation","Barcelona, Spain",SIGCOMM '09,,,,,,
1352,article,"Gollakota, Shyamnath and Perli, Samuel David and Katabi, Dina",Interference Alignment and Cancellation,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592588,10.1145/1594977.1592588,"The throughput of existing MIMO LANs is limited by the number of antennas on the AP. This paper shows how to overcome this limit. It presents interference alignment and cancellation (IAC), a new approach for decoding concurrent sender-receiver pairs in MIMO networks. IAC synthesizes two signal processing techniques, interference alignment and interference cancellation, showing that the combination applies to scenarios where neither interference alignment nor cancellation applies alone. We show analytically that IAC almost doubles the throughput of MIMO LANs. We also implement IAC in GNU-Radio, and experimentally demonstrate that for 2x2 MIMO LANs, IAC increases the average throughput by 1.5x on the downlink and 2x on the uplink.",,159–170,12,"interference alignment, interference cancellation",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1353,inproceedings,"Liu, Xi and Sheth, Anmol and Kaminsky, Michael and Papagiannaki, Konstantina and Seshan, Srinivasan and Steenkiste, Peter",DIRC: Increasing Indoor Wireless Capacity Using Directional Antennas,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592589,10.1145/1592568.1592589,"The demand for wireless bandwidth in indoor environments such as homes and offices continues to increase rapidly. Although wireless technologies such as MIMO can reach link throughputs of 100s of Mbps (802.11n) for a single link, the question of how we can deliver high throughput to a large number of densely-packed devices remains an open problem. Directional antennas have been shown to be an effective way to increase spatial reuse, but past work has focused largely on outdoor environments where the interactions between wireless links can usually be ignored. This assumption is not acceptable in dense indoor wireless networks since indoor deployments need to deal with rich scattering and multipath effects. In this paper we introduce DIRC, a wireless network design whose access points use phased array antennas to achieve high throughput in dense, indoor environments. The core of DIRC is an algorithm that increases spatial reuse and maximizes overall network capacity by optimizing the orientations of a network of directional antennas. We implemented DIRC and evaluated it on a nine node network in an enterprise setting. Our results show that DIRC improves overall network capacity in indoor environments, while being flexible enough to adapt to node mobility and changing traffic workloads.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,171–182,12,"indoor wireless capacity, directional antenna","Barcelona, Spain",SIGCOMM '09,,,,,,
1354,article,"Liu, Xi and Sheth, Anmol and Kaminsky, Michael and Papagiannaki, Konstantina and Seshan, Srinivasan and Steenkiste, Peter",DIRC: Increasing Indoor Wireless Capacity Using Directional Antennas,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592589,10.1145/1594977.1592589,"The demand for wireless bandwidth in indoor environments such as homes and offices continues to increase rapidly. Although wireless technologies such as MIMO can reach link throughputs of 100s of Mbps (802.11n) for a single link, the question of how we can deliver high throughput to a large number of densely-packed devices remains an open problem. Directional antennas have been shown to be an effective way to increase spatial reuse, but past work has focused largely on outdoor environments where the interactions between wireless links can usually be ignored. This assumption is not acceptable in dense indoor wireless networks since indoor deployments need to deal with rich scattering and multipath effects. In this paper we introduce DIRC, a wireless network design whose access points use phased array antennas to achieve high throughput in dense, indoor environments. The core of DIRC is an algorithm that increases spatial reuse and maximizes overall network capacity by optimizing the orientations of a network of directional antennas. We implemented DIRC and evaluated it on a nine node network in an enterprise setting. Our results show that DIRC improves overall network capacity in indoor environments, while being flexible enough to adapt to node mobility and changing traffic workloads.",,171–182,12,"indoor wireless capacity, directional antenna",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1355,inproceedings,"Flavel, Ashley and Roughan, Matthew",Stable and Flexible IBGP,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592591,10.1145/1592568.1592591,"Routing oscillation is highly detrimental. It can decrease performance and lead to a high level of update churn placing unnecessary workload on router the problem is distributed between many providers. However, iBGP --- the routing protocol used to distribute routes inside a single Autonomous System --- has also been shown to oscillate. Despite the fact that iBGP is configured by a single provider according to apparently straight forward rules, more than eight years of research has not solved the problem of iBGP oscillation. Various solutions have been proposed but they all lack critical features: either they are complicated to implement, restrict routing flexibility, or lack guarantees of stability. In this paper we propose a very simple adaptation to the BGP decision process. Despite its simplicity and negligible cost we prove algebraically that it prevents iBGP oscillation. We extend the idea to provide routing flexibility, such as respecting the MED attribute, without sacrificing network stability.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,183–194,12,"stability, BGP, metarouting, routing","Barcelona, Spain",SIGCOMM '09,,,,,,
1356,article,"Flavel, Ashley and Roughan, Matthew",Stable and Flexible IBGP,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592591,10.1145/1594977.1592591,"Routing oscillation is highly detrimental. It can decrease performance and lead to a high level of update churn placing unnecessary workload on router the problem is distributed between many providers. However, iBGP --- the routing protocol used to distribute routes inside a single Autonomous System --- has also been shown to oscillate. Despite the fact that iBGP is configured by a single provider according to apparently straight forward rules, more than eight years of research has not solved the problem of iBGP oscillation. Various solutions have been proposed but they all lack critical features: either they are complicated to implement, restrict routing flexibility, or lack guarantees of stability. In this paper we propose a very simple adaptation to the BGP decision process. Despite its simplicity and negligible cost we prove algebraically that it prevents iBGP oscillation. We extend the idea to provide routing flexibility, such as respecting the MED attribute, without sacrificing network stability.",,183–194,12,"routing, BGP, stability, metarouting",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1357,inproceedings,"Jokela, Petri and Zahemszky, Andr\'{a",LIPSIN: Line Speed Publish/Subscribe Inter-Networking,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592592,10.1145/1592568.1592592,"A large fraction of today's Internet applications are internally publish/subscribe in nature; the current architecture makes it cumbersome and inept to support them. In essence, supporting efficient publish/subscribe requires data-oriented naming, efficient multicast, and in-network caching. Deployment of native IP-based multicast has failed, and overlay-based multicast systems are inherently inefficient. We surmise that scalable and efficient publish/subscribe will require substantial architectural changes, such as moving from endpoint-oriented systems to information-centric architectures.In this paper, we propose a novel multicast forwarding fabric, suitable for large-scale topic-based publish/subscribe. Due to very simple forwarding decisions and small forwarding tables, the fabric may be more energy efficient than the currently used ones. To understand the limitations and potential, we provide efficiency and scalability analysis via simulations and early measurements from our two implementations. We show that the system scales up to metropolitan WAN sizes, and we discuss how to interconnect separate networks.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,195–206,12,"publish/subscribe, bloom filters, forwarding, multicast","Barcelona, Spain",SIGCOMM '09,,,,,,
1358,article,"Jokela, Petri and Zahemszky, Andr\'{a",LIPSIN: Line Speed Publish/Subscribe Inter-Networking,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592592,10.1145/1594977.1592592,"A large fraction of today's Internet applications are internally publish/subscribe in nature; the current architecture makes it cumbersome and inept to support them. In essence, supporting efficient publish/subscribe requires data-oriented naming, efficient multicast, and in-network caching. Deployment of native IP-based multicast has failed, and overlay-based multicast systems are inherently inefficient. We surmise that scalable and efficient publish/subscribe will require substantial architectural changes, such as moving from endpoint-oriented systems to information-centric architectures.In this paper, we propose a novel multicast forwarding fabric, suitable for large-scale topic-based publish/subscribe. Due to very simple forwarding decisions and small forwarding tables, the fabric may be more energy efficient than the currently used ones. To understand the limitations and potential, we provide efficiency and scalability analysis via simulations and early measurements from our two implementations. We show that the system scales up to metropolitan WAN sizes, and we discuss how to interconnect separate networks.",,195–206,12,"bloom filters, forwarding, multicast, publish/subscribe",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1359,inproceedings,"De Carli, Lorenzo and Pan, Yi and Kumar, Amit and Estan, Cristian and Sankaralingam, Karthikeyan",PLUG: Flexible Lookup Modules for Rapid Deployment of New Protocols in High-Speed Routers,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592593,10.1145/1592568.1592593,"New protocols for the data link and network layer are being proposed to address limitations of current protocols in terms of scalability, security, and manageability. High-speed routers and switches that implement these protocols traditionally perform packet processing using ASICs which offer high speed, low chip area, and low power. But with inflexible custom hardware, the deployment of new protocols could happen only through equipment upgrades. While newer routers use more flexible network processors for data plane processing, due to power and area constraints lookups in forwarding tables are done with custom lookup modules. Thus most of the proposed protocols can only be deployed with equipment upgrades. To speed up the deployment of new protocols, we propose a flexible lookup module, PLUG (Pipelined Lookup Grid). We can achieve generality without loosing efficiency because various custom lookup modules have the same fundamental features we retain: area dominated by memories, simple processing, and strict access patterns defined by the data structure. We implemented IPv4, Ethernet, Ethane, and SEATTLE in our dataflow-based programming model for the PLUG and mapped them to the PLUG hardware which consists of a grid of tiles. Throughput, area, power, and latency of PLUGs are close to those of specialized lookup modules.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,207–218,12,"flexibility, forwarding, dataflow, lookup, high-speed routers, tiled architectures","Barcelona, Spain",SIGCOMM '09,,,,,,
1360,article,"De Carli, Lorenzo and Pan, Yi and Kumar, Amit and Estan, Cristian and Sankaralingam, Karthikeyan",PLUG: Flexible Lookup Modules for Rapid Deployment of New Protocols in High-Speed Routers,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592593,10.1145/1594977.1592593,"New protocols for the data link and network layer are being proposed to address limitations of current protocols in terms of scalability, security, and manageability. High-speed routers and switches that implement these protocols traditionally perform packet processing using ASICs which offer high speed, low chip area, and low power. But with inflexible custom hardware, the deployment of new protocols could happen only through equipment upgrades. While newer routers use more flexible network processors for data plane processing, due to power and area constraints lookups in forwarding tables are done with custom lookup modules. Thus most of the proposed protocols can only be deployed with equipment upgrades. To speed up the deployment of new protocols, we propose a flexible lookup module, PLUG (Pipelined Lookup Grid). We can achieve generality without loosing efficiency because various custom lookup modules have the same fundamental features we retain: area dominated by memories, simple processing, and strict access patterns defined by the data structure. We implemented IPv4, Ethernet, Ethane, and SEATTLE in our dataflow-based programming model for the PLUG and mapped them to the PLUG hardware which consists of a grid of tiles. Throughput, area, power, and latency of PLUGs are close to those of specialized lookup modules.",,207–218,12,"high-speed routers, tiled architectures, lookup, dataflow, forwarding, flexibility",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1361,inproceedings,"Sung, Yu-Wei Eric and Lund, Carsten and Lyn, Mark and Rao, Sanjay G. and Sen, Subhabrata",Modeling and Understanding End-to-End Class of Service Policies in Operational Networks,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592595,10.1145/1592568.1592595,"Business and economic considerations are driving the extensive use of service differentiation in Virtual Private Networks (VPNs) operated for business enterprises today. The resulting Class of Service (CoS) designs embed complex policy decisions based on the described priorities of various applications, extent of bandwidth availability, and cost considerations. These inherently complex high-level policies are realized through low-level router configurations. The configuration process is tedious and error-prone given the highly intertwined nature of CoS configuration, the multiple router configurations over which the policies are instantiated, and the complex access control lists (ACLs) involved. Our contributions include (i) a formal approach to modeling CoS policies from router configuration files in a precise manner; (ii) a practical and computationally efficient tool that can determine the CoS treatment received by an arbitrary set of flows across multiple routers; and (iii) a validation of our approach in enabling applications such as troubleshooting, auditing, and visualization of network-wide CoS design, using router configuration data from a cross-section of 150 diverse enterprise VPNs. To our knowledge, this is the first effort aimed at modeling and analyzing CoS configurations.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,219–230,12,"configuration modeling, differentiated service","Barcelona, Spain",SIGCOMM '09,,,,,,
1362,article,"Sung, Yu-Wei Eric and Lund, Carsten and Lyn, Mark and Rao, Sanjay G. and Sen, Subhabrata",Modeling and Understanding End-to-End Class of Service Policies in Operational Networks,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592595,10.1145/1594977.1592595,"Business and economic considerations are driving the extensive use of service differentiation in Virtual Private Networks (VPNs) operated for business enterprises today. The resulting Class of Service (CoS) designs embed complex policy decisions based on the described priorities of various applications, extent of bandwidth availability, and cost considerations. These inherently complex high-level policies are realized through low-level router configurations. The configuration process is tedious and error-prone given the highly intertwined nature of CoS configuration, the multiple router configurations over which the policies are instantiated, and the complex access control lists (ACLs) involved. Our contributions include (i) a formal approach to modeling CoS policies from router configuration files in a precise manner; (ii) a practical and computationally efficient tool that can determine the CoS treatment received by an arbitrary set of flows across multiple routers; and (iii) a validation of our approach in enabling applications such as troubleshooting, auditing, and visualization of network-wide CoS design, using router configuration data from a cross-section of 150 diverse enterprise VPNs. To our knowledge, this is the first effort aimed at modeling and analyzing CoS configurations.",,219–230,12,"differentiated service, configuration modeling",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1363,inproceedings,"Mahimkar, Ajay Anil and Ge, Zihui and Shaikh, Aman and Wang, Jia and Yates, Jennifer and Zhang, Yin and Zhao, Qi",Towards Automated Performance Diagnosis in a Large IPTV Network,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592596,10.1145/1592568.1592596,"IPTV is increasingly being deployed and offered as a commercial service to residential broadband customers. Compared with traditional ISP networks, an IPTV distribution network (i) typically adopts a hierarchical instead of mesh-like structure, (ii) imposes more stringent requirements on both reliability and performance, (iii) has different distribution protocols (which make heavy use of IP multicast) and traffic patterns, and (iv) faces more serious scalability challenges in managing millions of network elements. These unique characteristics impose tremendous challenges in the effective management of IPTV network and service.In this paper, we focus on characterizing and troubleshooting performance issues in one of the largest IPTV networks in North America. We collect a large amount of measurement data from a wide range of sources, including device usage and error logs, user activity logs, video quality alarms, and customer trouble tickets. We develop a novel diagnosis tool called Giza that is specifically tailored to the enormous scale and hierarchical structure of the IPTV network. Giza applies multi-resolution data analysis to quickly detect and localize regions in the IPTV distribution hierarchy that are experiencing serious performance problems. Giza then uses several statistical data mining techniques to troubleshoot the identified problems and diagnose their root causes. Validation against operational experiences demonstrates the effectiveness of Giza in detecting important performance issues and identifying interesting dependencies. The methodology and algorithms in Giza promise to be of great use in IPTV network operations.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,231–242,12,"IPTV, network diagnosis","Barcelona, Spain",SIGCOMM '09,,,,,,
1364,article,"Mahimkar, Ajay Anil and Ge, Zihui and Shaikh, Aman and Wang, Jia and Yates, Jennifer and Zhang, Yin and Zhao, Qi",Towards Automated Performance Diagnosis in a Large IPTV Network,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592596,10.1145/1594977.1592596,"IPTV is increasingly being deployed and offered as a commercial service to residential broadband customers. Compared with traditional ISP networks, an IPTV distribution network (i) typically adopts a hierarchical instead of mesh-like structure, (ii) imposes more stringent requirements on both reliability and performance, (iii) has different distribution protocols (which make heavy use of IP multicast) and traffic patterns, and (iv) faces more serious scalability challenges in managing millions of network elements. These unique characteristics impose tremendous challenges in the effective management of IPTV network and service.In this paper, we focus on characterizing and troubleshooting performance issues in one of the largest IPTV networks in North America. We collect a large amount of measurement data from a wide range of sources, including device usage and error logs, user activity logs, video quality alarms, and customer trouble tickets. We develop a novel diagnosis tool called Giza that is specifically tailored to the enormous scale and hierarchical structure of the IPTV network. Giza applies multi-resolution data analysis to quickly detect and localize regions in the IPTV distribution hierarchy that are experiencing serious performance problems. Giza then uses several statistical data mining techniques to troubleshoot the identified problems and diagnose their root causes. Validation against operational experiences demonstrates the effectiveness of Giza in detecting important performance issues and identifying interesting dependencies. The methodology and algorithms in Giza promise to be of great use in IPTV network operations.",,231–242,12,"network diagnosis, IPTV",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1365,inproceedings,"Kandula, Srikanth and Mahajan, Ratul and Verkaik, Patrick and Agarwal, Sharad and Padhye, Jitendra and Bahl, Paramvir",Detailed Diagnosis in Enterprise Networks,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592597,10.1145/1592568.1592597,"By studying trouble tickets from small enterprise networks, we conclude that their operators need detailed fault diagnosis. That is, the diagnostic system should be able to diagnose not only generic faults (e.g., performance-related) but also application specific faults (e.g., error codes). It should also identify culprits at a fine granularity such as a process or firewall configuration. We build a system, called NetMedic, that enables detailed diagnosis by harnessing the rich information exposed by modern operating systems and applications. It formulates detailed diagnosis as an inference problem that more faithfully captures the behaviors and interactions of fine-grained network components such as processes. The primary challenge in solving this problem is inferring when a component might be impacting another. Our solution is based on an intuitive technique that uses the joint behavior of two components in the past to estimate the likelihood of them impacting one another in the present. We find that our deployed prototype is effective at diagnosing faults that we inject in a live environment. The faulty component is correctly identified as the most likely culprit in 80% of the cases and is almost always in the list of top five culprits.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,243–254,12,"applications, fault diagnosis, enterprise networks","Barcelona, Spain",SIGCOMM '09,,,,,,
1366,article,"Kandula, Srikanth and Mahajan, Ratul and Verkaik, Patrick and Agarwal, Sharad and Padhye, Jitendra and Bahl, Paramvir",Detailed Diagnosis in Enterprise Networks,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592597,10.1145/1594977.1592597,"By studying trouble tickets from small enterprise networks, we conclude that their operators need detailed fault diagnosis. That is, the diagnostic system should be able to diagnose not only generic faults (e.g., performance-related) but also application specific faults (e.g., error codes). It should also identify culprits at a fine granularity such as a process or firewall configuration. We build a system, called NetMedic, that enables detailed diagnosis by harnessing the rich information exposed by modern operating systems and applications. It formulates detailed diagnosis as an inference problem that more faithfully captures the behaviors and interactions of fine-grained network components such as processes. The primary challenge in solving this problem is inferring when a component might be impacting another. Our solution is based on an intuitive technique that uses the joint behavior of two components in the past to estimate the likelihood of them impacting one another in the present. We find that our deployed prototype is effective at diagnosing faults that we inject in a live environment. The faulty component is correctly identified as the most likely culprit in 80% of the cases and is almost always in the list of top five culprits.",,243–254,12,"applications, fault diagnosis, enterprise networks",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1367,inproceedings,"Kompella, Ramana Rao and Levchenko, Kirill and Snoeren, Alex C. and Varghese, George",Every Microsecond Counts: Tracking Fine-Grain Latencies with a Lossy Difference Aggregator,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592599,10.1145/1592568.1592599,"Many network applications have stringent end-to-end latency requirements, including VoIP and interactive video conferencing, automated trading, and high-performance computing---where even microsecond variations may be intolerable. The resulting fine-grain measurement demands cannot be met effectively by existing technologies, such as SNMP, NetFlow, or active probing. We propose instrumenting routers with a hash-based primitive that we call a Lossy Difference Aggregator (LDA) to measure latencies down to tens of microseconds and losses as infrequent as one in a million.Such measurement can be viewed abstractly as what we refer to as a coordinated streaming problem, which is fundamentally harder than standard streaming problems due to the need to coordinate values between nodes. We describe a compact data structure that efficiently computes the average and standard deviation of latency and loss rate in a coordinated streaming environment. Our theoretical results translate to an efficient hardware implementation at 40 Gbps using less than 1% of a typical 65-nm 400-MHz networking ASIC. When compared to Poisson-spaced active probing with similar overheads, our LDA mechanism delivers orders of magnitude smaller relative error; active probing requires 50--60 times as much bandwidth to deliver similar levels of accuracy.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,255–266,12,"passive measurement, packet sampling","Barcelona, Spain",SIGCOMM '09,,,,,,
1368,article,"Kompella, Ramana Rao and Levchenko, Kirill and Snoeren, Alex C. and Varghese, George",Every Microsecond Counts: Tracking Fine-Grain Latencies with a Lossy Difference Aggregator,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592599,10.1145/1594977.1592599,"Many network applications have stringent end-to-end latency requirements, including VoIP and interactive video conferencing, automated trading, and high-performance computing---where even microsecond variations may be intolerable. The resulting fine-grain measurement demands cannot be met effectively by existing technologies, such as SNMP, NetFlow, or active probing. We propose instrumenting routers with a hash-based primitive that we call a Lossy Difference Aggregator (LDA) to measure latencies down to tens of microseconds and losses as infrequent as one in a million.Such measurement can be viewed abstractly as what we refer to as a coordinated streaming problem, which is fundamentally harder than standard streaming problems due to the need to coordinate values between nodes. We describe a compact data structure that efficiently computes the average and standard deviation of latency and loss rate in a coordinated streaming environment. Our theoretical results translate to an efficient hardware implementation at 40 Gbps using less than 1% of a typical 65-nm 400-MHz networking ASIC. When compared to Poisson-spaced active probing with similar overheads, our LDA mechanism delivers orders of magnitude smaller relative error; active probing requires 50--60 times as much bandwidth to deliver similar levels of accuracy.",,255–266,12,"packet sampling, passive measurement",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1369,inproceedings,"Zhang, Yin and Roughan, Matthew and Willinger, Walter and Qiu, Lili",Spatio-Temporal Compressive Sensing and Internet Traffic Matrices,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592600,10.1145/1592568.1592600,"Many basic network engineering tasks (e.g., traffic engineering, capacity planning, anomaly detection) rely heavily on the availability and accuracy of traffic matrices. However, in practice it is challenging to reliably measure traffic matrices. Missing values are common. This observation brings us into the realm of compressive sensing, a generic technique for dealing with missing values that exploits the presence of structure and redundancy in many real-world systems. Despite much recent progress made in compressive sensing, existing compressive-sensing solutions often perform poorly for traffic matrix interpolation, because real traffic matrices rarely satisfy the technical conditions required for these solutions.To address this problem, we develop a novel spatio-temporal compressive sensing framework with two key components: (i) a new technique called Sparsity Regularized Matrix Factorization (SRMF) that leverages the sparse or low-rank nature of real-world traffic matrices and their spatio-temporal properties, and (ii) a mechanism for combining low-rank approximations with local interpolation procedures. We illustrate our new framework and demonstrate its superior performance in problems involving interpolation with real traffic matrices where we can successfully replace up to 98% of the values. Evaluation in applications such as network tomography, traffic prediction, and anomaly detection confirms the flexibility and effectiveness of our approach.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,267–278,12,"anomaly detection, tomography, interpolation, traffic matrix, prediction, compressive sensing","Barcelona, Spain",SIGCOMM '09,,,,,,
1370,article,"Zhang, Yin and Roughan, Matthew and Willinger, Walter and Qiu, Lili",Spatio-Temporal Compressive Sensing and Internet Traffic Matrices,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592600,10.1145/1594977.1592600,"Many basic network engineering tasks (e.g., traffic engineering, capacity planning, anomaly detection) rely heavily on the availability and accuracy of traffic matrices. However, in practice it is challenging to reliably measure traffic matrices. Missing values are common. This observation brings us into the realm of compressive sensing, a generic technique for dealing with missing values that exploits the presence of structure and redundancy in many real-world systems. Despite much recent progress made in compressive sensing, existing compressive-sensing solutions often perform poorly for traffic matrix interpolation, because real traffic matrices rarely satisfy the technical conditions required for these solutions.To address this problem, we develop a novel spatio-temporal compressive sensing framework with two key components: (i) a new technique called Sparsity Regularized Matrix Factorization (SRMF) that leverages the sparse or low-rank nature of real-world traffic matrices and their spatio-temporal properties, and (ii) a mechanism for combining low-rank approximations with local interpolation procedures. We illustrate our new framework and demonstrate its superior performance in problems involving interpolation with real traffic matrices where we can successfully replace up to 98% of the values. Evaluation in applications such as network tomography, traffic prediction, and anomaly detection confirms the flexibility and effectiveness of our approach.",,267–278,12,"compressive sensing, interpolation, tomography, prediction, traffic matrix, anomaly detection",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1371,inproceedings,"Papageorge, Pavlos and McCann, Justin and Hicks, Michael",Passive Aggressive Measurement with MGRP,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592601,10.1145/1592568.1592601,"We present the Measurement Manager Protocol (MGRP), an in-kernel service that schedules and transmits probes on behalf of active measurement tools. Unlike prior measurement services, MGRP transparently piggybacks application packets inside the often significant amounts of empty padding contained in typical probes. Using MGRP thus combines the modularity, flexibility, and accuracy of standalone active measurement tools with the lower overhead of passive measurement techniques. Microbenchmark experiments show that the resulting bandwidth savings makes it possible to measure the network accurately, but faster and more aggressively than without piggybacking, and with few ill effects to piggybacked application or competing traffic. When using MGRP to schedule measurements on behalf of MediaNet, an overlay service that adaptively schedules media streams, we show MediaNet can achieve significantly higher streaming rates under the same network conditions.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,279–290,12,"streaming, available bandwidth, passive, transport protocol, kernel module, piggybacking, probing, active","Barcelona, Spain",SIGCOMM '09,,,,,,
1372,article,"Papageorge, Pavlos and McCann, Justin and Hicks, Michael",Passive Aggressive Measurement with MGRP,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592601,10.1145/1594977.1592601,"We present the Measurement Manager Protocol (MGRP), an in-kernel service that schedules and transmits probes on behalf of active measurement tools. Unlike prior measurement services, MGRP transparently piggybacks application packets inside the often significant amounts of empty padding contained in typical probes. Using MGRP thus combines the modularity, flexibility, and accuracy of standalone active measurement tools with the lower overhead of passive measurement techniques. Microbenchmark experiments show that the resulting bandwidth savings makes it possible to measure the network accurately, but faster and more aggressively than without piggybacking, and with few ill effects to piggybacked application or competing traffic. When using MGRP to schedule measurements on behalf of MediaNet, an overlay service that adaptively schedules media streams, we show MediaNet can achieve significantly higher streaming rates under the same network conditions.",,279–290,12,"probing, streaming, transport protocol, active, available bandwidth, kernel module, passive, piggybacking",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1373,inproceedings,"Raiciu, Costin and Huici, Felipe and Handley, Mark and Rosenblum, David S.",ROAR: Increasing the Flexibility and Performance of Distributed Search,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592603,10.1145/1592568.1592603,"To search the web quickly, search engines partition the web index over many machines, and consult every partition when answering a query. To increase throughput, replicas are added for each of these machines. The key parameter of these algorithms is the trade-off between replication and partitioning: increasing the partitioning level improves query completion time since more servers handle the query, but may incur non-negligible startup costs for each sub-query. Finding the right operating point and adapting to it can significantly improve performance and reduce costs.We introduce Rendezvous On a Ring (ROAR), a novel distributed algorithm that enables on-the-fly re-configuration of the partitioning level. ROAR can add and remove servers without stopping the system, cope with server failures, and provide good load-balancing even with a heterogeneous server pool. We demonstrate these claims using a privacy-preserving search application built upon ROAR.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,291–302,12,,"Barcelona, Spain",SIGCOMM '09,,,,,,
1374,article,"Raiciu, Costin and Huici, Felipe and Handley, Mark and Rosenblum, David S.",ROAR: Increasing the Flexibility and Performance of Distributed Search,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592603,10.1145/1594977.1592603,"To search the web quickly, search engines partition the web index over many machines, and consult every partition when answering a query. To increase throughput, replicas are added for each of these machines. The key parameter of these algorithms is the trade-off between replication and partitioning: increasing the partitioning level improves query completion time since more servers handle the query, but may incur non-negligible startup costs for each sub-query. Finding the right operating point and adapting to it can significantly improve performance and reduce costs.We introduce Rendezvous On a Ring (ROAR), a novel distributed algorithm that enables on-the-fly re-configuration of the partitioning level. ROAR can add and remove servers without stopping the system, cope with server failures, and provide good load-balancing even with a heterogeneous server pool. We demonstrate these claims using a privacy-preserving search application built upon ROAR.",,291–302,12,,,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1375,inproceedings,"Vasudevan, Vijay and Phanishayee, Amar and Shah, Hiral and Krevat, Elie and Andersen, David G. and Ganger, Gregory R. and Gibson, Garth A. and Mueller, Brian",Safe and Effective Fine-Grained TCP Retransmissions for Datacenter Communication,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592604,10.1145/1592568.1592604,"This paper presents a practical solution to a problem facing high-fan-in, high-bandwidth synchronized TCP workloads in datacenter Ethernets---the TCP incast problem. In these networks, receivers can experience a drastic reduction in application throughput when simultaneously requesting data from many servers using TCP. Inbound data overfills small switch buffers, leading to TCP timeouts lasting hundreds of milliseconds. For many datacenter workloads that have a barrier synchronization requirement (e.g., filesystem reads and parallel data-intensive queries), throughput is reduced by up to 90%. For latency-sensitive applications, TCP timeouts in the datacenter impose delays of hundreds of milliseconds in networks with round-trip-times in microseconds.Our practical solution uses high-resolution timers to enable microsecond-granularity TCP timeouts. We demonstrate that this technique is effective in avoiding TCP incast collapse in simulation and in real-world experiments. We show that eliminating the minimum retransmission timeout bound is safe for all environments, including the wide-area.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,303–314,12,"datacenter networks, performance, incast, throughput","Barcelona, Spain",SIGCOMM '09,,,,,,
1376,article,"Vasudevan, Vijay and Phanishayee, Amar and Shah, Hiral and Krevat, Elie and Andersen, David G. and Ganger, Gregory R. and Gibson, Garth A. and Mueller, Brian",Safe and Effective Fine-Grained TCP Retransmissions for Datacenter Communication,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592604,10.1145/1594977.1592604,"This paper presents a practical solution to a problem facing high-fan-in, high-bandwidth synchronized TCP workloads in datacenter Ethernets---the TCP incast problem. In these networks, receivers can experience a drastic reduction in application throughput when simultaneously requesting data from many servers using TCP. Inbound data overfills small switch buffers, leading to TCP timeouts lasting hundreds of milliseconds. For many datacenter workloads that have a barrier synchronization requirement (e.g., filesystem reads and parallel data-intensive queries), throughput is reduced by up to 90%. For latency-sensitive applications, TCP timeouts in the datacenter impose delays of hundreds of milliseconds in networks with round-trip-times in microseconds.Our practical solution uses high-resolution timers to enable microsecond-granularity TCP timeouts. We demonstrate that this technique is effective in avoiding TCP incast collapse in simulation and in real-world experiments. We show that eliminating the minimum retransmission timeout bound is safe for all environments, including the wide-area.",,303–314,12,"performance, datacenter networks, incast, throughput",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1377,inproceedings,"Agarwal, Sharad and Lorch, Jacob R.",Matchmaking for Online Games and Other Latency-Sensitive P2P Systems,2009,9781605585949,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1592568.1592605,10.1145/1592568.1592605,"The latency between machines on the Internet can dramatically affect users' experience for many distributed applications. Particularly, in multiplayer online games, players seek to cluster themselves so that those in the same session have low latency to each other. A system that predicts latencies between machine pairs allows such matchmaking to consider many more machine pairs than can be probed in a scalable fashion while users are waiting. Using a far-reaching trace of latencies between players on over 3.5 million game consoles, we designed Htrae, a latency prediction system for game matchmaking scenarios. One novel feature of Htrae is its synthesis of geolocation with a network coordinate system. It uses geolocation to select reasonable initial network coordinates for new machines joining the system, allowing it to converge more quickly than standard network coordinate systems and produce substantially lower prediction error than state-of-the-art latency prediction systems. For instance, it produces 90th percentile errors less than half those of iPlane and Pyxida. Our design is general enough to make it a good fit for other latency-sensitive peer-to-peer applications besides game matchmaking.",Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication,315–326,12,"network coordinates, online gaming, latency estimation, matchmaking","Barcelona, Spain",SIGCOMM '09,,,,,,
1378,article,"Agarwal, Sharad and Lorch, Jacob R.",Matchmaking for Online Games and Other Latency-Sensitive P2P Systems,2009,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1594977.1592605,10.1145/1594977.1592605,"The latency between machines on the Internet can dramatically affect users' experience for many distributed applications. Particularly, in multiplayer online games, players seek to cluster themselves so that those in the same session have low latency to each other. A system that predicts latencies between machine pairs allows such matchmaking to consider many more machine pairs than can be probed in a scalable fashion while users are waiting. Using a far-reaching trace of latencies between players on over 3.5 million game consoles, we designed Htrae, a latency prediction system for game matchmaking scenarios. One novel feature of Htrae is its synthesis of geolocation with a network coordinate system. It uses geolocation to select reasonable initial network coordinates for new machines joining the system, allowing it to converge more quickly than standard network coordinate systems and produce substantially lower prediction error than state-of-the-art latency prediction systems. For instance, it produces 90th percentile errors less than half those of iPlane and Pyxida. Our design is general enough to make it a good fit for other latency-sensitive peer-to-peer applications besides game matchmaking.",,315–326,12,"online gaming, network coordinates, matchmaking, latency estimation",,,October 2009,39,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1379,inproceedings,"Subramanian, Lakshminarayanan",Session Details: Routing,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256734,10.1145/3256734,,Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,,,,"Seattle, WA, USA",SIGCOMM '08,,,,,,
1380,article,"Kim, Changhoon and Caesar, Matthew and Rexford, Jennifer",Floodless in Seattle: A Scalable Ethernet Architecture for Large Enterprises,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402961,10.1145/1402946.1402961,"IP networks today require massive effort to configure and manage. Ethernet is vastly simpler to manage, but does not scale beyond small local area networks. This paper describes an alternative network architecture called SEATTLE that achieves the best of both worlds: The scalability of IP combined with the simplicity of Ethernet. SEATTLE provides plug-and-play functionality via flat addressing, while ensuring scalability and efficiency through shortest-path routing and hash-based resolution of host information. In contrast to previous work on identity-based routing, SEATTLE ensures path predictability and stability, and simplifies network management. We performed a simulation study driven by real-world traffic traces and network topologies, and used Emulab to evaluate a prototype of our design based on the Click and XORP open-source routing platforms. Our experiments show that SEATTLE efficiently handles network failures and host mobility, while reducing control overhead and state requirements by roughly two orders of magnitude compared with Ethernet bridging.",,3–14,12,"routing, ethernet, scalability, enterprise network",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1381,inproceedings,"Kim, Changhoon and Caesar, Matthew and Rexford, Jennifer",Floodless in Seattle: A Scalable Ethernet Architecture for Large Enterprises,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402961,10.1145/1402958.1402961,"IP networks today require massive effort to configure and manage. Ethernet is vastly simpler to manage, but does not scale beyond small local area networks. This paper describes an alternative network architecture called SEATTLE that achieves the best of both worlds: The scalability of IP combined with the simplicity of Ethernet. SEATTLE provides plug-and-play functionality via flat addressing, while ensuring scalability and efficiency through shortest-path routing and hash-based resolution of host information. In contrast to previous work on identity-based routing, SEATTLE ensures path predictability and stability, and simplifies network management. We performed a simulation study driven by real-world traffic traces and network topologies, and used Emulab to evaluate a prototype of our design based on the Click and XORP open-source routing platforms. Our experiments show that SEATTLE efficiently handles network failures and host mobility, while reducing control overhead and state requirements by roughly two orders of magnitude compared with Ethernet bridging.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,3–14,12,"routing, enterprise network, scalability, ethernet","Seattle, WA, USA",SIGCOMM '08,,,,,,
1382,article,"Levchenko, Kirill and Voelker, Geoffrey M. and Paturi, Ramamohan and Savage, Stefan",Xl: An Efficient Network Routing Algorithm,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402962,10.1145/1402946.1402962,"In this paper, we present a new link-state routing algorithm called Approximate Link state (XL) aimed at increasing routing efficiency by suppressing updates from parts of the network. We prove that three simple criteria for update propagation are sufficient to guarantee soundness, completeness and bounded optimality for any such algorithm. We show, via simulation, that XL significantly outperforms standard link-state and distance vector algorithms - in some cases reducing overhead by more than an order of magnitude - while having negligible impact on path length. Finally, we argue that existing link-state protocols, such as OSPF, can incorporate XL routing in a backwards compatible and incrementally deployable fashion.",,15–26,12,"routing protocol, link-state",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1383,inproceedings,"Levchenko, Kirill and Voelker, Geoffrey M. and Paturi, Ramamohan and Savage, Stefan",Xl: An Efficient Network Routing Algorithm,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402962,10.1145/1402958.1402962,"In this paper, we present a new link-state routing algorithm called Approximate Link state (XL) aimed at increasing routing efficiency by suppressing updates from parts of the network. We prove that three simple criteria for update propagation are sufficient to guarantee soundness, completeness and bounded optimality for any such algorithm. We show, via simulation, that XL significantly outperforms standard link-state and distance vector algorithms - in some cases reducing overhead by more than an order of magnitude - while having negligible impact on path length. Finally, we argue that existing link-state protocols, such as OSPF, can incorporate XL routing in a backwards compatible and incrementally deployable fashion.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,15–26,12,"link-state, routing protocol","Seattle, WA, USA",SIGCOMM '08,,,,,,
1384,article,"Motiwala, Murtaza and Elmore, Megan and Feamster, Nick and Vempala, Santosh",Path Splicing,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402963,10.1145/1402946.1402963,"We present path splicing, a new routing primitive that allows network paths to be constructed by combining multiple routing trees (""slices"") to each destination over a single network topology. Path splicing allows traffic to switch trees at any hop en route to the destination. End systems can change the path on which traffic is forwarded by changing a small number of additional bits in the packet header. We evaluate path splicing for intradomain routing using slices generated from perturbed link weights and find that splicing achieves reliability that approaches the best possible using a small number of slices, for only a small increase in latency and no adverse effects on traffic in the network. In the case of interdomain routing, where splicing derives multiple trees from edges in alternate backup routes, path splicing achieves near-optimal reliability and can provide significant benefits even when only a fraction of ASes deploy it. We also describe several other applications of path splicing, as well as various possible deployment paths.",,27–38,12,"path diversity, path splicing, multi-path routing",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1385,inproceedings,"Motiwala, Murtaza and Elmore, Megan and Feamster, Nick and Vempala, Santosh",Path Splicing,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402963,10.1145/1402958.1402963,"We present path splicing, a new routing primitive that allows network paths to be constructed by combining multiple routing trees (""slices"") to each destination over a single network topology. Path splicing allows traffic to switch trees at any hop en route to the destination. End systems can change the path on which traffic is forwarded by changing a small number of additional bits in the packet header. We evaluate path splicing for intradomain routing using slices generated from perturbed link weights and find that splicing achieves reliability that approaches the best possible using a small number of slices, for only a small increase in latency and no adverse effects on traffic in the network. In the case of interdomain routing, where splicing derives multiple trees from edges in alternate backup routes, path splicing achieves near-optimal reliability and can provide significant benefits even when only a fraction of ASes deploy it. We also describe several other applications of path splicing, as well as various possible deployment paths.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,27–38,12,"path splicing, path diversity, multi-path routing","Seattle, WA, USA",SIGCOMM '08,,,,,,
1386,article,"Le, Franck and Xie, Geoffrey G. and Pei, Dan and Wang, Jia and Zhang, Hui",Shedding Light on the Glue Logic of the Internet Routing Architecture,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402964,10.1145/1402946.1402964,"Recent studies reveal that the routing structures of operational networks are much more complex than a simple BGP/IGP hierarchy, highlighted by the presence of many distinct instances of routing protocols. However, the glue (how routing protocol instances interact and exchange routes among themselves) is still little understood or studied. For example, although Route Redistribution (RR), the implementation of the glue in router software, has been used in the Internet for more than a decade, it was only recently shown that RR is extremely vulnerable to anomalies similar to the permanent route oscillations in BGP. This paper takes an important step toward understanding how RR is used and how fundamental the role RR plays in practice. We developed a complete model and associated tools for characterizing interconnections between routing instances based on analysis of router configuration data. We analyzed and characterized the RR usage in more than 1600 operational networks. The findings are: (i) RR is indeed widely used; (ii) operators use RR to achieve important design objectives not realizable with existing routing protocols alone; (iii) RR configurations can be very diverse and complex. These empirical discoveries not only confirm that the RR glue constitutes a critical component of the current Internet routing architecture, but also emphasize the urgent need for more research to improve its safety and flexibility to support important design objectives.",,39–50,12,"routing glue logic, route redistribution, route selection",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1387,inproceedings,"Le, Franck and Xie, Geoffrey G. and Pei, Dan and Wang, Jia and Zhang, Hui",Shedding Light on the Glue Logic of the Internet Routing Architecture,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402964,10.1145/1402958.1402964,"Recent studies reveal that the routing structures of operational networks are much more complex than a simple BGP/IGP hierarchy, highlighted by the presence of many distinct instances of routing protocols. However, the glue (how routing protocol instances interact and exchange routes among themselves) is still little understood or studied. For example, although Route Redistribution (RR), the implementation of the glue in router software, has been used in the Internet for more than a decade, it was only recently shown that RR is extremely vulnerable to anomalies similar to the permanent route oscillations in BGP. This paper takes an important step toward understanding how RR is used and how fundamental the role RR plays in practice. We developed a complete model and associated tools for characterizing interconnections between routing instances based on analysis of router configuration data. We analyzed and characterized the RR usage in more than 1600 operational networks. The findings are: (i) RR is indeed widely used; (ii) operators use RR to achieve important design objectives not realizable with existing routing protocols alone; (iii) RR configurations can be very diverse and complex. These empirical discoveries not only confirm that the RR glue constitutes a critical component of the current Internet routing architecture, but also emphasize the urgent need for more research to improve its safety and flexibility to support important design objectives.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,39–50,12,"routing glue logic, route redistribution, route selection","Seattle, WA, USA",SIGCOMM '08,,,,,,
1388,inproceedings,"Minshall, Greg",Session Details: Data Center Networking,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256735,10.1145/3256735,,Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,,,,"Seattle, WA, USA",SIGCOMM '08,,,,,,
1389,article,"Joseph, Dilip A. and Tavakoli, Arsalan and Stoica, Ion",A Policy-Aware Switching Layer for Data Centers,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402966,10.1145/1402946.1402966,"Data centers deploy a variety of middleboxes (e.g., firewalls, load balancers and SSL offloaders) to protect, manage and improve the performance of applications and services they run. Since existing networks provide limited support for middleboxes, administrators typically overload path selection mechanisms to coerce traffic through the desired sequences of middleboxes placed on the network path. These ad-hoc practices result in a data center network that is hard to configure and maintain, wastes middlebox resources, and cannot guarantee middlebox traversal under network churn.To address these issues, we propose the policy-aware switching layer or PLayer, a new layer-2 for data centers consisting of inter-connected policy-aware switches or pswitches. Unmodified middleboxes are placed off the network path by plugging them into pswitches. Based on policies specified by administrators, pswitches explicitly forward different types of traffic through different sequences of middleboxes. Experiments using our prototype software pswitches suggest that the PLayer is flexible, uses middleboxes efficiently, and guarantees correct middlebox traversal under churn.",,51–62,12,"indirection, policies, data center, middlebox, switching, layer-2",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1390,inproceedings,"Joseph, Dilip A. and Tavakoli, Arsalan and Stoica, Ion",A Policy-Aware Switching Layer for Data Centers,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402966,10.1145/1402958.1402966,"Data centers deploy a variety of middleboxes (e.g., firewalls, load balancers and SSL offloaders) to protect, manage and improve the performance of applications and services they run. Since existing networks provide limited support for middleboxes, administrators typically overload path selection mechanisms to coerce traffic through the desired sequences of middleboxes placed on the network path. These ad-hoc practices result in a data center network that is hard to configure and maintain, wastes middlebox resources, and cannot guarantee middlebox traversal under network churn.To address these issues, we propose the policy-aware switching layer or PLayer, a new layer-2 for data centers consisting of inter-connected policy-aware switches or pswitches. Unmodified middleboxes are placed off the network path by plugging them into pswitches. Based on policies specified by administrators, pswitches explicitly forward different types of traffic through different sequences of middleboxes. Experiments using our prototype software pswitches suggest that the PLayer is flexible, uses middleboxes efficiently, and guarantees correct middlebox traversal under churn.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,51–62,12,"switching, layer-2, indirection, policies, data center, middlebox","Seattle, WA, USA",SIGCOMM '08,,,,,,
1391,article,"Al-Fares, Mohammad and Loukissas, Alexander and Vahdat, Amin","A Scalable, Commodity Data Center Network Architecture",2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402967,10.1145/1402946.1402967,"Today's data centers may contain tens of thousands of computers with significant aggregate bandwidth requirements. The network architecture typically consists of a tree of routing and switching elements with progressively more specialized and expensive equipment moving up the network hierarchy. Unfortunately, even when deploying the highest-end IP switches/routers, resulting topologies may only support 50% of the aggregate bandwidth available at the edge of the network, while still incurring tremendous cost. Non-uniform bandwidth among data center nodes complicates application design and limits overall system performance.In this paper, we show how to leverage largely commodity Ethernet switches to support the full aggregate bandwidth of clusters consisting of tens of thousands of elements. Similar to how clusters of commodity computers have largely replaced more specialized SMPs and MPPs, we argue that appropriately architected and interconnected commodity switches may deliver more performance at less cost than available from today's higher-end solutions. Our approach requires no modifications to the end host network interface, operating system, or applications; critically, it is fully backward compatible with Ethernet, IP, and TCP.",,63–74,12,"data center topology, equal-cost routing",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1392,inproceedings,"Al-Fares, Mohammad and Loukissas, Alexander and Vahdat, Amin","A Scalable, Commodity Data Center Network Architecture",2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402967,10.1145/1402958.1402967,"Today's data centers may contain tens of thousands of computers with significant aggregate bandwidth requirements. The network architecture typically consists of a tree of routing and switching elements with progressively more specialized and expensive equipment moving up the network hierarchy. Unfortunately, even when deploying the highest-end IP switches/routers, resulting topologies may only support 50% of the aggregate bandwidth available at the edge of the network, while still incurring tremendous cost. Non-uniform bandwidth among data center nodes complicates application design and limits overall system performance.In this paper, we show how to leverage largely commodity Ethernet switches to support the full aggregate bandwidth of clusters consisting of tens of thousands of elements. Similar to how clusters of commodity computers have largely replaced more specialized SMPs and MPPs, we argue that appropriately architected and interconnected commodity switches may deliver more performance at less cost than available from today's higher-end solutions. Our approach requires no modifications to the end host network interface, operating system, or applications; critically, it is fully backward compatible with Ethernet, IP, and TCP.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,63–74,12,"data center topology, equal-cost routing","Seattle, WA, USA",SIGCOMM '08,,,,,,
1393,article,"Guo, Chuanxiong and Wu, Haitao and Tan, Kun and Shi, Lei and Zhang, Yongguang and Lu, Songwu",Dcell: A Scalable and Fault-Tolerant Network Structure for Data Centers,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402968,10.1145/1402946.1402968,"A fundamental challenge in data center networking is how to efficiently interconnect an exponentially increasing number of servers. This paper presents DCell, a novel network structure that has many desirable features for data center networking. DCell is a recursively defined structure, in which a high-level DCell is constructed from many low-level DCells and DCells at the same level are fully connected with one another. DCell scales doubly exponentially as the node degree increases. DCell is fault tolerant since it does not have single point of failure and its distributed fault-tolerant routing protocol performs near shortest-path routing even in the presence of severe link or node failures. DCell also provides higher network capacity than the traditional tree-based structure for various types of services. Furthermore, DCell can be incrementally expanded and a partial DCell provides the same appealing features. Results from theoretical analysis, simulations, and experiments show that DCell is a viable interconnection structure for data centers.",,75–86,12,"fault-tolerance, data center, throughput, network topology",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1394,inproceedings,"Guo, Chuanxiong and Wu, Haitao and Tan, Kun and Shi, Lei and Zhang, Yongguang and Lu, Songwu",Dcell: A Scalable and Fault-Tolerant Network Structure for Data Centers,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402968,10.1145/1402958.1402968,"A fundamental challenge in data center networking is how to efficiently interconnect an exponentially increasing number of servers. This paper presents DCell, a novel network structure that has many desirable features for data center networking. DCell is a recursively defined structure, in which a high-level DCell is constructed from many low-level DCells and DCells at the same level are fully connected with one another. DCell scales doubly exponentially as the node degree increases. DCell is fault tolerant since it does not have single point of failure and its distributed fault-tolerant routing protocol performs near shortest-path routing even in the presence of severe link or node failures. DCell also provides higher network capacity than the traditional tree-based structure for various types of services. Furthermore, DCell can be incrementally expanded and a partial DCell provides the same appealing features. Results from theoretical analysis, simulations, and experiments show that DCell is a viable interconnection structure for data centers.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,75–86,12,"data center, fault-tolerance, network topology, throughput","Seattle, WA, USA",SIGCOMM '08,,,,,,
1395,inproceedings,"Francis, Paul",Session Details: Management,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256736,10.1145/3256736,,Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,,,,"Seattle, WA, USA",SIGCOMM '08,,,,,,
1396,article,"Kandula, Srikanth and Chandra, Ranveer and Katabi, Dina",What's Going on? Learning Communication Rules in Edge Networks,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402970,10.1145/1402946.1402970,"Existing traffic analysis tools focus on traffic volume. They identify the heavy-hitters - flows that exchange high volumes of data, yet fail to identify the structure implicit in network traffic - do certain flows happen before, after or along with each other repeatedly over time? Since most traffic is generated by applications (web browsing, email, p2p), network traffic tends to be governed by a set of underlying rules. Malicious traffic such as network-wide scans for vulnerable hosts (mySQLbot) also presents distinct patterns.We present eXpose, a technique to learn the underlying rules that govern communication over a network. From packet timing information, eXpose learns rules for network communication that may be spread across multiple hosts, protocols or applications. Our key contribution is a novel statistical rule mining technique to extract significant communication patterns in a packet trace without explicitly being told what to look for. Going beyond rules involving flow pairs, eXpose introduces templates to systematically abstract away parts of flows thereby capturing rules that are otherwise unidentifiable. Deployments within our lab and within a large enterprise show that eXpose discovers rules that help with network monitoring, diagnosis, and intrusion detection with few false positives.",,87–98,12,"rule mining, expose, communication rules, correlation",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1397,inproceedings,"Kandula, Srikanth and Chandra, Ranveer and Katabi, Dina",What's Going on? Learning Communication Rules in Edge Networks,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402970,10.1145/1402958.1402970,"Existing traffic analysis tools focus on traffic volume. They identify the heavy-hitters - flows that exchange high volumes of data, yet fail to identify the structure implicit in network traffic - do certain flows happen before, after or along with each other repeatedly over time? Since most traffic is generated by applications (web browsing, email, p2p), network traffic tends to be governed by a set of underlying rules. Malicious traffic such as network-wide scans for vulnerable hosts (mySQLbot) also presents distinct patterns.We present eXpose, a technique to learn the underlying rules that govern communication over a network. From packet timing information, eXpose learns rules for network communication that may be spread across multiple hosts, protocols or applications. Our key contribution is a novel statistical rule mining technique to extract significant communication patterns in a packet trace without explicitly being told what to look for. Going beyond rules involving flow pairs, eXpose introduces templates to systematically abstract away parts of flows thereby capturing rules that are otherwise unidentifiable. Deployments within our lab and within a large enterprise show that eXpose discovers rules that help with network monitoring, diagnosis, and intrusion detection with few false positives.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,87–98,12,"rule mining, correlation, expose, communication rules","Seattle, WA, USA",SIGCOMM '08,,,,,,
1398,article,"Tariq, Mukarram and Zeitoun, Amgad and Valancius, Vytautas and Feamster, Nick and Ammar, Mostafa",Answering What-If Deployment and Configuration Questions with Wise,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402971,10.1145/1402946.1402971,"Designers of content distribution networks often need to determine how changes to infrastructure deployment and configuration affect service response times when they deploy a new data center, change ISP peering, or change the mapping of clients to servers. Today, the designers use coarse, back-of-the-envelope calculations, or costly field deployments; they need better ways to evaluate the effects of such hypothetical ""what-if"" questions before the actual deployments. This paper presents What-If Scenario Evaluator (WISE), a tool that predicts the effects of possible configuration and deployment changes in content distribution networks. WISE makes three contributions: (1) an algorithm that uses traces from existing deployments to learn causality among factors that affect service response-time distributions; (2) an algorithm that uses the learned causal structure to estimate a dataset that is representative of the hypothetical scenario that a designer may wish to evaluate, and uses these datasets to predict future response-time distributions; (3) a scenario specification language that allows a network designer to easily express hypothetical deployment scenarios without being cognizant of the dependencies between variables that affect service response times. Our evaluation, both in a controlled setting and in a real-world field deployment at a large, global CDN, shows that WISE can quickly and accurately predict service response-time distributions for many practical What-If scenarios.",,99–110,12,"content distribution networks, performance modeling, what-if scenario evaluation",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1399,inproceedings,"Tariq, Mukarram and Zeitoun, Amgad and Valancius, Vytautas and Feamster, Nick and Ammar, Mostafa",Answering What-If Deployment and Configuration Questions with Wise,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402971,10.1145/1402958.1402971,"Designers of content distribution networks often need to determine how changes to infrastructure deployment and configuration affect service response times when they deploy a new data center, change ISP peering, or change the mapping of clients to servers. Today, the designers use coarse, back-of-the-envelope calculations, or costly field deployments; they need better ways to evaluate the effects of such hypothetical ""what-if"" questions before the actual deployments. This paper presents What-If Scenario Evaluator (WISE), a tool that predicts the effects of possible configuration and deployment changes in content distribution networks. WISE makes three contributions: (1) an algorithm that uses traces from existing deployments to learn causality among factors that affect service response-time distributions; (2) an algorithm that uses the learned causal structure to estimate a dataset that is representative of the hypothetical scenario that a designer may wish to evaluate, and uses these datasets to predict future response-time distributions; (3) a scenario specification language that allows a network designer to easily express hypothetical deployment scenarios without being cognizant of the dependencies between variables that affect service response times. Our evaluation, both in a controlled setting and in a real-world field deployment at a large, global CDN, shows that WISE can quickly and accurately predict service response-time distributions for many practical What-If scenarios.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,99–110,12,"performance modeling, what-if scenario evaluation, content distribution networks","Seattle, WA, USA",SIGCOMM '08,,,,,,
1400,article,"Alimi, Richard and Wang, Ye and Yang, Y. Richard",Shadow Configuration as a Network Management Primitive,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402972,10.1145/1402946.1402972,"Configurations for today's IP networks are becoming increasingly complex. As a result, configuration management is becoming a major cost factor for network providers and configuration errors are becoming a major cause of network disruptions. In this paper, we present and evaluate the novel idea of shadow configurations. Shadow configurations allow configuration evaluation before deployment and thus can reduce potential network disruptions. We demonstrate using real implementation that shadow configurations can be implemented with low overhead.",,111–122,12,"network diagnostics, network management",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1401,inproceedings,"Alimi, Richard and Wang, Ye and Yang, Y. Richard",Shadow Configuration as a Network Management Primitive,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402972,10.1145/1402958.1402972,"Configurations for today's IP networks are becoming increasingly complex. As a result, configuration management is becoming a major cost factor for network providers and configuration errors are becoming a major cause of network disruptions. In this paper, we present and evaluate the novel idea of shadow configurations. Shadow configurations allow configuration evaluation before deployment and thus can reduce potential network disruptions. We demonstrate using real implementation that shadow configurations can be implemented with low overhead.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,111–122,12,"network management, network diagnostics","Seattle, WA, USA",SIGCOMM '08,,,,,,
1402,article,"Karagiannis, Thomas and Mortier, Richard and Rowstron, Antony",Network Exception Handlers: Host-Network Control in Enterprise Networks,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402973,10.1145/1402946.1402973,"Enterprise network architecture and management have followed the Internet's design principles despite different requirements and characteristics: enterprise hosts are administered by a single authority, which intrinsically assigns different values to traffic from different business applications.We advocate a new approach where hosts are no longer relegated to the network's periphery, but actively participate in network-related decisions. To enable host participation, network information, such as dynamic network topology and per-link characteristics and costs, is exposed to the hosts, and network administrators specify conditions on the propagated network information that trigger actions to be performed while a condition holds. The combination of a condition and its actions embodies the concept of the network exception handler, defined analogous to a program exception handler. Conceptually, network exception handlers execute on hosts with actions parameterized by network and host state.Network exception handlers allow hosts to participate in network management, traffic engineering and other operational decisions by explicitly controlling host traffic under predefined conditions. This flexibility improves overall performance by allowing efficient use of network resources. We outline several sample network exception handlers, present an architecture to support them, and evaluate them using data collected from our own enterprise network.",,123–134,12,"enterprise networks, management, network exception handlers",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1403,inproceedings,"Karagiannis, Thomas and Mortier, Richard and Rowstron, Antony",Network Exception Handlers: Host-Network Control in Enterprise Networks,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402973,10.1145/1402958.1402973,"Enterprise network architecture and management have followed the Internet's design principles despite different requirements and characteristics: enterprise hosts are administered by a single authority, which intrinsically assigns different values to traffic from different business applications.We advocate a new approach where hosts are no longer relegated to the network's periphery, but actively participate in network-related decisions. To enable host participation, network information, such as dynamic network topology and per-link characteristics and costs, is exposed to the hosts, and network administrators specify conditions on the propagated network information that trigger actions to be performed while a condition holds. The combination of a condition and its actions embodies the concept of the network exception handler, defined analogous to a program exception handler. Conceptually, network exception handlers execute on hosts with actions parameterized by network and host state.Network exception handlers allow hosts to participate in network management, traffic engineering and other operational decisions by explicitly controlling host traffic under predefined conditions. This flexibility improves overall performance by allowing efficient use of network resources. We outline several sample network exception handlers, present an architecture to support them, and evaluate them using data collected from our own enterprise network.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,123–134,12,"management, enterprise networks, network exception handlers","Seattle, WA, USA",SIGCOMM '08,,,,,,
1404,inproceedings,"Qiu, Lili",Session Details: Wireless I,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256737,10.1145/3256737,,Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,,,,"Seattle, WA, USA",SIGCOMM '08,,,,,,
1405,article,"Chandra, Ranveer and Mahajan, Ratul and Moscibroda, Thomas and Raghavendra, Ramya and Bahl, Paramvir",A Case for Adapting Channel Width in Wireless Networks,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402975,10.1145/1402946.1402975,"We study a fundamental yet under-explored facet in wireless communication -- the width of the spectrum over which transmitters spread their signals, or the channel width. Through detailed measurements in controlled and live environments, and using only commodity 802.11 hardware, we first quantify the impact of channel width on throughput, range, and power consumption. Taken together, our findings make a strong case for wireless systems that adapt channel width. Such adaptation brings unique benefits. For instance, when the throughput required is low, moving to a narrower channel increases range and reduces power consumption; in fixed-width systems, these two quantities are always in conflict. We then present a channel width adaptation algorithm, called SampleWidth, for the base case of two communicating nodes. This algorithm is based on a simple search process that builds on top of existing techniques for adapting modulation. Per specified policy, it can maximize throughput or minimize power consumption. Evaluation using a prototype implementation shows that SampleWidth correctly identities the optimal width under a range of scenarios. In our experiments with mobility, it increases throughput by more than 60% compared to the best fixed-width configuration.",,135–146,12,channel width,,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1406,inproceedings,"Chandra, Ranveer and Mahajan, Ratul and Moscibroda, Thomas and Raghavendra, Ramya and Bahl, Paramvir",A Case for Adapting Channel Width in Wireless Networks,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402975,10.1145/1402958.1402975,"We study a fundamental yet under-explored facet in wireless communication -- the width of the spectrum over which transmitters spread their signals, or the channel width. Through detailed measurements in controlled and live environments, and using only commodity 802.11 hardware, we first quantify the impact of channel width on throughput, range, and power consumption. Taken together, our findings make a strong case for wireless systems that adapt channel width. Such adaptation brings unique benefits. For instance, when the throughput required is low, moving to a narrower channel increases range and reduces power consumption; in fixed-width systems, these two quantities are always in conflict. We then present a channel width adaptation algorithm, called SampleWidth, for the base case of two communicating nodes. This algorithm is based on a simple search process that builds on top of existing techniques for adapting modulation. Per specified policy, it can maximize throughput or minimize power consumption. Evaluation using a prototype implementation shows that SampleWidth correctly identities the optimal width under a range of scenarios. In our experiments with mobility, it increases throughput by more than 60% compared to the best fixed-width configuration.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,135–146,12,channel width,"Seattle, WA, USA",SIGCOMM '08,,,,,,
1407,article,"Rahul, Hariharan and Kushman, Nate and Katabi, Dina and Sodini, Charles and Edalat, Farinaz",Learning to Share: Narrowband-Friendly Wideband Networks,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402976,10.1145/1402946.1402976,"Wideband technologies in the unlicensed spectrum can satisfy the ever-increasing demands for wireless bandwidth created by emerging rich media applications. The key challenge for such systems, however, is to allow narrowband technologies that share these bands (say, 802.11 a/b/g/n, Zigbee) to achieve their normal performance, without compromising the throughput or range of the wideband network.This paper presents SWIFT, the first system where high-throughput wideband nodes are shown in a working deployment to coexist with unknown narrowband devices, while forming a network of their own. Prior work avoids narrowband devices by operating below the noise level and limiting itself to a single contiguous unused band. While this achieves coexistence, it sacrifices the throughput and operating distance of the wideband device. In contrast, SWIFT creates high throughput wireless links by weaving together non-contiguous unused frequency bands that change as narrowband devices enter or leave the environment. This design principle of cognitive aggregation allows SWIFT to achieve coexistence, while operating at normal power, and thereby obtaining higher throughput and greater operating range. We implement SWIFT on a wideband hardware platform, and evaluate it in the presence of 802.11 devices. In comparison to a baseline that coexists with narrowband devices by operating below their noise level, SWIFT is equally narrowband-friendly but achieves 3.6-10.5x higher throughput and 6x greater range.",,147–158,12,"cognitive radios, wideband radios, wireless networks",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1408,inproceedings,"Rahul, Hariharan and Kushman, Nate and Katabi, Dina and Sodini, Charles and Edalat, Farinaz",Learning to Share: Narrowband-Friendly Wideband Networks,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402976,10.1145/1402958.1402976,"Wideband technologies in the unlicensed spectrum can satisfy the ever-increasing demands for wireless bandwidth created by emerging rich media applications. The key challenge for such systems, however, is to allow narrowband technologies that share these bands (say, 802.11 a/b/g/n, Zigbee) to achieve their normal performance, without compromising the throughput or range of the wideband network.This paper presents SWIFT, the first system where high-throughput wideband nodes are shown in a working deployment to coexist with unknown narrowband devices, while forming a network of their own. Prior work avoids narrowband devices by operating below the noise level and limiting itself to a single contiguous unused band. While this achieves coexistence, it sacrifices the throughput and operating distance of the wideband device. In contrast, SWIFT creates high throughput wireless links by weaving together non-contiguous unused frequency bands that change as narrowband devices enter or leave the environment. This design principle of cognitive aggregation allows SWIFT to achieve coexistence, while operating at normal power, and thereby obtaining higher throughput and greater operating range. We implement SWIFT on a wideband hardware platform, and evaluate it in the presence of 802.11 devices. In comparison to a baseline that coexists with narrowband devices by operating below their noise level, SWIFT is equally narrowband-friendly but achieves 3.6-10.5x higher throughput and 6x greater range.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,147–158,12,"wideband radios, wireless networks, cognitive radios","Seattle, WA, USA",SIGCOMM '08,,,,,,
1409,article,"Gollakota, Shyamnath and Katabi, Dina",Zigzag Decoding: Combating Hidden Terminals in Wireless Networks,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402977,10.1145/1402946.1402977,"This paper presents ZigZag, an 802.11 receiver design that combats hidden terminals. ZigZag's core contribution is a new form of interference cancellation that exploits asynchrony across successive collisions. Specifically, 802.11 retransmissions, in the case of hidden terminals, cause successive collisions. These collisions have different interference-free stretches at their start, which ZigZag exploits to bootstrap its decoding.ZigZag makes no changes to the 802.11 MAC and introduces no overhead when there are no collisions. But, when senders collide, ZigZag attains the same throughput as if the colliding packets were a priori scheduled in separate time slots. We build a prototype of ZigZag in GNU Radio. In a testbed of 14 USRP nodes, ZigZag reduces the average packet loss rate at hidden terminals from 72.6% to about 0.7%.",,159–170,12,"interference cancellation, wireless, hidden terminals",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1410,inproceedings,"Gollakota, Shyamnath and Katabi, Dina",Zigzag Decoding: Combating Hidden Terminals in Wireless Networks,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402977,10.1145/1402958.1402977,"This paper presents ZigZag, an 802.11 receiver design that combats hidden terminals. ZigZag's core contribution is a new form of interference cancellation that exploits asynchrony across successive collisions. Specifically, 802.11 retransmissions, in the case of hidden terminals, cause successive collisions. These collisions have different interference-free stretches at their start, which ZigZag exploits to bootstrap its decoding.ZigZag makes no changes to the 802.11 MAC and introduces no overhead when there are no collisions. But, when senders collide, ZigZag attains the same throughput as if the colliding packets were a priori scheduled in separate time slots. We build a prototype of ZigZag in GNU Radio. In a testbed of 14 USRP nodes, ZigZag reduces the average packet loss rate at hidden terminals from 72.6% to about 0.7%.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,159–170,12,"interference cancellation, hidden terminals, wireless","Seattle, WA, USA",SIGCOMM '08,,,,,,
1411,inproceedings,"Mao, Z. Morley",Session Details: Security I,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256738,10.1145/3256738,,Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,,,,"Seattle, WA, USA",SIGCOMM '08,,,,,,
1412,article,"Xie, Yinglian and Yu, Fang and Achan, Kannan and Panigrahy, Rina and Hulten, Geoff and Osipkov, Ivan",Spamming Botnets: Signatures and Characteristics,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402979,10.1145/1402946.1402979,"In this paper, we focus on characterizing spamming botnets by leveraging both spam payload and spam server traffic properties. Towards this goal, we developed a spam signature generation framework called AutoRE to detect botnet-based spam emails and botnet membership. AutoRE does not require pre-classified training data or white lists. Moreover, it outputs high quality regular expression signatures that can detect botnet spam with a low false positive rate. Using a three-month sample of emails from Hotmail, AutoRE successfully identified 7,721 botnet-based spam campaigns together with 340,050 unique botnet host IP addresses.Our in-depth analysis of the identified botnets revealed several interesting findings regarding the degree of email obfuscation, properties of botnet IP addresses, sending patterns, and their correlation with network scanning traffic. We believe these observations are useful information in the design of botnet detection schemes.",,171–182,12,"botnet, regular expression, signature generation, spam",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1413,inproceedings,"Xie, Yinglian and Yu, Fang and Achan, Kannan and Panigrahy, Rina and Hulten, Geoff and Osipkov, Ivan",Spamming Botnets: Signatures and Characteristics,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402979,10.1145/1402958.1402979,"In this paper, we focus on characterizing spamming botnets by leveraging both spam payload and spam server traffic properties. Towards this goal, we developed a spam signature generation framework called AutoRE to detect botnet-based spam emails and botnet membership. AutoRE does not require pre-classified training data or white lists. Moreover, it outputs high quality regular expression signatures that can detect botnet spam with a low false positive rate. Using a three-month sample of emails from Hotmail, AutoRE successfully identified 7,721 botnet-based spam campaigns together with 340,050 unique botnet host IP addresses.Our in-depth analysis of the identified botnets revealed several interesting findings regarding the degree of email obfuscation, properties of botnet IP addresses, sending patterns, and their correlation with network scanning traffic. We believe these observations are useful information in the design of botnet detection schemes.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,171–182,12,"regular expression, botnet, spam, signature generation","Seattle, WA, USA",SIGCOMM '08,,,,,,
1414,article,"Maier, Gregor and Sommer, Robin and Dreger, Holger and Feldmann, Anja and Paxson, Vern and Schneider, Fabian",Enriching Network Security Analysis with Time Travel,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402980,10.1145/1402946.1402980,"In many situations it can be enormously helpful to archive the raw contents of a network traffic stream to disk, to enable later inspection of activity that becomes interesting only in retrospect. We present a Time Machine (TM) for network traffic that provides such a capability. The TM leverages the heavy-tailed nature of network flows to capture nearly all of the likely-interesting traffic while storing only a small fraction of the total volume. An initial proof-of-principle prototype established the forensic value of such an approach, contributing to the investigation of numerous attacks at a site with thousands of users. Based on these experiences, a rearchitected implementation of the system provides flexible, highperformance traffic stream capture, indexing and retrieval, including an interface between the TM and a real-time network intrusion detection system (NIDS). The NIDS controls the TM by dynamically adjusting recording parameters, instructing it to permanently store suspicious activity for offline forensics, and fetching traffic from the past for retrospective analysis. We present a detailed performance evaluation of both stand-alone and joint setups, and report on experiences with running the system live in high-volume environments.",,183–194,12,"packet capture, intrusion detection, forensics",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1415,inproceedings,"Maier, Gregor and Sommer, Robin and Dreger, Holger and Feldmann, Anja and Paxson, Vern and Schneider, Fabian",Enriching Network Security Analysis with Time Travel,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402980,10.1145/1402958.1402980,"In many situations it can be enormously helpful to archive the raw contents of a network traffic stream to disk, to enable later inspection of activity that becomes interesting only in retrospect. We present a Time Machine (TM) for network traffic that provides such a capability. The TM leverages the heavy-tailed nature of network flows to capture nearly all of the likely-interesting traffic while storing only a small fraction of the total volume. An initial proof-of-principle prototype established the forensic value of such an approach, contributing to the investigation of numerous attacks at a site with thousands of users. Based on these experiences, a rearchitected implementation of the system provides flexible, highperformance traffic stream capture, indexing and retrieval, including an interface between the TM and a real-time network intrusion detection system (NIDS). The NIDS controls the TM by dynamically adjusting recording parameters, instructing it to permanently store suspicious activity for offline forensics, and fetching traffic from the past for retrospective analysis. We present a detailed performance evaluation of both stand-alone and joint setups, and report on experiences with running the system live in high-volume environments.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,183–194,12,"packet capture, intrusion detection, forensics","Seattle, WA, USA",SIGCOMM '08,,,,,,
1416,article,"Liu, Xin and Yang, Xiaowei and Lu, Yanbin",To Filter or to Authorize: Network-Layer DoS Defense against Multimillion-Node Botnets,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402981,10.1145/1402946.1402981,"This paper presents the design and implementation of a filter-based DoS defense system (StopIt) and a comparison study on the effectiveness of filters and capabilities. Central to the StopIt design is a novel closed-control, open-service architecture: any receiver can use StopIt to block the undesired traffic it receives, yet the design is robust to various strategic attacks from millions of bots, including filter exhaustion attacks and bandwidth flooding attacks that aim to disrupt the timely installation of filters. Our evaluation shows that StopIt can block the attack traffic from a few millions of attackers within tens of minutes with bounded router memory. We compare StopIt with existing filter-based and capability-based DoS defense systems under simulated DoS attacks of various types and scales. Our results show that StopIt outperforms existing filter-based systems, and can prevent legitimate communications from being disrupted by various DoS flooding attacks. It also outperforms capability-based systems in most attack scenarios, but a capability-based system is more effective in a type of attack that the attack traffic does not reach a victim, but congests a link shared by the victim. These results suggest that both filters and capabilities are highly effective DoS defense mechanisms, but neither is more effective than the other in all types of DoS attacks.",,195–206,12,"denial-of-service, capability, internet, filter",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1417,inproceedings,"Liu, Xin and Yang, Xiaowei and Lu, Yanbin",To Filter or to Authorize: Network-Layer DoS Defense against Multimillion-Node Botnets,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402981,10.1145/1402958.1402981,"This paper presents the design and implementation of a filter-based DoS defense system (StopIt) and a comparison study on the effectiveness of filters and capabilities. Central to the StopIt design is a novel closed-control, open-service architecture: any receiver can use StopIt to block the undesired traffic it receives, yet the design is robust to various strategic attacks from millions of bots, including filter exhaustion attacks and bandwidth flooding attacks that aim to disrupt the timely installation of filters. Our evaluation shows that StopIt can block the attack traffic from a few millions of attackers within tens of minutes with bounded router memory. We compare StopIt with existing filter-based and capability-based DoS defense systems under simulated DoS attacks of various types and scales. Our results show that StopIt outperforms existing filter-based systems, and can prevent legitimate communications from being disrupted by various DoS flooding attacks. It also outperforms capability-based systems in most attack scenarios, but a capability-based system is more effective in a type of attack that the attack traffic does not reach a victim, but congests a link shared by the victim. These results suggest that both filters and capabilities are highly effective DoS defense mechanisms, but neither is more effective than the other in all types of DoS attacks.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,195–206,12,"denial-of-service, filter, capability, internet","Seattle, WA, USA",SIGCOMM '08,,,,,,
1418,inproceedings,"Lockwood, John",Session Details: Router Primitives,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256739,10.1145/3256739,,Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,,,,"Seattle, WA, USA",SIGCOMM '08,,,,,,
1419,article,"Smith, Randy and Estan, Cristian and Jha, Somesh and Kong, Shijin",Deflating the Big Bang: Fast and Scalable Deep Packet Inspection with Extended Finite Automata,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402983,10.1145/1402946.1402983,"Deep packet inspection is playing an increasingly important role in the design of novel network services. Regular expressions are the language of choice for writing signatures, but standard DFA or NFA representations are unsuitable for high-speed environments, requiring too much memory, too much time, or too much per-flow state. DFAs are fast and can be readily combined, but doing so often leads to state-space explosion. NFAs, while small, require large per-flow state and are slow.We propose a solution that simultaneously addresses all these problems. We start with a first-principles characterization of state-space explosion and give conditions that eliminate it when satisfied. We show how auxiliary variables can be used to transform automata so that they satisfy these conditions, which we codify in a formal model that augments DFAs with auxiliary variables and simple instructions for manipulating them. Building on this model, we present techniques, inspired by principles used in compiler optimization, that systematically reduce runtime and per-flow state. In our experiments, signature sets from Snort and Cisco Systems achieve state-space reductions of over four orders of magnitude, per-flow state reductions of up to a factor of six, and runtimes that approach DFAs.",,207–218,12,"deep packet inspection, regular expressions, signature matching, xfa",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1420,inproceedings,"Smith, Randy and Estan, Cristian and Jha, Somesh and Kong, Shijin",Deflating the Big Bang: Fast and Scalable Deep Packet Inspection with Extended Finite Automata,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402983,10.1145/1402958.1402983,"Deep packet inspection is playing an increasingly important role in the design of novel network services. Regular expressions are the language of choice for writing signatures, but standard DFA or NFA representations are unsuitable for high-speed environments, requiring too much memory, too much time, or too much per-flow state. DFAs are fast and can be readily combined, but doing so often leads to state-space explosion. NFAs, while small, require large per-flow state and are slow.We propose a solution that simultaneously addresses all these problems. We start with a first-principles characterization of state-space explosion and give conditions that eliminate it when satisfied. We show how auxiliary variables can be used to transform automata so that they satisfy these conditions, which we codify in a formal model that augments DFAs with auxiliary variables and simple instructions for manipulating them. Building on this model, we present techniques, inspired by principles used in compiler optimization, that systematically reduce runtime and per-flow state. In our experiments, signature sets from Snort and Cisco Systems achieve state-space reductions of over four orders of magnitude, per-flow state reductions of up to a factor of six, and runtimes that approach DFAs.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,207–218,12,"regular expressions, xfa, deep packet inspection, signature matching","Seattle, WA, USA",SIGCOMM '08,,,,,,
1421,article,"Anand, Ashok and Gupta, Archit and Akella, Aditya and Seshan, Srinivasan and Shenker, Scott",Packet Caches on Routers: The Implications of Universal Redundant Traffic Elimination,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402984,10.1145/1402946.1402984,"Many past systems have explored how to eliminate redundant transfers from network links and improve network efficiency. Several of these systems operate at the application layer, while the more recent systems operate on individual packets. A common aspect of these systems is that they apply to localized settings, e.g. at stub network access links. In this paper, we explore the benefits of deploying packet-level redundant content elimination as a universal primitive on all Internet routers. Such a universal deployment would immediately reduce link loads everywhere. However, we argue that far more significant network-wide benefits can be derived by redesigning network routing protocols to leverage the universal deployment. We develop ""redundancy-aware"" intra- and inter-domain routing algorithms and show that they enable better traffic engineering, reduce link usage costs, and enhance ISPs' responsiveness to traffic variations. In particular, employing redundancy elimination approaches across redundancy-aware routes can lower intra and inter-domain link loads by 10-50%. We also address key challenges that may hinder implementation of redundancy elimination on fast routers. Our current software router implementation can run at OC48 speeds.",,219–230,12,"traffic redundancy, routing, traffic engineering",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1422,inproceedings,"Anand, Ashok and Gupta, Archit and Akella, Aditya and Seshan, Srinivasan and Shenker, Scott",Packet Caches on Routers: The Implications of Universal Redundant Traffic Elimination,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402984,10.1145/1402958.1402984,"Many past systems have explored how to eliminate redundant transfers from network links and improve network efficiency. Several of these systems operate at the application layer, while the more recent systems operate on individual packets. A common aspect of these systems is that they apply to localized settings, e.g. at stub network access links. In this paper, we explore the benefits of deploying packet-level redundant content elimination as a universal primitive on all Internet routers. Such a universal deployment would immediately reduce link loads everywhere. However, we argue that far more significant network-wide benefits can be derived by redesigning network routing protocols to leverage the universal deployment. We develop ""redundancy-aware"" intra- and inter-domain routing algorithms and show that they enable better traffic engineering, reduce link usage costs, and enhance ISPs' responsiveness to traffic variations. In particular, employing redundancy elimination approaches across redundancy-aware routes can lower intra and inter-domain link loads by 10-50%. We also address key challenges that may hinder implementation of redundancy elimination on fast routers. Our current software router implementation can run at OC48 speeds.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,219–230,12,"traffic engineering, traffic redundancy, routing","Seattle, WA, USA",SIGCOMM '08,,,,,,
1423,article,"Wang, Yi and Keller, Eric and Biskeborn, Brian and van der Merwe, Jacobus and Rexford, Jennifer",Virtual Routers on the Move: Live Router Migration as a Network-Management Primitive,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402985,10.1145/1402946.1402985,"The complexity of network management is widely recognized as one of the biggest challenges facing the Internet today. Point solutions for individual problems further increase system complexity while not addressing the underlying causes. In this paper, we argue that many network-management problems stem from the same root cause---the need to maintain consistency between the physical and logical configuration of the routers. Hence, we propose VROOM (Virtual ROuters On the Move), a new network-management primitive that avoids unnecessary changes to the logical topology by allowing (virtual) routers to freely move from one physical node to another. In addition to simplifying existing network-management tasks like planned maintenance and service deployment, VROOM can also help tackle emerging challenges such as reducing energy consumption. We present the design, implementation, and evaluation of novel migration techniques for virtual routers with either hardware or software data planes. Our evaluation shows that VROOM is transparent to routing protocols and results in no performance impact on the data traffic when a hardware-based data plane is used.",,231–242,12,"architecture, internet, routing, migration, virtual router",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1424,inproceedings,"Wang, Yi and Keller, Eric and Biskeborn, Brian and van der Merwe, Jacobus and Rexford, Jennifer",Virtual Routers on the Move: Live Router Migration as a Network-Management Primitive,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402985,10.1145/1402958.1402985,"The complexity of network management is widely recognized as one of the biggest challenges facing the Internet today. Point solutions for individual problems further increase system complexity while not addressing the underlying causes. In this paper, we argue that many network-management problems stem from the same root cause---the need to maintain consistency between the physical and logical configuration of the routers. Hence, we propose VROOM (Virtual ROuters On the Move), a new network-management primitive that avoids unnecessary changes to the logical topology by allowing (virtual) routers to freely move from one physical node to another. In addition to simplifying existing network-management tasks like planned maintenance and service deployment, VROOM can also help tackle emerging challenges such as reducing energy consumption. We present the design, implementation, and evaluation of novel migration techniques for virtual routers with either hardware or software data planes. Our evaluation shows that VROOM is transparent to routing protocols and results in no performance impact on the data traffic when a hardware-based data plane is used.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,231–242,12,"architecture, virtual router, routing, migration, internet","Seattle, WA, USA",SIGCOMM '08,,,,,,
1425,inproceedings,"Snoeren, Alex",Session Details: Incentives,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256740,10.1145/3256740,,Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,,,,"Seattle, WA, USA",SIGCOMM '08,,,,,,
1426,article,"Levin, Dave and LaCurts, Katrina and Spring, Neil and Bhattacharjee, Bobby",Bittorrent is an Auction: Analyzing and Improving Bittorrent's Incentives,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402987,10.1145/1402946.1402987,"Incentives play a crucial role in BitTorrent, motivating users to upload to others to achieve fast download times for all peers. Though long believed to be robust to strategic manipulation, recent work has empirically shown that BitTorrent does not provide its users incentive to follow the protocol. We propose an auction-based model to study and improve upon BitTorrent's incentives. The insight behind our model is that BitTorrent uses, not tit-for-tat as widely believed, but an auction to decide which peers to serve. Our model not only captures known, performance-improving strategies, it shapes our thinking toward new, effective strategies. For example, our analysis demonstrates, counter-intuitively, that BitTorrent peers have incentive to intelligently under-report what pieces of the file they have to their neighbors. We implement and evaluate a modification to BitTorrent in which peers reward one another with proportional shares of bandwidth. Within our game-theoretic model, we prove that a proportional-share client is strategy-proof. With experiments on PlanetLab, a local cluster, and live downloads, we show that a proportional-share unchoker yields faster downloads against BitTorrent and BitTyrant clients, and that under-reporting pieces yields prolonged neighbor interest.",,243–254,12,"tit-for-tat, auctions, bittorrent, incentive systems, proportional share",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1427,inproceedings,"Levin, Dave and LaCurts, Katrina and Spring, Neil and Bhattacharjee, Bobby",Bittorrent is an Auction: Analyzing and Improving Bittorrent's Incentives,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402987,10.1145/1402958.1402987,"Incentives play a crucial role in BitTorrent, motivating users to upload to others to achieve fast download times for all peers. Though long believed to be robust to strategic manipulation, recent work has empirically shown that BitTorrent does not provide its users incentive to follow the protocol. We propose an auction-based model to study and improve upon BitTorrent's incentives. The insight behind our model is that BitTorrent uses, not tit-for-tat as widely believed, but an auction to decide which peers to serve. Our model not only captures known, performance-improving strategies, it shapes our thinking toward new, effective strategies. For example, our analysis demonstrates, counter-intuitively, that BitTorrent peers have incentive to intelligently under-report what pieces of the file they have to their neighbors. We implement and evaluate a modification to BitTorrent in which peers reward one another with proportional shares of bandwidth. Within our game-theoretic model, we prove that a proportional-share client is strategy-proof. With experiments on PlanetLab, a local cluster, and live downloads, we show that a proportional-share unchoker yields faster downloads against BitTorrent and BitTyrant clients, and that under-reporting pieces yields prolonged neighbor interest.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,243–254,12,"auctions, proportional share, bittorrent, tit-for-tat, incentive systems","Seattle, WA, USA",SIGCOMM '08,,,,,,
1428,article,"Podlesny, Maxim and Gorinsky, Sergey",Rd Network Services: Differentiation through Performance Incentives,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402988,10.1145/1402946.1402988,"With the Internet offering a single best-effort service, there have been numerous proposals of diversified network services that align better with the divergent needs of different distributed applications. The failure of these innovative architectures to gain wide deployment is primarily due to economic and legacy issues, rather than technical shortcomings. We propose a new paradigm for network service differentiation where design principles account explicitly for the multiplicity of Internet service providers and users as well as their economic interests in environments with partly deployed new services. Our key idea is to base the service differentiation on performance itself, rather than price. The proposed RD (Rate-Delay) services enable a user to choose between a higher transmission rate or low queuing delay at a congested network link. An RD router supports the two services by maintaining two queues per output link and achieves the intended rate-delay differentiation through simple link scheduling and dynamic buffer sizing. After analytically deriving specific rules for RD router operation, we conduct extensive simulations that confirm effectiveness of the RD services geared for incremental deployment in the Internet.",,255–266,12,"performance incentive, transmission rate, legacy infrastructure, legacy traffic, service differentiation, incremental deployment, queuing delay",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1429,inproceedings,"Podlesny, Maxim and Gorinsky, Sergey",Rd Network Services: Differentiation through Performance Incentives,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402988,10.1145/1402958.1402988,"With the Internet offering a single best-effort service, there have been numerous proposals of diversified network services that align better with the divergent needs of different distributed applications. The failure of these innovative architectures to gain wide deployment is primarily due to economic and legacy issues, rather than technical shortcomings. We propose a new paradigm for network service differentiation where design principles account explicitly for the multiplicity of Internet service providers and users as well as their economic interests in environments with partly deployed new services. Our key idea is to base the service differentiation on performance itself, rather than price. The proposed RD (Rate-Delay) services enable a user to choose between a higher transmission rate or low queuing delay at a congested network link. An RD router supports the two services by maintaining two queues per output link and achieves the intended rate-delay differentiation through simple link scheduling and dynamic buffer sizing. After analytically deriving specific rules for RD router operation, we conduct extensive simulations that confirm effectiveness of the RD services geared for incremental deployment in the Internet.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,255–266,12,"service differentiation, transmission rate, incremental deployment, performance incentive, queuing delay, legacy infrastructure, legacy traffic","Seattle, WA, USA",SIGCOMM '08,,,,,,
1430,article,"Goldberg, Sharon and Halevi, Shai and Jaggard, Aaron D. and Ramachandran, Vijay and Wright, Rebecca N.",Rationality and Traffic Attraction: Incentives for Honest Path Announcements in Bgp,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402989,10.1145/1402946.1402989,"We study situations in which autonomous systems (ASes) may have incentives to send BGP announcements differing from the AS-level paths that packets traverse in the data plane. Prior work on this issue assumed that ASes seek only to obtain the best possible outgoing path for their traffic. In reality, other factors can influence a rational AS's behavior. Here we consider a more natural model, in which an AS is also interested in attracting incoming traffic (e.g., because other ASes pay it to carry their traffic). We ask what combinations of BGP enhancements and restrictions on routing policies can ensure that ASes have no incentive to lie about their data-plane paths. We find that protocols like S-BGP alone are insufficient, but that S-BGP does suffice if coupled with additional (quite unrealistic) restrictions on routing policies. Our game-theoretic analysis illustrates the high cost of ensuring that the ASes honestly announce data-plane paths in their BGP path announcements.",,267–278,12,"bgp, incentives",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1431,inproceedings,"Goldberg, Sharon and Halevi, Shai and Jaggard, Aaron D. and Ramachandran, Vijay and Wright, Rebecca N.",Rationality and Traffic Attraction: Incentives for Honest Path Announcements in Bgp,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402989,10.1145/1402958.1402989,"We study situations in which autonomous systems (ASes) may have incentives to send BGP announcements differing from the AS-level paths that packets traverse in the data plane. Prior work on this issue assumed that ASes seek only to obtain the best possible outgoing path for their traffic. In reality, other factors can influence a rational AS's behavior. Here we consider a more natural model, in which an AS is also interested in attracting incoming traffic (e.g., because other ASes pay it to carry their traffic). We ask what combinations of BGP enhancements and restrictions on routing policies can ensure that ASes have no incentive to lie about their data-plane paths. We find that protocols like S-BGP alone are insufficient, but that S-BGP does suffice if coupled with additional (quite unrealistic) restrictions on routing policies. Our game-theoretic analysis illustrates the high cost of ensuring that the ASes honestly announce data-plane paths in their BGP path announcements.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,267–278,12,"bgp, incentives","Seattle, WA, USA",SIGCOMM '08,,,,,,
1432,inproceedings,"Andersen, David",Session Details: Measurement,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256741,10.1145/3256741,,Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,,,,"Seattle, WA, USA",SIGCOMM '08,,,,,,
1433,article,"Trestian, Ionut and Ranjan, Supranamaya and Kuzmanovi, Aleksandar and Nucci, Antonio",Unconstrained Endpoint Profiling (Googling the Internet),2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402991,10.1145/1402946.1402991,"Understanding Internet access trends at a global scale, i.e., what do people do on the Internet, is a challenging problem that is typically addressed by analyzing network traces. However, obtaining such traces presents its own set of challenges owing to either privacy concerns or to other operational difficulties. The key hypothesis of our work here is that most of the information needed to profile the Internet endpoints is already available around us - on the web.In this paper, we introduce a novel approach for profiling and classifying endpoints. We implement and deploy a Google-based profiling tool, which accurately characterizes endpoint behavior by collecting and strategically combining information freely available on the web. Our 'unconstrained endpoint profiling' approach shows remarkable advances in the following scenarios: (i) Even when no packet traces are available, it can accurately predict application and protocol usage trends at arbitrary networks; (ii) When network traces are available, it dramatically outperforms state-of-the-art classification tools; (iii) When sampled flow-level traces are available, it retains high classification capabilities when other schemes literally fall apart. Using this approach, we perform unconstrained endpoint profiling at a global scale: for clients in four different world regions (Asia, South and North America and Europe). We provide the first-of-its-kind endpoint analysis which reveals fascinating similarities and differences among these regions.",,279–290,12,"clustering, traffic locality, endpoint profiling, traffic classification, google",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1434,inproceedings,"Trestian, Ionut and Ranjan, Supranamaya and Kuzmanovi, Aleksandar and Nucci, Antonio",Unconstrained Endpoint Profiling (Googling the Internet),2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402991,10.1145/1402958.1402991,"Understanding Internet access trends at a global scale, i.e., what do people do on the Internet, is a challenging problem that is typically addressed by analyzing network traces. However, obtaining such traces presents its own set of challenges owing to either privacy concerns or to other operational difficulties. The key hypothesis of our work here is that most of the information needed to profile the Internet endpoints is already available around us - on the web.In this paper, we introduce a novel approach for profiling and classifying endpoints. We implement and deploy a Google-based profiling tool, which accurately characterizes endpoint behavior by collecting and strategically combining information freely available on the web. Our 'unconstrained endpoint profiling' approach shows remarkable advances in the following scenarios: (i) Even when no packet traces are available, it can accurately predict application and protocol usage trends at arbitrary networks; (ii) When network traces are available, it dramatically outperforms state-of-the-art classification tools; (iii) When sampled flow-level traces are available, it retains high classification capabilities when other schemes literally fall apart. Using this approach, we perform unconstrained endpoint profiling at a global scale: for clients in four different world regions (Asia, South and North America and Europe). We provide the first-of-its-kind endpoint analysis which reveals fascinating similarities and differences among these regions.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,279–290,12,"traffic classification, traffic locality, google, clustering, endpoint profiling","Seattle, WA, USA",SIGCOMM '08,,,,,,
1435,article,"Eriksson, Brian and Barford, Paul and Nowak, Robert",Network Discovery from Passive Measurements,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402992,10.1145/1402946.1402992,"Understanding the Internet's structure through empirical measurements is important in the development of new topology generators, new protocols, traffic engineering, and troubleshooting, among other things. While prior studies of Internet topology have been based on active (traceroute-like) measurements, passive measurements of packet traffic offer the possibility of a greatly expanded perspective of Internet structure with much lower impact and management overhead. In this paper we describe a methodology for inferring network structure from passive measurements of IP packet traffic. We describe algorithms that enable 1) traffic sources that share network paths to be clustered accurately without relying on IP address or autonomous system information, 2) topological structure to be inferred accurately with only a small number of active measurements, 3) missing information to be recovered, which is a serious challenge in the use of passive packet measurements. We demonstrate our techniques using a series of simulated topologies and empirical data sets. Our experiments show that the clusters established by our method closely correspond to sources that actually share paths. We also show the trade-offs between selectively applied active probes and the accuracy of the inferred topology between sources. Finally, we characterize the degree to which missing information can be recovered from passive measurements, which further enhances the accuracy of the inferred topologies.",,291–302,12,"topology, imputation, measurement, embedding, inference",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1436,inproceedings,"Eriksson, Brian and Barford, Paul and Nowak, Robert",Network Discovery from Passive Measurements,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402992,10.1145/1402958.1402992,"Understanding the Internet's structure through empirical measurements is important in the development of new topology generators, new protocols, traffic engineering, and troubleshooting, among other things. While prior studies of Internet topology have been based on active (traceroute-like) measurements, passive measurements of packet traffic offer the possibility of a greatly expanded perspective of Internet structure with much lower impact and management overhead. In this paper we describe a methodology for inferring network structure from passive measurements of IP packet traffic. We describe algorithms that enable 1) traffic sources that share network paths to be clustered accurately without relying on IP address or autonomous system information, 2) topological structure to be inferred accurately with only a small number of active measurements, 3) missing information to be recovered, which is a serious challenge in the use of passive packet measurements. We demonstrate our techniques using a series of simulated topologies and empirical data sets. Our experiments show that the clusters established by our method closely correspond to sources that actually share paths. We also show the trade-offs between selectively applied active probes and the accuracy of the inferred topology between sources. Finally, we characterize the degree to which missing information can be recovered from passive measurements, which further enhances the accuracy of the inferred topologies.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,291–302,12,"imputation, measurement, topology, inference, embedding","Seattle, WA, USA",SIGCOMM '08,,,,,,
1437,article,"Sherwood, Rob and Bender, Adam and Spring, Neil",Discarte: A Disjunctive Internet Cartographer,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402993,10.1145/1402946.1402993,"Internet topology discovery consists of inferring the inter-router connectivity (""links"") and the mapping from IP addresses to routers (""alias resolution""). Current topology discovery techniques use TTL-limited ""traceroute"" probes to discover links and use direct router probing to resolve aliases. The often-ignored record route (RR) IP option provides a source of disparate topology data that could augment existing techniques, but it is difficult to properly align with traceroute-based topologies because router RR implementations are under-standardized. Correctly aligned RR and traceroute topologies have fewer false links, include anonymous and hidden routers, and discover aliases for routers that do not respond to direct probing. More accurate and feature-rich topologies benefit overlay construction and network diagnostics, modeling, and measurement.We present DisCarte, a system for aligning and cross-validating RR and traceroute topology data using observed engineering practices DisCarte uses disjunctive logic programming (DLP), a logical inference and constraint solving technique, to intelligently merge RR and traceroute data. We demonstrate that the resultant topology is more accurate and complete than previous techniques by validating its internal consistency and by comparing to publicly-available topologies. We classify irregularities in router implementations and introduce a divide-and-conquer technique used to scale DLP to Internet-sized systems.",,303–314,12,"discarte, network topology discovery, disjunctive logic programming, record route, alias resolution",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1438,inproceedings,"Sherwood, Rob and Bender, Adam and Spring, Neil",Discarte: A Disjunctive Internet Cartographer,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402993,10.1145/1402958.1402993,"Internet topology discovery consists of inferring the inter-router connectivity (""links"") and the mapping from IP addresses to routers (""alias resolution""). Current topology discovery techniques use TTL-limited ""traceroute"" probes to discover links and use direct router probing to resolve aliases. The often-ignored record route (RR) IP option provides a source of disparate topology data that could augment existing techniques, but it is difficult to properly align with traceroute-based topologies because router RR implementations are under-standardized. Correctly aligned RR and traceroute topologies have fewer false links, include anonymous and hidden routers, and discover aliases for routers that do not respond to direct probing. More accurate and feature-rich topologies benefit overlay construction and network diagnostics, modeling, and measurement.We present DisCarte, a system for aligning and cross-validating RR and traceroute topology data using observed engineering practices DisCarte uses disjunctive logic programming (DLP), a logical inference and constraint solving technique, to intelligently merge RR and traceroute data. We demonstrate that the resultant topology is more accurate and complete than previous techniques by validating its internal consistency and by comparing to publicly-available topologies. We classify irregularities in router implementations and introduce a divide-and-conquer technique used to scale DLP to Internet-sized systems.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,303–314,12,"network topology discovery, disjunctive logic programming, record route, discarte, alias resolution","Seattle, WA, USA",SIGCOMM '08,,,,,,
1439,article,"Dischinger, Marcel and Haeberlen, Andreas and Beschastnikh, Ivan and Gummadi, Krishna P. and Saroiu, Stefan",Satellitelab: Adding Heterogeneity to Planetary-Scale Network Testbeds,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402994,10.1145/1402946.1402994,"Planetary-scale network testbeds like PlanetLab and RON have become indispensable for evaluating prototypes of distributed systems under realistic Internet conditions. However, current testbeds lack the heterogeneity that characterizes the commercial Internet. For example, most testbed nodes are connected to well-provisioned research networks, whereas most Internet nodes are in edge networks.In this paper, we present the design, implementation, and evaluation of SatelliteLab, a testbed that includes nodes from a diverse set of Internet edge networks. SatelliteLab has a two-tier architecture, in which well-provisioned nodes called planets form the core, and lightweight nodes called satellites connect to the planets from the periphery. The application code of an experiment runs on the planets, whereas the satellites only forward network traffic. Thus, the traffic is subjected to the network conditions of the satellites, which greatly improves the testbed's network heterogeneity. The separation of code execution and traffic forwarding enables satellites to remain lightweight, which lowers the barrier to entry for Internet edge nodes.Our prototype of SatelliteLab uses PlanetLab nodes as planets and a set of 32 volunteered satellites with diverse network characteristics. These satellites consist of desktops, laptops, and handhelds connected to the Internet via cable, DSL, ISDN, Wi-Fi, Bluetooth, and cellular links. We evaluate SatelliteLab's design, and we demonstrate the benefits of evaluating applications on SatelliteLab.",,315–326,12,"internet, network testbed, distributed systems",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1440,inproceedings,"Dischinger, Marcel and Haeberlen, Andreas and Beschastnikh, Ivan and Gummadi, Krishna P. and Saroiu, Stefan",Satellitelab: Adding Heterogeneity to Planetary-Scale Network Testbeds,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402994,10.1145/1402958.1402994,"Planetary-scale network testbeds like PlanetLab and RON have become indispensable for evaluating prototypes of distributed systems under realistic Internet conditions. However, current testbeds lack the heterogeneity that characterizes the commercial Internet. For example, most testbed nodes are connected to well-provisioned research networks, whereas most Internet nodes are in edge networks.In this paper, we present the design, implementation, and evaluation of SatelliteLab, a testbed that includes nodes from a diverse set of Internet edge networks. SatelliteLab has a two-tier architecture, in which well-provisioned nodes called planets form the core, and lightweight nodes called satellites connect to the planets from the periphery. The application code of an experiment runs on the planets, whereas the satellites only forward network traffic. Thus, the traffic is subjected to the network conditions of the satellites, which greatly improves the testbed's network heterogeneity. The separation of code execution and traffic forwarding enables satellites to remain lightweight, which lowers the barrier to entry for Internet edge nodes.Our prototype of SatelliteLab uses PlanetLab nodes as planets and a set of 32 volunteered satellites with diverse network characteristics. These satellites consist of desktops, laptops, and handhelds connected to the Internet via cable, DSL, ISDN, Wi-Fi, Bluetooth, and cellular links. We evaluate SatelliteLab's design, and we demonstrate the benefits of evaluating applications on SatelliteLab.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,315–326,12,"distributed systems, internet, network testbed","Seattle, WA, USA",SIGCOMM '08,,,,,,
1441,inproceedings,"Wetherall, David",Session Details: Security II,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256742,10.1145/3256742,,Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,,,,"Seattle, WA, USA",SIGCOMM '08,,,,,,
1442,article,"Zhang, Zheng and Zhang, Ying and Hu, Y. Charlie and Mao, Z. Morley and Bush, Randy",Ispy: Detecting Ip Prefix Hijacking on My Own,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402996,10.1145/1402946.1402996,"IP prefix hijacking remains a major threat to the security of the Internet routing system due to a lack of authoritative prefix ownership information. Despite many efforts in designing IP prefix hijack detection schemes, no existing design can satisfy all the critical requirements of a truly effective system: real-time, accurate, light-weight, easily and incrementally deployable, as well as robust in victim notification. In this paper, we present a novel approach that fulfills all these goals by monitoring network reachability from key external transit networks to one's own network through lightweight prefix-owner-based active probing. Using the prefix-owner's view of reachability, our detection system, iSPY, can differentiate between IP prefix hijacking and network failures based on the observation that hijacking is likely to result in topologically more diverse polluted networks and unreachability. Through detailed simulations of Internet routing, 25-day deployment in 88 ASes (108 prefixes), and experiments with hijacking events of our own prefix from multiple locations, we demonstrate that iSPY is accurate with false negative ratio below 0.45% and false positive ratio below 0.17%. Furthermore, iSPY is truly real-time; it can detect hijacking events within a few minutes.",,327–338,12,"detection, hijacking, routing, bgp",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1443,inproceedings,"Zhang, Zheng and Zhang, Ying and Hu, Y. Charlie and Mao, Z. Morley and Bush, Randy",Ispy: Detecting Ip Prefix Hijacking on My Own,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402996,10.1145/1402958.1402996,"IP prefix hijacking remains a major threat to the security of the Internet routing system due to a lack of authoritative prefix ownership information. Despite many efforts in designing IP prefix hijack detection schemes, no existing design can satisfy all the critical requirements of a truly effective system: real-time, accurate, light-weight, easily and incrementally deployable, as well as robust in victim notification. In this paper, we present a novel approach that fulfills all these goals by monitoring network reachability from key external transit networks to one's own network through lightweight prefix-owner-based active probing. Using the prefix-owner's view of reachability, our detection system, iSPY, can differentiate between IP prefix hijacking and network failures based on the observation that hijacking is likely to result in topologically more diverse polluted networks and unreachability. Through detailed simulations of Internet routing, 25-day deployment in 88 ASes (108 prefixes), and experiments with hijacking events of our own prefix from multiple locations, we demonstrate that iSPY is accurate with false negative ratio below 0.45% and false positive ratio below 0.17%. Furthermore, iSPY is truly real-time; it can detect hijacking events within a few minutes.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,327–338,12,"bgp, detection, hijacking, routing","Seattle, WA, USA",SIGCOMM '08,,,,,,
1444,article,"Andersen, David G. and Balakrishnan, Hari and Feamster, Nick and Koponen, Teemu and Moon, Daekyeong and Shenker, Scott",Accountable Internet Protocol (Aip),2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402997,10.1145/1402946.1402997,"This paper presents AIP (Accountable Internet Protocol), a network architecture that provides accountability as a first-order property. AIP uses a hierarchy of self-certifying addresses, in which each component is derived from the public key of the corresponding entity. We discuss how AIP enables simple solutions to source spoofing, denial-of-service, route hijacking, and route forgery. We also discuss how AIP's design meets the challenges of scaling, key management, and traffic engineering.",,339–350,12,"address, accountability, security, internet architecture, scalability",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1445,inproceedings,"Andersen, David G. and Balakrishnan, Hari and Feamster, Nick and Koponen, Teemu and Moon, Daekyeong and Shenker, Scott",Accountable Internet Protocol (Aip),2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402997,10.1145/1402958.1402997,"This paper presents AIP (Accountable Internet Protocol), a network architecture that provides accountability as a first-order property. AIP uses a hierarchy of self-certifying addresses, in which each component is derived from the public key of the corresponding entity. We discuss how AIP enables simple solutions to source spoofing, denial-of-service, route hijacking, and route forgery. We also discuss how AIP's design meets the challenges of scaling, key management, and traffic engineering.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,339–350,12,"security, scalability, accountability, internet architecture, address","Seattle, WA, USA",SIGCOMM '08,,,,,,
1446,inproceedings,"Gummadi, Krishna",Session Details: P2P,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256743,10.1145/3256743,,Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,,,,"Seattle, WA, USA",SIGCOMM '08,,,,,,
1447,article,"Xie, Haiyong and Yang, Y. Richard and Krishnamurthy, Arvind and Liu, Yanbin Grace and Silberschatz, Abraham",P4p: Provider Portal for Applications,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1402999,10.1145/1402946.1402999,"As peer-to-peer (P2P) emerges as a major paradigm for scalable network application design, it also exposes significant new challenges in achieving efficient and fair utilization of Internet network resources. Being largely network-oblivious, many P2P applications may lead to inefficient network resource usage and/or low application performance. In this paper, we propose a simple architecture called P4P to allow for more effective cooperative traffic control between applications and network providers. We conducted extensive simulations and real-life experiments on the Internet to demonstrate the feasibility and effectiveness of P4P. Our experiments demonstrated that P4P either improves or maintains the same level of application performance of native P2P applications, while, at the same time, it substantially reduces network provider cost compared with either native or latency-based localized P2P applications.",,351–362,12,"p2p, network application, network architecture",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1448,inproceedings,"Xie, Haiyong and Yang, Y. Richard and Krishnamurthy, Arvind and Liu, Yanbin Grace and Silberschatz, Abraham",P4p: Provider Portal for Applications,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1402999,10.1145/1402958.1402999,"As peer-to-peer (P2P) emerges as a major paradigm for scalable network application design, it also exposes significant new challenges in achieving efficient and fair utilization of Internet network resources. Being largely network-oblivious, many P2P applications may lead to inefficient network resource usage and/or low application performance. In this paper, we propose a simple architecture called P4P to allow for more effective cooperative traffic control between applications and network providers. We conducted extensive simulations and real-life experiments on the Internet to demonstrate the feasibility and effectiveness of P4P. Our experiments demonstrated that P4P either improves or maintains the same level of application performance of native P2P applications, while, at the same time, it substantially reduces network provider cost compared with either native or latency-based localized P2P applications.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,351–362,12,"p2p, network application, network architecture","Seattle, WA, USA",SIGCOMM '08,,,,,,
1449,article,"Choffnes, David R. and Bustamante, Fabi\'{a",Taming the Torrent: A Practical Approach to Reducing Cross-Isp Traffic in Peer-to-Peer Systems,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1403000,10.1145/1402946.1403000,"Peer-to-peer (P2P) systems, which provide a variety of popular services, such as file sharing, video streaming and voice-over-IP, contribute a significant portion of today's Internet traffic. By building overlay networks that are oblivious to the underlying Internet topology and routing, these systems have become one of the greatest traffic-engineering challenges for Internet Service Providers (ISPs) and the source of costly data traffic flows. In an attempt to reduce these operational costs, ISPs have tried to shape, block or otherwise limit P2P traffic, much to the chagrin of their subscribers, who consistently finds ways to eschew these controls or simply switch providers.In this paper, we present the design, deployment and evaluation of an approach to reducing this costly cross-ISP traffic without sacrificing system performance. Our approach recycles network views gathered at low cost from content distribution networks to drive biased neighbor selection without any path monitoring or probing. Using results collected from a deployment in BitTorrent with over 120,000 users in nearly 3,000 networks, we show that our lightweight approach significantly reduces cross-ISP traffic and, over 33% of the time, it selects peers along paths that are within a single autonomous system (AS). Further, we find that our system locates peers along paths that have two orders of magnitude lower latency and 30% lower loss rates than those picked at random, and that these high-quality paths can lead to significant improvements in transfer rates. In challenged settings where peers are overloaded in terms of available bandwidth, our approach provides 31% average download-rate improvement; in environments with large available bandwidth, it increases download rates by 207% on average (and improves median rates by 883%",,363–374,12,"measurement reuse, p2p, cross-isp traffic, peer selection, isp",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1450,inproceedings,"Choffnes, David R. and Bustamante, Fabi\'{a",Taming the Torrent: A Practical Approach to Reducing Cross-Isp Traffic in Peer-to-Peer Systems,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1403000,10.1145/1402958.1403000,"Peer-to-peer (P2P) systems, which provide a variety of popular services, such as file sharing, video streaming and voice-over-IP, contribute a significant portion of today's Internet traffic. By building overlay networks that are oblivious to the underlying Internet topology and routing, these systems have become one of the greatest traffic-engineering challenges for Internet Service Providers (ISPs) and the source of costly data traffic flows. In an attempt to reduce these operational costs, ISPs have tried to shape, block or otherwise limit P2P traffic, much to the chagrin of their subscribers, who consistently finds ways to eschew these controls or simply switch providers.In this paper, we present the design, deployment and evaluation of an approach to reducing this costly cross-ISP traffic without sacrificing system performance. Our approach recycles network views gathered at low cost from content distribution networks to drive biased neighbor selection without any path monitoring or probing. Using results collected from a deployment in BitTorrent with over 120,000 users in nearly 3,000 networks, we show that our lightweight approach significantly reduces cross-ISP traffic and, over 33% of the time, it selects peers along paths that are within a single autonomous system (AS). Further, we find that our system locates peers along paths that have two orders of magnitude lower latency and 30% lower loss rates than those picked at random, and that these high-quality paths can lead to significant improvements in transfer rates. In challenged settings where peers are overloaded in terms of available bandwidth, our approach provides 31% average download-rate improvement; in environments with large available bandwidth, it increases download rates by 207% on average (and improves median rates by 883%",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,363–374,12,"measurement reuse, p2p, cross-isp traffic, isp, peer selection","Seattle, WA, USA",SIGCOMM '08,,,,,,
1451,article,"Huang, Yan and Fu, Tom Z.J. and Chiu, Dah-Ming and Lui, John C.S. and Huang, Cheng","Challenges, Design and Analysis of a Large-Scale P2p-Vod System",2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1403001,10.1145/1402946.1403001,"P2P file downloading and streaming have already become very popular Internet applications. These systems dramatically reduce the server loading, and provide a platform for scalable content distribution, as long as there is interest for the content. P2P-based video-on-demand (P2P-VoD) is a new challenge for the P2P technology. Unlike streaming live content, P2P-VoD has less synchrony in the users sharing video content, therefore it is much more difficult to alleviate the server loading and at the same time maintaining the streaming performance. To compensate, a small storage is contributed by every peer, and new mechanisms for coordinating content replication, content discovery, and peer scheduling are carefully designed. In this paper, we describe and discuss the challenges and the architectural design issues of a large-scale P2P-VoD system based on the experiences of a real system deployed by PPLive. The system is also designed and instrumented with monitoring capability to measure both system and component specific performance metrics (for design improvements) as well as user satisfaction. After analyzing a large amount of collected data, we present a number of results on user behavior, various system performance metrics, including user satisfaction, and discuss what we observe based on the system design. The study of a real life system provides valuable insights for the future development of P2P-VoD technology.",,375–388,14,"video-on-demand, peer-to-peer/overlay networks, content distribution",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1452,inproceedings,"Huang, Yan and Fu, Tom Z.J. and Chiu, Dah-Ming and Lui, John C.S. and Huang, Cheng","Challenges, Design and Analysis of a Large-Scale P2p-Vod System",2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1403001,10.1145/1402958.1403001,"P2P file downloading and streaming have already become very popular Internet applications. These systems dramatically reduce the server loading, and provide a platform for scalable content distribution, as long as there is interest for the content. P2P-based video-on-demand (P2P-VoD) is a new challenge for the P2P technology. Unlike streaming live content, P2P-VoD has less synchrony in the users sharing video content, therefore it is much more difficult to alleviate the server loading and at the same time maintaining the streaming performance. To compensate, a small storage is contributed by every peer, and new mechanisms for coordinating content replication, content discovery, and peer scheduling are carefully designed. In this paper, we describe and discuss the challenges and the architectural design issues of a large-scale P2P-VoD system based on the experiences of a real system deployed by PPLive. The system is also designed and instrumented with monitoring capability to measure both system and component specific performance metrics (for design improvements) as well as user satisfaction. After analyzing a large amount of collected data, we present a number of results on user behavior, various system performance metrics, including user satisfaction, and discuss what we observe based on the system design. The study of a real life system provides valuable insights for the future development of P2P-VoD technology.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,375–388,14,"video-on-demand, peer-to-peer/overlay networks, content distribution","Seattle, WA, USA",SIGCOMM '08,,,,,,
1453,article,"Bharambe, Ashwin and Douceur, John R. and Lorch, Jacob R. and Moscibroda, Thomas and Pang, Jeffrey and Seshan, Srinivasan and Zhuang, Xinyu","Donnybrook: Enabling Large-Scale, High-Speed, Peer-to-Peer Games",2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1403002,10.1145/1402946.1403002,"Without well-provisioned dedicated servers, modern fast-paced action games limit the number of players who can interact simultaneously to 16-32. This is because interacting players must frequently exchange state updates, and high player counts would exceed the bandwidth available to participating machines. In this paper, we describe Donnybrook, a system that enables epic-scale battles without dedicated server resources, even in a fast-paced game with tight latency bounds. It achieves this scalability through two novel components. First, it reduces bandwidth demand by estimating what players are paying attention to, thereby enabling it to reduce the frequency of sending less important state updates. Second, it overcomes resource and interest heterogeneity by disseminating updates via a multicast system designed for the special requirements of games: that they have multiple sources, are latency-sensitive, and have frequent group membership changes. We present user study results using a prototype implementation based on Quake III that show our approach provides a desirable user experience. We also present simulation results that demonstrate Donnybrook's efficacy in enabling battles of up to 900 players.",,389–400,12,"doppelg\""{a",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1454,inproceedings,"Bharambe, Ashwin and Douceur, John R. and Lorch, Jacob R. and Moscibroda, Thomas and Pang, Jeffrey and Seshan, Srinivasan and Zhuang, Xinyu","Donnybrook: Enabling Large-Scale, High-Speed, Peer-to-Peer Games",2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1403002,10.1145/1402958.1403002,"Without well-provisioned dedicated servers, modern fast-paced action games limit the number of players who can interact simultaneously to 16-32. This is because interacting players must frequently exchange state updates, and high player counts would exceed the bandwidth available to participating machines. In this paper, we describe Donnybrook, a system that enables epic-scale battles without dedicated server resources, even in a fast-paced game with tight latency bounds. It achieves this scalability through two novel components. First, it reduces bandwidth demand by estimating what players are paying attention to, thereby enabling it to reduce the frequency of sending less important state updates. Second, it overcomes resource and interest heterogeneity by disseminating updates via a multicast system designed for the special requirements of games: that they have multiple sources, are latency-sensitive, and have frequent group membership changes. We present user study results using a prototype implementation based on Quake III that show our approach provides a desirable user experience. We also present simulation results that demonstrate Donnybrook's efficacy in enabling battles of up to 900 players.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,389–400,12,"computer games, interest sets, doppelg\""{a","Seattle, WA, USA",SIGCOMM '08,,,,,,
1455,inproceedings,"Keshav, S.",Session Details: Wireless II,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3256744,10.1145/3256744,,Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,,,,"Seattle, WA, USA",SIGCOMM '08,,,,,,
1456,article,"Katti, Sachin and Katabi, Dina and Balakrishnan, Hari and Medard, Muriel",Symbol-Level Network Coding for Wireless Mesh Networks,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1403004,10.1145/1402946.1403004,"This paper describes MIXIT, a system that improves the throughput of wireless mesh networks. MIXIT exploits a basic property of mesh networks: even when no node receives a packet correctly, any given bit is likely to be received by some node correctly. Instead of insisting on forwarding only correct packets, MIXIT routers use physical layer hints to make their best guess about which bits in a corrupted packet are likely to be correct and forward them to the destination. Even though this approach inevitably lets erroneous bits through, we find that it can achieve high throughput without compromising end-to-end reliability.The core component of MIXIT is a novel network code that operates on small groups of bits, called symbols. It allows the nodes to opportunistically route groups of bits to their destination with low overhead. MIXIT's network code also incorporates an end-to-end error correction component that the destination uses to correct any errors that might seep through. We have implemented MIXIT on a software radio platform running the Zigbee radio protocol. Our experiments on a 25-node indoor testbed show that MIXIT has a throughput gain of 2.8x over MORE, a state-of-the-art opportunistic routing scheme, and about 3.9x over traditional routing using the ETX metric.",,401–412,12,"wireless networks, cooperative diversity, network coding",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1457,inproceedings,"Katti, Sachin and Katabi, Dina and Balakrishnan, Hari and Medard, Muriel",Symbol-Level Network Coding for Wireless Mesh Networks,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1403004,10.1145/1402958.1403004,"This paper describes MIXIT, a system that improves the throughput of wireless mesh networks. MIXIT exploits a basic property of mesh networks: even when no node receives a packet correctly, any given bit is likely to be received by some node correctly. Instead of insisting on forwarding only correct packets, MIXIT routers use physical layer hints to make their best guess about which bits in a corrupted packet are likely to be correct and forward them to the destination. Even though this approach inevitably lets erroneous bits through, we find that it can achieve high throughput without compromising end-to-end reliability.The core component of MIXIT is a novel network code that operates on small groups of bits, called symbols. It allows the nodes to opportunistically route groups of bits to their destination with low overhead. MIXIT's network code also incorporates an end-to-end error correction component that the destination uses to correct any errors that might seep through. We have implemented MIXIT on a software radio platform running the Zigbee radio protocol. Our experiments on a 25-node indoor testbed show that MIXIT has a throughput gain of 2.8x over MORE, a state-of-the-art opportunistic routing scheme, and about 3.9x over traditional routing using the ETX metric.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,401–412,12,"network coding, wireless networks, cooperative diversity","Seattle, WA, USA",SIGCOMM '08,,,,,,
1458,article,"Li, Yi and Qiu, Lili and Zhang, Yin and Mahajan, Ratul and Rozner, Eric",Predictable Performance Optimization for Wireless Networks,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1403005,10.1145/1402946.1403005,"We present a novel approach to optimize the performance of IEEE 802.11-based multi-hop wireless networks. A unique feature of our approach is that it enables an accurate prediction of the resulting throughput of individual flows. At its heart lies a simple yet model of the network that captures interference, traffic, and MAC-induced dependencies. Unless properly accounted for, these dependencies lead to unpredictable behaviors. For instance, we show that even a simple network of two links with one flow is vulnerable to severe performance degradation. We design algorithms that build on this model to optimize the network for fairness and throughput. Given traffic demands as input, these algorithms compute rates at which individual flows must send to meet the objective. Evaluation using a multi-hop wireless testbed as well as simulations show that our approach is very effective. When optimizing for fairness, our methods result in close to perfect fairness. When optimizing for throughput, they lead to 100-200% improvement for UDP traffic and 10-50% for TCP traffic.",,413–426,14,"interference, optimization, multi-hop wireless networks, modeling",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1459,inproceedings,"Li, Yi and Qiu, Lili and Zhang, Yin and Mahajan, Ratul and Rozner, Eric",Predictable Performance Optimization for Wireless Networks,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1403005,10.1145/1402958.1403005,"We present a novel approach to optimize the performance of IEEE 802.11-based multi-hop wireless networks. A unique feature of our approach is that it enables an accurate prediction of the resulting throughput of individual flows. At its heart lies a simple yet model of the network that captures interference, traffic, and MAC-induced dependencies. Unless properly accounted for, these dependencies lead to unpredictable behaviors. For instance, we show that even a simple network of two links with one flow is vulnerable to severe performance degradation. We design algorithms that build on this model to optimize the network for fairness and throughput. Given traffic demands as input, these algorithms compute rates at which individual flows must send to meet the objective. Evaluation using a multi-hop wireless testbed as well as simulations show that our approach is very effective. When optimizing for fairness, our methods result in close to perfect fairness. When optimizing for throughput, they lead to 100-200% improvement for UDP traffic and 10-50% for TCP traffic.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,413–426,14,"interference, modeling, multi-hop wireless networks, optimization","Seattle, WA, USA",SIGCOMM '08,,,,,,
1460,article,"Balasubramanian, Aruna and Mahajan, Ratul and Venkataramani, Arun and Levine, Brian Neil and Zahorjan, John",Interactive Wifi Connectivity for Moving Vehicles,2008,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402946.1403006,10.1145/1402946.1403006,"We ask if the ubiquity of WiFi can be leveraged to provide cheap connectivity from moving vehicles for common applications such as Web browsing and VoIP. Driven by this question, we conduct a study of connection quality available to vehicular WiFi clients based on measurements from testbeds in two different cities. We find that current WiFi handoff methods, in which clients communicate with one basestation at a time, lead to frequent disruptions in connectivity. We also find that clients can overcome many disruptions by communicating with multiple basestations simultaneously. These findings lead us to develop ViFi, a protocol that opportunistically exploits basestation diversity to minimize disruptions and support interactive applications for mobile clients. ViFi uses a decentralized and lightweight probabilistic algorithm for coordination between participating basestations. Our evaluation using a two-month long deployment and trace-driven simulations shows that its link-layer performance comes close to an ideal diversity-based protocol. Using two applications, VoIP and short TCP transfers, we show that the link layer performance improvement translates to better application performance. In our deployment, ViFi doubles the number of successful short TCP transfers and doubles the length of disruption-free VoIP sessions compared to an existing WiFi-style handoff protocol.",,427–438,12,"wifi, handoff, diversity, applications, vehicular networks",,,October 2008,38,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1461,inproceedings,"Balasubramanian, Aruna and Mahajan, Ratul and Venkataramani, Arun and Levine, Brian Neil and Zahorjan, John",Interactive Wifi Connectivity for Moving Vehicles,2008,9781605581750,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1402958.1403006,10.1145/1402958.1403006,"We ask if the ubiquity of WiFi can be leveraged to provide cheap connectivity from moving vehicles for common applications such as Web browsing and VoIP. Driven by this question, we conduct a study of connection quality available to vehicular WiFi clients based on measurements from testbeds in two different cities. We find that current WiFi handoff methods, in which clients communicate with one basestation at a time, lead to frequent disruptions in connectivity. We also find that clients can overcome many disruptions by communicating with multiple basestations simultaneously. These findings lead us to develop ViFi, a protocol that opportunistically exploits basestation diversity to minimize disruptions and support interactive applications for mobile clients. ViFi uses a decentralized and lightweight probabilistic algorithm for coordination between participating basestations. Our evaluation using a two-month long deployment and trace-driven simulations shows that its link-layer performance comes close to an ideal diversity-based protocol. Using two applications, VoIP and short TCP transfers, we show that the link layer performance improvement translates to better application performance. In our deployment, ViFi doubles the number of successful short TCP transfers and doubles the length of disruption-free VoIP sessions compared to an existing WiFi-style handoff protocol.",Proceedings of the ACM SIGCOMM 2008 Conference on Data Communication,427–438,12,"vehicular networks, handoff, diversity, wifi, applications","Seattle, WA, USA",SIGCOMM '08,,,,,,
1462,inproceedings,"Cha, Meeyoung and Kwak, Haewoon and Rodriguez, Pablo and Ahn, Yong-Yeol and Moon, Sue","I Tube, You Tube, Everybody Tubes: Analyzing the World's Largest User Generated Content Video System",2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298309,10.1145/1298306.1298309,"User Generated Content (UGC) is re-shaping the way people watch video and TV, with millions of video producers and consumers. In particular, UGC sites are creating new viewing patterns and social interactions, empowering users to be more creative, and developing new business opportunities. To better understand the impact of UGC systems, we have analyzed YouTube, the world's largest UGC VoD system. Based on a large amount of data collected, we provide an in-depth study of YouTube and other similar UGC systems. In particular, we study the popularity life-cycle of videos, the intrinsic statistical properties of requests and their relationship with video age, and the level of content aliasing or of illegal content in the system. We also provide insights on the potential for more efficient UGC VoD systems (e.g. utilizing P2P techniques or making better use of caching). Finally, we discuss the opportunities to leverage the latent demand for niche videos that are not reached today due to information filtering effects or other system scarcity distortions. Overall, we believe that the results presented in this paper are crucial in understanding UGC systems and can provide valuable information to ISPs, site administrators, and content owners with major commercial and technical implications.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,1–14,14,"long tail, vod, popularity analysis, p2p, content aliasing, user generated content, power-law, caching","San Diego, California, USA",IMC '07,,,,,,
1463,inproceedings,"Krishnamurthy, A.",Session Details: Social Networks,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3250733,10.1145/3250733,,Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,,,,"San Diego, California, USA",IMC '07,,,,,,
1464,inproceedings,"Gill, Phillipa and Arlitt, Martin and Li, Zongpeng and Mahanti, Anirban",Youtube Traffic Characterization: A View from the Edge,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298310,10.1145/1298306.1298310,"This paper presents a traffic characterization study of the popular video sharing service, YouTube. Over a three month period we observed almost 25 million transactions between users on an edge network and YouTube, including more than 600,000 video downloads. We also monitored the globally popular videos over this period of time.In the paper we examine usage patterns, file properties, popularity and referencing characteristics, and transfer behaviors of YouTube, and compare them to traditional Web and media streaming workload characteristics. We conclude the paper with a discussion of the implications of the observed characteristics. For example, we find that as with the traditional Web, caching could improve the end user experience, reduce network bandwidth consumption, and reduce the load on YouTube's core server infrastructure. Unlike traditional Web caching, Web 2.0 provides additional meta-data that should be exploited to improve the effectiveness of strategies like caching.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,15–28,14,"characterization, youtube, multimedia, web 2.0","San Diego, California, USA",IMC '07,,,,,,
1465,inproceedings,"Mislove, Alan and Marcon, Massimiliano and Gummadi, Krishna P. and Druschel, Peter and Bhattacharjee, Bobby",Measurement and Analysis of Online Social Networks,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298311,10.1145/1298306.1298311,"Online social networking sites like Orkut, YouTube, and Flickr are among the most popular sites on the Internet. Users of these sites form a social network, which provides a powerful means of sharing, organizing, and finding content and contacts. The popularity of these sites provides an opportunity to study the characteristics of online social network graphs at large scale. Understanding these graphs is important, both to improve current systems and to design new applications of online social networks.This paper presents a large-scale measurement study and analysis of the structure of multiple online social networks. We examine data gathered from four popular online social networks: Flickr, YouTube, LiveJournal, and Orkut. We crawled the publicly accessible user links on each site, obtaining a large portion of each social network's graph. Our data set contains over 11.3 million users and 328 million links. We believe that this is the first study to examine multiple online social networks at scale.Our results confirm the power-law, small-world, and scale-free properties of online social networks. We observe that the indegree of user nodes tends to match the outdegree; that the networks contain a densely connected core of high-degree nodes; and that this core links small groups of strongly clustered, low-degree nodes at the fringes of the network. Finally, we discuss the implications of these structural properties for the design of social network based systems.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,29–42,14,"analysis, measurement, social networks","San Diego, California, USA",IMC '07,,,,,,
1466,inproceedings,"Crovella, M.",Session Details: Measurements,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3250734,10.1145/3250734,,Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,,,,"San Diego, California, USA",IMC '07,,,,,,
1467,inproceedings,"Dischinger, Marcel and Haeberlen, Andreas and Gummadi, Krishna P. and Saroiu, Stefan",Characterizing Residential Broadband Networks,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298313,10.1145/1298306.1298313,"A large and rapidly growing proportion of users connect to the Internet via residential broadband networks such as Digital Subscriber Lines (DSL) and cable. Residential networks are often the bottleneck in the last mile of today's Internet. Their characteristics critically affect Internet applications, including voice-over-IP, online games, and peer-to-peer content sharing/delivery systems. However, to date, few studies have investigated commercial broadband deployments, and rigorous measurement data that characterize these networks at scale are lacking.In this paper, we present the first large-scale measurement study of major cable and DSL providers in North America and Europe. We describe and evaluate the measurement tools we developed for this purpose. Our study characterizes several properties of broadband networks, including link capacities, packet round-trip times and jitter, packet loss rates, queue lengths, and queue drop policies. Our analysis reveals important ways in which residential networks differ from how the Internet is conventionally thought to operate. We also discuss the implications of our findings for many emerging protocols and systems, including delay-based congestion control (e.g., PCP) and network coordinate systems (e.g., Vivaldi).",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,43–56,14,"dsl, cable, network measurement, broadband access networks","San Diego, California, USA",IMC '07,,,,,,
1468,inproceedings,"Bartlett, Genevieve and Heidemann, John and Papadopoulos, Christos",Understanding Passive and Active Service Discovery,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298314,10.1145/1298306.1298314,"Increasingly, network operators do not directly operate computers on their network, yet are responsible for assessing network vulnerabilities to ensure compliance with policies about information disclosure, and tracking services that affect provisioning. Thus, with decentralized network management, service discovery becomes an important part of maintaining and protecting computer networks.We explore two approaches to service discovery: active probing and passive monitoring. Active probing finds all services currently on the network, except services temporarily unavailable or hidden by firewalls; however, it is often too invasive, especially if used across administrative boundaries. Passive monitoring can find transient services, but misses services that are idle. We compare the accuracy of passive and active approaches to service discovery and show that they are complimentary, highlighting the need for multiple active scans coupled with long-duration passive monitoring. We find passive monitoring is well suited for quickly finding popular services, finding servers responsible for 99% of incoming connections within minutes. Active scanning is better suited to rapidly finding all servers, which is important for vulnerability detection - one scan finds 98% of services in two hours, missing only a handful. External scans are an unexpected ally to passive monitoring, speeding service discovery by the equivalent of 9-15 days of additional observation. Finally, we show how the use of static or dynamic addresses changes the effectiveness of service discovery, both due to address reuse and VPN effects.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,57–70,14,"active measurement, passive monitoring, situational awareness, service discovery, network reconnaissance","San Diego, California, USA",IMC '07,,,,,,
1469,inproceedings,"Khadilkar, Manas and Feamster, Nick and Sanders, Matt and Clark, Russ",Usage-Based Dhcp Lease Time Optimization,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298315,10.1145/1298306.1298315,"The Dynamic Host Configuration Protocol (DHCP) is used to dynamically allocate address space to hosts on a local area network. Despite its widespread usage, few studies exist on DHCP usage patterns, and even less is known about the importance of setting the lease time (the time that a client retains ownership over some IP address) to an appropriate value. Lease time can greatly affect the tradeoff between address space utilization and the number of both renewal messages and client session expirations. In this paper, using a DHCP trace for 5 weekdays from the Georgia Tech campus network, we present the largest known study of DHCP utilization. We also explore how various strategies for setting lease times can dramatically reduce the number of renewals and expirations without prohibitively increasing address space utilization.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,71–76,6,"dhcp, usage, network management, optimization","San Diego, California, USA",IMC '07,,,,,,
1470,inproceedings,"Allman, Mark and Paxson, Vern and Terrell, Jeff",A Brief History of Scanning,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298316,10.1145/1298306.1298316,"Incessant scanning of hosts by attackers looking for vulnerable servers has become a fact of Internet life. In this paper we present an initial study of the scanning activity observed at one site over the past 12.5 years. We study the onset of scanning in the late 1990s and its evolution in terms of characteristics such as the number of scanners, targets and probing patterns. While our study is preliminary in many ways, it provides the first longitudinal examination of a now ubiquitous Internet phenomenon.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,77–82,6,"malicious activity, longitudinal, scanning","San Diego, California, USA",IMC '07,,,,,,
1471,inproceedings,"Barford, P.",Session Details: Security and Anomaly Detection,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3250735,10.1145/3250735,,Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,,,,"San Diego, California, USA",IMC '07,,,,,,
1472,inproceedings,"Lee, Homin K. and Malkin, Tal and Nahum, Erich",Cryptographic Strength of Ssl/Tls Servers: Current and Recent Practices,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298318,10.1145/1298306.1298318,"The Secure Socket Layer (SSL) and its variant, Transport Layer Security (TLS), are used toward ensuring server security. In this paper, we characterize the cryptographic strength of public servers running SSL/TLS. We present a tool developed for this purpose, the Probing SSL Security Tool (PSST), and evaluate over 19,000 servers. We expose the great diversity in the levels of cryptographic strength that is supported on the Internet. Some of our discouraging results show that most sites still support the insecure SSL 2.0, weak export-level grades of encryption ciphers, or weak RSA key strengths. We also observe encouraging behavior such as sensible default choices by servers when presented with multiple options, the quick adoption of AES (more than half the servers support strong key AES as their default choice), and the use of strong RSA key sizes of 1024 bits and above. Comparing results of running our tool over the last two years points to a positive trend that is moving in the right direction, though perhaps not as quickly as it should.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,83–92,10,"network security, servers, ssl","San Diego, California, USA",IMC '07,,,,,,
1473,inproceedings,"Collins, M. Patrick and Shimeall, Timothy J. and Faber, Sidney and Janies, Jeff and Weaver, Rhiannon and De Shon, Markus and Kadane, Joseph",Using Uncleanliness to Predict Future Botnet Addresses,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298319,10.1145/1298306.1298319,"The increased use of botnets as an attack tool and the awareness attackers have of blocking lists leads to the question of whether we can effectively predict future bot locations. To that end, we introduce a network quality that we term uncleanliness: an indicator of the propensity for hosts in a network to be compromised by outside parties.We hypothesize that unclean networks will demonstrate two properties: spatial and temporal uncleanliness. Spatial uncleanliness is the tendency for compromised hosts to cluster within unclean networks. Temporal uncleanliness is the tendency for unclean networks to contain compromised hosts for extended periods.We test for these properties by collating data from multiple indicators (spamming, phishing, scanning and botnet IRC log monitoring). We demonstrate evidence for both spatial and temporal uncleanliness. We further show evidence for cross-relationship between the various datasets, showing that botnet activity predicts spamming and scanning, while phishing activity appears to be unrelated to the other indicators.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,93–104,12,"reputation management, blacklists, botnets","San Diego, California, USA",IMC '07,,,,,,
1474,inproceedings,"Soule, Augustin and Silveira, Fernando and Ringberg, Haakon and Diot, Christophe",Challenging the Supremacy of Traffic Matrices in Anomaly Detection,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298320,10.1145/1298306.1298320,"Multiple network-wide anomaly detection techniques proposed in the literature define an anomaly as a statistical outlier in aggregated network traffic. The most popular way to aggregate the traffic is as a Traffic Matrix, where the traffic is divided according to its ingress and egress points in the network. However, the reasons for choosing traffic matrices instead of any other formalism have not been studied yet. In this paper we compare three network-driven traffic aggregation formalisms: ingress routers, input links and origin-destination pairs (i.e. traffic matrices). Each formalism is computed on data collected from two research backbones. Then, a network-wide anomaly detection method is applied to each formalism. All anomalies are manually labeled, as a true or false positive. Our results show that the traffic aggregation level has asignificant impact on the number of anomalies detected and on the false positive rate. We show that aggregating by OD pairs is indeed the most appropriate choice for the data sets and the detection method we consider. We correlate our observations with time series statistics in order to explain how aggregation impacts anomaly detection.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,105–110,6,"traffic aggregation, anomaly detection","San Diego, California, USA",IMC '07,,,,,,
1475,inproceedings,"John, Wolfgang and Tafvelin, Sven",Analysis of Internet Backbone Traffic and Header Anomalies Observed,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298321,10.1145/1298306.1298321,"The dominating Internet protocols, IP and TCP, allow some flexibility in implementation, including a variety of optional features. To support research and further development of these protocols, it is crucial to know about current deployment of protocol specific features and accompanying anomalies. This work is intended to reflect the current characteristics of Internet backbone traffic and point out misbehaviors and potential problems. On 20 consecutive days in April 2006 bidirectional traffic was collected on an OC-192 backbone link. The analysis of the data provides a comprehensive summary about current protocol usage including comparisons to prior studies. Furthermore, header misbehaviors and anomalies were found within almost every aspect analyzed and are discussed in detail. These observations are important information for designers of network protocols, network application and network attack detection systems.1..",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,111–116,6,"internet measurement, traffic analysis, header anomalies","San Diego, California, USA",IMC '07,,,,,,
1476,inproceedings,"Rodriguez, P.",Session Details: Applications,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3250736,10.1145/3250736,,Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,,,,"San Diego, California, USA",IMC '07,,,,,,
1477,inproceedings,"Steiner, Moritz and En-Najjary, Taoufik and Biersack, Ernst W.",A Global View of Kad,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298323,10.1145/1298306.1298323,"Distributed hash tables (DHTs) have been actively studied in literature and many different proposals have been made on how to organize peers in a DHT. However, very few DHT shave been implemented in real systems and deployed on alarge scale. One exception is <scp>KAD</scp>, a DHT based on Kademlia, which is part of eDonkey2000, a peer-to-peer file sharing system with several million simultaneous users. We have been crawling <scp>KAD</scp> continuously for about six months and obtained information about the total number of peers online and their geographical distribution.Peers are identified by the so called <scp>KAD</scp> ID, which was up to now assumed to remain the same across sessions. However, we observed that this is not the case: There is a large number of peers, in particular in China, that change their <scp>KAD</scp> ID, sometimes as frequently as after each session. This change of <scp>KAD</scp> IDs makes it difficult to characterize end-user availability or membership turnover.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,117–122,6,"peer-to-peer, distributed hash table, lookup","San Diego, California, USA",IMC '07,,,,,,
1478,inproceedings,"Shue, Craig A. and Kalafut, Andrew J. and Gupta, Minaxi",The Web is Smaller than It Seems,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298324,10.1145/1298306.1298324,"The Web has grown beyond anybody's imagination. While significant research has been devoted to understanding aspects of the Web from the perspective of the documents that comprise it, we have little data on the relationship among servers that comprise the Web. In this paper, we explore the extent to which Web servers are co-located with other Web servers in the Internet. In terms of the location of servers, we find that the Web is surprisingly smaller than it seems. Our work has important implications for the availability of Web servers in case of DoS attacks and blocklisting.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,123–128,6,"world wide web, block lists, dos attacks, server co-location","San Diego, California, USA",IMC '07,,,,,,
1479,inproceedings,"Falkner, Jarret and Piatek, Michael and John, John P. and Krishnamurthy, Arvind and Anderson, Thomas",Profiling a Million User Dht,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298325,10.1145/1298306.1298325,"Distributed hash tables (DHTs) provide scalable, key-based lookup of objects in dynamic network environments. Although DHTs have been studied extensively from an analytical perspective, only recently have wide deployments enabled empirical examination. This paper reports measurements of the Azureus BitTorrent client's DHT, which is in active use by more than 1 million nodes on a daily basis. The Azureus DHT operates on untrusted, unreliable end-hosts, offering a glimpse into the implementation challenges associated with making structured overlays work in practice. Our measurements provide characterizations of churn, overhead, and performance in this environment. We leverage these measurements to drive the design of a modified DHT lookup algorithm that reduces median DHT lookup time by an order of magnitude for a nominal increase in overhead.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,129–134,6,,"San Diego, California, USA",IMC '07,,,,,,
1480,inproceedings,"Dovrolis, C.",Session Details: Ethics and Legality,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3250737,10.1145/3250737,,Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,,,,"San Diego, California, USA",IMC '07,,,,,,
1481,inproceedings,"Alllman, Mark and Paxson, Vern",Issues and Etiquette Concerning Use of Shared Measurement Data,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298327,10.1145/1298306.1298327,"In this note we discuss issues surrounding how to provide and use network measurement data made available for sharing among researchers. While previous work has focused on the technical details of enabling sharing via traffic anonymization, we focus on higher-level aspects of the process such as potential harm to the provider (e.g., by de-anonymizing a shared dataset) or interactions to strengthen subsequent research (e.g., helping to establish ground truth). We believe the community would benefit from a dialog regarding expectations and responsibilities of data providers, and the etiquette involved with using others' measurement data. To this end, we provide a set of guidelines that aim to aid the process of sharing measurement data. We present these not as specific rules, but rather a framework under which providers and users can better attain a mutual understanding about how to treat particular datasets.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,135–140,6,"data sharing, anonymization","San Diego, California, USA",IMC '07,,,,,,
1482,inproceedings,"Wang, J.",Session Details: Routing and Topology I,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3250738,10.1145/3250738,,Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,,,,"San Diego, California, USA",IMC '07,,,,,,
1483,inproceedings,"Augustin, Brice and Friedman, Timur and Teixeira, Renata",Measuring Load-Balanced Paths in the Internet,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298329,10.1145/1298306.1298329,"Tools to measure internet properties usually assume the existence of a single path from a source to a destination. However, load-balancing capabilities, which create multiple active paths between two end-hosts, are available in most contemporary routers. This paper proposes a methodology to identify load-balancing routers and characterize load-balanced paths. We enhance our traceroute-like tool, called Paris traceroute, to find all paths between a pair of hosts, and use it from 15 sources to over 68 thousand destinations. Our results show that the traditional concept of a single network path between hosts no longer holds. For instance, 39% of the source-destination pairs in our traces traverse a load balancer. Furthermore, this fraction increases to 70% if we consider the paths between a source and a destination network.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,149–160,12,"load balancing, multipath, path diversity, traceroute","San Diego, California, USA",IMC '07,,,,,,
1484,inproceedings,"Erramilli, Vijay and Chaintreau, Augustin and Crovella, Mark and Diot, Christophe",Diversity of Forwarding Paths in Pocket Switched Networks,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298330,10.1145/1298306.1298330,"Forwarding in Delay Tolerant Networks (DTNs) is a challenging problem. We focus on the specific issue of forwarding in an environment where mobile devices are carried by people in a restricted physical space (a conference) and contact patterns are not predictable. We show for the first time a path explosion phenomenon between most pairs of nodes. This means that, once the first path reaches the destination, the number of subsequent paths grows rapidly with time, so there usually exist many near-optimal paths. We study the path explosion phenomenon both analytically and empirically. Our results highlight the importance of unequal contact rates across nodes for understanding the performance of forwarding algorithms. We also find that a variety of well-known forwarding algorithms show surprisingly similar performance in our setting and we interpret this fact in light of the path explosion phenomenon.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,161–174,14,"dtn, forwarding, pocket switched networks, path diversity","San Diego, California, USA",IMC '07,,,,,,
1485,inproceedings,"Wang, Guohui and Zhang, Bo and Ng, T. S. Eugene",Towards Network Triangle Inequality Violation Aware Distributed Systems,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298331,10.1145/1298306.1298331,"Many distributed systems rely on neighbor selection mechanisms to create overlay structures that have good network performance. These neighbor selection mechanisms often assume the triangle inequality holds for Internet delays. However, the reality is that the triangle inequality is violated by Internet delays. This phenomenon creates astrange environment that confuses neighbor selection mechanisms. This paper investigates the properties of triangle inequality violation (TIV) in Internet delays, the impacts of TIV on representative neighbor selection mechanisms, specifically Vivaldi and Meridian, and avenues to reduce these impacts. We propose a TIV alert mechanism that can inform neighbor selection mechanisms to avoid the pitfalls caused by TIVs and improve their effectiveness.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,175–188,14,"distributed system, internet delay space, triangle inequality violations, analysis, neighbor selection","San Diego, California, USA",IMC '07,,,,,,
1486,inproceedings,"Alderson, D.",Session Details: Routing and Topology II,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3250739,10.1145/3250739,,Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,,,,"San Diego, California, USA",IMC '07,,,,,,
1487,inproceedings,"Mao, Yun and Jamjoom, Hani and Tao, Shu and Smith, Jonathan M.",Networkmd: Topology Inference and Failure Diagnosis in the Last Mile,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298333,10.1145/1298306.1298333,"Health monitoring, automated failure localization and diagnosis have all become critical to service providers of large distribution networks (e.g., digital cable and fiber-to-the-home), due to the increases in scale and complexity of their offered services. Existing automated failure diagnosis solutions typically assume complete knowledge of network topology, which in practice is rarely available. The solution presented in this paper - Network Management and Diagnosis (NetworkMD) - is an automated failure diagnosis system that can infer failure groups based on historical failure data, and optionally geographical information. The inferred failure groups mirror missing topologies, and can be used to localize failures, diagnose root causes of problems, and detect misconfiguration in known topologies. NetworkMD uses an unsupervised learning algorithm based on non-negative matrix factorization (NMF) to infer failure groups. Using cable network as the primary example, we demonstrate the effectiveness of NetworkMD in both simulated settings and real environment using data collected from a commercial network serving hundreds of thousands of customers via thousands of intermediate network devices.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,189–202,14,"network topology inference, failure diagnosis","San Diego, California, USA",IMC '07,,,,,,
1488,inproceedings,"Gunes, Mehmet H. and Sarac, Kamil",Inferring Subnets in Router-Level Topology Collection Studies,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298334,10.1145/1298306.1298334,"Internet measurement studies require availability of representative topology maps. Depending on the map resolution (e.g., autonomous system level or router level), the procedure of collecting and processing an Internet topology map involves different tasks. In this paper, we present a new task, i.e., subnet inference, to advance the current state of the art in topology collection studies. Utilizing a technique to infer the subnet relations among the routers in the resulting topology map, we identify IP addresses that are connected over the same connection medium. We believe that the successful inclusion of subnet relations among the routers will yield topology maps that are closer, at the network layer, to the sampled segments of the Internet in router level topology measurement studies.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,203–208,6,"topology discovery, router-level map, subnet inference","San Diego, California, USA",IMC '07,,,,,,
1489,inproceedings,"Eriksson, Brian and Barford, Paul and Nowak, Robert and Crovella, Mark",Learning Network Structure from Passive Measurements,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298335,10.1145/1298306.1298335,"The ability to discover network organization, whether in the form of explicit topology reconstruction or as embeddings that approximate topological distance, is a valuable tool. To date, network discovery has been based on active measurements. However, it is feasible to envision passive discovery of network topology and distance, simply by monitoring packet traffic. Unfortunately, the lack of explicit control over the choices of which endpoints are measured means that passive network discovery must deal with the problem of missing information. We consider one such example, namely reconstructing embeddings and some network structure information from unwanted network traffic captured at a set of honeypots. We develop a number of algorithms for reconstruction of missing measurements. Our algorithms use insights derived from the known topology of the Internet as well as local imputation techniques from approximation theory. We characterize the degree to which missing information can be reconstructed and show that a limited but useful amount of reconstruction is possible, allowing the recovery of network embeddings and some topological relationships from passively collected data.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,209–214,6,"imputation, embedding, measurement, topology, inference","San Diego, California, USA",IMC '07,,,,,,
1490,inproceedings,"Zhang, Ying and Zhang, Zheng and Mao, Zhuoqing Morley and Hu, Charlie and MacDowell Maggs, Bruce",On the Impact of Route Monitor Selection,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298336,10.1145/1298306.1298336,"Several route monitoring systems have been set up to help understand the Internet routing system. They operate by gathering real-time BGP updates from different networks. Many studies have relied on such data sources by assuming reasonably good coverage and thus representative visibility into the Internet routing system. However, different deployment strategies of route monitors directly impact the accuracy and generality of conclusions.Our work is the first to critically examine the visibility constraints imposed by the deployment of route monitors on various applications. We study the difference due to diverse deployment schemes on three important classes of applications: (1) discovery of relatively stable Internet properties such as the AS topology and prefix to origin AS mappings, (2) discovery of dynamic routing behavior such as IP prefix hijack attacks and routing instability, and (3) inference of important network properties such as AS relationships and AS-level paths. We study several simple schemes of route monitor selection and provide insights on improving monitor placement.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,215–220,6,"internet measurement, bgp","San Diego, California, USA",IMC '07,,,,,,
1491,inproceedings,"Hummel, Benjamin and Kosub, Sven",Acyclic Type-of-Relationship Problems on the Internet: An Experimental Analysis,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298337,10.1145/1298306.1298337,"An experimental study of the feasibility and accuracy of the acyclicity approach introduced in [14] for the inference of business relationships among autonomous systems (ASes) is provided. We investigate the maximum acyclic type-of-relationship problem: on a given set of AS paths, find a maximum-cardinality subset which allows an acyclic and valley-free orientation. Inapproximability and NP-hardness results for this problem are presented and a heuristic is designed. The heuristic is experimentally compared to most of the state-of-the-art algorithms on a reliable data set. It turns out that the proposed heuristic produces the least number of misclassified customer-to-provider relationships among the tested algorithms. Moreover, it is flexible in handling pre-knowledge in the sense that already a small amount of correct relationships is enough to produce a high-quality relationship classification. Furthermore, the reliable data set is used to validate the acyclicity assumptions. The findings demonstrate that acyclicity notions should be an integral part of models of AS relationships.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,221–226,6,"algorithms, inter-domain routing, as relationships","San Diego, California, USA",IMC '07,,,,,,
1492,inproceedings,"Teixeira, R.",Session Details: Network Tomography,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3250740,10.1145/3250740,,Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,,,,"San Diego, California, USA",IMC '07,,,,,,
1493,inproceedings,"Nguyen, Hung X. and Thiran, Patrick",Network Loss Inference with Second Order Statistics of End-to-End Flows,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298339,10.1145/1298306.1298339,"We address the problem of calculating link loss rates from end-to-end measurements. Contrary to existing works that use only the average end-to-end loss rates or strict temporal correlations between probes, we exploit second-order moments of end-to-end flows. We first prove that the variances of link loss rates can be uniquely calculated from the covariances of the measured end-to-end loss rates in any realistic topology. After calculating the link variances, we remove the un-congested links with small variances from the first-order moment equations to obtain a full rank linear system of equations, from which we can calculate precisely the loss rates of the remaining congested links. This operation is possible because losses due to congestion occur in bursts and hence the loss rates of congested links have high variances. On the contrary, most links on the Internet are un-congested, and hence the averages and variances of their loss rates are virtually zero. Our proposed solution uses only regular unicast probes and thus is applicable in today's Internet. It is accurate and scalable, as shown in our simulations and experiments on PlanetLab.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,227–240,14,"inference, network tomography, identifiability","San Diego, California, USA",IMC '07,,,,,,
1494,inproceedings,"Coates, Mark and Pointurier, Yvan and Rabbat, Michael",Compressed Network Monitoring for Ip and All-Optical Networks,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298340,10.1145/1298306.1298340,"We address the problem of efficient end-to-end network monitoring of path metrics in communication networks. Our goal is to minimize the number of measurements or monitors required to maintain an acceptable estimation accuracy. We present a framework based on diffusion wavelets and nonlinear estimation. Our procedure involves the development of a diffusion wavelet basis that is adapted to the monitoring problem. This basis exploits spatial and temporal correlations in the measured phenomena to provide a compressible representation of the path metrics. The framework employs nonlinear estimation techniques using l1 minimization to generate estimates for the unmeasured paths. We describe heuristic approaches for the selection of the paths that should be monitored, or equivalently, where hardware monitors should be located. We demonstrate how our estimation framework can improve the efficiency of end-to-end delay estimation in IP networks and reduce the number of hardware monitors required to track bit-error rates in all-optical networks (networks with no electrical regenerators).",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,241–252,12,"diffusion wavelets, compressed sensing, network monitoring","San Diego, California, USA",IMC '07,,,,,,
1495,inproceedings,"Schnitter, Stefan and Hartleb, Franz and Horneffer, Martin",Quality-of-Service Class Specific Traffic Matrices in Ip/Mpls Networks,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298341,10.1145/1298306.1298341,"In this paper we consider the problem of determining traffic matrices for end-to-end demands in an IP/MPLS network that supports multiple quality of service (QoS) classes. More precisely, we want to determine the set of traffic matrices Ti for each QoS class i separately. Ti contains average bandwidth levels for QoS class i for every pair of routers within the network. We propose a new method for obtaining QoS class specific traffic matrices that combines estimation and measurement methods: We take advantage of the fact that the total traffic matrix can be measured precisely in MPLS networks using either the LDP or RSVP-TE protocol. These measurements can then be used in a mathematical model to improve estimation methods - known as network tomography - that estimate QoS class specific traffic matrices from QoS class specific link utilizations. In addition to the mathematical model, we present results of the proposed method from its application in Deutsche Telekom's global IP/MPLS backbone network and we show that the estimation accuracy (mean relative error) is improved by a factor of 2.5 compared to results from network tomogravity. We investigate the structure of the estimated traffic matrices for the different QoS classes and motivate in this paper why QoS class specific traffic matrices will be essential for efficient network planning and network engineering in the future.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,253–258,6,"mpls, qos, ldp, traffic matrices","San Diego, California, USA",IMC '07,,,,,,
1496,inproceedings,"Chandalia, Gaurav and Rish, Irina",Blind Source Separation Approach to Performance Diagnosis and Dependency Discovery,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298342,10.1145/1298306.1298342,"We consider the problem of diagnosing performance problems in distributed system and networks given end-to-end performance measurements provided by test transactions, or probes. Common techniques for problem diagnosis such as, for example, codebook and network tomography usually assume a known dependency (e.g., routing) matrix that describes how each probe depends on the systems components. However, collecting full information about routing and/or probe dependencies on all systems components can be very costly, if not impossible, in large-scale, dynamic networks and distributed systems. We propose an approach to problem diagnosis and dependency discovery from end-to-end performance measurements in cases when the dependency/routing information is unknown or partially known. Our method is based on Blind Source Separation (BSS) approach that aims at reconstructing unobserved input signals and the mixing-weights matrix from the observed mixtures of signals. Particularly, we apply sparse non-negative matrix factorization techniques that appear particularly fitted to the problem of recovering network bottlenecks and dependency (routing) matrix, and show promising experimental results on several realistic network topologies.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,259–264,6,"sparse optimization, network tomography, end-to-end probes, matrix factorization, blind source separation","San Diego, California, USA",IMC '07,,,,,,
1497,inproceedings,"Veitch, D.",Session Details: Sampling and Streaming,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3250741,10.1145/3250741,,Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,,,,"San Diego, California, USA",IMC '07,,,,,,
1498,inproceedings,"Cohen, Edith and Duffield, Nick and Kaplan, Haim and Lund, Carsten and Thorup, Mikkel",Algorithms and Estimators for Accurate Summarization of Internet Traffic,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298344,10.1145/1298306.1298344,"Statistical summaries of traffic in IP networks are at the heart of network operation and are used to recover information on the traffic of arbitrary subpopulations of flows. It is therefore of great importance to collect the most accurate and informative summaries given the router's resource constraints. Cisco's sampled NetFlow, based on aggregating a sampled packet stream into flows, is the most widely deployed such system.We observe two sources of inefficiency in current methods. Firstly, a single parameter (the sampling rate) is used to control utilization of both memory and processing/access speed, which means that it has to be set according to the bottleneck resource. Secondly, the unbiased estimators are applicable to summaries that in effect are collected through uneven use of resources during the measurement period (information from the earlier part of the measurement period is either not collected at all and fewer counter are utilized or discarded when performing a sampling rate adaptation).We develop algorithms that collect more informative summaries through an even and more efficient use of available resources. The heart of our approach is a novel derivation of unbiased estimators that use these more informative counts. We show how to efficiently compute these estimators and prove analytically that they are superior (have smaller variance on all packet streams and subpopulations) to previous approaches. Simulations on Pareto distributions and IP flow data show that the new summaries provide significantly more accurate estimates. We provide an implementation design that can be efficiently deployed at routers.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,265–278,14,"data streams, network management, sketches, subpopulation queries, ip flows, netflow","San Diego, California, USA",IMC '07,,,,,,
1499,inproceedings,"Zhao, Haiquan (Chuck) and Lall, Ashwin and Ogihara, Mitsunori and Spatscheck, Oliver and Wang, Jia and Xu, Jun",A Data Streaming Algorithm for Estimating Entropies of Od Flows,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298345,10.1145/1298306.1298345,"Entropy has recently gained considerable significance as an important metric for network measurement. Previous research has shown its utility in clustering traffic and detecting traffic anomalies. While measuring the entropy of the traffic observed at a single point has already been studied, an interesting open problem is to measure the entropy of the traffic between every origin-destination pair. In this paper, we propose the first solution to this challenging problem. Our sketch builds upon and extends the Lp sketch of Indyk with significant additional innovations. We present calculations showing that our data streaming algorithm is feasible for high link speeds using commodity CPU/memory at a reasonable cost. Our algorithm is shown to be very accurate in practice via simulations, using traffic traces collected at a tier-1 ISP backbone link.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,279–290,12,"stable distributions, traffic matrix, network measurement, entropy estimation, data streaming","San Diego, California, USA",IMC '07,,,,,,
1500,inproceedings,"Baccelli, Francois and Machiraju, Sridhar and Veitch, Darryl and Bolot, Jean C.",On Optimal Probing for Delay and Loss Measurement,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298346,10.1145/1298306.1298346,"Packet delay and loss are two fundamental measures of performance. Using active probing to measure delay and loss typically involves sending Poisson probes, on the basis of the PASTA property (Poisson Arrivals See Time Averages), which ensures that Poisson probing yields unbiased estimates. Recent work, however, has questioned the utility of PASTA for probing and shown that, for delay measurements, i) a wide variety of processes other than Poisson can be used to probe with zero bias and ii) Poisson probing does not necessarily minimize the variance of delay estimates.In this paper, we determine optimal probing processes that minimize the mean-square error of measurement estimates for both delay and loss. Our contributions are twofold. First, we show that a family of probing processes, specifically Gamma renewal probing processes, has optimal properties in terms of bias and variance. The optimality result is general, and only assumes that the target process we seek to optimally measure via probing, such as a loss or delay process, has a convex auto-covariance function. Second, we use empirical datasets to demonstrate the applicability of our results in practice, specifically to show that the convexity condition holds true and that Gamma probing is indeed superior to Poisson probing. Together, these results lead to explicit guidelines on designing the best probe streams for both delay and loss estimation.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,291–302,12,"active probing, variance, auto-covariance, pasta, convexity","San Diego, California, USA",IMC '07,,,,,,
1501,inproceedings,"Machiraju, S.",Session Details: Measurements II,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3250742,10.1145/3250742,,Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,,,,"San Diego, California, USA",IMC '07,,,,,,
1502,inproceedings,"Sommers, Joel and Barford, Paul",An Active Measurement System for Shared Environments,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298348,10.1145/1298306.1298348,"Testbeds composed of end hosts deployed across the Internet enable researchers to simultaneously conduct a wide variety of experiments. Active measurement studies of Internet path properties that require precisely crafted probe streams can be problematic in these environments. The reason is that load on the host systems from concurrently executing experiments (as is typical in PlanetLab) can significantly alter probe stream timings. In this paper we measure and characterize how packet streams from our local PlanetLab nodes are affected by experimental concurrency. We find that the effects can be extreme. We then set up a simple PlanetLab deployment in a laboratory testbed to evaluate these effects in a controlled fashion. We find that even relatively low load levels can cause serious problems in probe streams. Based on these results, we develop a novel system called <scp>MAD</scp> that can operate as a Linux kernel module or as a stand-alone daemon to support real-time scheduling of probe streams. <scp>MAD</scp> coordinates probe packet emission for all active measurement experiments on a node. We demonstrate the capabilities of <scp>MAD</scp>, showing that it performs effectively even under very high levels of multiplexing and host system load.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,303–314,12,"active measurement, mad","San Diego, California, USA",IMC '07,,,,,,
1503,inproceedings,"Iliofotou, Marios and Pappu, Prashanth and Faloutsos, Michalis and Mitzenmacher, Michael and Singh, Sumeet and Varghese, George",Network Monitoring Using Traffic Dispersion Graphs (Tdgs),2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298349,10.1145/1298306.1298349,"Monitoring network traffic and detecting unwanted applications has become a challenging problem, since many applications obfuscate their traffic using unregistered port numbers or payload encryption. Apart from some notable exceptions, most traffic monitoring tools use two types of approaches: (a) keeping traffic statistics such as packet sizes and interarrivals, flow counts, byte volumes, etc., or (b) analyzing packet content. In this paper, we propose the use of Traffic Dispersion Graphs (TDGs) as a way to monitor, analyze, and visualize network traffic. TDGs model the social behavior of hosts (""who talks to whom""), where the edges can be defined to represent different interactions (e.g. the exchange of a certain number or type of packets). With the introduction of TDGs, we are able to harness a wealth of tools and graph modeling techniques from a diverse set of disciplines.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,315–320,6,"network traffic visualization, network monitoring, hosts' connection graphs, behavioral approach","San Diego, California, USA",IMC '07,,,,,,
1504,inproceedings,"Iannaccone, G.",Session Details: Wireless II,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3250744,10.1145/3250744,,Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,,,,"San Diego, California, USA",IMC '07,,,,,,
1505,inproceedings,"Niculescu, Drago\c{s",Interference Map for 802.11 Networks,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298355,10.1145/1298306.1298355,"The interference map of an 802.11 network is a collection of data structures that can help heuristics for routing, channel assignment and call admission in dense wireless networks. The map can be obtained from detailed measurements, which are time consuming and require network down time. We explore methods and models to produce the interference map with a reduced number of measurements, by identifying interference properties that help to extrapolate complex measurements from simple measurements. Actual interference in an 802.11a testbed is shown to follow certain regularities - it is linear with respect to packet rate of the source, packet rate of the interferer, and shows independence among interferers. When multiple cards are available, they behave differently, and even different channels of the same card have different performance. We find that while current methods of gathering the interference map may be appropriate for characterizing interference in one card networks, they are unscalable for multiple card networks when considering: 802.11 characteristics (card and channel asymmetries, time variation), required downtime, and complexity of the measurement procedure.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,339–350,12,"interference, model, measurement, 802.11","San Diego, California, USA",IMC '07,,,,,,
1506,inproceedings,"Shrivastava, Vivek and Agrawal, Dheeraj and Mishra, Arunesh and Banerjee, Suman and Nadeem, Tamer",Understanding the Limitations of Transmit Power Control for Indoor Wlans,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298356,10.1145/1298306.1298356,"A wide range of transmit power control (TPC) algorithms have been proposed in recent literature to reduce interference and increase capacity in 802.11 wireless networks. However, few of them have made it to practice. In many cases this gap is attributed to lack of suitable hardware support in wireless cards to implement these algorithms. In particular, many research efforts have indicated that wireless card vendors need to support power control mechanisms in a fine-grained manner - both in the number of possible power levels and the time granularity at which the controls can be applied. In this paper we claim that even if fine-grained power control mechanisms were to be made available by wireless card vendors, algorithms would not be able to properly leverage such degrees of control in typical indoor environments. We prove this claim through rigorous empirical analysis and then build a tunable empirical model (Model-TPC) that can determine the granularity of power control that is actually useful. To illustrate the importance of our solution, we conclude by demonstrating the impact of choice of power control granularity on Internet applications where wireless clients interact with servers on the Internet. We observe that the number of feasible power was found to be between 2-4 for most indoor environments. We believe that the results from this study can serve as the right set of assumptions to build practically realizable TPC algorithms in the future.",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,351–364,14,"rssi modeling, ieee 802.11, transmit power control, indoor wlan, limitations, fine grained, kullback-leibler","San Diego, California, USA",IMC '07,,,,,,
1507,inproceedings,"Wei, Wei and Suh, Kyoungwon and Wang, Bing and Gu, Yu and Kurose, Jim and Towsley, Don",Passive Online Rogue Access Point Detection Using Sequential Hypothesis Testing with TCP ACK-Pairs,2007,9781595939081,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1298306.1298357,10.1145/1298306.1298357,"Rogue (unauthorized) wireless access points pose serious security threats to local networks. In this paper, we propose two online algorithms to detect rogue access points using sequential hypothesis tests applied to packet-header data collected passively at a monitoring point. One algorithm requires training sets, while the other does not. Both algorithms extend our earlier TCP ACK-pair technique to differentiate wired and wireless LAN TCP traffic, and exploit the fundamental properties of the 802.11 CSMA/CA MAC protocol and the half duplex nature of wireless channels. Our algorithms make prompt decisions as TCP ACK-pairs are observed, and only incur minimum computation and storage overhead. We have built a system for online rogue-access-point detection using these algorithms and deployed it at a university gateway router. Extensive experiments in various scenarios have demonstrated the excellent performance of our approach: the algorithm that requires training provides rapid detection and is extremely accurate (the detection is mostly within 10 seconds, with very low false positive and false negative ratios); the algorithm that does not require training detects 60\%-76\% of the wireless hosts without any false positives; both algorithms are light-weight (with computation and storage overhead well within the capability of commodity equipment).",Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement,365–378,14,"sequential hypothesis testing, rogue access point detection, TCP ACK-pairs","San Diego, California, USA",IMC '07,,,,,,
1508,inproceedings,"Balakrishnan, H.",Session Details: Architecture,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3249784,10.1145/3249784,,"Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Pisa, Italy",SIGCOMM '06,,,,,,
1509,article,"Bavier, Andy and Feamster, Nick and Huang, Mark and Peterson, Larry and Rexford, Jennifer",In VINI Veritas: Realistic and Controlled Network Experimentation,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159916,10.1145/1151659.1159916,"This paper describes VINI, a virtual network infrastructure that allows network researchers to evaluate their protocols and services in a realistic environment that also provides a high degree of control over network conditions. VINI allows researchers to deploy and evaluate their ideas with real routing software, traffic loads, and network events. To provide researchers flexibility in designing their experiments, VINI supports simultaneous experiments with arbitrary network topologies on a shared physical infrastructure. This paper tackles the following important design question: What set of concepts and techniques facilitate flexible, realistic, and controlled experimentation (e.g., multiple topologies and the ability to tweak routing algorithms) on a fixed physical infrastructure? We first present VINI's high-level design and the challenges of virtualizing a single network. We then present PL-VINI, an implementation of VINI on PlanetLab, running the ""Internet In a Slice"". Our evaluation of PL-VINI shows that it provides a realistic and controlled environment for evaluating new protocols and services.",,3–14,12,"internet, virtualization, experimentation, routing, architecture",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1510,inproceedings,"Bavier, Andy and Feamster, Nick and Huang, Mark and Peterson, Larry and Rexford, Jennifer",In VINI Veritas: Realistic and Controlled Network Experimentation,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159916,10.1145/1159913.1159916,"This paper describes VINI, a virtual network infrastructure that allows network researchers to evaluate their protocols and services in a realistic environment that also provides a high degree of control over network conditions. VINI allows researchers to deploy and evaluate their ideas with real routing software, traffic loads, and network events. To provide researchers flexibility in designing their experiments, VINI supports simultaneous experiments with arbitrary network topologies on a shared physical infrastructure. This paper tackles the following important design question: What set of concepts and techniques facilitate flexible, realistic, and controlled experimentation (e.g., multiple topologies and the ability to tweak routing algorithms) on a fixed physical infrastructure? We first present VINI's high-level design and the challenges of virtualizing a single network. We then present PL-VINI, an implementation of VINI on PlanetLab, running the ""Internet In a Slice"". Our evaluation of PL-VINI shows that it provides a realistic and controlled environment for evaluating new protocols and services.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",3–14,12,"internet, routing, experimentation, architecture, virtualization","Pisa, Italy",SIGCOMM '06,,,,,,
1511,article,"Ratnasamy, Sylvia and Ermolinskiy, Andrey and Shenker, Scott",Revisiting IP Multicast,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159917,10.1145/1151659.1159917,This paper revisits a much explored topic in networking - the search for a simple yet fully-general multicast design. The many years of research into multicast routing have led to a generally pessimistic view that the complexity of multicast routing-and inter-domain multicast routing in particular - can only be overcome by restricting the service model (as in single-source) multicast. This paper proposes a new approach to implementing IP multicast that we hope leads to a reevaluation of this commonly held view.,,15–26,12,"routing, multicas",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1512,inproceedings,"Ratnasamy, Sylvia and Ermolinskiy, Andrey and Shenker, Scott",Revisiting IP Multicast,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159917,10.1145/1159913.1159917,This paper revisits a much explored topic in networking - the search for a simple yet fully-general multicast design. The many years of research into multicast routing have led to a generally pessimistic view that the complexity of multicast routing-and inter-domain multicast routing in particular - can only be overcome by restricting the service model (as in single-source) multicast. This paper proposes a new approach to implementing IP multicast that we hope leads to a reevaluation of this commonly held view.,"Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",15–26,12,"multicas, routing","Pisa, Italy",SIGCOMM '06,,,,,,
1513,article,"Kohler, Eddie and Handley, Mark and Floyd, Sally",Designing DCCP: Congestion Control without Reliability,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159918,10.1145/1151659.1159918,"Fast-growing Internet applications like streaming media and telephony prefer timeliness to reliability, making TCP a poor fit. Unfortunately, UDP, the natural alternative, lacks congestion control. High-bandwidth UDP applications must implement congestion control themselves-a difficult task-or risk rendering congested networks unusable. We set out to ease the safe deployment of these applications by designing a congestion-controlled unreliable transport protocol. The outcome, the Datagram Congestion Control Protocol or DCCP, adds to a UDP-like foundation the minimum mechanisms necessary to support congestion control. We thought those mechanisms would resemble TCP's, but without reliability and, especially, cumulative acknowledgements, we had to reconsider almost every aspect of TCP's design. The resulting protocol sheds light on how congestion control interacts with unreliable transport, how modern network constraints impact protocol design, and how TCP's reliable bytestream semantics intertwine with its other mechanisms, including congestion control.",,27–38,12,"congestion control, Internet telephony, streaming media, TCP, unreliable, transfer, transport protocols, DCCP",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1514,inproceedings,"Kohler, Eddie and Handley, Mark and Floyd, Sally",Designing DCCP: Congestion Control without Reliability,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159918,10.1145/1159913.1159918,"Fast-growing Internet applications like streaming media and telephony prefer timeliness to reliability, making TCP a poor fit. Unfortunately, UDP, the natural alternative, lacks congestion control. High-bandwidth UDP applications must implement congestion control themselves-a difficult task-or risk rendering congested networks unusable. We set out to ease the safe deployment of these applications by designing a congestion-controlled unreliable transport protocol. The outcome, the Datagram Congestion Control Protocol or DCCP, adds to a UDP-like foundation the minimum mechanisms necessary to support congestion control. We thought those mechanisms would resemble TCP's, but without reliability and, especially, cumulative acknowledgements, we had to reconsider almost every aspect of TCP's design. The resulting protocol sheds light on how congestion control interacts with unreliable transport, how modern network constraints impact protocol design, and how TCP's reliable bytestream semantics intertwine with its other mechanisms, including congestion control.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",27–38,12,"streaming media, unreliable, Internet telephony, congestion control, transport protocols, DCCP, TCP, transfer","Pisa, Italy",SIGCOMM '06,,,,,,
1515,inproceedings,"Karp, B.",Session Details: Wireless,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3249785,10.1145/3249785,,"Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Pisa, Italy",SIGCOMM '06,,,,,,
1516,article,"Cheng, Yu-Chung and Bellardo, John and Benk\""{o",Jigsaw: Solving the Puzzle of Enterprise 802.11 Analysis,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159920,10.1145/1151659.1159920,"The combination of unlicensed spectrum, cheap wireless interfaces and the inherent convenience of untethered computing have made 802.11 based networks ubiquitous in the enterprise. Modern universities, corporate campuses and government offices routinely de-ploy scores of access points to blanket their sites with wireless Internet access. However, while the fine-grained behavior of the 802.11 protocol itself has been well studied, our understanding of how large 802.11 networks behave in their full empirical complex-ity is surprisingly limited. In this paper, we present a system called Jigsaw that uses multiple monitors to provide a single unified view of all physical, link, network and transport-layer activity on an 802.11 network. To drive this analysis, we have deployed an infrastructure of over 150 radio monitors that simultaneously capture all 802.11b and 802.11g activity in a large university building (1M+ cubic feet). We describe the challenges posed by both the scale and ambiguity inherent in such an architecture, and explain the algorithms and inference techniques we developed to address them. Finally, using a 24-hour distributed trace containing more than 1.5 billion events, we use Jigsaw's global cross-layer viewpoint to isolate performance artifacts, both explicit, such as management inefficiencies, and implicit, such as co-channel interference. We believe this is the first analysis combining this scale and level of detail for a production 802.11 network.",,39–50,12,"monitoring, wireless networks, measurement, 802.11",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1517,inproceedings,"Cheng, Yu-Chung and Bellardo, John and Benk\""{o",Jigsaw: Solving the Puzzle of Enterprise 802.11 Analysis,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159920,10.1145/1159913.1159920,"The combination of unlicensed spectrum, cheap wireless interfaces and the inherent convenience of untethered computing have made 802.11 based networks ubiquitous in the enterprise. Modern universities, corporate campuses and government offices routinely de-ploy scores of access points to blanket their sites with wireless Internet access. However, while the fine-grained behavior of the 802.11 protocol itself has been well studied, our understanding of how large 802.11 networks behave in their full empirical complex-ity is surprisingly limited. In this paper, we present a system called Jigsaw that uses multiple monitors to provide a single unified view of all physical, link, network and transport-layer activity on an 802.11 network. To drive this analysis, we have deployed an infrastructure of over 150 radio monitors that simultaneously capture all 802.11b and 802.11g activity in a large university building (1M+ cubic feet). We describe the challenges posed by both the scale and ambiguity inherent in such an architecture, and explain the algorithms and inference techniques we developed to address them. Finally, using a 24-hour distributed trace containing more than 1.5 billion events, we use Jigsaw's global cross-layer viewpoint to isolate performance artifacts, both explicit, such as management inefficiencies, and implicit, such as co-channel interference. We believe this is the first analysis combining this scale and level of detail for a production 802.11 network.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",39–50,12,"802.11, monitoring, measurement, wireless networks","Pisa, Italy",SIGCOMM '06,,,,,,
1518,article,"Reis, Charles and Mahajan, Ratul and Rodrig, Maya and Wetherall, David and Zahorjan, John",Measurement-Based Models of Delivery and Interference in Static Wireless Networks,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159921,10.1145/1151659.1159921,"We present practical models for the physical layer behaviors of packet reception and carrier sense with interference in static wireless networks. These models use measurements of a real network rather than abstract RF propagation models as the basis for accuracy in complex environments. Seeding our models requires N trials in an N node network, in which each sender transmits in turn and receivers measure RSSI values and packet counts, both of which are easily obtainable. The models then predict packet delivery and throughput in the same network for different sets of transmitters with the same node placements. We evaluate our models for the base case of two senders that broadcast packets simultaneously. We find that they are effective at predicting when there will be significant interference effects. Across many predictions, we obtain an RMS error for 802.11a and 802.11b of a half and a third, respectively, of a measurement-based model that ignores interference.",,51–62,12,"RSSI, interference, modeling",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1519,inproceedings,"Reis, Charles and Mahajan, Ratul and Rodrig, Maya and Wetherall, David and Zahorjan, John",Measurement-Based Models of Delivery and Interference in Static Wireless Networks,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159921,10.1145/1159913.1159921,"We present practical models for the physical layer behaviors of packet reception and carrier sense with interference in static wireless networks. These models use measurements of a real network rather than abstract RF propagation models as the basis for accuracy in complex environments. Seeding our models requires N trials in an N node network, in which each sender transmits in turn and receivers measure RSSI values and packet counts, both of which are easily obtainable. The models then predict packet delivery and throughput in the same network for different sets of transmitters with the same node placements. We evaluate our models for the base case of two senders that broadcast packets simultaneously. We find that they are effective at predicting when there will be significant interference effects. Across many predictions, we obtain an RMS error for 802.11a and 802.11b of a half and a third, respectively, of a measurement-based model that ignores interference.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",51–62,12,"RSSI, interference, modeling","Pisa, Italy",SIGCOMM '06,,,,,,
1520,article,"Rangwala, Sumit and Gummadi, Ramakrishna and Govindan, Ramesh and Psounis, Konstantinos",Interference-Aware Fair Rate Control in Wireless Sensor Networks,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159922,10.1145/1151659.1159922,"In a wireless sensor network of N nodes transmitting data to a single base station, possibly over multiple hops, what distributed mechanisms should be implemented in order to dynamically allocate fair and efficient transmission rates to each node? Our interferenceaware fair rate control (IFRC) detects incipient congestion at a node by monitoring the average queue length, communicates congestion state to exactly the set of potential interferers using a novel low-overhead congestion sharing mechanism, and converges to a fair and efficient rate using an AIMD control law. We evaluate IFRC extensively on a 40-node wireless sensor network testbed. IFRC achieves a fair and efficient rate allocation that is within 20-40% of the optimal fair rate allocation on some network topologies. Its rate adaptation mechanism is highly effective: we did not observe a single instance of queue overflow in our many experiments. Finally, IFRC can be extended easily to support situations where only a subset of the nodes transmit, where the network has multiple base stations, or where nodes are assigned different transmission weights.",,63–74,12,"congestion, ControlRate control, IFRC, sensor network, fairness, wireless",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1521,inproceedings,"Rangwala, Sumit and Gummadi, Ramakrishna and Govindan, Ramesh and Psounis, Konstantinos",Interference-Aware Fair Rate Control in Wireless Sensor Networks,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159922,10.1145/1159913.1159922,"In a wireless sensor network of N nodes transmitting data to a single base station, possibly over multiple hops, what distributed mechanisms should be implemented in order to dynamically allocate fair and efficient transmission rates to each node? Our interferenceaware fair rate control (IFRC) detects incipient congestion at a node by monitoring the average queue length, communicates congestion state to exactly the set of potential interferers using a novel low-overhead congestion sharing mechanism, and converges to a fair and efficient rate using an AIMD control law. We evaluate IFRC extensively on a 40-node wireless sensor network testbed. IFRC achieves a fair and efficient rate allocation that is within 20-40% of the optimal fair rate allocation on some network topologies. Its rate adaptation mechanism is highly effective: we did not observe a single instance of queue overflow in our many experiments. Finally, IFRC can be extended easily to support situations where only a subset of the nodes transmit, where the network has multiple base stations, or where nodes are assigned different transmission weights.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",63–74,12,"wireless, sensor network, fairness, congestion, IFRC, ControlRate control","Pisa, Italy",SIGCOMM '06,,,,,,
1522,article,"Mahajan, Ratul and Rodrig, Maya and Wetherall, David and Zahorjan, John",Analyzing the MAC-Level Behavior of Wireless Networks in the Wild,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159923,10.1145/1151659.1159923,"We present Wit, a non-intrusive tool that builds on passive monitoring to analyze the detailed MAC-level behavior of operational wireless networks. Wit uses three processing steps to construct an enhanced trace of system activity. First, a robust merging procedure combines the necessarily incomplete views from multiple, independent monitors into a single, more complete trace of wireless activity. Next, a novel inference engine based on formal language methods reconstructs packets that were not captured by any monitor and determines whether each packet was received by its destination. Finally, Wit derives network performance measures from this enhanced trace; we show how to estimate the number of stations competing for the medium. We assess Wit with a mix of real traces and simulation tests. We find that merging and inference both significantly enhance the originally captured trace. We apply Wit to multi-monitor traces from a live network to show how it facilitates 802.11 MAC analyses that would otherwise be difficult or rely on less accurate heuristics.",,75–86,12,"passive monitoring, 802.11 MAC, measurement tool",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1523,inproceedings,"Mahajan, Ratul and Rodrig, Maya and Wetherall, David and Zahorjan, John",Analyzing the MAC-Level Behavior of Wireless Networks in the Wild,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159923,10.1145/1159913.1159923,"We present Wit, a non-intrusive tool that builds on passive monitoring to analyze the detailed MAC-level behavior of operational wireless networks. Wit uses three processing steps to construct an enhanced trace of system activity. First, a robust merging procedure combines the necessarily incomplete views from multiple, independent monitors into a single, more complete trace of wireless activity. Next, a novel inference engine based on formal language methods reconstructs packets that were not captured by any monitor and determines whether each packet was received by its destination. Finally, Wit derives network performance measures from this enhanced trace; we show how to estimate the number of stations competing for the medium. We assess Wit with a mix of real traces and simulation tests. We find that merging and inference both significantly enhance the originally captured trace. We apply Wit to multi-monitor traces from a live network to show how it facilitates 802.11 MAC analyses that would otherwise be difficult or rely on less accurate heuristics.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",75–86,12,"passive monitoring, measurement tool, 802.11 MAC","Pisa, Italy",SIGCOMM '06,,,,,,
1524,inproceedings,"Rexford, J.",Session Details: Network,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3249786,10.1145/3249786,,"Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Pisa, Italy",SIGCOMM '06,,,,,,
1525,article,"Menth, Michael and Martin, R\""{u",Capacity Overprovisioning for Networks with Resilience Requirements,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159925,10.1145/1151659.1159925,"This work focuses on capacity overprovisioning (CO) as an alternative to admission control (AC) to implement quality of service (QoS) in packet-switched communication networks. CO prevents potential overload while AC protects the QoS of the traffic during overload situations. Overload may be caused, e. g., by uctuations of the traffic rate on a link due to its normal stochastic behavior (a), by traffic shifts within the network due to popular contents (b), or by redirected traffic due to network failures (c). Capacity dimensioning methods for CO need to take into account all potential sources of overload while AC can block excess traffic caused by (a) and (b) if the capacity does not suffice. The contributions of this paper are (1) the presentation of a capacity dimensioning method for networks with resilience requirements and changing traffic matrices, (2) the investigation of the impact of the mentioned sources of overload (a-c) on the required capacity for CO in networks with and without resilience requirements, and (3) a comparison of this equired capacity with the one for AC. Our results show that in the presence of strong traffic shifts CO requires more capacity than AC. However, if resilience against network failures is required, both CO and AC need additional backup capacity for the redirected traffic. In this case, CO can use the backup capacity to absorb other types of overload. As a consequence, CO and AC have similar bandwidth requirements. These findings are robust against the network size.",,87–98,12,"admission control, capacity overprovisioning, QoS",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1526,inproceedings,"Menth, Michael and Martin, R\""{u",Capacity Overprovisioning for Networks with Resilience Requirements,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159925,10.1145/1159913.1159925,"This work focuses on capacity overprovisioning (CO) as an alternative to admission control (AC) to implement quality of service (QoS) in packet-switched communication networks. CO prevents potential overload while AC protects the QoS of the traffic during overload situations. Overload may be caused, e. g., by uctuations of the traffic rate on a link due to its normal stochastic behavior (a), by traffic shifts within the network due to popular contents (b), or by redirected traffic due to network failures (c). Capacity dimensioning methods for CO need to take into account all potential sources of overload while AC can block excess traffic caused by (a) and (b) if the capacity does not suffice. The contributions of this paper are (1) the presentation of a capacity dimensioning method for networks with resilience requirements and changing traffic matrices, (2) the investigation of the impact of the mentioned sources of overload (a-c) on the required capacity for CO in networks with and without resilience requirements, and (3) a comparison of this equired capacity with the one for AC. Our results show that in the presence of strong traffic shifts CO requires more capacity than AC. However, if resilience against network failures is required, both CO and AC need additional backup capacity for the redirected traffic. In this case, CO can use the backup capacity to absorb other types of overload. As a consequence, CO and AC have similar bandwidth requirements. These findings are robust against the network size.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",87–98,12,"QoS, admission control, capacity overprovisioning","Pisa, Italy",SIGCOMM '06,,,,,,
1527,article,"Wang, Hao and Xie, Haiyong and Qiu, Lili and Yang, Yang Richard and Zhang, Yin and Greenberg, Albert",COPE: Traffic Engineering in Dynamic Networks,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159926,10.1145/1151659.1159926,"Traffic engineering plays a critical role in determining the performance and reliability of a network. A major challenge in traffic engineering is how to cope with dynamic and unpredictable changes in traffic demand. In this paper, we propose COPE, a class of traffic engineering algorithms that optimize for the expected scenarios while providing a worst-case guarantee for unexpected scenarios. Using extensive evaluations based on real topologies and traffic traces, we show that COPE can achieve efficient resource utilization and avoid network congestion in a wide variety of scenarios.",,99–110,12,"unpredictable traffic, traffic engineering, optimization, oblivious routing, COPE",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1528,inproceedings,"Wang, Hao and Xie, Haiyong and Qiu, Lili and Yang, Yang Richard and Zhang, Yin and Greenberg, Albert",COPE: Traffic Engineering in Dynamic Networks,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159926,10.1145/1159913.1159926,"Traffic engineering plays a critical role in determining the performance and reliability of a network. A major challenge in traffic engineering is how to cope with dynamic and unpredictable changes in traffic demand. In this paper, we propose COPE, a class of traffic engineering algorithms that optimize for the expected scenarios while providing a worst-case guarantee for unexpected scenarios. Using extensive evaluations based on real topologies and traffic traces, we show that COPE can achieve efficient resource utilization and avoid network congestion in a wide variety of scenarios.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",99–110,12,"oblivious routing, traffic engineering, optimization, COPE, unpredictable traffic","Pisa, Italy",SIGCOMM '06,,,,,,
1529,inproceedings,"Willinger, W.",Session Details: Analysis,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3249787,10.1145/3249787,,"Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Pisa, Italy",SIGCOMM '06,,,,,,
1530,article,"Vishwanath, Kashi Venkatesh and Vahdat, Amin",Realistic and Responsive Network Traffic Generation,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159928,10.1145/1151659.1159928,"This paper presents Swing, a closed-loop, network-responsive traffic generator that accurately captures the packet interactions of a range of applications using a simple structural model. Starting from observed traffic at a single point in the network, Swing automatically extracts distributions for user, application, and network behavior. It then generates live traffic corresponding to the underlying models in a network emulation environment running commodity network protocol stacks. We find that the generated traces are statistically similar to the original traces. Further, to the best of our knowledge, we are the first to reproduce burstiness in traffic across a range of timescales using a model applicable to a variety of network settings. An initial sensitivity analysis reveals the importance of capturing and recreating user, application, and network characteristics to accurately reproduce such burstiness. Finally, we explore Swing's ability to vary user characteristics, application properties, and wide-area network conditions to project traffic characteristics into alternate scenarios.",,111–122,12,"burstiness, wavelets, structural model, internet, traffic, energy plot, modeling, generator",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1531,inproceedings,"Vishwanath, Kashi Venkatesh and Vahdat, Amin",Realistic and Responsive Network Traffic Generation,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159928,10.1145/1159913.1159928,"This paper presents Swing, a closed-loop, network-responsive traffic generator that accurately captures the packet interactions of a range of applications using a simple structural model. Starting from observed traffic at a single point in the network, Swing automatically extracts distributions for user, application, and network behavior. It then generates live traffic corresponding to the underlying models in a network emulation environment running commodity network protocol stacks. We find that the generated traces are statistically similar to the original traces. Further, to the best of our knowledge, we are the first to reproduce burstiness in traffic across a range of timescales using a model applicable to a variety of network settings. An initial sensitivity analysis reveals the importance of capturing and recreating user, application, and network characteristics to accurately reproduce such burstiness. Finally, we explore Swing's ability to vary user characteristics, application properties, and wide-area network conditions to project traffic characteristics into alternate scenarios.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",111–122,12,"modeling, wavelets, energy plot, traffic, internet, burstiness, structural model, generator","Pisa, Italy",SIGCOMM '06,,,,,,
1532,article,"Jiang, Yuming",A Basic Stochastic Network Calculus,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159929,10.1145/1151659.1159929,"A basic calculus is presented for stochastic service guarantee analysis in communication networks. Central to the calculus are two definitions, maximum-(virtual)-backlog-centric (m. b. c) stochastic arrival curve and stochastic service curve, which respectively generalize arrival curve and service curve in the deterministic network calculus framework. With m. b. c stochastic arrival curve and stochastic service curve, various basic results are derived under the (min, +)algebra for the general case analysis, which are crucial to the development of stochastic network calculus. These results include (i)superposition of flows, (ii)concatenation of servers, (iii) output characterization, (iv)per-flow service under aggregation, and (v)stochastic backlog and delay guarantees. In addition, to perform independent case analysis, stochastic strict server is defined, which uses an ideal service process and an impairment process to characterize a server. The concept of stochastic strict server not only allows us to improve the basic results (i)-(v)under the independent case, but also provides a convenient way to find the stochastic service curve of a serve. Moreover, an approach is introduced to find the m.b.c stochastic arrival curve of a flow and the stochastic service curve of a server.",,123–134,12,"stochastic quality of service guarantee, stochastic strict server, stochastic service curve, stochastic arrival curve, stochastic network calculus, independent case analysis",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1533,inproceedings,"Jiang, Yuming",A Basic Stochastic Network Calculus,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159929,10.1145/1159913.1159929,"A basic calculus is presented for stochastic service guarantee analysis in communication networks. Central to the calculus are two definitions, maximum-(virtual)-backlog-centric (m. b. c) stochastic arrival curve and stochastic service curve, which respectively generalize arrival curve and service curve in the deterministic network calculus framework. With m. b. c stochastic arrival curve and stochastic service curve, various basic results are derived under the (min, +)algebra for the general case analysis, which are crucial to the development of stochastic network calculus. These results include (i)superposition of flows, (ii)concatenation of servers, (iii) output characterization, (iv)per-flow service under aggregation, and (v)stochastic backlog and delay guarantees. In addition, to perform independent case analysis, stochastic strict server is defined, which uses an ideal service process and an impairment process to characterize a server. The concept of stochastic strict server not only allows us to improve the basic results (i)-(v)under the independent case, but also provides a convenient way to find the stochastic service curve of a serve. Moreover, an approach is introduced to find the m.b.c stochastic arrival curve of a flow and the stochastic service curve of a server.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",123–134,12,"stochastic quality of service guarantee, stochastic service curve, stochastic arrival curve, stochastic network calculus, stochastic strict server, independent case analysis","Pisa, Italy",SIGCOMM '06,,,,,,
1534,article,"Mahadevan, Priya and Krioukov, Dmitri and Fall, Kevin and Vahdat, Amin",Systematic Topology Analysis and Generation Using Degree Correlations,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159930,10.1145/1151659.1159930,"Researchers have proposed a variety of metrics to measure important graph properties, for instance, in social, biological, and computer networks. Values for a particular graph metric may capture a graph's resilience to failure or its routing efficiency. Knowledge of appropriate metric values may influence the engineering of future topologies, repair strategies in the face of failure, and understanding of fundamental properties of existing networks. Unfortunately, there are typically no algorithms to generate graphs matching one or more proposed metrics and there is little understanding of the relationships among individual metrics or their applicability to different settings. We present a new, systematic approach for analyzing network topologies. We first introduce the dK-series of probability distributions specifying all degree correlations within d-sized subgraphs of a given graph G. Increasing values of d capture progressively more properties of G at the cost of more complex representation of the probability distribution. Using this series, we can quantitatively measure the distance between two graphs and construct random graphs that accurately reproduce virtually all metrics proposed in the literature. The nature of the dK-series implies that it will also capture any future metrics that may be proposed. Using our approach, we construct graphs for ",,135–146,12,"degree correlations, network topology",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1535,inproceedings,"Mahadevan, Priya and Krioukov, Dmitri and Fall, Kevin and Vahdat, Amin",Systematic Topology Analysis and Generation Using Degree Correlations,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159930,10.1145/1159913.1159930,"Researchers have proposed a variety of metrics to measure important graph properties, for instance, in social, biological, and computer networks. Values for a particular graph metric may capture a graph's resilience to failure or its routing efficiency. Knowledge of appropriate metric values may influence the engineering of future topologies, repair strategies in the face of failure, and understanding of fundamental properties of existing networks. Unfortunately, there are typically no algorithms to generate graphs matching one or more proposed metrics and there is little understanding of the relationships among individual metrics or their applicability to different settings. We present a new, systematic approach for analyzing network topologies. We first introduce the dK-series of probability distributions specifying all degree correlations within d-sized subgraphs of a given graph G. Increasing values of d capture progressively more properties of G at the cost of more complex representation of the probability distribution. Using this series, we can quantitatively measure the distance between two graphs and construct random graphs that accurately reproduce virtually all metrics proposed in the literature. The nature of the dK-series implies that it will also capture any future metrics that may be proposed. Using our approach, we construct graphs for ","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",135–146,12,"degree correlations, network topology","Pisa, Italy",SIGCOMM '06,,,,,,
1536,article,"Godfrey, P. Brighten and Shenker, Scott and Stoica, Ion",Minimizing Churn in Distributed Systems,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159931,10.1145/1151659.1159931,"A pervasive requirement of distributed systems is to deal with churn-change in the set of participating nodes due to joins, graceful leaves, and failures. A high churn rate can increase costs or decrease service quality. This paper studies how to reduce churn by selecting which subset of a set of available nodes to use.First, we provide a comparison of the performance of a range of different node selection strategies in five real-world traces. Among our findings is that the simple strategy of picking a uniform-random replacement whenever a node fails performs surprisingly well. We explain its performance through analysis in a stochastic model.Second, we show that a class of strategies, which we call ""Preference List"" strategies, arise commonly as a result of optimizing for a metric other than churn, and produce high churn relative to more randomized strategies under realistic node failure patterns. Using this insight, we demonstrate and explain differences in performance for designs that incorporate varying degrees of randomization. We give examples from a variety of protocols, including anycast, over-lay multicast, and distributed hash tables. In many cases, simply adding some randomization can go a long way towards reducing churn.",,147–158,12,"churn, multicast, node selection, DHT",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1537,inproceedings,"Godfrey, P. Brighten and Shenker, Scott and Stoica, Ion",Minimizing Churn in Distributed Systems,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159931,10.1145/1159913.1159931,"A pervasive requirement of distributed systems is to deal with churn-change in the set of participating nodes due to joins, graceful leaves, and failures. A high churn rate can increase costs or decrease service quality. This paper studies how to reduce churn by selecting which subset of a set of available nodes to use.First, we provide a comparison of the performance of a range of different node selection strategies in five real-world traces. Among our findings is that the simple strategy of picking a uniform-random replacement whenever a node fails performs surprisingly well. We explain its performance through analysis in a stochastic model.Second, we show that a class of strategies, which we call ""Preference List"" strategies, arise commonly as a result of optimizing for a metric other than churn, and produce high churn relative to more randomized strategies under realistic node failure patterns. Using this insight, we demonstrate and explain differences in performance for designs that incorporate varying degrees of randomization. We give examples from a variety of protocols, including anycast, over-lay multicast, and distributed hash tables. In many cases, simply adding some randomization can go a long way towards reducing churn.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",147–158,12,"multicast, node selection, DHT, churn","Pisa, Italy",SIGCOMM '06,,,,,,
1538,inproceedings,"Davie, B.",Session Details: Routing 1,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3249788,10.1145/3249788,,"Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Pisa, Italy",SIGCOMM '06,,,,,,
1539,article,"Yang, Xiaowei and Wetherall, David",Source Selectable Path Diversity via Routing Deflections,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159933,10.1145/1151659.1159933,"We present the design of a routing system in which end-systems set tags to select non-shortest path routes as an alternative to explicit source routes. Routers collectively generate these routes by using tags as hints to independently deflect packets to neighbors that lie off the shortest-path. We show how this can be done simply, by local extensions of the shortest path machinery, and safely, so that loops are provably not formed. The result is to provide end-systems with a high-level of path diversity that allows them to bypass unde-sirable locations within the network. Unlike explicit source routing, our scheme is inherently scalable and compatible with ISP policies because it derives from the deployed Internet routing. We also sug-gest an encoding that is compatible with common IP usage, making our scheme incrementally deployable at the granularity of individual routers.",,159–170,12,"routing deflections, source routin, path diversity",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1540,inproceedings,"Yang, Xiaowei and Wetherall, David",Source Selectable Path Diversity via Routing Deflections,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159933,10.1145/1159913.1159933,"We present the design of a routing system in which end-systems set tags to select non-shortest path routes as an alternative to explicit source routes. Routers collectively generate these routes by using tags as hints to independently deflect packets to neighbors that lie off the shortest-path. We show how this can be done simply, by local extensions of the shortest path machinery, and safely, so that loops are provably not formed. The result is to provide end-systems with a high-level of path diversity that allows them to bypass unde-sirable locations within the network. Unlike explicit source routing, our scheme is inherently scalable and compatible with ISP policies because it derives from the deployed Internet routing. We also sug-gest an encoding that is compatible with common IP usage, making our scheme incrementally deployable at the granularity of individual routers.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",159–170,12,"source routin, routing deflections, path diversity","Pisa, Italy",SIGCOMM '06,,,,,,
1541,article,"Xu, Wen and Rexford, Jennifer",MIRO: Multi-Path Interdomain Routing,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159934,10.1145/1151659.1159934,"The Internet consists of thousands of independent domains with different, and sometimes competing, business interests. However, the current interdomain routing protocol (BGP) limits each router to using a single route for each destination prefix, which may not satisfy the diverse requirements of end users. Recent proposals for source routing offer an alternative where end hosts or edge routers select the end-to-end paths. However, source routing leaves transit domains with very little control and introduces difficult scalability and security challenges. In this paper, we present a multi-path inter-domain routing protocol called MIRO that offers substantial flexiility, while giving transit domains control over the flow of traffic through their infrastructure and avoiding state explosion in disseminating reachability information. In MIRO, routers learn default routes through the existing BGP protocol, and arbitrary pairs of domains can negotiate the use of additional paths (bound to tunnels in the data plane) tailored to their special needs. MIRO retains the simplicity of BGP for most traffic, and remains backwards compatible with BGP to allow for incremental deployability. Experiments with Internet topology and routing data illustrate that MIRO offers tremendous flexibility for path selection with reasonable overhead.",,171–182,12,"BGP, flexibility, inter-domain routing, scalability, multipath routing",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1542,inproceedings,"Xu, Wen and Rexford, Jennifer",MIRO: Multi-Path Interdomain Routing,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159934,10.1145/1159913.1159934,"The Internet consists of thousands of independent domains with different, and sometimes competing, business interests. However, the current interdomain routing protocol (BGP) limits each router to using a single route for each destination prefix, which may not satisfy the diverse requirements of end users. Recent proposals for source routing offer an alternative where end hosts or edge routers select the end-to-end paths. However, source routing leaves transit domains with very little control and introduces difficult scalability and security challenges. In this paper, we present a multi-path inter-domain routing protocol called MIRO that offers substantial flexiility, while giving transit domains control over the flow of traffic through their infrastructure and avoiding state explosion in disseminating reachability information. In MIRO, routers learn default routes through the existing BGP protocol, and arbitrary pairs of domains can negotiate the use of additional paths (bound to tunnels in the data plane) tailored to their special needs. MIRO retains the simplicity of BGP for most traffic, and remains backwards compatible with BGP to allow for incremental deployability. Experiments with Internet topology and routing data illustrate that MIRO offers tremendous flexibility for path selection with reasonable overhead.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",171–182,12,"scalability, inter-domain routing, BGP, multipath routing, flexibility","Pisa, Italy",SIGCOMM '06,,,,,,
1543,article,"Laskowski, Paul and Chuang, John",Network Monitors and Contracting Systems: Competition and Innovation,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159935,10.1145/1151659.1159935,"Today's Internet industry suffers from several well-known pathologies, but none is as destructive in the long term as its resistance to evolution. Rather than introducing new services, ISPs are presently moving towards greater commoditization. It is apparent that the network's primitive system of contracts does not align incentives properly. In this study, we identify the network's lack of accountability as a fundamental obstacle to correcting this problem: Employing an economic model, we argue that optimal routes and innovation are impossible unless new monitoring capability is introduced and incorporated with the contracting system. Furthermore, we derive the minimum requirements a monitoring system must meet to support first-best routing and innovation characteristics. Our work does not constitute a new protocol; rather, we provide practical and specific guidance for the design of monitoring systems, as well as a theoretical framework to explore the factors that influence innovation.",,183–194,12,"monitoring, commoditization, contracts, innovation, incentives",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1544,inproceedings,"Laskowski, Paul and Chuang, John",Network Monitors and Contracting Systems: Competition and Innovation,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159935,10.1145/1159913.1159935,"Today's Internet industry suffers from several well-known pathologies, but none is as destructive in the long term as its resistance to evolution. Rather than introducing new services, ISPs are presently moving towards greater commoditization. It is apparent that the network's primitive system of contracts does not align incentives properly. In this study, we identify the network's lack of accountability as a fundamental obstacle to correcting this problem: Employing an economic model, we argue that optimal routes and innovation are impossible unless new monitoring capability is introduced and incorporated with the contracting system. Furthermore, we derive the minimum requirements a monitoring system must meet to support first-best routing and innovation characteristics. Our work does not constitute a new protocol; rather, we provide practical and specific guidance for the design of monitoring systems, as well as a theoretical framework to explore the factors that influence innovation.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",183–194,12,"incentives, contracts, commoditization, monitoring, innovation","Pisa, Italy",SIGCOMM '06,,,,,,
1545,inproceedings,"Byers, J.",Session Details: Measurement,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3249789,10.1145/3249789,,"Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Pisa, Italy",SIGCOMM '06,,,,,,
1546,article,"M\""{u",Building an AS-Topology Model That Captures Route Diversity,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159937,10.1145/1151659.1159937,"An understanding of the topological structure of the Internet is needed for quite a number of networking tasks, e. g., making decisions about peering relationships, choice of upstream providers, inter-domain traffic engineering. One essential component of these tasks is the ability to predict routes in the Internet. However, the Internet is composed of a large number of independent autonomous systems (ASes) resulting in complex interactions, and until now no model of the Internet has succeeded in producing predictions of acceptable accuracy.We demonstrate that there are two limitations of prior models: (i) they have all assumed that an Autonomous System (AS) is an atomic structure - it is not, and (ii) models have tended to oversimplify the relationships between ASes. Our approach uses multiple quasi-routers to capture route diversity within the ASes, and is deliberately agnostic regarding the types of relationships between ASes. The resulting model ensures that its routing is consistent with the observed routes. Exploiting a large number of observation points, we show that our model provides accurate predictions for unobserved routes, a first step towards developing structural mod-els of the Internet that enable real applications.",,195–206,12,"BGP, route diversity, routing, inter-domain routing, policies",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1547,inproceedings,"M\""{u",Building an AS-Topology Model That Captures Route Diversity,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159937,10.1145/1159913.1159937,"An understanding of the topological structure of the Internet is needed for quite a number of networking tasks, e. g., making decisions about peering relationships, choice of upstream providers, inter-domain traffic engineering. One essential component of these tasks is the ability to predict routes in the Internet. However, the Internet is composed of a large number of independent autonomous systems (ASes) resulting in complex interactions, and until now no model of the Internet has succeeded in producing predictions of acceptable accuracy.We demonstrate that there are two limitations of prior models: (i) they have all assumed that an Autonomous System (AS) is an atomic structure - it is not, and (ii) models have tended to oversimplify the relationships between ASes. Our approach uses multiple quasi-routers to capture route diversity within the ASes, and is deliberately agnostic regarding the types of relationships between ASes. The resulting model ensures that its routing is consistent with the observed routes. Exploiting a large number of observation points, we show that our model provides accurate predictions for unobserved routes, a first step towards developing structural mod-els of the Internet that enable real applications.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",195–206,12,"BGP, policies, routing, route diversity, inter-domain routing","Pisa, Italy",SIGCOMM '06,,,,,,
1548,article,"Cho, Kenjiro and Fukuda, Kensuke and Esaki, Hiroshi and Kato, Akira",The Impact and Implications of the Growth in Residential User-to-User Traffic,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159938,10.1145/1151659.1159938,"It has been reported worldwide that peer-to-peer traffic is taking up a significant portion of backbone networks. In particular, it is prominent in Japan because of the high penetration rate of fiber-based broadband access. In this paper, we first report aggregated traffic measurements collected over 21 months from seven ISPs covering 42% of the Japanese backbone traffic. The backbone is dominated by symmetric residential traffic which increased 37%in 2005. We further investigate residential per-customer trafficc in one of the ISPs by comparing DSL and fiber users, heavy-hitters and normal users, and geographic traffic matrices. The results reveal that a small segment of users dictate the overall behavior; 4% of heavy-hitters account for 75% of the inbound volume, and the fiber users account for 86%of the inbound volume. About 63%of the total residential volume is user-to-user traffic. The dominant applications exhibit poor locality and communicate with a wide range and number of peers. The distribution of heavy-hitters is heavy-tailed without a clear boundary between heavy-hitters and normal users, which suggests that users start playing with peer-to-peer applications, become heavy-hitters, and eventually shift from DSL to fiber. We provide conclusive empirical evidence from a large and diverse set of commercial backbone data that the emergence of new attractive applications has drastically affected traffic usage and capacity engineering requirements.",,207–218,12,"traffic growth, ISP backbone traffic, residential broadband",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1549,inproceedings,"Cho, Kenjiro and Fukuda, Kensuke and Esaki, Hiroshi and Kato, Akira",The Impact and Implications of the Growth in Residential User-to-User Traffic,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159938,10.1145/1159913.1159938,"It has been reported worldwide that peer-to-peer traffic is taking up a significant portion of backbone networks. In particular, it is prominent in Japan because of the high penetration rate of fiber-based broadband access. In this paper, we first report aggregated traffic measurements collected over 21 months from seven ISPs covering 42% of the Japanese backbone traffic. The backbone is dominated by symmetric residential traffic which increased 37%in 2005. We further investigate residential per-customer trafficc in one of the ISPs by comparing DSL and fiber users, heavy-hitters and normal users, and geographic traffic matrices. The results reveal that a small segment of users dictate the overall behavior; 4% of heavy-hitters account for 75% of the inbound volume, and the fiber users account for 86%of the inbound volume. About 63%of the total residential volume is user-to-user traffic. The dominant applications exhibit poor locality and communicate with a wide range and number of peers. The distribution of heavy-hitters is heavy-tailed without a clear boundary between heavy-hitters and normal users, which suggests that users start playing with peer-to-peer applications, become heavy-hitters, and eventually shift from DSL to fiber. We provide conclusive empirical evidence from a large and diverse set of commercial backbone data that the emergence of new attractive applications has drastically affected traffic usage and capacity engineering requirements.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",207–218,12,"residential broadband, ISP backbone traffic, traffic growth","Pisa, Italy",SIGCOMM '06,,,,,,
1550,article,"Zhao, Yao and Chen, Yan and Bindel, David",Towards Unbiased End-to-End Network Diagnosis,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159939,10.1145/1151659.1159939,"Internet fault diagnosis is extremely important for end users, overlay network service providers (like Akamai [1]) and even Internet service providers (ISPs). However, because link-level properties cannot be uniquely determined from end-to-end measurements, the accuracy of existing statistical diagnosis approaches is subject to uncertainty from statistical assumptions about the network. In this paper, we propose a novel Least-biased End-to-end Network Diagnosis (in short, LEND) system for inferring link-level properties like loss rate. We define a minimal identifiable link sequence (MILS) as a link sequence of minimal length whose properties can be uniquely identified from end-to-end measurements. We also design efficient algorithms to find all the MILSes and infer their loss rates for diagnosis. Our LEND system works for any network topology and for both directed and undirected properties, and incrementally adapts to network topology and property changes. It gives highly accurate estimates of the loss rates of MILSes, as indicated by both extensive simulations and Internet experiments. Furthermore, we demonstrate that such diagnosis can be achieved with fine granularity and in near real-time even for reasonably large overlay networks. Finally, LEND can supplement existing statistical inference approaches and provide smooth tradeoff between diagnosis accuracy and granularity.",,219–230,12,"internet diagnosis, linear algebra, network measurement",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1551,inproceedings,"Zhao, Yao and Chen, Yan and Bindel, David",Towards Unbiased End-to-End Network Diagnosis,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159939,10.1145/1159913.1159939,"Internet fault diagnosis is extremely important for end users, overlay network service providers (like Akamai [1]) and even Internet service providers (ISPs). However, because link-level properties cannot be uniquely determined from end-to-end measurements, the accuracy of existing statistical diagnosis approaches is subject to uncertainty from statistical assumptions about the network. In this paper, we propose a novel Least-biased End-to-end Network Diagnosis (in short, LEND) system for inferring link-level properties like loss rate. We define a minimal identifiable link sequence (MILS) as a link sequence of minimal length whose properties can be uniquely identified from end-to-end measurements. We also design efficient algorithms to find all the MILSes and infer their loss rates for diagnosis. Our LEND system works for any network topology and for both directed and undirected properties, and incrementally adapts to network topology and property changes. It gives highly accurate estimates of the loss rates of MILSes, as indicated by both extensive simulations and Internet experiments. Furthermore, we demonstrate that such diagnosis can be achieved with fine granularity and in near real-time even for reasonably large overlay networks. Finally, LEND can supplement existing statistical inference approaches and provide smooth tradeoff between diagnosis accuracy and granularity.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",219–230,12,"network measurement, linear algebra, internet diagnosis","Pisa, Italy",SIGCOMM '06,,,,,,
1552,article,"Baccelli, Franc\c{c",The Role of PASTA in Network Measurement,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159940,10.1145/1151659.1159940,"Poisson Arrivals See Time Averages (PASTA) is a well known property applicable to many stochastic systems. In active probing, PASTA is invoked to justify the sending of probe packets (or trains) at Poisson times in a variety of contexts. However, due to the diversity of aims and analysis techniques used in active probing, the benefits of Poisson based measurement, and the utility and role of PASTA, are unclear. Using a combination of rigorous results and carefully constructed examples and counter-examples, we map out the issues involved, and argue that PASTA is of very limited use in active probing. In particular, Poisson probes are not unique in their ability to sample without bias. Furthermore, PASTA ignores the issue of estimation variance, and the central need for an inversion phase to estimate the quantity of interest ased on what is directly observable. We give concrete examples of when Poisson probes should not be used, and explain why, and offer initial guidelines on suitable alternative sending processes.",,231–242,12,"PASTA, NIMASTA, active measurement, probing",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1553,inproceedings,"Baccelli, Franc\c{c",The Role of PASTA in Network Measurement,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159940,10.1145/1159913.1159940,"Poisson Arrivals See Time Averages (PASTA) is a well known property applicable to many stochastic systems. In active probing, PASTA is invoked to justify the sending of probe packets (or trains) at Poisson times in a variety of contexts. However, due to the diversity of aims and analysis techniques used in active probing, the benefits of Poisson based measurement, and the utility and role of PASTA, are unclear. Using a combination of rigorous results and carefully constructed examples and counter-examples, we map out the issues involved, and argue that PASTA is of very limited use in active probing. In particular, Poisson probes are not unique in their ability to sample without bias. Furthermore, PASTA ignores the issue of estimation variance, and the central need for an inversion phase to estimate the quantity of interest ased on what is directly observable. We give concrete examples of when Poisson probes should not be used, and explain why, and offer initial guidelines on suitable alternative sending processes.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",231–242,12,"active measurement, probing, NIMASTA, PASTA","Pisa, Italy",SIGCOMM '06,,,,,,
1554,inproceedings,"Feldmann, A.",Session Details: Coding,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3249790,10.1145/3249790,,"Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Pisa, Italy",SIGCOMM '06,,,,,,
1555,article,"Katti, Sachin and Rahul, Hariharan and Hu, Wenjun and Katabi, Dina and M\'{e",XORs in the Air: Practical Wireless Network Coding,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159942,10.1145/1151659.1159942,"This paper proposes COPE, a new architecture for wireless mesh networks. In addition to forwarding packets, routers mix (i.e., code) packets from different sources to increase the information content of each transmission. We show that intelligently mixing packets increases network throughput. Our design is rooted in the theory of network coding. Prior work on network coding is mainly theoretical and focuses on multicast traffic. This paper aims to bridge theory with practice; it addresses the common case of unicast traffic, dynamic and potentially bursty flows, and practical issues facing the integration of network coding in the current network stack. We evaluate our design on a 20-node wireless network, and discuss the results of the first testbed deployment of wireless network coding. The results show that COPE largely increases network throughput. The gains vary from a few percent to several folds depending on the traffic pattern, congestion level, and transport protocol.",,243–254,12,"network coding, wireless networks",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1556,inproceedings,"Katti, Sachin and Rahul, Hariharan and Hu, Wenjun and Katabi, Dina and M\'{e",XORs in the Air: Practical Wireless Network Coding,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159942,10.1145/1159913.1159942,"This paper proposes COPE, a new architecture for wireless mesh networks. In addition to forwarding packets, routers mix (i.e., code) packets from different sources to increase the information content of each transmission. We show that intelligently mixing packets increases network throughput. Our design is rooted in the theory of network coding. Prior work on network coding is mainly theoretical and focuses on multicast traffic. This paper aims to bridge theory with practice; it addresses the common case of unicast traffic, dynamic and potentially bursty flows, and practical issues facing the integration of network coding in the current network stack. We evaluate our design on a 20-node wireless network, and discuss the results of the first testbed deployment of wireless network coding. The results show that COPE largely increases network throughput. The gains vary from a few percent to several folds depending on the traffic pattern, congestion level, and transport protocol.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",243–254,12,"network coding, wireless networks","Pisa, Italy",SIGCOMM '06,,,,,,
1557,article,"Kamra, Abhinav and Misra, Vishal and Feldman, Jon and Rubenstein, Dan",Growth Codes: Maximizing Sensor Network Data Persistence,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159943,10.1145/1151659.1159943,"Sensor networks are especially useful in catastrophic or emergency scenarios such as floods, fires, terrorist attacks or earthquakes where human participation may be too dangerous. However, such disaster scenarios pose an interesting design challenge since the sensor nodes used to collect and communicate data may themselves fail suddenly and unpredictably, resulting in the loss of valuable data. Furthermore, because these networks are often expected to be deployed in response to a disaster, or because of sudden configuration changes due to failure, these networks are often expected to operate in a ""zero-configuration"" paradigm, where data collection and transmission must be initiated immediately, before the nodes have a chance to assess the current network topology. In this paper, we design and analyze techniques to increase ""persistence"" of sensed data, so that data is more likely to reach a data sink, even as network nodes fail. This is done by replicating data compactly at neighboring nodes using novel ""Growth Codes"" that increase in efficiency as data accumulates at the sink. We show that Growth Codes preserve more data in the presence of node failures than previously proposed erasure resilient techniques.",,255–266,12,"network resilience, LDPC codes",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1558,inproceedings,"Kamra, Abhinav and Misra, Vishal and Feldman, Jon and Rubenstein, Dan",Growth Codes: Maximizing Sensor Network Data Persistence,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159943,10.1145/1159913.1159943,"Sensor networks are especially useful in catastrophic or emergency scenarios such as floods, fires, terrorist attacks or earthquakes where human participation may be too dangerous. However, such disaster scenarios pose an interesting design challenge since the sensor nodes used to collect and communicate data may themselves fail suddenly and unpredictably, resulting in the loss of valuable data. Furthermore, because these networks are often expected to be deployed in response to a disaster, or because of sudden configuration changes due to failure, these networks are often expected to operate in a ""zero-configuration"" paradigm, where data collection and transmission must be initiated immediately, before the nodes have a chance to assess the current network topology. In this paper, we design and analyze techniques to increase ""persistence"" of sensed data, so that data is more likely to reach a data sink, even as network nodes fail. This is done by replicating data compactly at neighboring nodes using novel ""Growth Codes"" that increase in efficiency as data accumulates at the sink. We show that Growth Codes preserve more data in the presence of node failures than previously proposed erasure resilient techniques.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",255–266,12,"LDPC codes, network resilience","Pisa, Italy",SIGCOMM '06,,,,,,
1559,inproceedings,"Paxson, V.",Session Details: Security,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3249791,10.1145/3249791,,"Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Pisa, Italy",SIGCOMM '06,,,,,,
1560,article,"Yu, Haifeng and Kaminsky, Michael and Gibbons, Phillip B. and Flaxman, Abraham",SybilGuard: Defending against Sybil Attacks via Social Networks,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159945,10.1145/1151659.1159945,"Peer-to-peer and other decentralized,distributed systems are known to be particularly vulnerable to sybil attacks. In a sybil attack,a malicious user obtains multiple fake identities and pretends to be multiple, distinct nodes in the system. By controlling a large fraction of the nodes in the system,the malicious user is able to ""out vote"" the honest users in collaborative tasks such as Byzantine failure defenses. This paper presents SybilGuard, a novel protocol for limiting the corruptive influences of sybil attacks.Our protocol is based on the ""social network ""among user identities, where an edge between two identities indicates a human-established trust relationship. Malicious users can create many identities but few trust relationships. Thus, there is a disproportionately-small ""cut"" in the graph between the sybil nodes and the honest nodes. SybilGuard exploits this property to bound the number of identities a malicious user can create.We show the effectiveness of SybilGuard both analytically and experimentally.",,267–278,12,"sybilGuard, sybil attack, sybil identity, social networks",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1561,inproceedings,"Yu, Haifeng and Kaminsky, Michael and Gibbons, Phillip B. and Flaxman, Abraham",SybilGuard: Defending against Sybil Attacks via Social Networks,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159945,10.1145/1159913.1159945,"Peer-to-peer and other decentralized,distributed systems are known to be particularly vulnerable to sybil attacks. In a sybil attack,a malicious user obtains multiple fake identities and pretends to be multiple, distinct nodes in the system. By controlling a large fraction of the nodes in the system,the malicious user is able to ""out vote"" the honest users in collaborative tasks such as Byzantine failure defenses. This paper presents SybilGuard, a novel protocol for limiting the corruptive influences of sybil attacks.Our protocol is based on the ""social network ""among user identities, where an edge between two identities indicates a human-established trust relationship. Malicious users can create many identities but few trust relationships. Thus, there is a disproportionately-small ""cut"" in the graph between the sybil nodes and the honest nodes. SybilGuard exploits this property to bound the number of identities a malicious user can create.We show the effectiveness of SybilGuard both analytically and experimentally.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",267–278,12,"sybil attack, sybil identity, sybilGuard, social networks","Pisa, Italy",SIGCOMM '06,,,,,,
1562,article,"Chan, Haowen and Dash, Debabrata and Perrig, Adrian and Zhang, Hui",Modeling Adoptability of Secure BGP Protocol,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159946,10.1145/1151659.1159946,"Despite the existence of several secure BGP routing protocols, there has been little progress to date on actual adoption. Although feasibility for widespread adoption remains the greatest hurdle for BGP security, there has been little quantitative research into what properties contribute the most to the adoptability of a security scheme. In this paper, we provide a model for assessing the adoptability of a secure BGP routing protocol. We perform this evaluation by simulating incentives compatible adoption decisions of ISPs on the Internet under a variety of assumptions. Our results include: (a) the existence of a sharp threshold, where, if the cost of adoption is below the threshold, complete adoption takes place, while almost no adoption takes place above the threshold; (b) under a strong attacker model, adding a single hop of path authentication to origin authentication yields similar adoptability characteristics as a full path security scheme; (c) under a weaker attacker model, adding full path authentication (e.g., via S-BGP [9]) significantly improves the adoptability of BGP security over weaker path security schemes such as soBGP [16]. These results provide insight into the development of more adoptable secure BGP protocols and demonstrate the importance of studying adoptability of protocols.",,279–290,12,"incentives-compatibility, adoptability, adoption dynamics",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1563,inproceedings,"Chan, Haowen and Dash, Debabrata and Perrig, Adrian and Zhang, Hui",Modeling Adoptability of Secure BGP Protocol,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159946,10.1145/1159913.1159946,"Despite the existence of several secure BGP routing protocols, there has been little progress to date on actual adoption. Although feasibility for widespread adoption remains the greatest hurdle for BGP security, there has been little quantitative research into what properties contribute the most to the adoptability of a security scheme. In this paper, we provide a model for assessing the adoptability of a secure BGP routing protocol. We perform this evaluation by simulating incentives compatible adoption decisions of ISPs on the Internet under a variety of assumptions. Our results include: (a) the existence of a sharp threshold, where, if the cost of adoption is below the threshold, complete adoption takes place, while almost no adoption takes place above the threshold; (b) under a strong attacker model, adding a single hop of path authentication to origin authentication yields similar adoptability characteristics as a full path security scheme; (c) under a weaker attacker model, adding full path authentication (e.g., via S-BGP [9]) significantly improves the adoptability of BGP security over weaker path security schemes such as soBGP [16]. These results provide insight into the development of more adoptable secure BGP protocols and demonstrate the importance of studying adoptability of protocols.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",279–290,12,"adoptability, adoption dynamics, incentives-compatibility","Pisa, Italy",SIGCOMM '06,,,,,,
1564,article,"Ramachandran, Anirudh and Feamster, Nick",Understanding the Network-Level Behavior of Spammers,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159947,10.1145/1151659.1159947,"This paper studies the network-level behavior of spammers, including: IP address ranges that send the most spam, common spamming modes (e.g., BGP route hijacking, bots), how persistent across time each spamming host is, and characteristics of spamming botnets. We try to answer these questions by analyzing a 17-month trace of over 10 million spam messages collected at an Internet ""spam sinkhole"", and by correlating this data with the results of IP-based blacklist lookups, passive TCP fingerprinting information, routing information, and botnet ""command and control"" traces.We find that most spam is being sent from a few regions of IP address space, and that spammers appear to be using transient ""bots"" that send only a few pieces of email over very short periods of time. Finally, a small, yet non-negligible, amount of spam is received from IP addresses that correspond to short-lived BGP routes, typically for hijacked prefixes. These trends suggest that developing algorithms to identify botnet membership, filtering email messages based on network-level properties (which are less variable than email content), and improving the security of the Internet routing infrastructure, may prove to be extremely effective for combating spam.",,291–302,12,"security, network management, botnet, spam, BGP",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1565,inproceedings,"Ramachandran, Anirudh and Feamster, Nick",Understanding the Network-Level Behavior of Spammers,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159947,10.1145/1159913.1159947,"This paper studies the network-level behavior of spammers, including: IP address ranges that send the most spam, common spamming modes (e.g., BGP route hijacking, bots), how persistent across time each spamming host is, and characteristics of spamming botnets. We try to answer these questions by analyzing a 17-month trace of over 10 million spam messages collected at an Internet ""spam sinkhole"", and by correlating this data with the results of IP-based blacklist lookups, passive TCP fingerprinting information, routing information, and botnet ""command and control"" traces.We find that most spam is being sent from a few regions of IP address space, and that spammers appear to be using transient ""bots"" that send only a few pieces of email over very short periods of time. Finally, a small, yet non-negligible, amount of spam is received from IP addresses that correspond to short-lived BGP routes, typically for hijacked prefixes. These trends suggest that developing algorithms to identify botnet membership, filtering email messages based on network-level properties (which are less variable than email content), and improving the security of the Internet routing infrastructure, may prove to be extremely effective for combating spam.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",291–302,12,"spam, BGP, network management, security, botnet","Pisa, Italy",SIGCOMM '06,,,,,,
1566,article,"Walfish, Michael and Vutukuru, Mythili and Balakrishnan, Hari and Karger, David and Shenker, Scott",DDoS Defense by Offense,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159948,10.1145/1151659.1159948,"This paper presents the design, implementation, analysis, and experimental evaluation of speak-up, a defense against application-level distributed denial-of-service (DDoS), in which attackers cripple a server by sending legitimate-looking requests that consume computational resources (e.g., CPU cycles, disk). With speak-up, a victimized server encourages all clients, resources permitting, to automatically send higher volumes of traffic. We suppose that attackers are already using most of their upload bandwidth so cannot react to the encouragement. Good clients, however, have spare upload bandwidth and will react to the encouragement with drastically higher volumes of traffic. The intended outcome of this traffic inflation is that the good clients crowd out the bad ones, thereby capturing a much larger fraction of the server's resources than before. We experiment under various conditions and find that speak-up causes the server to spend resources on a group of clients in rough proportion to their aggregate upload bandwidth. This result makes the defense viable and effective for a class of real attacks.",,303–314,12,"bandwidth, DoS attack, currency",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1567,inproceedings,"Walfish, Michael and Vutukuru, Mythili and Balakrishnan, Hari and Karger, David and Shenker, Scott",DDoS Defense by Offense,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159948,10.1145/1159913.1159948,"This paper presents the design, implementation, analysis, and experimental evaluation of speak-up, a defense against application-level distributed denial-of-service (DDoS), in which attackers cripple a server by sending legitimate-looking requests that consume computational resources (e.g., CPU cycles, disk). With speak-up, a victimized server encourages all clients, resources permitting, to automatically send higher volumes of traffic. We suppose that attackers are already using most of their upload bandwidth so cannot react to the encouragement. Good clients, however, have spare upload bandwidth and will react to the encouragement with drastically higher volumes of traffic. The intended outcome of this traffic inflation is that the good clients crowd out the bad ones, thereby capturing a much larger fraction of the server's resources than before. We experiment under various conditions and find that speak-up causes the server to spend resources on a group of clients in rough proportion to their aggregate upload bandwidth. This result makes the defense viable and effective for a class of real attacks.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",303–314,12,"currency, bandwidth, DoS attack","Pisa, Italy",SIGCOMM '06,,,,,,
1568,inproceedings,"Zegura, E.",Session Details: Hardware,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3249792,10.1145/3249792,,"Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Pisa, Italy",SIGCOMM '06,,,,,,
1569,article,"Bonomi, Flavio and Mitzenmacher, Michael and Panigrah, Rina and Singh, Sushil and Varghese, George",Beyond Bloom Filters: From Approximate Membership Checks to Approximate State Machines,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159950,10.1145/1151659.1159950,"Many networking applications require fast state lookups in a concurrent state machine,which tracks the state of a large number of flows simultaneously.We consider the question of how to compactly represent such concurrent state machines. To achieve compactness,we consider data structures for Approximate Concurrent State Machines (ACSMs)that can return false positives,false negatives,or a ""don 't know ""response.We describe three techniques based on Bloom filters and hashing,and evaluate them using both theoretical analysis and simulation.Our analysis leads us to an extremely efficient hashing-based scheme with several parameters that can be chosen to trade off space,computation,and the pact of errors.Our hashing approach also yields a simple alternative structure with the same functionality as a counting Bloom filter that uses much less space.We show how ACSMs can be used for video congestion control.Using an ACSM,a router can implement sophisticated Active Queue Management (AQM)techniques for video traffic (without the need for standards changes to mark packets or change video formats),with a factor of four reduction in memory compared to full-state schemes and with very little error.We also show that ACSMs show promise for real-time detection of P2P traffic.",,315–326,12,"bloom filters, network flows, state machines",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1570,inproceedings,"Bonomi, Flavio and Mitzenmacher, Michael and Panigrah, Rina and Singh, Sushil and Varghese, George",Beyond Bloom Filters: From Approximate Membership Checks to Approximate State Machines,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159950,10.1145/1159913.1159950,"Many networking applications require fast state lookups in a concurrent state machine,which tracks the state of a large number of flows simultaneously.We consider the question of how to compactly represent such concurrent state machines. To achieve compactness,we consider data structures for Approximate Concurrent State Machines (ACSMs)that can return false positives,false negatives,or a ""don 't know ""response.We describe three techniques based on Bloom filters and hashing,and evaluate them using both theoretical analysis and simulation.Our analysis leads us to an extremely efficient hashing-based scheme with several parameters that can be chosen to trade off space,computation,and the pact of errors.Our hashing approach also yields a simple alternative structure with the same functionality as a counting Bloom filter that uses much less space.We show how ACSMs can be used for video congestion control.Using an ACSM,a router can implement sophisticated Active Queue Management (AQM)techniques for video traffic (without the need for standards changes to mark packets or change video formats),with a factor of four reduction in memory compared to full-state schemes and with very little error.We also show that ACSMs show promise for real-time detection of P2P traffic.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",315–326,12,"state machines, bloom filters, network flows","Pisa, Italy",SIGCOMM '06,,,,,,
1571,article,"Varghese, George and Fingerhut, J. Andrew and Bonomi, Flavio",Detecting Evasion Attacks at High Speeds without Reassembly,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159951,10.1145/1151659.1159951,"Ptacek and Newsham [14] showed how to evade signature detection at Intrusion Prevention Systems (IPS) using TCP and IP Fragmentation. These attacks are implemented in tools like FragRoute, and are institutionalized in IPS product tests. The classic defense is for the IPS to reassemble TCP and IP packets,and to consistently normalize the output stream. Current IPS standards require keeping state for 1 million connections. Both the state and processing requirements of reassembly and normalization are barriers to scalability for an IPS at speeds higher than 10 Gbps.In this paper, we suggest breaking with this paradigm using an approach we call Split-Detect. We focus on the simplest form of signature, an exact string match, and start by splitting the signature into pieces. By doing so the attacker is either forced to include at least one piece completely in a packet, or to display potentially abnormal behavior (e.g., several small TCP fragments or out-of-order packets) that cause the attacker's flow to be diverted to a slow path. We prove that under certain assumptions this scheme can detect all byte-string evasions. We also show using real traces that the processing and storage requirements of this scheme can be 10% of that required by a conventional IPS, allowing reasonable cost implementations at 20 Gbps. While the changes required by Split-Detect may be a barrier to adoption, this paper exposes the assumptions that must be changed to avoid normalization and reassembly in the fast path.",,327–338,12,"TCP reassembly, intrusion prevention systems, evasion attacks, normalization",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1572,inproceedings,"Varghese, George and Fingerhut, J. Andrew and Bonomi, Flavio",Detecting Evasion Attacks at High Speeds without Reassembly,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159951,10.1145/1159913.1159951,"Ptacek and Newsham [14] showed how to evade signature detection at Intrusion Prevention Systems (IPS) using TCP and IP Fragmentation. These attacks are implemented in tools like FragRoute, and are institutionalized in IPS product tests. The classic defense is for the IPS to reassemble TCP and IP packets,and to consistently normalize the output stream. Current IPS standards require keeping state for 1 million connections. Both the state and processing requirements of reassembly and normalization are barriers to scalability for an IPS at speeds higher than 10 Gbps.In this paper, we suggest breaking with this paradigm using an approach we call Split-Detect. We focus on the simplest form of signature, an exact string match, and start by splitting the signature into pieces. By doing so the attacker is either forced to include at least one piece completely in a packet, or to display potentially abnormal behavior (e.g., several small TCP fragments or out-of-order packets) that cause the attacker's flow to be diverted to a slow path. We prove that under certain assumptions this scheme can detect all byte-string evasions. We also show using real traces that the processing and storage requirements of this scheme can be 10% of that required by a conventional IPS, allowing reasonable cost implementations at 20 Gbps. While the changes required by Split-Detect may be a barrier to adoption, this paper exposes the assumptions that must be changed to avoid normalization and reassembly in the fast path.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",327–338,12,"normalization, evasion attacks, TCP reassembly, intrusion prevention systems","Pisa, Italy",SIGCOMM '06,,,,,,
1573,article,"Kumar, Sailesh and Dharmapurikar, Sarang and Yu, Fang and Crowley, Patrick and Turner, Jonathan",Algorithms to Accelerate Multiple Regular Expressions Matching for Deep Packet Inspection,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159952,10.1145/1151659.1159952,"There is a growing demand for network devices capable of examining the content of data packets in order to improve network security and provide application-specific services. Most high performance systems that perform deep packet inspection implement simple string matching algorithms to match packets against a large, but finite set of strings. owever, there is growing interest in the use of regular expression-based pattern matching, since regular expressions offer superior expressive power and flexibility. Deterministic finite automata (DFA) representations are typically used to implement regular expressions. However, DFA representations of regular expression sets arising in network applications require large amounts of memory, limiting their practical application.In this paper, we introduce a new representation for regular expressions, called the Delayed Input DFA (D2FA), which substantially reduces space equirements as compared to a DFA. A D2FA is constructed by transforming a DFA via incrementally replacing several transitions of the automaton with a single default transition. Our approach dramatically reduces the number of distinct transitions between states. For a collection of regular expressions drawn from current commercial and academic systems, a D2FA representation reduces transitions by more than 95%. Given the substantially reduced space equirements, we describe an efficient architecture that can perform deep packet inspection at multi-gigabit rates. Our architecture uses multiple on-chip memories in such a way that each remains uniformly occupied and accessed over a short duration, thus effectively distributing the load and enabling high throughput. Our architecture can provide ostffective packet content scanning at OC-192 rates with memory requirements that are consistent with current ASIC technology.",,339–350,12,"regular expressions, DFA, deep packet inspection",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1574,inproceedings,"Kumar, Sailesh and Dharmapurikar, Sarang and Yu, Fang and Crowley, Patrick and Turner, Jonathan",Algorithms to Accelerate Multiple Regular Expressions Matching for Deep Packet Inspection,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159952,10.1145/1159913.1159952,"There is a growing demand for network devices capable of examining the content of data packets in order to improve network security and provide application-specific services. Most high performance systems that perform deep packet inspection implement simple string matching algorithms to match packets against a large, but finite set of strings. owever, there is growing interest in the use of regular expression-based pattern matching, since regular expressions offer superior expressive power and flexibility. Deterministic finite automata (DFA) representations are typically used to implement regular expressions. However, DFA representations of regular expression sets arising in network applications require large amounts of memory, limiting their practical application.In this paper, we introduce a new representation for regular expressions, called the Delayed Input DFA (D2FA), which substantially reduces space equirements as compared to a DFA. A D2FA is constructed by transforming a DFA via incrementally replacing several transitions of the automaton with a single default transition. Our approach dramatically reduces the number of distinct transitions between states. For a collection of regular expressions drawn from current commercial and academic systems, a D2FA representation reduces transitions by more than 95%. Given the substantially reduced space equirements, we describe an efficient architecture that can perform deep packet inspection at multi-gigabit rates. Our architecture uses multiple on-chip memories in such a way that each remains uniformly occupied and accessed over a short duration, thus effectively distributing the load and enabling high throughput. Our architecture can provide ostffective packet content scanning at OC-192 rates with memory requirements that are consistent with current ASIC technology.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",339–350,12,"regular expressions, DFA, deep packet inspection","Pisa, Italy",SIGCOMM '06,,,,,,
1575,inproceedings,"Handley, M.",Session Details: Routing 2,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3249793,10.1145/3249793,,"Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Pisa, Italy",SIGCOMM '06,,,,,,
1576,article,"Caesar, Matthew and Castro, Miguel and Nightingale, Edmund B. and O'Shea, Greg and Rowstron, Antony",Virtual Ring Routing: Network Routing Inspired by DHTs,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159954,10.1145/1151659.1159954,"This paper presents Virtual Ring Routing (VRR), a new network routing protocol that occupies a unique point in the design space. VRR is inspired by overlay routing algorithms in Distributed Hash Tables (DHTs) but it does not rely on an underlying network routing protocol. It is implemented directly on top of the link layer. VRR provides both raditional point-to-point network routing and DHT routing to the node responsible for a hash table key.VRR can be used with any link layer technology but this paper describes a design and several implementations of VRR that are tuned for wireless networks. We evaluate the performance of VRR using simulations and measurements from a sensor network and an 802.11a testbed. The experimental results show that VRR provides robust performance across a wide range of environments and workloads. It performs comparably to, or better than, the best wireless routing protocol in each experiment. VRR performs well because of its unique features: it does not require network flooding or trans-lation between fixed identifiers and location-dependent addresses.",,351–362,12,"wireless, network routing, distributed hash table",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1577,inproceedings,"Caesar, Matthew and Castro, Miguel and Nightingale, Edmund B. and O'Shea, Greg and Rowstron, Antony",Virtual Ring Routing: Network Routing Inspired by DHTs,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159954,10.1145/1159913.1159954,"This paper presents Virtual Ring Routing (VRR), a new network routing protocol that occupies a unique point in the design space. VRR is inspired by overlay routing algorithms in Distributed Hash Tables (DHTs) but it does not rely on an underlying network routing protocol. It is implemented directly on top of the link layer. VRR provides both raditional point-to-point network routing and DHT routing to the node responsible for a hash table key.VRR can be used with any link layer technology but this paper describes a design and several implementations of VRR that are tuned for wireless networks. We evaluate the performance of VRR using simulations and measurements from a sensor network and an 802.11a testbed. The experimental results show that VRR provides robust performance across a wide range of environments and workloads. It performs comparably to, or better than, the best wireless routing protocol in each experiment. VRR performs well because of its unique features: it does not require network flooding or trans-lation between fixed identifiers and location-dependent addresses.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",351–362,12,"network routing, distributed hash table, wireless","Pisa, Italy",SIGCOMM '06,,,,,,
1578,article,"Caesar, Matthew and Condie, Tyson and Kannan, Jayanthkumar and Lakshminarayanan, Karthik and Stoica, Ion and Shenker, Scott",ROFL: Routing on Flat Labels,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159955,10.1145/1151659.1159955,"It is accepted wisdom that the current Internet architecture conflates network locations and host identities, but there is no agreement on how a future architecture should distinguish the two. One could sidestep this quandary by routing directly on host identities themselves, and eliminating the need for network-layer protocols to include any mention of network location. The key to achieving this is the ability to route on flat labels. In this paper we take an initial stab at this challenge, proposing and analyzing our ROFL routing algorithm. While its scaling and efficiency properties are far from ideal, our results suggest that the idea of routing on flat labels cannot be immediately dismissed.",,363–374,12,"internet architecture, naming, routing",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1579,inproceedings,"Caesar, Matthew and Condie, Tyson and Kannan, Jayanthkumar and Lakshminarayanan, Karthik and Stoica, Ion and Shenker, Scott",ROFL: Routing on Flat Labels,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159955,10.1145/1159913.1159955,"It is accepted wisdom that the current Internet architecture conflates network locations and host identities, but there is no agreement on how a future architecture should distinguish the two. One could sidestep this quandary by routing directly on host identities themselves, and eliminating the need for network-layer protocols to include any mention of network location. The key to achieving this is the ability to route on flat labels. In this paper we take an initial stab at this challenge, proposing and analyzing our ROFL routing algorithm. While its scaling and efficiency properties are far from ideal, our results suggest that the idea of routing on flat labels cannot be immediately dismissed.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",363–374,12,"internet architecture, naming, routing","Pisa, Italy",SIGCOMM '06,,,,,,
1580,article,"Wang, Feng and Mao, Zhuoqing Morley and Wang, Jia and Gao, Lixin and Bush, Randy",A Measurement Study on the Impact of Routing Events on End-to-End Internet Path Performance,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159956,10.1145/1151659.1159956,"Extensive measurement studies have shown that end-to-end Internet path performance degradation is correlated with routing dynamics. However, the root cause of the correlation between routing dynamics and such performance degradation is poorly understood. In particular, how do routing changes result in degraded end-to-end path performance in the first place? How do factors such as topological properties, routing policies, and iBGP configurations affect the extent to which such routing events can cause performance degradation? Answers to these questions are critical for improving network performance.In this paper, we conduct extensive measurement that involves both controlled routing updates through two tier-1 ISPs and active probes of a diverse set of end-to-end paths on the Internet. We find that routing changes contribute to end-to-end packet loss significantly. Specifically, we study failover events in which a link failure leads to a routing change and recovery events in which a link repair causes a routing change. In both cases, it is possible to experience data plane performance degradation in terms of increased long loss burst as well as forwarding loops. Furthermore, we find that common routing policies and iBGP configurations of ISPs can directly affect the end-to-end path performance during routing changes. Our work provides new insights into potential measures that network operators can undertake to enhance network performance.",,375–386,12,"recovery, BGP, packet reordering, event, active probing, routing dynamics, packet loss, failover event",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1581,inproceedings,"Wang, Feng and Mao, Zhuoqing Morley and Wang, Jia and Gao, Lixin and Bush, Randy",A Measurement Study on the Impact of Routing Events on End-to-End Internet Path Performance,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159956,10.1145/1159913.1159956,"Extensive measurement studies have shown that end-to-end Internet path performance degradation is correlated with routing dynamics. However, the root cause of the correlation between routing dynamics and such performance degradation is poorly understood. In particular, how do routing changes result in degraded end-to-end path performance in the first place? How do factors such as topological properties, routing policies, and iBGP configurations affect the extent to which such routing events can cause performance degradation? Answers to these questions are critical for improving network performance.In this paper, we conduct extensive measurement that involves both controlled routing updates through two tier-1 ISPs and active probes of a diverse set of end-to-end paths on the Internet. We find that routing changes contribute to end-to-end packet loss significantly. Specifically, we study failover events in which a link failure leads to a routing change and recovery events in which a link repair causes a routing change. In both cases, it is possible to experience data plane performance degradation in terms of increased long loss burst as well as forwarding loops. Furthermore, we find that common routing policies and iBGP configurations of ISPs can directly affect the end-to-end path performance during routing changes. Our work provides new insights into potential measures that network operators can undertake to enhance network performance.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",375–386,12,"active probing, BGP, routing dynamics, packet reordering, failover event, packet loss, event, recovery","Pisa, Italy",SIGCOMM '06,,,,,,
1582,article,"Chau, Chi-kin",Policy-Based Routing with Non-Strict Preferences,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159957,10.1145/1151659.1159957,"Traditional studies of routing problems often assumed strict preferences on paths, by eliminating ambiguity in path comparisons, or imposing a priori deterministic tie-breaking. Such an assumption is outpaced by today's common practice of non-deterministic,multi-path routing, which is crucial to traffic engineering, QoS routing, multicasting and virtual private networking. A pair of paths may be incomparable or equally preferred. In the presence of ambiguous preferences at pairs, or even multiple collections of paths, a challenge is to ensure robustness in the complex and sophisticated situations of policy-based routing where heterogeneous routing policies are allowed among routing systems. This paper presents an extensive study of policy-based routing with non-strict preferences, deriving sufficient conditions that ensure the existence, optimality and asynchronous convergence of stable routings.",,387–398,12,"policy-based routing, robustness",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1583,inproceedings,"Chau, Chi-kin",Policy-Based Routing with Non-Strict Preferences,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159957,10.1145/1159913.1159957,"Traditional studies of routing problems often assumed strict preferences on paths, by eliminating ambiguity in path comparisons, or imposing a priori deterministic tie-breaking. Such an assumption is outpaced by today's common practice of non-deterministic,multi-path routing, which is crucial to traffic engineering, QoS routing, multicasting and virtual private networking. A pair of paths may be incomparable or equally preferred. In the presence of ambiguous preferences at pairs, or even multiple collections of paths, a challenge is to ensure robustness in the complex and sophisticated situations of policy-based routing where heterogeneous routing policies are allowed among routing systems. This paper presents an extensive study of policy-based routing with non-strict preferences, deriving sufficient conditions that ensure the existence, optimality and asynchronous convergence of stable routings.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",387–398,12,"robustness, policy-based routing","Pisa, Italy",SIGCOMM '06,,,,,,
1584,inproceedings,"Seshan, S.",Session Details: Applications,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3249794,10.1145/3249794,,"Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Pisa, Italy",SIGCOMM '06,,,,,,
1585,article,"Chen, Kuan-Ta and Huang, Chun-Ying and Huang, Polly and Lei, Chin-Laung",Quantifying Skype User Satisfaction,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159959,10.1145/1151659.1159959,"The success of Skype has inspired a generation of peer-to-peer-based solutions for satisfactory real-time multimedia services over the Internet. However, fundamental questions, such as whether VoIP services like Skype are good enough in terms of user satisfaction,have not been formally addressed. One of the major challenges lies in the lack of an easily accessible and objective index to quantify the degree of user satisfaction.In this work, we propose a model, geared to Skype, but generalizable to other VoIP services, to quantify VoIP user satisfaction based on a rigorous analysis of the call duration from actual Skype traces. The User Satisfaction Index (USI) derived from the model is unique in that 1) it is composed by objective source-and network-level metrics, such as the bit rate, bit rate jitter, and round-trip time, 2) unlike speech quality measures based on voice signals, such as the PESQ model standardized by ITU-T, the metrics are easily accessible and computable for real-time adaptation, and 3) the model development only requires network measurements, i.e., no user surveys or voice signals are necessary. Our model is validated by an independent set of metrics that quantifies the degree of user interaction from the actual traces.",,399–410,12,"human perception, internet measurement, survival analysis, VoIP, quality of service, wavelet denoising",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1586,inproceedings,"Chen, Kuan-Ta and Huang, Chun-Ying and Huang, Polly and Lei, Chin-Laung",Quantifying Skype User Satisfaction,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159959,10.1145/1159913.1159959,"The success of Skype has inspired a generation of peer-to-peer-based solutions for satisfactory real-time multimedia services over the Internet. However, fundamental questions, such as whether VoIP services like Skype are good enough in terms of user satisfaction,have not been formally addressed. One of the major challenges lies in the lack of an easily accessible and objective index to quantify the degree of user satisfaction.In this work, we propose a model, geared to Skype, but generalizable to other VoIP services, to quantify VoIP user satisfaction based on a rigorous analysis of the call duration from actual Skype traces. The User Satisfaction Index (USI) derived from the model is unique in that 1) it is composed by objective source-and network-level metrics, such as the bit rate, bit rate jitter, and round-trip time, 2) unlike speech quality measures based on voice signals, such as the PESQ model standardized by ITU-T, the metrics are easily accessible and computable for real-time adaptation, and 3) the model development only requires network measurements, i.e., no user surveys or voice signals are necessary. Our model is validated by an independent set of metrics that quantifies the degree of user interaction from the actual traces.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",399–410,12,"human perception, quality of service, VoIP, wavelet denoising, internet measurement, survival analysis","Pisa, Italy",SIGCOMM '06,,,,,,
1587,article,"Sung, Yu-Wei and Bishop, Michael and Rao, Sanjay",Enabling Contribution Awareness in an Overlay Broadcasting System,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159960,10.1145/1151659.1159960,"We consider the design of bandwidth-demanding broadcasting applications using overlays in environments characterized by hosts with limited and asymmetric bandwidth, and significant heterogeneity in outgoing bandwidth. Such environments are critical to consider to extend the applicability of overlay multicast to mainstream Internet environments where insufficient bandwidth exists to support all hosts, but have not received adequate attention from the research community. We leverage the multi-tree framework and design heuristics to enable it to consider host contribution and operate in bandwidth-scarce environments. Our extensions seek to simultaneously achieve good utilization of system resources, performance to hosts commensurate to their contributions, and consistent performance. We have implemented the system and conducted an Internet evaluation on Planet-Lab using real traces from previous operational deployments of an overlay broadcasting system. Our results indicate for these traces, our heuristics can improve the performance of high contributors by 10-240% and facilitate equitable bandwidth distribution among hosts with similar contributions.",,411–422,12,"multi-tree, overlay multicast, incentive",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1588,inproceedings,"Sung, Yu-Wei and Bishop, Michael and Rao, Sanjay",Enabling Contribution Awareness in an Overlay Broadcasting System,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159960,10.1145/1159913.1159960,"We consider the design of bandwidth-demanding broadcasting applications using overlays in environments characterized by hosts with limited and asymmetric bandwidth, and significant heterogeneity in outgoing bandwidth. Such environments are critical to consider to extend the applicability of overlay multicast to mainstream Internet environments where insufficient bandwidth exists to support all hosts, but have not received adequate attention from the research community. We leverage the multi-tree framework and design heuristics to enable it to consider host contribution and operate in bandwidth-scarce environments. Our extensions seek to simultaneously achieve good utilization of system resources, performance to hosts commensurate to their contributions, and consistent performance. We have implemented the system and conducted an Internet evaluation on Planet-Lab using real traces from previous operational deployments of an overlay broadcasting system. Our results indicate for these traces, our heuristics can improve the performance of high contributors by 10-240% and facilitate equitable bandwidth distribution among hosts with similar contributions.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",411–422,12,"overlay multicast, incentive, multi-tree","Pisa, Italy",SIGCOMM '06,,,,,,
1589,article,"Gkantsidis, Christos and Karagiannis, Thomas and VojnoviC, Milan",Planet Scale Software Updates,2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159961,10.1145/1151659.1159961,"Fast and effective distribution of software updates (a.k.a. patches) to millions of Internet users has evolved into a critical task over the last years. In this paper, we characterize ""Windows Update"", one of the largest update services in the world, with the aim to draw general guidelines on how to best design and architect a fast and effective planet-scale patch dissemination system. To this end, we analyze an extensive set of data traces collected over the period of a year, consisting of billions of queries from over 300 million computers. Based on empirical observations and analytical results, we identify interesting properties of today's update traffic and user behavior.Building on this analysis, we consider alternative patch delivery strategies such as caching and peer-to-peer and evaluate their performance. We identify key factors that determine the effectiveness of these schemes in reducing the server workload and the network traffic, and in speeding-up the patch delivery. Most of our findings are invariant properties induced by either user behavior or architectural characteristics of today's Internet, and thus apply to the general problem of Internet-wide dissemination of software updates.",,423–434,12,"peer-to-peer, software updates, caching",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1590,inproceedings,"Gkantsidis, Christos and Karagiannis, Thomas and VojnoviC, Milan",Planet Scale Software Updates,2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159961,10.1145/1159913.1159961,"Fast and effective distribution of software updates (a.k.a. patches) to millions of Internet users has evolved into a critical task over the last years. In this paper, we characterize ""Windows Update"", one of the largest update services in the world, with the aim to draw general guidelines on how to best design and architect a fast and effective planet-scale patch dissemination system. To this end, we analyze an extensive set of data traces collected over the period of a year, consisting of billions of queries from over 300 million computers. Based on empirical observations and analytical results, we identify interesting properties of today's update traffic and user behavior.Building on this analysis, we consider alternative patch delivery strategies such as caching and peer-to-peer and evaluate their performance. We identify key factors that determine the effectiveness of these schemes in reducing the server workload and the network traffic, and in speeding-up the patch delivery. Most of our findings are invariant properties induced by either user behavior or architectural characteristics of today's Internet, and thus apply to the general problem of Internet-wide dissemination of software updates.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",423–434,12,"software updates, peer-to-peer, caching","Pisa, Italy",SIGCOMM '06,,,,,,
1591,article,"Su, Ao-Jan and Choffnes, David R. and Kuzmanovic, Aleksandar and Bustamante, Fabi\'{a",Drafting behind Akamai (Travelocity-Based Detouring),2006,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1151659.1159962,10.1145/1151659.1159962,"To enhance web browsing experiences, content distribution networks (CDNs) move web content ""closer"" to clients by caching copies of web objects on thousands of servers worldwide. Additionally, to minimize client download times, such systems perform extensive network and server measurements, and use them to redirect clients to different servers over short time scales. In this paper, we explore techniques for inferring and exploiting network measurements performed by the largest CDN, Akamai; our objective is to locate and utilize quality Internet paths without performing extensive path probing or monitoring.Our contributions are threefold. First, we conduct a broad measurement study of Akamai's CDN. We probe Akamai's network from 140 PlanetLab vantage points for two months. We find that Akamai redirection times, while slightly higher than advertised, are sufficiently low to be useful for network control. Second, we empirically show that Akamai redirections overwhelmingly correlate with network latencies on the paths between clients and the Akamai servers. Finally, we illustrate how large-scale overlay networks can exploit Akamai redirections to identify the best detouring nodes for one-hop source routing. Our research shows that in more than 50% of investigated scenarios, it is better to route through the nodes ""recommended"" by Akamai, than to use the direct paths. Because this is not the case for the rest of the scenarios, we develop lowoverhead pruning algorithms that avoid Akamai-driven paths when they are not beneficial.",,435–446,12,"one-hop, DNS, measurement reuse, CDN, source routing, edge server, Akamai",,,October 2006,36,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1592,inproceedings,"Su, Ao-Jan and Choffnes, David R. and Kuzmanovic, Aleksandar and Bustamante, Fabi\'{a",Drafting behind Akamai (Travelocity-Based Detouring),2006,1595933085,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1159913.1159962,10.1145/1159913.1159962,"To enhance web browsing experiences, content distribution networks (CDNs) move web content ""closer"" to clients by caching copies of web objects on thousands of servers worldwide. Additionally, to minimize client download times, such systems perform extensive network and server measurements, and use them to redirect clients to different servers over short time scales. In this paper, we explore techniques for inferring and exploiting network measurements performed by the largest CDN, Akamai; our objective is to locate and utilize quality Internet paths without performing extensive path probing or monitoring.Our contributions are threefold. First, we conduct a broad measurement study of Akamai's CDN. We probe Akamai's network from 140 PlanetLab vantage points for two months. We find that Akamai redirection times, while slightly higher than advertised, are sufficiently low to be useful for network control. Second, we empirically show that Akamai redirections overwhelmingly correlate with network latencies on the paths between clients and the Akamai servers. Finally, we illustrate how large-scale overlay networks can exploit Akamai redirections to identify the best detouring nodes for one-hop source routing. Our research shows that in more than 50% of investigated scenarios, it is better to route through the nodes ""recommended"" by Akamai, than to use the direct paths. Because this is not the case for the rest of the scenarios, we develop lowoverhead pruning algorithms that avoid Akamai-driven paths when they are not beneficial.","Proceedings of the 2006 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",435–446,12,"one-hop, DNS, Akamai, edge server, source routing, CDN, measurement reuse","Pisa, Italy",SIGCOMM '06,,,,,,
1593,inproceedings,"Wang, Yong and Jain, Sushant and Martonosi, Margaret and Fall, Kevin",Erasure-Coding Based Routing for Opportunistic Networks,2005,1595930264,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1080139.1080140,10.1145/1080139.1080140,"Routing in Delay Tolerant Networks (DTN) with unpredictable node mobility is a challenging problem because disconnections are prevalent and lack of knowledge about network dynamics hinders good decision making. Current approaches are primarily based on redundant transmissions. They have either high overhead due to excessive transmissions or long delays due to the possibility of making wrong choices when forwarding a few redundant copies. In this paper, we propose a novel forwarding algorithm based on the idea of erasure codes. Erasure coding allows use of a large number of relays while maintaining a constant overhead, which results in fewer cases of long delays.We use simulation to compare the routing performance of using erasure codes in DTN with four other categories of forwarding algorithms proposed in the literature. Our simulations are based on a real-world mobility trace collected in a large outdoor wild-life environment. The results show that the erasure-coding based algorithm provides the best worst-case delay performance with a fixed amount of overhead. We also present a simple analytical model to capture the delay characteristics of erasure-coding based forwarding, which provides insights on the potential of our approach.",Proceedings of the 2005 ACM SIGCOMM Workshop on Delay-Tolerant Networking,229–236,8,"delay tolerant network, routing, erasure coding","Philadelphia, Pennsylvania, USA",WDTN '05,,,,,,
1594,inproceedings,"Jones, Evan P. C. and Li, Lily and Ward, Paul A. S.",Practical Routing in Delay-Tolerant Networks,2005,1595930264,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1080139.1080141,10.1145/1080139.1080141,"Delay-tolerant networks (DTNs) have the potential to connect devices and areas of the world that are under-served by current networks. A critical challenge for DTNs is determining routes through the network without ever having an end-to-end connection, or even knowing which ""routers"" will be connected at any given time. Prior approaches have focused either on epidemic message replication or on knowledge of the connectivity schedule. The epidemic approach of replicating messages to all nodes is expensive and does not appear to scale well with increasing load. It can, however, operate without any prior network configuration. The alternatives, by requiring a priori connectivity knowledge, appear infeasible for a self-configuring network.In this paper we present a practical routing protocol that only uses observed information about the network. We designed a metric that estimates how long a message will have to wait before it can be transferred to the next hop. The topology is distributed using a link-state routing protocol, where the link-state packets are ""flooded"" using epidemic routing. The routing is recomputed when connections are established. Messages are exchanged if the topology suggests that a connected node is ""closer"" than the current node.We demonstrate through simulation that our protocol provides performance similar to that of schemes that have global knowledge of the network topology, yet without requiring that knowledge. Further, it requires a significantly smaller quantity of buffer, suggesting that our approach will scale with the number of messages in the network, where replication approaches may not.",Proceedings of the 2005 ACM SIGCOMM Workshop on Delay-Tolerant Networking,237–243,7,"delay tolerant network, route metrics, routing","Philadelphia, Pennsylvania, USA",WDTN '05,,,,,,
1595,inproceedings,"Hui, Pan and Chaintreau, Augustin and Scott, James and Gass, Richard and Crowcroft, Jon and Diot, Christophe",Pocket Switched Networks and Human Mobility in Conference Environments,2005,1595930264,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1080139.1080142,10.1145/1080139.1080142,"Pocket Switched Networks (PSN) make use of both human mobility and local/global connectivity in order to transfer data between mobile users' devices. This falls under the Delay Tolerant Networking (DTN) space, focusing on the use of opportunistic networking. One key problem in PSN is in designing forwarding algorithms which cope with human mobility patterns. We present an experiment measuring forty-one humans' mobility at the Infocom 2005 conference. The results of this experiment are similar to our previous experiments in corporate and academic working environments, in exhibiting a power-law distrbution for the time between node contacts. We then discuss the implications of these results on the design of forwarding algorithms for PSN.",Proceedings of the 2005 ACM SIGCOMM Workshop on Delay-Tolerant Networking,244–251,8,"mobile networking, network measurement, delay-tolerant networking, wireless networking","Philadelphia, Pennsylvania, USA",WDTN '05,,,,,,
1596,inproceedings,"Spyropoulos, Thrasyvoulos and Psounis, Konstantinos and Raghavendra, Cauligi S.",Spray and Wait: An Efficient Routing Scheme for Intermittently Connected Mobile Networks,2005,1595930264,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1080139.1080143,10.1145/1080139.1080143,"Intermittently connected mobile networks are sparse wireless networks where most of the time there does not exist a complete path from the source to the destination. These networks fall into the general category of Delay Tolerant Networks. There are many real networks that follow this paradigm, for example, wildlife tracking sensor networks, military networks, inter-planetary networks, etc. In this context, conventional routing schemes would fail.To deal with such networks researchers have suggested to use flooding-based routing schemes. While flooding-based schemes have a high probability of delivery, they waste a lot of energy and suffer from severe contention, which can significantly degrade their performance. Furthermore, proposed efforts to significantly reduce the overhead of flooding-based schemes have often be plagued by large delays. With this in mind, we introduce a new routing scheme, called Spray and Wait, that ""sprays"" a number of copies into the network, and then ""waits"" till one of these nodes meets the destination.Using theory and simulations we show that Spray and Wait outperforms all existing schemes with respect to both average message delivery delay and number of transmissions per message delivered; its overall performance is close to the optimal scheme. Furthermore, it is highly scalable retaining good performance under a large range of scenarios, unlike other schemes. Finally, it is simple to implement and to optimize in order to achieve given performance goals in practice.",Proceedings of the 2005 ACM SIGCOMM Workshop on Delay-Tolerant Networking,252–259,8,"ad-hoc networks, delay tolerant networks, intermittent connectivity, routing","Philadelphia, Pennsylvania, USA",WDTN '05,,,,,,
1597,inproceedings,"Small, Tara and Haas, Zygmunt J.",Resource and Performance Tradeoffs in Delay-Tolerant Wireless Networks,2005,1595930264,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1080139.1080144,10.1145/1080139.1080144,"Wireless and mobile network technologies often impose severe limitations on the availability of resources, resulting in poor and often unsatisfactory performance of the commonly used wireless networking protocols. For instance, power and memory/storage constraints of miniaturized network nodes reduce the throughput capacity and increase the network latency. Through various approaches and technological advances, researchers attempt to somehow compensate for such hardware limitations. However, this is not always necessary. Sometimes, the required performance of such networks does not need to adhere to the level of services that would be required for performance-critical applications. For example, for some applications of sensor networks, minimal latency is not a critical factor and it could be traded off for a more limited resource, such as energy or throughput. Such networks are termed delay-tolerant networks. Thus, to reduce the energy expenditure, transmission range of such sensor nodes would be quite short, leading to network topologies in which the average number of neighbors of the network nodes is very small. If the sensor nodes are mobile, then most of the time a node has no neighbors; only infrequently another node migrates into its neighborhood. This means that the classical networking approach of store-and-forward would not work well, as there is nearly never an intact path between a source and a destination. Several routing protocols have been proposed for this type of networking environment, one example is the Shared Wireless Infostation Model (SWIM), where a packet propagates through the network by being copied (rather than forwarded) from a node to a node, as links are sporadically created. The goal is that one of the copies of the packet reaches the destination. SWIM is an example of the way that non-critical performance could be traded off for insufficient resources, such as the tradeoffs between energy, delay, storage, capacity, and processing complexity. In this paper, we examine some of these tradeoffs, exposing the ways in which resources could be saved by compromising on the level of performance, as to satisfy the particular limitations of network technologies.",Proceedings of the 2005 ACM SIGCOMM Workshop on Delay-Tolerant Networking,260–267,8,"store and forward, data MULEs, SWIM, sparse networks, sensor networks, carry and forward, shared wireless infostation model, delay-tolerant networks, resource tradeoffs, message ferrying, epidemic routing","Philadelphia, Pennsylvania, USA",WDTN '05,,,,,,
1598,inproceedings,"Zhao, Wenrui and Ammar, Mostafa and Zegura, Ellen",Multicasting in Delay Tolerant Networks: Semantic Models and Routing Algorithms,2005,1595930264,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1080139.1080145,10.1145/1080139.1080145,"Delay tolerant networks (DTNs) are a class of emerging networks that experience frequent and long-duration partitions. These networks have a variety of applications in situations such as crisis environments and deep-space communication. In this paper, we study the problem of multicasting in DTNs. Multicast supports the distribution of data to a group of users, a service needed for many potential DTN applications. While multicasting in the Internet and mobile ad hoc networks has been studied extensively, due to the unique characteristic of frequent partitioning in DTNs, multicasting in DTNs is a considerably different and challenging problem. It not only requires new definitions of multicast semantics but also brings new issues to the design of routing algorithms. In this paper, we propose new semantic models for DTN multicast and develop several multicast routing algorithms with different routing strategies. We present a framework to evaluate these algorithms in DTNs. To the best of our knowledge, this is the first study of multicasting in DTNs. Our objectives are to understand how routing performance is affected by the availability of knowledge about network topology and group membership and to guide the design of DTN routing protocols. Using ns simulations, we find that efficient multicast routing for DTNs can be constructed using only partial knowledge. In addition, accurate topology information is generally more important in routing than up-to-date membership information. We also find that routing algorithms that forward data along multiple paths achieve better delivery ratios, especially when available knowledge is limited.",Proceedings of the 2005 ACM SIGCOMM Workshop on Delay-Tolerant Networking,268–275,8,"multicast, semantic model, delay tolerant networks","Philadelphia, Pennsylvania, USA",WDTN '05,,,,,,
1599,inproceedings,"Leguay, J\'{e",DTN Routing in a Mobility Pattern Space,2005,1595930264,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1080139.1080146,10.1145/1080139.1080146,Routing in delay tolerant networks (DTNs) benefits considerably if one can take advantage of knowledge concerning node mobility. The main contribution of this paper is the definition of a generic routing scheme for DTNs using a high-dimensional Euclidean space constructed upon nodes' mobility patterns. We call this the MobySpace. One way of representing nodes in this space is to give them coordinates that correspond to their probability of being found in each possible location. We present simulation results indicating that such a scheme can be beneficial in a scenario inspired by studies done on real mobility traces. This work should open the way to further use of the virtual space formalism in DTN routing.,Proceedings of the 2005 ACM SIGCOMM Workshop on Delay-Tolerant Networking,276–283,8,"delay tolerant networks, mobility, routing","Philadelphia, Pennsylvania, USA",WDTN '05,,,,,,
1600,inproceedings,"Widmer, J\""{o",Network Coding for Efficient Communication in Extreme Networks,2005,1595930264,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1080139.1080147,10.1145/1080139.1080147,"Some forms of ad-hoc networks need to operate in extremely performance-challenged environments where end-to-end connectivity is rare. Such environments can be found for example in very sparse mobile networks where nodes ""meet"" only occasionally and are able to exchange information, or in wireless sensor networks where nodes sleep most of the time to conserve energy. Forwarding mechanisms in such networks usually resort to some form of intelligent flooding, as for example in probabilistic routing.We propose a communication algorithm that significantly reduces the overhead of probabilistic routing algorithms, making it a suitable building block for a delay-tolerant network architecture. Our forwarding scheme is based on network coding. Nodes do not simply forward packets they overhear but may send out information that is coded over the contents of several packets they received. We show by simulation that this algorithm achieves the reliability and robustness of flooding at a small fraction of the overhead.",Proceedings of the 2005 ACM SIGCOMM Workshop on Delay-Tolerant Networking,284–291,8,"network coding, delay-tolerant network","Philadelphia, Pennsylvania, USA",WDTN '05,,,,,,
1601,inproceedings,"Calvert, Ken",Session Details: Network Geometry and Design,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244271,10.1145/3244271,,"Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Portland, Oregon, USA",SIGCOMM '04,,,,,,
1602,article,"Li, Lun and Alderson, David and Willinger, Walter and Doyle, John",A First-Principles Approach to Understanding the Internet's Router-Level Topology,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015470,10.1145/1030194.1015470,"A detailed understanding of the many facets of the Internet's topological structure is critical for evaluating the performance of networking protocols, for assessing the effectiveness of proposed techniques to protect the network from nefarious intrusions and attacks, or for developing improved designs for resource provisioning. Previous studies of topology have focused on interpreting measurements or on phenomenological descriptions and evaluation of graph-theoretic properties of topology generators. We propose a complementary approach of combining a more subtle use of statistics and graph theory with a first-principles theory of router-level topology that reflects practical constraints and tradeoffs. While there is an inevitable tradeoff between model complexity and fidelity, a challenge is to distill from the seemingly endless list of potentially relevant technological and economic issues the features that are most essential to a solid understanding of the intrinsic fundamentals of network topology. We claim that very simple models that incorporate hard technological constraints on router and link bandwidth and connectivity, together with abstract models of user demand and network performance, can successfully address this challenge and further resolve much of the confusion and controversy that has surrounded topology generation and evaluation.",,3–14,12,"heuristically optimal topology, network topology, topology metrics, degree-based generators",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1603,inproceedings,"Li, Lun and Alderson, David and Willinger, Walter and Doyle, John",A First-Principles Approach to Understanding the Internet's Router-Level Topology,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015470,10.1145/1015467.1015470,"A detailed understanding of the many facets of the Internet's topological structure is critical for evaluating the performance of networking protocols, for assessing the effectiveness of proposed techniques to protect the network from nefarious intrusions and attacks, or for developing improved designs for resource provisioning. Previous studies of topology have focused on interpreting measurements or on phenomenological descriptions and evaluation of graph-theoretic properties of topology generators. We propose a complementary approach of combining a more subtle use of statistics and graph theory with a first-principles theory of router-level topology that reflects practical constraints and tradeoffs. While there is an inevitable tradeoff between model complexity and fidelity, a challenge is to distill from the seemingly endless list of potentially relevant technological and economic issues the features that are most essential to a solid understanding of the intrinsic fundamentals of network topology. We claim that very simple models that incorporate hard technological constraints on router and link bandwidth and connectivity, together with abstract models of user demand and network performance, can successfully address this challenge and further resolve much of the confusion and controversy that has surrounded topology generation and evaluation.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",3–14,12,"network topology, topology metrics, degree-based generators, heuristically optimal topology","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1604,article,"Dabek, Frank and Cox, Russ and Kaashoek, Frans and Morris, Robert",Vivaldi: A Decentralized Network Coordinate System,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015471,10.1145/1030194.1015471,"Large-scale Internet applications can benefit from an ability to predict round-trip times to other hosts without having to contact them first. Explicit measurements are often unattractive because the cost of measurement can outweigh the benefits of exploiting proximity information. Vivaldi is a simple, light-weight algorithm that assigns synthetic coordinates to hosts such that the distance between the coordinates of two hosts accurately predicts the communication latency between the hosts. Vivaldi is fully distributed, requiring no fixed network infrastructure and no distinguished hosts. It is also efficient: a new host can compute good coordinates for itself after collecting latency information from only a few other hosts. Because it requires little com-munication, Vivaldi can piggy-back on the communication patterns of the application using it and scale to a large number of hosts. An evaluation of Vivaldi using a simulated network whose latencies are based on measurements among 1740 Internet hosts shows that a 2-dimensional Euclidean model with height vectors embeds these hosts with low error (the median relative error in round-trip time prediction is 11 percent).",,15–26,12,"internet topology, Vivaldi, network coordinates",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1605,inproceedings,"Dabek, Frank and Cox, Russ and Kaashoek, Frans and Morris, Robert",Vivaldi: A Decentralized Network Coordinate System,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015471,10.1145/1015467.1015471,"Large-scale Internet applications can benefit from an ability to predict round-trip times to other hosts without having to contact them first. Explicit measurements are often unattractive because the cost of measurement can outweigh the benefits of exploiting proximity information. Vivaldi is a simple, light-weight algorithm that assigns synthetic coordinates to hosts such that the distance between the coordinates of two hosts accurately predicts the communication latency between the hosts. Vivaldi is fully distributed, requiring no fixed network infrastructure and no distinguished hosts. It is also efficient: a new host can compute good coordinates for itself after collecting latency information from only a few other hosts. Because it requires little com-munication, Vivaldi can piggy-back on the communication patterns of the application using it and scale to a large number of hosts. An evaluation of Vivaldi using a simulated network whose latencies are based on measurements among 1740 Internet hosts shows that a 2-dimensional Euclidean model with height vectors embeds these hosts with low error (the median relative error in round-trip time prediction is 11 percent).","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",15–26,12,"internet topology, network coordinates, Vivaldi","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1606,article,"Maltz, David A. and Xie, Geoffrey and Zhan, Jibin and Zhang, Hui and Hj\'{a",Routing Design in Operational Networks: A Look from the Inside,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015472,10.1145/1030194.1015472,"In any IP network, routing protocols provide the intelligence that takes a collection of physical links and transforms them into a network that enables packets to travel from one host to another. Though routing design is arguably the single most important design task for large IP networks, there has been very little systematic investigation into how routing protocols are actually used in production networks to implement the goals of network architects. We have developed a methodology for reverse engineering a coherent global view of a network's routing design from the static analysis of dumps of the local configuration state of each router. Starting with a set of 8,035 configuration files, we have applied this method to 31 production networks. In this paper we present a detailed examination of how routing protocols are used in operational networks. In particular, the results show the conventional model of ""interior"" and ""exterior"" gateway protocols is insufficient to describe the diverse set of mechanisms used by architects, and we provide examples of the more unusual designs and examine their trade-offs. We discuss the strengths and weaknesses of our methodology, and argue that it opens paths towards new understandings of network behavior and design.",,27–40,14,"routing design, network modeling, static configuration analysis, reverse engineering",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1607,inproceedings,"Maltz, David A. and Xie, Geoffrey and Zhan, Jibin and Zhang, Hui and Hj\'{a",Routing Design in Operational Networks: A Look from the Inside,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015472,10.1145/1015467.1015472,"In any IP network, routing protocols provide the intelligence that takes a collection of physical links and transforms them into a network that enables packets to travel from one host to another. Though routing design is arguably the single most important design task for large IP networks, there has been very little systematic investigation into how routing protocols are actually used in production networks to implement the goals of network architects. We have developed a methodology for reverse engineering a coherent global view of a network's routing design from the static analysis of dumps of the local configuration state of each router. Starting with a set of 8,035 configuration files, we have applied this method to 31 production networks. In this paper we present a detailed examination of how routing protocols are used in operational networks. In particular, the results show the conventional model of ""interior"" and ""exterior"" gateway protocols is insufficient to describe the diverse set of mechanisms used by architects, and we provide examples of the more unusual designs and examine their trade-offs. We discuss the strengths and weaknesses of our methodology, and argue that it opens paths towards new understandings of network behavior and design.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",27–40,14,"routing design, static configuration analysis, reverse engineering, network modeling","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1608,inproceedings,"Dovrolis, Constantinos",Session Details: Inference of Network Properties,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244272,10.1145/3244272,,"Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Portland, Oregon, USA",SIGCOMM '04,,,,,,
1609,article,"Hu, Ningning and Li, Li (Erran) and Mao, Zhuoqing Morley and Steenkiste, Peter and Wang, Jia","Locating Internet Bottlenecks: Algorithms, Measurements, and Implications",2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015474,10.1145/1030194.1015474,"The ability to locate network bottlenecks along end-to-end paths on the Internet is of great interest to both network operators and researchers. For example, knowing where bottleneck links are, network operators can apply traffic engineering either at the interdomain or intradomain level to improve routing. Existing tools either fail to identify the location of bottlenecks, or generate a large amount of probing packets. In addition, they often require access to both end points. In this paper we present Pathneck, a tool that allows end users to efficiently and accurately locate the bottleneck link on an Internet path. Pathneck is based on a novel probing technique called Recursive Packet Train (RPT) and does not require access to the destination. We evaluate Pathneck using wide area Internet experiments and trace-driven emulation. In addition, we present the results of an extensive study on bottlenecks in the Internet using carefully selected, geographically diverse probing sources and destinations. We found that Pathneck can successfully detect bottlenecks for almost 80% of the Internet paths we probed. We also report our success in using the bottleneck location and bandwidth bounds provided by Pathneck to infer bottlenecks and to avoid bottlenecks in multihoming and overlay routing.",,41–54,14,"active probing, bottleneck location, packet train, available bandwidth",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1610,inproceedings,"Hu, Ningning and Li, Li (Erran) and Mao, Zhuoqing Morley and Steenkiste, Peter and Wang, Jia","Locating Internet Bottlenecks: Algorithms, Measurements, and Implications",2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015474,10.1145/1015467.1015474,"The ability to locate network bottlenecks along end-to-end paths on the Internet is of great interest to both network operators and researchers. For example, knowing where bottleneck links are, network operators can apply traffic engineering either at the interdomain or intradomain level to improve routing. Existing tools either fail to identify the location of bottlenecks, or generate a large amount of probing packets. In addition, they often require access to both end points. In this paper we present Pathneck, a tool that allows end users to efficiently and accurately locate the bottleneck link on an Internet path. Pathneck is based on a novel probing technique called Recursive Packet Train (RPT) and does not require access to the destination. We evaluate Pathneck using wide area Internet experiments and trace-driven emulation. In addition, we present the results of an extensive study on bottlenecks in the Internet using carefully selected, geographically diverse probing sources and destinations. We found that Pathneck can successfully detect bottlenecks for almost 80% of the Internet paths we probed. We also report our success in using the bottleneck location and bandwidth bounds provided by Pathneck to infer bottlenecks and to avoid bottlenecks in multihoming and overlay routing.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",41–54,14,"bottleneck location, available bandwidth, active probing, packet train","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1611,article,"Chen, Yan and Bindel, David and Song, Hanhee and Katz, Randy H.",An Algebraic Approach to Practical and Scalable Overlay Network Monitoring,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015475,10.1145/1030194.1015475,"Overlay network monitoring enables distributed Internet applications to detect and recover from path outages and periods of degraded performance within seconds. For an overlay network with n end hosts, existing systems either require O(n2) measurements, and thus lack scalability, or can only estimate the latency but not congestion or failures. Our earlier extended abstract [1] briefly proposes an algebraic approach that selectively monitors k linearly independent paths that can fully describe all the O(n2) paths. The loss rates and latency of these k paths can be used to estimate the loss rates and latency of all other paths. Our scheme only assumes knowledge of the underlying IP topology, with links dynamically varying between lossy and normal.In this paper, we improve, implement and extensively evaluate such a monitoring system. We further make the following contributions: i) scalability analysis indicating that for reasonably large n (e.g., 100), the growth of k is bounded as O(n log n), ii) efficient adaptation algorithms for topology changes, such as the addition or removal of end hosts and routing changes, iii) measurement load balancing schemes, and iv) topology measurement error handling. Both simulation and Internet experiments demonstrate we obtain highly accurate path loss rate estimation while adapting to topology changes within seconds and handling topology errors.",,55–66,12,"scalability, load balancing, overlay, dynamics, numerical linear algebra, network measurement and monitoring",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1612,inproceedings,"Chen, Yan and Bindel, David and Song, Hanhee and Katz, Randy H.",An Algebraic Approach to Practical and Scalable Overlay Network Monitoring,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015475,10.1145/1015467.1015475,"Overlay network monitoring enables distributed Internet applications to detect and recover from path outages and periods of degraded performance within seconds. For an overlay network with n end hosts, existing systems either require O(n2) measurements, and thus lack scalability, or can only estimate the latency but not congestion or failures. Our earlier extended abstract [1] briefly proposes an algebraic approach that selectively monitors k linearly independent paths that can fully describe all the O(n2) paths. The loss rates and latency of these k paths can be used to estimate the loss rates and latency of all other paths. Our scheme only assumes knowledge of the underlying IP topology, with links dynamically varying between lossy and normal.In this paper, we improve, implement and extensively evaluate such a monitoring system. We further make the following contributions: i) scalability analysis indicating that for reasonably large n (e.g., 100), the growth of k is bounded as O(n log n), ii) efficient adaptation algorithms for topology changes, such as the addition or removal of end hosts and routing changes, iii) measurement load balancing schemes, and iv) topology measurement error handling. Both simulation and Internet experiments demonstrate we obtain highly accurate path loss rate estimation while adapting to topology changes within seconds and handling topology errors.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",55–66,12,"network measurement and monitoring, scalability, load balancing, dynamics, overlay, numerical linear algebra","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1613,article,"Kapoor, Rohit and Chen, Ling-Jyh and Lao, Li and Gerla, Mario and Sanadidi, M. Y.",CapProbe: A Simple and Accurate Capacity Estimation Technique,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015476,10.1145/1030194.1015476,"We present a new capacity estimation technique, called CapProbe. CapProbe combines delay as well as dispersion measurements of packet pairs to filter out samples distorted by cross-traffic. CapProbe algorithms include convergence tests and convergence speed-up techniques by varying probing parameters. Our study of CapProbe includes a probability analysis to determine the time it takes CapProbe to converge on the average. Through simulations and measurements, we found CapProbe to be quick and accurate across a wide range of traffic scenarios. We also compared CapProbe with two previous well-known techniques, pathchar and pathrate. We found CapProbe to be much more accurate than pathchar and similar in accuracy to pathrate, while providing faster estimation than both. Another advantage of CapProbe is its lower computation cost, since no statistical post processing of probing data is required.",,67–78,12,"network capacity, packet pair dispersion, bottleneck bandwidth",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1614,inproceedings,"Kapoor, Rohit and Chen, Ling-Jyh and Lao, Li and Gerla, Mario and Sanadidi, M. Y.",CapProbe: A Simple and Accurate Capacity Estimation Technique,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015476,10.1145/1015467.1015476,"We present a new capacity estimation technique, called CapProbe. CapProbe combines delay as well as dispersion measurements of packet pairs to filter out samples distorted by cross-traffic. CapProbe algorithms include convergence tests and convergence speed-up techniques by varying probing parameters. Our study of CapProbe includes a probability analysis to determine the time it takes CapProbe to converge on the average. Through simulations and measurements, we found CapProbe to be quick and accurate across a wide range of traffic scenarios. We also compared CapProbe with two previous well-known techniques, pathchar and pathrate. We found CapProbe to be much more accurate than pathchar and similar in accuracy to pathrate, while providing faster estimation than both. Another advantage of CapProbe is its lower computation cost, since no statistical post processing of probing data is required.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",67–78,12,"bottleneck bandwidth, packet pair dispersion, network capacity","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1615,inproceedings,"Byers, John",Session Details: Multihoming and Overlays,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244273,10.1145/3244273,,"Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Portland, Oregon, USA",SIGCOMM '04,,,,,,
1616,article,"Goldenberg, David K. and Qiuy, Lili and Xie, Haiyong and Yang, Yang Richard and Zhang, Yin",Optimizing Cost and Performance for Multihoming,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015478,10.1145/1030194.1015478,"Multihoming is often used by large enterprises and stub ISPs to connect to the Internet. In this paper, we design a series of novel smart routing algorithms to optimize cost and performance for multihomed users. We evaluate our algorithms through both analysis and extensive simulations based on realistic charging models, traffic demands, performance data, and network topologies. Our results suggest that these algorithms are very effective in minimizing cost and at the same time improving performance. We further examine the equilibrium performance of smart routing in a global setting and show that a smart routing user can improve its performance without adversely affecting other users.",,79–92,14,"algorithms, multihoming, optimization, smart routing",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1617,inproceedings,"Goldenberg, David K. and Qiuy, Lili and Xie, Haiyong and Yang, Yang Richard and Zhang, Yin",Optimizing Cost and Performance for Multihoming,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015478,10.1145/1015467.1015478,"Multihoming is often used by large enterprises and stub ISPs to connect to the Internet. In this paper, we design a series of novel smart routing algorithms to optimize cost and performance for multihomed users. We evaluate our algorithms through both analysis and extensive simulations based on realistic charging models, traffic demands, performance data, and network topologies. Our results suggest that these algorithms are very effective in minimizing cost and at the same time improving performance. We further examine the equilibrium performance of smart routing in a global setting and show that a smart routing user can improve its performance without adversely affecting other users.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",79–92,14,"algorithms, multihoming, optimization, smart routing","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1618,article,"Akella, Aditya and Pang, Jeffrey and Maggs, Bruce and Seshan, Srinivasan and Shaikh, Anees",A Comparison of Overlay Routing and Multihoming Route Control,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015479,10.1145/1030194.1015479,"The limitations of BGP routing in the Internet are often blamed for poor end-to-end performance and prolonged connectivity interruptions. Recent work advocates using overlays to effectively bypass BGP's path selection in order to improve performance and fault tolerance. In this paper, we explore the possibility that intelligent control of BGP routes, coupled with ISP multihoming, can provide competitive end-to-end performance and reliability. Using extensive measurements of paths between nodes in a large content distribution network, we compare the relative benefits of overlay routing and multihoming route control in terms of round-trip latency, TCP connection throughput, and path availability. We observe that the performance achieved by route control together with multihoming to three ISPs (3-multihoming), is within 5-15% of overlay routing employed in conjunction 3-multihoming, in terms of both end-to-end RTT and throughput. We also show that while multihoming cannot offer the nearly perfect resilience of overlays, it can eliminate almost all failures experienced by a singly-homed end-network. Our results demonstrate that, by leveraging the capability of multihoming route control, it is not necessary to circumvent BGP routing to extract good wide-area performance and availability from the existing routing system.",,93–106,14,"multihoming, overlay routing, route control",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1619,inproceedings,"Akella, Aditya and Pang, Jeffrey and Maggs, Bruce and Seshan, Srinivasan and Shaikh, Anees",A Comparison of Overlay Routing and Multihoming Route Control,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015479,10.1145/1015467.1015479,"The limitations of BGP routing in the Internet are often blamed for poor end-to-end performance and prolonged connectivity interruptions. Recent work advocates using overlays to effectively bypass BGP's path selection in order to improve performance and fault tolerance. In this paper, we explore the possibility that intelligent control of BGP routes, coupled with ISP multihoming, can provide competitive end-to-end performance and reliability. Using extensive measurements of paths between nodes in a large content distribution network, we compare the relative benefits of overlay routing and multihoming route control in terms of round-trip latency, TCP connection throughput, and path availability. We observe that the performance achieved by route control together with multihoming to three ISPs (3-multihoming), is within 5-15% of overlay routing employed in conjunction 3-multihoming, in terms of both end-to-end RTT and throughput. We also show that while multihoming cannot offer the nearly perfect resilience of overlays, it can eliminate almost all failures experienced by a singly-homed end-network. Our results demonstrate that, by leveraging the capability of multihoming route control, it is not necessary to circumvent BGP routing to extract good wide-area performance and availability from the existing routing system.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",93–106,14,"multihoming, overlay routing, route control","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1620,article,"Sripanidkulchai, Kunwadee and Ganjam, Aditya and Maggs, Bruce and Zhang, Hui",The Feasibility of Supporting Large-Scale Live Streaming Applications with Dynamic Application End-Points,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015480,10.1145/1030194.1015480,"While application end-point architectures have proven to be viable solutions for large-scale distributed applications such as distributed computing and file-sharing, there is little known about its feasibility for more bandwidth-demanding applications such as live streaming. Heterogeneity in bandwidth resources and dynamic group membership, inherent properties of application end-points, may adversely affect the construction of a usable and efficient overlay. At large scales, the problems become even more challenging. In this paper, we study one of the most prominent architectural issues in overlay multicast: the feasibility of supporting large-scale groups using an application end-point architecture. We look at three key requirements for feasibility: (i) are there enough resources to construct an overlay, (ii) can a stable and connected overlay be maintained in the presence of group dynamics, and (iii) can an efficient overlay be constructed? Using traces from a large content delivery network, we characterize the behavior of users watching live audio and video streams. We show that in many common real-world scenarios, all three requirements are satisfied. In addition, we evaluate the performance of several design alternatives and show that simple algorithms have the potential to meet these requirements in practice. Overall, our results argue for the feasibility of supporting large-scale live streaming using an application end-point architecture.",,107–120,14,"overlay multicast, live streaming, peer-to-peer, application-level multicast",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1621,inproceedings,"Sripanidkulchai, Kunwadee and Ganjam, Aditya and Maggs, Bruce and Zhang, Hui",The Feasibility of Supporting Large-Scale Live Streaming Applications with Dynamic Application End-Points,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015480,10.1145/1015467.1015480,"While application end-point architectures have proven to be viable solutions for large-scale distributed applications such as distributed computing and file-sharing, there is little known about its feasibility for more bandwidth-demanding applications such as live streaming. Heterogeneity in bandwidth resources and dynamic group membership, inherent properties of application end-points, may adversely affect the construction of a usable and efficient overlay. At large scales, the problems become even more challenging. In this paper, we study one of the most prominent architectural issues in overlay multicast: the feasibility of supporting large-scale groups using an application end-point architecture. We look at three key requirements for feasibility: (i) are there enough resources to construct an overlay, (ii) can a stable and connected overlay be maintained in the presence of group dynamics, and (iii) can an efficient overlay be constructed? Using traces from a large content delivery network, we characterize the behavior of users watching live audio and video streams. We show that in many common real-world scenarios, all three requirements are satisfied. In addition, we evaluate the performance of several design alternatives and show that simple algorithms have the potential to meet these requirements in practice. Overall, our results argue for the feasibility of supporting large-scale live streaming using an application end-point architecture.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",107–120,14,"application-level multicast, overlay multicast, peer-to-peer, live streaming","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1622,inproceedings,"Padmanabhan, Venkat",Session Details: Wireless and Delay-Tolerant Networks,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244274,10.1145/3244274,,"Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Portland, Oregon, USA",SIGCOMM '04,,,,,,
1623,article,"Aguayo, Daniel and Bicket, John and Biswas, Sanjit and Judd, Glenn and Morris, Robert",Link-Level Measurements from an 802.11b Mesh Network,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015482,10.1145/1030194.1015482,"This paper analyzes the causes of packet loss in a 38-node urban multi-hop 802.11b network. The patterns and causes of loss are important in the design of routing and error-correction protocols, as well as in network planning.The paper makes the following observations. The distribution of inter-node loss rates is relatively uniform over the whole range of loss rates; there is no clear threshold separating ""in range"" and ""out of range."" Most links have relatively stable loss rates from one second to the next, though a small minority have very bursty losses at that time scale. Signal-to-noise ratio and distance have little predictive value for loss rate. The large number of links with intermediate loss rates is probably due to multi-path fading rather than attenuation or interference.The phenomena discussed here are all well-known. The contributions of this paper are an understanding of their relative importance, of how they interact, and of the implications for MAC and routing protocol design.",,121–132,12,"wireless, mesh, 802.11b",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1624,inproceedings,"Aguayo, Daniel and Bicket, John and Biswas, Sanjit and Judd, Glenn and Morris, Robert",Link-Level Measurements from an 802.11b Mesh Network,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015482,10.1145/1015467.1015482,"This paper analyzes the causes of packet loss in a 38-node urban multi-hop 802.11b network. The patterns and causes of loss are important in the design of routing and error-correction protocols, as well as in network planning.The paper makes the following observations. The distribution of inter-node loss rates is relatively uniform over the whole range of loss rates; there is no clear threshold separating ""in range"" and ""out of range."" Most links have relatively stable loss rates from one second to the next, though a small minority have very bursty losses at that time scale. Signal-to-noise ratio and distance have little predictive value for loss rate. The large number of links with intermediate loss rates is probably due to multi-path fading rather than attenuation or interference.The phenomena discussed here are all well-known. The contributions of this paper are an understanding of their relative importance, of how they interact, and of the implications for MAC and routing protocol design.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",121–132,12,"wireless, 802.11b, mesh","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1625,article,"Draves, Richard and Padhye, Jitendra and Zill, Brian",Comparison of Routing Metrics for Static Multi-Hop Wireless Networks,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015483,10.1145/1030194.1015483,"Routing protocols for wireless ad hoc networks have traditionally focused on finding paths with minimum hop count. However, such paths can include slow or lossy links, leading to poor throughput. A routing algorithm can select better paths by explicitly taking the quality of the wireless links into account. In this paper, we conduct a detailed, empirical evaluation of the performance of three link-quality metrics---ETX, per-hop RTT, and per-hop packet pair---and compare them against minimum hop count. We study these metrics using a DSR-based routing protocol running in a wireless testbed. We find that the ETX metric has the best performance when all nodes are stationary. We also find that the per-hop RTT and per-hop packet-pair metrics perform poorly due to self-interference. Interestingly, the hop-count metric outperforms all of the link-quality metrics in a scenario where the sender is mobile.",,133–144,12,"wireless multi-hop networks, routing",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1626,inproceedings,"Draves, Richard and Padhye, Jitendra and Zill, Brian",Comparison of Routing Metrics for Static Multi-Hop Wireless Networks,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015483,10.1145/1015467.1015483,"Routing protocols for wireless ad hoc networks have traditionally focused on finding paths with minimum hop count. However, such paths can include slow or lossy links, leading to poor throughput. A routing algorithm can select better paths by explicitly taking the quality of the wireless links into account. In this paper, we conduct a detailed, empirical evaluation of the performance of three link-quality metrics---ETX, per-hop RTT, and per-hop packet pair---and compare them against minimum hop count. We study these metrics using a DSR-based routing protocol running in a wireless testbed. We find that the ETX metric has the best performance when all nodes are stationary. We also find that the per-hop RTT and per-hop packet-pair metrics perform poorly due to self-interference. Interestingly, the hop-count metric outperforms all of the link-quality metrics in a scenario where the sender is mobile.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",133–144,12,"routing, wireless multi-hop networks","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1627,article,"Jain, Sushant and Fall, Kevin and Patra, Rabin",Routing in a Delay Tolerant Network,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015484,10.1145/1030194.1015484,"We formulate the delay-tolerant networking routing problem, where messages are to be moved end-to-end across a connectivity graph that is time-varying but whose dynamics may be known in advance. The problem has the added constraints of finite buffers at each node and the general property that no contemporaneous end-to-end path may ever exist. This situation limits the applicability of traditional routing approaches that tend to treat outages as failures and seek to find an existing end-to-end path. We propose a framework for evaluating routing algorithms in such environments. We then develop several algorithms and use simulations to compare their performance with respect to the amount of knowledge they require about network topology. We find that, as expected, the algorithms using the least knowledge tend to perform poorly. We also find that with limited additional knowledge, far less than complete global knowledge, efficient algorithms can be constructed for routing in such environments. To the best of our knowledge this is the first such investigation of routing issues in DTNs.",,145–158,14,"delay tolerant network, routing",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1628,inproceedings,"Jain, Sushant and Fall, Kevin and Patra, Rabin",Routing in a Delay Tolerant Network,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015484,10.1145/1015467.1015484,"We formulate the delay-tolerant networking routing problem, where messages are to be moved end-to-end across a connectivity graph that is time-varying but whose dynamics may be known in advance. The problem has the added constraints of finite buffers at each node and the general property that no contemporaneous end-to-end path may ever exist. This situation limits the applicability of traditional routing approaches that tend to treat outages as failures and seek to find an existing end-to-end path. We propose a framework for evaluating routing algorithms in such environments. We then develop several algorithms and use simulations to compare their performance with respect to the amount of knowledge they require about network topology. We find that, as expected, the algorithms using the least knowledge tend to perform poorly. We also find that with limited additional knowledge, far less than complete global knowledge, efficient algorithms can be constructed for routing in such environments. To the best of our knowledge this is the first such investigation of routing issues in DTNs.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",145–158,14,"delay tolerant network, routing","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1629,article,"Wang, Randolph Y. and Sobti, Sumeet and Garg, Nitin and Ziskind, Elisha and Lai, Junwen and Krishnamurthy, Arvind",Turning the Postal System into a Generic Digital Communication Mechanism,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015485,10.1145/1030194.1015485,"The phenomenon that rural residents and people with low incomes lag behind in Internet access is known as the ""digital divide."" This problem is particularly acute in developing countries, where most of the world's population lives. Bridging this digital divide, especially by attempting to increase the accessibility of broadband connectivity, can be challenging. The improvement of wide-area connectivity is constrained by factors such as how quickly we can dig ditches to bury fibers in the ground; and the cost of furnishing ""last-mile"" wiring can be prohibitively high.In this paper, we explore the use of digital storage media transported by the postal system as a general digital communication mechanism. While some companies have used the postal system to deliver software and movies, none of them has turned the postal system into a truly generic digital communication medium supporting a wide variety of applications. We call such a generic system a Postmanet. Compared to traditional wide-area connectivity options, the Postmanet has several important advantages, including wide global reach, great bandwidth potential and low cost.Manually preparing mobile storage devices for shipment may appear deceptively simple, but with many applications, communicating parties and messages, manual management becomes infeasible, and systems support at several levels becomes necessary. We explore the simultaneous exploitation of the Internet and the Postmanet, so we can combine their latency and bandwidth advantages to enable sophisticated bandwidth-intensive applications.",,159–166,8,"network architecture, storage devices, postal network",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1630,inproceedings,"Wang, Randolph Y. and Sobti, Sumeet and Garg, Nitin and Ziskind, Elisha and Lai, Junwen and Krishnamurthy, Arvind",Turning the Postal System into a Generic Digital Communication Mechanism,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015485,10.1145/1015467.1015485,"The phenomenon that rural residents and people with low incomes lag behind in Internet access is known as the ""digital divide."" This problem is particularly acute in developing countries, where most of the world's population lives. Bridging this digital divide, especially by attempting to increase the accessibility of broadband connectivity, can be challenging. The improvement of wide-area connectivity is constrained by factors such as how quickly we can dig ditches to bury fibers in the ground; and the cost of furnishing ""last-mile"" wiring can be prohibitively high.In this paper, we explore the use of digital storage media transported by the postal system as a general digital communication mechanism. While some companies have used the postal system to deliver software and movies, none of them has turned the postal system into a truly generic digital communication medium supporting a wide variety of applications. We call such a generic system a Postmanet. Compared to traditional wide-area connectivity options, the Postmanet has several important advantages, including wide global reach, great bandwidth potential and low cost.Manually preparing mobile storage devices for shipment may appear deceptively simple, but with many applications, communicating parties and messages, manual management becomes infeasible, and systems support at several levels becomes necessary. We explore the simultaneous exploitation of the Internet and the Postmanet, so we can combine their latency and bandwidth advantages to enable sophisticated bandwidth-intensive applications.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",159–166,8,"storage devices, postal network, network architecture","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1631,inproceedings,"Barford, Paul",Session Details: Secure Networks,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244275,10.1145/3244275,,"Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Portland, Oregon, USA",SIGCOMM '04,,,,,,
1632,article,"Raghavan, Barath and Snoeren, Alex C.",A System for Authenticated Policy-Compliant Routing,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015487,10.1145/1030194.1015487,"Internet end users and ISPs alike have little control over how packets are routed outside of their own AS, restricting their ability to achieve levels of performance, reliability, and utility that might otherwise be attained. While researchers have proposed a number of source-routing techniques to combat this limitation, there has thus far been no way for independent ASes to ensure that such traffic does not circumvent local traffic policies, nor to accurately determine the correct party to charge for forwarding the traffic.We present Platypus, an authenticated source routing system built around the concept of network capabilities. Network capabilities allow for accountable, fine-grained path selection by cryptographically attesting to policy compliance at each hop along a source route. Capabilities can be composed to construct routes through multiple ASes and can be delegated to third parties. Platypus caters to the needs of both end users and ISPs: users gain the ability to pool their resources and select routes other than the default, while ISPs maintain control over where, when, and whose packets traverse their networks. We describe how Platypus can be used to address several well-known issues in wide-area routing at both the edge and the core, and evaluate its performance, security, and interactions with existing protocols. Our results show that incremental deployment of Platypus can achieve immediate gains.",,167–178,12,"source routing, capabilities, overlay networks, authentication",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1633,inproceedings,"Raghavan, Barath and Snoeren, Alex C.",A System for Authenticated Policy-Compliant Routing,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015487,10.1145/1015467.1015487,"Internet end users and ISPs alike have little control over how packets are routed outside of their own AS, restricting their ability to achieve levels of performance, reliability, and utility that might otherwise be attained. While researchers have proposed a number of source-routing techniques to combat this limitation, there has thus far been no way for independent ASes to ensure that such traffic does not circumvent local traffic policies, nor to accurately determine the correct party to charge for forwarding the traffic.We present Platypus, an authenticated source routing system built around the concept of network capabilities. Network capabilities allow for accountable, fine-grained path selection by cryptographically attesting to policy compliance at each hop along a source route. Capabilities can be composed to construct routes through multiple ASes and can be delegated to third parties. Platypus caters to the needs of both end users and ISPs: users gain the ability to pool their resources and select routes other than the default, while ISPs maintain control over where, when, and whose packets traverse their networks. We describe how Platypus can be used to address several well-known issues in wide-area routing at both the edge and the core, and evaluate its performance, security, and interactions with existing protocols. Our results show that incremental deployment of Platypus can achieve immediate gains.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",167–178,12,"source routing, overlay networks, authentication, capabilities","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1634,article,"Hu, Yih-Chun and Perrig, Adrian and Sirbu, Marvin",SPV: Secure Path Vector Routing for Securing BGP,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015488,10.1145/1030194.1015488,"As our economy and critical infrastructure increasingly relies on the Internet, the insecurity of the underlying border gateway routing protocol (BGP) stands out as the Achilles heel. Recent misconfigurations and attacks have demonstrated the brittleness of BGP. Securing BGP has become a priority.In this paper, we focus on a viable deployment path to secure BGP. We analyze security requirements, and consider tradeoffs of mechanisms that achieve the requirements. In particular, we study how to secure BGP update messages against attacks. We design an efficient cryptographic mechanism that relies only on symmetric cryptographic primitives to guard an ASPATH from alteration, and propose the Secure Path Vector (SPV) protocol. In contrast to the previously proposed S-BGP protocol, SPV is around 22 times faster. With the current effort to secure BGP, we anticipate that SPV will contribute several alternative mechanisms to secure BGP, especially for the case of incremental deployments.",,179–192,14,"Border Gateway Protocol, interdomain routing, BGP, security, routing",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1635,inproceedings,"Hu, Yih-Chun and Perrig, Adrian and Sirbu, Marvin",SPV: Secure Path Vector Routing for Securing BGP,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015488,10.1145/1015467.1015488,"As our economy and critical infrastructure increasingly relies on the Internet, the insecurity of the underlying border gateway routing protocol (BGP) stands out as the Achilles heel. Recent misconfigurations and attacks have demonstrated the brittleness of BGP. Securing BGP has become a priority.In this paper, we focus on a viable deployment path to secure BGP. We analyze security requirements, and consider tradeoffs of mechanisms that achieve the requirements. In particular, we study how to secure BGP update messages against attacks. We design an efficient cryptographic mechanism that relies only on symmetric cryptographic primitives to guard an ASPATH from alteration, and propose the Secure Path Vector (SPV) protocol. In contrast to the previously proposed S-BGP protocol, SPV is around 22 times faster. With the current effort to secure BGP, we anticipate that SPV will contribute several alternative mechanisms to secure BGP, especially for the case of incremental deployments.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",179–192,14,"routing, interdomain routing, BGP, security, Border Gateway Protocol","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1636,article,"Wang, Helen J. and Guo, Chuanxiong and Simon, Daniel R. and Zugenmaier, Alf",Shield: Vulnerability-Driven Network Filters for Preventing Known Vulnerability Exploits,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015489,10.1145/1030194.1015489,"Software patching has not been effective as a first-line defense against large-scale worm attacks, even when patches have long been available for their corresponding vulnerabilities. Generally, people have been reluctant to patch their systems immediately, because patches are perceived to be unreliable and disruptive to apply. To address this problem, we propose a first-line worm defense in the network stack, using shields -- vulnerability-specific, exploit-generic network filters installed in end systems once a vulnerability is discovered, but before a patch is applied. These filters examine the incoming or outgoing traffic of vulnerable applications, and correct traffic that exploits vulnerabilities. Shields are less disruptive to install and uninstall, easier to test for bad side effects, and hence more reliable than traditional software patches. Further, shields are resilient to polymorphic or metamorphic variations of exploits [43].In this paper, we show that this concept is feasible by describing a prototype Shield framework implementation that filters traffic above the transport layer. We have designed a safe and restrictive language to describe vulnerabilities as partial state machines of the vulnerable application. The expressiveness of the language has been verified by encoding the signatures of several known vulnerabilites. Our evaluation provides evidence of Shield's low false positive rate and small impact on application throughput. An examination of a sample set of known vulnerabilities suggests that Shield could be used to prevent exploitation of a substantial fraction of the most dangerous ones.",,193–204,12,"generic protocol analyzer, patching, worm defense, network filter, vulnerability signature",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1637,inproceedings,"Wang, Helen J. and Guo, Chuanxiong and Simon, Daniel R. and Zugenmaier, Alf",Shield: Vulnerability-Driven Network Filters for Preventing Known Vulnerability Exploits,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015489,10.1145/1015467.1015489,"Software patching has not been effective as a first-line defense against large-scale worm attacks, even when patches have long been available for their corresponding vulnerabilities. Generally, people have been reluctant to patch their systems immediately, because patches are perceived to be unreliable and disruptive to apply. To address this problem, we propose a first-line worm defense in the network stack, using shields -- vulnerability-specific, exploit-generic network filters installed in end systems once a vulnerability is discovered, but before a patch is applied. These filters examine the incoming or outgoing traffic of vulnerable applications, and correct traffic that exploits vulnerabilities. Shields are less disruptive to install and uninstall, easier to test for bad side effects, and hence more reliable than traditional software patches. Further, shields are resilient to polymorphic or metamorphic variations of exploits [43].In this paper, we show that this concept is feasible by describing a prototype Shield framework implementation that filters traffic above the transport layer. We have designed a safe and restrictive language to describe vulnerabilities as partial state machines of the vulnerable application. The expressiveness of the language has been verified by encoding the signatures of several known vulnerabilites. Our evaluation provides evidence of Shield's low false positive rate and small impact on application throughput. An examination of a sample set of known vulnerabilities suggests that Shield could be used to prevent exploitation of a substantial fraction of the most dangerous ones.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",193–204,12,"patching, vulnerability signature, generic protocol analyzer, worm defense, network filter","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1638,inproceedings,"Taft, Nina",Session Details: Network Troubleshooting,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244276,10.1145/3244276,,"Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Portland, Oregon, USA",SIGCOMM '04,,,,,,
1639,article,"Feldmann, Anja and Maennel, Olaf and Mao, Z. Morley and Berger, Arthur and Maggs, Bruce",Locating Internet Routing Instabilities,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015491,10.1145/1030194.1015491,"This paper presents a methodology for identifying the autonomous system (or systems) responsible when a routing change is observed and propagated by BGP. The origin of such a routing instability is deduced by examining and correlating BGP updates for many prefixes gathered at many observation points. Although interpreting BGP updates can be perplexing, we find that we can pinpoint the origin to either a single AS or a session between two ASes in most cases. We verify our methodology in two phases. First, we perform simulations on an AS topology derived from actual BGP updates using routing policies that are compatible with inferred peering/customer/provider relationships. In these simulations, in which network and router behavior are ""ideal"", we inject inter-AS link failures and demonstrate that our methodology can effectively identify most origins of instability. We then develop several heuristics to cope with the limitations of the actual BGP update propagation process and monitoring infrastructure, and apply our methodology and evaluation techniques to actual BGP updates gathered at hundreds of observation points. This approach of relying on data from BGP simulations as well as from measurements enables us to evaluate the inference quality achieved by our approach under ideal situations and how it is correlated with the actual quality and the number of observation points.",,205–218,14,"root cause analysis, simulation, analysis, routing instability, instability origin, BGP",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1640,inproceedings,"Feldmann, Anja and Maennel, Olaf and Mao, Z. Morley and Berger, Arthur and Maggs, Bruce",Locating Internet Routing Instabilities,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015491,10.1145/1015467.1015491,"This paper presents a methodology for identifying the autonomous system (or systems) responsible when a routing change is observed and propagated by BGP. The origin of such a routing instability is deduced by examining and correlating BGP updates for many prefixes gathered at many observation points. Although interpreting BGP updates can be perplexing, we find that we can pinpoint the origin to either a single AS or a session between two ASes in most cases. We verify our methodology in two phases. First, we perform simulations on an AS topology derived from actual BGP updates using routing policies that are compatible with inferred peering/customer/provider relationships. In these simulations, in which network and router behavior are ""ideal"", we inject inter-AS link failures and demonstrate that our methodology can effectively identify most origins of instability. We then develop several heuristics to cope with the limitations of the actual BGP update propagation process and monitoring infrastructure, and apply our methodology and evaluation techniques to actual BGP updates gathered at hundreds of observation points. This approach of relying on data from BGP simulations as well as from measurements enables us to evaluate the inference quality achieved by our approach under ideal situations and how it is correlated with the actual quality and the number of observation points.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",205–218,14,"analysis, root cause analysis, BGP, simulation, instability origin, routing instability","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1641,article,"Lakhina, Anukool and Crovella, Mark and Diot, Christophe",Diagnosing Network-Wide Traffic Anomalies,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015492,10.1145/1030194.1015492,"Anomalies are unusual and significant changes in a network's traffic levels, which can often span multiple links. Diagnosing anomalies is critical for both network operators and end users. It is a difficult problem because one must extract and interpret anomalous patterns from large amounts of high-dimensional, noisy data.In this paper we propose a general method to diagnose anomalies. This method is based on a separation of the high-dimensional space occupied by a set of network traffic measurements into disjoint subspaces corresponding to normal and anomalous network conditions. We show that this separation can be performed effectively by Principal Component Analysis.Using only simple traffic measurements from links, we study volume anomalies and show that the method can: (1) accurately detect when a volume anomaly is occurring; (2) correctly identify the underlying origin-destination (OD) flow which is the source of the anomaly; and (3) accurately estimate the amount of traffic involved in the anomalous OD flow.We evaluate the method's ability to diagnose (i.e., detect, identify, and quantify) both existing and synthetically injected volume anomalies in real traffic from two backbone networks. Our method consistently diagnoses the largest volume anomalies, and does so with a very low false alarm rate.",,219–230,12,"network traffic analysis, anomaly detection",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1642,inproceedings,"Lakhina, Anukool and Crovella, Mark and Diot, Christophe",Diagnosing Network-Wide Traffic Anomalies,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015492,10.1145/1015467.1015492,"Anomalies are unusual and significant changes in a network's traffic levels, which can often span multiple links. Diagnosing anomalies is critical for both network operators and end users. It is a difficult problem because one must extract and interpret anomalous patterns from large amounts of high-dimensional, noisy data.In this paper we propose a general method to diagnose anomalies. This method is based on a separation of the high-dimensional space occupied by a set of network traffic measurements into disjoint subspaces corresponding to normal and anomalous network conditions. We show that this separation can be performed effectively by Principal Component Analysis.Using only simple traffic measurements from links, we study volume anomalies and show that the method can: (1) accurately detect when a volume anomaly is occurring; (2) correctly identify the underlying origin-destination (OD) flow which is the source of the anomaly; and (3) accurately estimate the amount of traffic involved in the anomalous OD flow.We evaluate the method's ability to diagnose (i.e., detect, identify, and quantify) both existing and synthetically injected volume anomalies in real traffic from two backbone networks. Our method consistently diagnoses the largest volume anomalies, and does so with a very low false alarm rate.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",219–230,12,"anomaly detection, network traffic analysis","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1643,article,"Teixeira, Renata and Shaikh, Aman and Griffin, Tim and Voelker, Geoffrey M.",Network Sensitivity to Hot-Potato Disruptions,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015493,10.1145/1030194.1015493,"Hot-potato routing is a mechanism employed when there are multiple (equally good) interdomain routes available for a given destination. In this scenario, the Border Gateway Protocol (BGP) selects the interdomain route associated with the closest egress point based upon intradomain path costs. Consequently, intradomain routing changes can impact interdomain routing and cause abrupt swings of external routes, which we call hot-potato disruptions. Recent work has shown that hot-potato disruptions can have a substantial impact on large ISP backbones and thereby jeopardize the network robustness. As a result, there is a need for guidelines and tools to assist in the design of networks that minimize hot-potato disruptions. However, developing these tools is challenging due to the complex and subtle nature of the interactions between exterior and interior routing. In this paper, we address these challenges using an analytic model of hot-potato routing that incorporates metrics to evaluate network sensitivity to hot-potato disruptions. We then present a methodology for computing these metrics using measurements of real ISP networks. We demonstrate the utility of our model by analyzing the sensitivity of a large AS in a tier~1 ISP network.",,231–244,14,"hot-potato routing, IGP, sensitivity analysis, OSPF, BGP, network robustness",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1644,inproceedings,"Teixeira, Renata and Shaikh, Aman and Griffin, Tim and Voelker, Geoffrey M.",Network Sensitivity to Hot-Potato Disruptions,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015493,10.1145/1015467.1015493,"Hot-potato routing is a mechanism employed when there are multiple (equally good) interdomain routes available for a given destination. In this scenario, the Border Gateway Protocol (BGP) selects the interdomain route associated with the closest egress point based upon intradomain path costs. Consequently, intradomain routing changes can impact interdomain routing and cause abrupt swings of external routes, which we call hot-potato disruptions. Recent work has shown that hot-potato disruptions can have a substantial impact on large ISP backbones and thereby jeopardize the network robustness. As a result, there is a need for guidelines and tools to assist in the design of networks that minimize hot-potato disruptions. However, developing these tools is challenging due to the complex and subtle nature of the interactions between exterior and interior routing. In this paper, we address these challenges using an analytic model of hot-potato routing that incorporates metrics to evaluate network sensitivity to hot-potato disruptions. We then present a methodology for computing these metrics using measurements of real ISP networks. We demonstrate the utility of our model by analyzing the sensitivity of a large AS in a tier~1 ISP network.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",231–244,14,"network robustness, hot-potato routing, BGP, sensitivity analysis, OSPF, IGP","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1645,inproceedings,"Stoica, Ion",Session Details: Router Design,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244277,10.1145/3244277,,"Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Portland, Oregon, USA",SIGCOMM '04,,,,,,
1646,article,"Estan, Cristian and Keys, Ken and Moore, David and Varghese, George",Building a Better NetFlow,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015495,10.1145/1030194.1015495,"Network operators need to determine the composition of the traffic mix on links when looking for dominant applications, users, or estimating traffic matrices. Cisco's NetFlow has evolved into a solution that satisfies this need by reporting flow records that summarize a sample of the traffic traversing the link. But sampled NetFlow has shortcomings that hinder the collection and analysis of traffic data. First, during flooding attacks router memory and network bandwidth consumed by flow records can increase beyond what is available; second, selecting the right static sampling rate is difficult because no single rate gives the right tradeoff of memory use versus accuracy for all traffic mixes; third, the heuristics routers use to decide when a flow is reported are a poor match to most applications that work with time bins; finally, it is impossible to estimate without bias the number of active flows for aggregates with non-TCP traffic.In this paper we propose Adaptive NetFlow, deployable through an update to router software, which addresses many shortcomings of NetFlow by dynamically adapting the sampling rate to achieve robustness without sacrificing accuracy. To enable counting of non-TCP flows, we propose an optional Flow Counting Extension that requires augmenting existing hardware at routers. Both our proposed solutions readily provide descriptions of the traffic of progressively smaller sizes. Transmitting these at progressively higher levels of reliability allows graceful degradation of the accuracy of traffic reports in response to network congestion on the reporting path.",,245–256,12,"data summarization, traffic measurement, network monitoring",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1647,inproceedings,"Estan, Cristian and Keys, Ken and Moore, David and Varghese, George",Building a Better NetFlow,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015495,10.1145/1015467.1015495,"Network operators need to determine the composition of the traffic mix on links when looking for dominant applications, users, or estimating traffic matrices. Cisco's NetFlow has evolved into a solution that satisfies this need by reporting flow records that summarize a sample of the traffic traversing the link. But sampled NetFlow has shortcomings that hinder the collection and analysis of traffic data. First, during flooding attacks router memory and network bandwidth consumed by flow records can increase beyond what is available; second, selecting the right static sampling rate is difficult because no single rate gives the right tradeoff of memory use versus accuracy for all traffic mixes; third, the heuristics routers use to decide when a flow is reported are a poor match to most applications that work with time bins; finally, it is impossible to estimate without bias the number of active flows for aggregates with non-TCP traffic.In this paper we propose Adaptive NetFlow, deployable through an update to router software, which addresses many shortcomings of NetFlow by dynamically adapting the sampling rate to achieve robustness without sacrificing accuracy. To enable counting of non-TCP flows, we propose an optional Flow Counting Extension that requires augmenting existing hardware at routers. Both our proposed solutions readily provide descriptions of the traffic of progressively smaller sizes. Transmitting these at progressively higher levels of reliability allows graceful degradation of the accuracy of traffic reports in response to network congestion on the reporting path.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",245–256,12,"network monitoring, traffic measurement, data summarization","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1648,article,"Pappu, Prashanth and Turner, Jonathan and Wong, Ken",Work-Conserving Distributed Schedulers for Terabit Routers,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015496,10.1145/1030194.1015496,"Buffered multistage interconnection networks offer one of the most scalable and cost-effective approaches to building high capacity routers. Unfortunately, the performance of such systems has been difficult to predict in the presence of the extreme traffic conditions that can arise in the Internet. Recent work introduced distributed scheduling, to regulate the flow of traffic in such systems. This work demonstrated, using simulation and experimental measurements, that distributed scheduling can deliver robust performance for extreme traffic. Here, we show that distributed schedulers can be provably work-conserving for speedups of 2 or more. Two of the three schedulers we describe were inspired by previously published crossbar schedulers. The third has no direct counterpart in crossbar scheduling. In our analysis, we show that distributed schedulers based on blocking flows in small-depth acyclic flow graphs can be work-conserving, just as certain crossbar schedulers based on maximal bipartite matchings have been shown to be work-conserving. We also study the performance of practical variants of these schedulers when the speedup is less than 2, using simulation.",,257–268,12,"high performance routers, CIOQ switches, crossbar scheduling, distributed scheduling",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1649,inproceedings,"Pappu, Prashanth and Turner, Jonathan and Wong, Ken",Work-Conserving Distributed Schedulers for Terabit Routers,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015496,10.1145/1015467.1015496,"Buffered multistage interconnection networks offer one of the most scalable and cost-effective approaches to building high capacity routers. Unfortunately, the performance of such systems has been difficult to predict in the presence of the extreme traffic conditions that can arise in the Internet. Recent work introduced distributed scheduling, to regulate the flow of traffic in such systems. This work demonstrated, using simulation and experimental measurements, that distributed scheduling can deliver robust performance for extreme traffic. Here, we show that distributed schedulers can be provably work-conserving for speedups of 2 or more. Two of the three schedulers we describe were inspired by previously published crossbar schedulers. The third has no direct counterpart in crossbar scheduling. In our analysis, we show that distributed schedulers based on blocking flows in small-depth acyclic flow graphs can be work-conserving, just as certain crossbar schedulers based on maximal bipartite matchings have been shown to be work-conserving. We also study the performance of practical variants of these schedulers when the speedup is less than 2, using simulation.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",257–268,12,"distributed scheduling, CIOQ switches, crossbar scheduling, high performance routers","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1650,article,"Valente, Paolo","Exact GPS Simulation with Logarithmic Complexity, and Its Application to an Optimally Fair Scheduler",2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015497,10.1145/1030194.1015497,"Generalized Processor Sharing (GPS) is a fluid scheduling policy providing perfect fairness. The minimum deviation (lead/lag) with respect to the GPS service achievable by a packet scheduler is one packet size. To the best of our knowledge, the only packet scheduler guaranteeing such minimum deviation is Worst-case Fair Weighted Fair Queueing (WF2Q), that requires on-line GPS simulation. Existing algorithms to perform GPS simulation have O(N) complexity per packet transmission (N being the number of competing flows). Hence WF2Q has been charged for O(N) complexity too. Schedulers with lower complexity have been devised, but at the price of at least O(N) deviation from the GPS service, which has been shown to be detrimental for real-time adaptive applications and feedback based applications. Furthermore, it has been proven that the lower bound complexity to guarantee O(1) deviation is Ω(log N), yet a scheduler achieving such result has remained elusive so far.In this paper we present an algorithm that performs exact GPS simulation with O(log N) worst-case complexity and small constants. As such it improves the complexity of all the packet schedulers based on GPS simulation. In particular, using our algorithm within WF2Q, we achieve the minimum deviation from the GPS service with O(log N) complexity, thus matching the aforementioned lower bound. Furthermore, we assess the effectiveness of the proposed solution by simulating real-world scenarios.",,269–280,12,"computational complexity, quality of service, packet scheduling, data structures",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1651,inproceedings,"Valente, Paolo","Exact GPS Simulation with Logarithmic Complexity, and Its Application to an Optimally Fair Scheduler",2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015497,10.1145/1015467.1015497,"Generalized Processor Sharing (GPS) is a fluid scheduling policy providing perfect fairness. The minimum deviation (lead/lag) with respect to the GPS service achievable by a packet scheduler is one packet size. To the best of our knowledge, the only packet scheduler guaranteeing such minimum deviation is Worst-case Fair Weighted Fair Queueing (WF2Q), that requires on-line GPS simulation. Existing algorithms to perform GPS simulation have O(N) complexity per packet transmission (N being the number of competing flows). Hence WF2Q has been charged for O(N) complexity too. Schedulers with lower complexity have been devised, but at the price of at least O(N) deviation from the GPS service, which has been shown to be detrimental for real-time adaptive applications and feedback based applications. Furthermore, it has been proven that the lower bound complexity to guarantee O(1) deviation is Ω(log N), yet a scheduler achieving such result has remained elusive so far.In this paper we present an algorithm that performs exact GPS simulation with O(log N) worst-case complexity and small constants. As such it improves the complexity of all the packet schedulers based on GPS simulation. In particular, using our algorithm within WF2Q, we achieve the minimum deviation from the GPS service with O(log N) complexity, thus matching the aforementioned lower bound. Furthermore, we assess the effectiveness of the proposed solution by simulating real-world scenarios.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",269–280,12,"quality of service, computational complexity, data structures, packet scheduling","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1652,inproceedings,"Snoeren, Alex",Session Details: Congestion Control,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244278,10.1145/3244278,,"Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Portland, Oregon, USA",SIGCOMM '04,,,,,,
1653,article,"Appenzeller, Guido and Keslassy, Isaac and McKeown, Nick",Sizing Router Buffers,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015499,10.1145/1030194.1015499,"All Internet routers contain buffers to hold packets during times of congestion. Today, the size of the buffers is determined by the dynamics of TCP's congestion control algorithm. In particular, the goal is to make sure that when a link is congested, it is busy 100% of the time; which is equivalent to making sure its buffer never goes empty. A widely used rule-of-thumb states that each link needs a buffer of size B",,281–292,12,"TCP, internet router, buffer size, bandwidth delay product",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1654,inproceedings,"Appenzeller, Guido and Keslassy, Isaac and McKeown, Nick",Sizing Router Buffers,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015499,10.1145/1015467.1015499,"All Internet routers contain buffers to hold packets during times of congestion. Today, the size of the buffers is determined by the dynamics of TCP's congestion control algorithm. In particular, the goal is to make sure that when a link is congested, it is busy 100% of the time; which is equivalent to making sure its buffer never goes empty. A widely used rule-of-thumb states that each link needs a buffer of size B","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",281–292,12,"internet router, buffer size, bandwidth delay product, TCP","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1655,article,"Kim, Min Sik and Kim, Taekhyun and Shin, YongJune and Lam, Simon S. and Powers, Edward J.",A Wavelet-Based Approach to Detect Shared Congestion,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015500,10.1145/1030194.1015500,"Per-flow congestion control helps endpoints fairly and efficiently share network resources. Better utilization of network resources can be achieved, however, if congestion management algorithms can determine when two different flows share a congested link. Such knowledge can be used to implement cooperative congestion control or improve the overlay topology of a P2P system. Previous techniques to detect shared congestion either assume a common source or destination node, drop-tail queueing, or a single point of congestion. We propose in this paper a novel technique, applicable to any pair of paths on the Internet, without such limitations. Our technique employs a signal processing method, wavelet denoising, to separate queueing delay caused by network congestion from various other delay variations. Our wavelet-based technique is evaluated through both simulations and Internet experiments. We show that, when detecting shared congestion of paths with a common endpoint, our technique provides faster convergence and higher accuracy while using fewer packets than previous techniques, and that it also accurately determines when there is no shared congestion. Furthermore, we show that our technique is robust and accurate for paths without a common endpoint or synchronized clocks; more specifically, it can tolerate a synchronization offset of up to one second between two packet flows.",,293–306,14,"wavelet denoising, shared congestion",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1656,inproceedings,"Kim, Min Sik and Kim, Taekhyun and Shin, YongJune and Lam, Simon S. and Powers, Edward J.",A Wavelet-Based Approach to Detect Shared Congestion,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015500,10.1145/1015467.1015500,"Per-flow congestion control helps endpoints fairly and efficiently share network resources. Better utilization of network resources can be achieved, however, if congestion management algorithms can determine when two different flows share a congested link. Such knowledge can be used to implement cooperative congestion control or improve the overlay topology of a P2P system. Previous techniques to detect shared congestion either assume a common source or destination node, drop-tail queueing, or a single point of congestion. We propose in this paper a novel technique, applicable to any pair of paths on the Internet, without such limitations. Our technique employs a signal processing method, wavelet denoising, to separate queueing delay caused by network congestion from various other delay variations. Our wavelet-based technique is evaluated through both simulations and Internet experiments. We show that, when detecting shared congestion of paths with a common endpoint, our technique provides faster convergence and higher accuracy while using fewer packets than previous techniques, and that it also accurately determines when there is no shared congestion. Furthermore, we show that our technique is robust and accurate for paths without a common endpoint or synchronized clocks; more specifically, it can tolerate a synchronization offset of up to one second between two packet flows.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",293–306,14,"shared congestion, wavelet denoising","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1657,article,"Zhang, Yueping and Kang, Seong-Ryong and Loguinov, Dmitri",Delayed Stability and Performance of Distributed Congestion Control,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015501,10.1145/1030194.1015501,"Recent research efforts to design better Internet transport protocols combined with scalable Active Queue Management (AQM) have led to significant advances in congestion control. One of the hottest topics in this area is the design of discrete congestion control algorithms that are asymptotically stable under heterogeneous feedback delay and whose control equations do not explicitly depend on the RTTs of end-flows. In this paper, we show that max-min fair congestion control methods with a stable symmetric Jacobian remain stable under arbitrary feedback delay (including heterogeneous directional delays) and that the stability condition of such methods does not involve any of the delays. To demonstrate the practicality of the obtained result, we change the original controller in Kelly's work [14] to become robust under random feedback delay and fixed constants of the control equation. We call the resulting framework Max-min Kelly Control (MKC) and show that it offers smooth sending rate, exponential convergence to efficiency, and fast convergence to fairness, all of which make it appealing for future high-speed networks.",,307–318,12,"discrete congestion control, heterogenous delay, stability",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1658,inproceedings,"Zhang, Yueping and Kang, Seong-Ryong and Loguinov, Dmitri",Delayed Stability and Performance of Distributed Congestion Control,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015501,10.1145/1015467.1015501,"Recent research efforts to design better Internet transport protocols combined with scalable Active Queue Management (AQM) have led to significant advances in congestion control. One of the hottest topics in this area is the design of discrete congestion control algorithms that are asymptotically stable under heterogeneous feedback delay and whose control equations do not explicitly depend on the RTTs of end-flows. In this paper, we show that max-min fair congestion control methods with a stable symmetric Jacobian remain stable under arbitrary feedback delay (including heterogeneous directional delays) and that the stability condition of such methods does not involve any of the delays. To demonstrate the practicality of the obtained result, we change the original controller in Kelly's work [14] to become robust under random feedback delay and fixed constants of the control equation. We call the resulting framework Max-min Kelly Control (MKC) and show that it offers smooth sending rate, exponential convergence to efficiency, and fast convergence to fairness, all of which make it appealing for future high-speed networks.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",307–318,12,"discrete congestion control, heterogenous delay, stability","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1659,inproceedings,"Seshan, Srini",Session Details: DNS and Naming,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244279,10.1145/3244279,,"Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Portland, Oregon, USA",SIGCOMM '04,,,,,,
1660,article,"Pappas, Vasileios and Xu, Zhiguo and Lu, Songwu and Massey, Daniel and Terzis, Andreas and Zhang, Lixia",Impact of Configuration Errors on DNS Robustness,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015503,10.1145/1030194.1015503,"During the past twenty years the Domain Name System (DNS) has sustained phenomenal growth while maintaining satisfactory performance. However, the original design focused mainly on system robustness against physical failures, and neglected the impact of operational errors such as misconfigurations. Our recent measurement effort revealed three specific types of misconfigurations in DNS today: lame delegation, diminished server redundancy, and cyclic zone dependency. Zones with configuration errors suffer from reduced availability and increased query delays up to an order of magnitude. Furthermore, while the original DNS design assumed that redundant DNS servers fail independently, our measurements show that operational choices made at individual zones can severely affect the availability of other zones. We found that, left unchecked, DNS configuration errors are widespread, with lame delegation affecting 15% of the DNS zones, diminished server redundancy being even more prevalent, and cyclic dependency appearing in 2% of the zones. We also noted that the degrees of misconfiguration vary from zone to zone, with most popular zones having the lowest percentage of errors. Our results indicate that DNS, as well as any other truly robust large-scale system, must include systematic checking mechanisms to cope with operational errors.",,319–330,12,"resiliency, DNS, misconfigurations",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1661,inproceedings,"Pappas, Vasileios and Xu, Zhiguo and Lu, Songwu and Massey, Daniel and Terzis, Andreas and Zhang, Lixia",Impact of Configuration Errors on DNS Robustness,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015503,10.1145/1015467.1015503,"During the past twenty years the Domain Name System (DNS) has sustained phenomenal growth while maintaining satisfactory performance. However, the original design focused mainly on system robustness against physical failures, and neglected the impact of operational errors such as misconfigurations. Our recent measurement effort revealed three specific types of misconfigurations in DNS today: lame delegation, diminished server redundancy, and cyclic zone dependency. Zones with configuration errors suffer from reduced availability and increased query delays up to an order of magnitude. Furthermore, while the original DNS design assumed that redundant DNS servers fail independently, our measurements show that operational choices made at individual zones can severely affect the availability of other zones. We found that, left unchecked, DNS configuration errors are widespread, with lame delegation affecting 15% of the DNS zones, diminished server redundancy being even more prevalent, and cyclic dependency appearing in 2% of the zones. We also noted that the degrees of misconfiguration vary from zone to zone, with most popular zones having the lowest percentage of errors. Our results indicate that DNS, as well as any other truly robust large-scale system, must include systematic checking mechanisms to cope with operational errors.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",319–330,12,"DNS, resiliency, misconfigurations","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1662,article,"Ramasubramanian, Venugopalan and Sirer, Emin G\""{u",The Design and Implementation of a next Generation Name Service for the Internet,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015504,10.1145/1030194.1015504,"Name services are critical for mapping logical resource names to physical resources in large-scale distributed systems. The Domain Name System (DNS) used on the Internet, however, is slow, vulnerable to denial of service attacks, and does not support fast updates. These problems stem fundamentally from the structure of the legacy DNS.This paper describes the design and implementation of the Cooperative Domain Name System (CoDoNS), a novel name service, which provides high lookup performance through proactive caching, resilience to denial of service attacks through automatic load-balancing, and fast propagation of updates. CoDoNS derives its scalability, decentralization, self-organization, and failure resilience from peer-to-peer overlays, while it achieves high performance using the Beehive replication framework. Cryptographic delegation, instead of host-based physical delegation, limits potential malfeasance by namespace operators and creates a competitive market for namespace management. Backwards compatibility with existing protocols and wire formats enables CoDoNS to serve as a backup for legacy DNS, as well as a complete replacement. Performance measurements from a real-life deployment of the system in PlanetLab shows that CoDoNS provides fast lookups, automatically reconfigures around faults without manual involvement and thwarts distributed denial of service attacks by promptly redistributing load across nodes.",,331–342,12,"peer to peer, proactive caching, DNS",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1663,inproceedings,"Ramasubramanian, Venugopalan and Sirer, Emin G\""{u",The Design and Implementation of a next Generation Name Service for the Internet,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015504,10.1145/1015467.1015504,"Name services are critical for mapping logical resource names to physical resources in large-scale distributed systems. The Domain Name System (DNS) used on the Internet, however, is slow, vulnerable to denial of service attacks, and does not support fast updates. These problems stem fundamentally from the structure of the legacy DNS.This paper describes the design and implementation of the Cooperative Domain Name System (CoDoNS), a novel name service, which provides high lookup performance through proactive caching, resilience to denial of service attacks through automatic load-balancing, and fast propagation of updates. CoDoNS derives its scalability, decentralization, self-organization, and failure resilience from peer-to-peer overlays, while it achieves high performance using the Beehive replication framework. Cryptographic delegation, instead of host-based physical delegation, limits potential malfeasance by namespace operators and creates a competitive market for namespace management. Backwards compatibility with existing protocols and wire formats enables CoDoNS to serve as a backup for legacy DNS, as well as a complete replacement. Performance measurements from a real-life deployment of the system in PlanetLab shows that CoDoNS provides fast lookups, automatically reconfigures around faults without manual involvement and thwarts distributed denial of service attacks by promptly redistributing load across nodes.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",331–342,12,"proactive caching, peer to peer, DNS","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1664,article,"Balakrishnan, Hari and Lakshminarayanan, Karthik and Ratnasamy, Sylvia and Shenker, Scott and Stoica, Ion and Walfish, Michael",A Layered Naming Architecture for the Internet,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015505,10.1145/1030194.1015505,"Currently the Internet has only one level of name resolution, DNS, which converts user-level domain names into IP addresses. In this paper we borrow liberally from the literature to argue that there should be three levels of name resolution: from user-level descriptors to service identifiers; from service identifiers to endpoint identifiers; and from endpoint identifiers to IP addresses. These additional levels of naming and resolution (1) allow services and data to be first class Internet objects (in that they can be directly and persistently named), (2) seamlessly accommodate mobility and multi-homing and (3) integrate middleboxes (such as NATs and firewalls) into the Internet architecture. We further argue that flat names are a natural choice for the service and endpoint identifiers. Hence, this architecture requires scalable resolution of flat names, a capability that distributed hash tables (DHTs) can provide.",,343–352,10,"distributed hash tables, internet architecture, name resolution, global identifiers, middleboxes, naming",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1665,inproceedings,"Balakrishnan, Hari and Lakshminarayanan, Karthik and Ratnasamy, Sylvia and Shenker, Scott and Stoica, Ion and Walfish, Michael",A Layered Naming Architecture for the Internet,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015505,10.1145/1015467.1015505,"Currently the Internet has only one level of name resolution, DNS, which converts user-level domain names into IP addresses. In this paper we borrow liberally from the literature to argue that there should be three levels of name resolution: from user-level descriptors to service identifiers; from service identifiers to endpoint identifiers; and from endpoint identifiers to IP addresses. These additional levels of naming and resolution (1) allow services and data to be first class Internet objects (in that they can be directly and persistently named), (2) seamlessly accommodate mobility and multi-homing and (3) integrate middleboxes (such as NATs and firewalls) into the Internet architecture. We further argue that flat names are a natural choice for the service and endpoint identifiers. Hence, this architecture requires scalable resolution of flat names, a capability that distributed hash tables (DHTs) can provide.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",343–352,10,"middleboxes, name resolution, distributed hash tables, global identifiers, internet architecture, naming","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1666,inproceedings,"Rowstron, Antony",Session Details: Distributed Information Systems,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244280,10.1145/3244280,,"Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Portland, Oregon, USA",SIGCOMM '04,,,,,,
1667,article,"Bharambe, Ashwin R. and Agrawal, Mukesh and Seshan, Srinivasan",Mercury: Supporting Scalable Multi-Attribute Range Queries,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015507,10.1145/1030194.1015507,"This paper presents the design of Mercury, a scalable protocol for supporting multi-attribute range-based searches. Mercury differs from previous range-based query systems in that it supports multiple attributes as well as performs explicit load balancing. To guarantee efficient routing and load balancing, Mercury uses novel light-weight sampling mechanisms for uniformly sampling random nodes in a highly dynamic overlay network. Our evaluation shows that Mercury is able to achieve its goals of logarithmic-hop routing and near-uniform load balancing.We also show that Mercury can be used to solve a key problem for an important class of distributed applications: distributed state maintenance for distributed games. We show that the Mercury-based solution is easy to use, and that it reduces the game's messaging overheard significantly compared to a na\""{\i",,353–366,14,"range queries, load balancing, distributed hash tables, random sampling, peer-to-peer systems",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1668,inproceedings,"Bharambe, Ashwin R. and Agrawal, Mukesh and Seshan, Srinivasan",Mercury: Supporting Scalable Multi-Attribute Range Queries,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015507,10.1145/1015467.1015507,"This paper presents the design of Mercury, a scalable protocol for supporting multi-attribute range-based searches. Mercury differs from previous range-based query systems in that it supports multiple attributes as well as performs explicit load balancing. To guarantee efficient routing and load balancing, Mercury uses novel light-weight sampling mechanisms for uniformly sampling random nodes in a highly dynamic overlay network. Our evaluation shows that Mercury is able to achieve its goals of logarithmic-hop routing and near-uniform load balancing.We also show that Mercury can be used to solve a key problem for an important class of distributed applications: distributed state maintenance for distributed games. We show that the Mercury-based solution is easy to use, and that it reduces the game's messaging overheard significantly compared to a na\""{\i","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",353–366,14,"peer-to-peer systems, random sampling, load balancing, range queries, distributed hash tables","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1669,article,"Qiu, Dongyu and Srikant, R.",Modeling and Performance Analysis of BitTorrent-like Peer-to-Peer Networks,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015508,10.1145/1030194.1015508,"In this paper, we develop simple models to study the performance of BitTorrent, a second generation peer-to-peer (P2P) application. We first present a simple fluid model and study the scalability, performance and efficiency of such a file-sharing mechanism. We then consider the built-in incentive mechanism of BitTorrent and study its effect on network performance. We also provide numerical results based on both simulations and real traces obtained from the Internet.",,367–378,12,"game theory, fluid model, peer-to-peer networks",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1670,inproceedings,"Qiu, Dongyu and Srikant, R.",Modeling and Performance Analysis of BitTorrent-like Peer-to-Peer Networks,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015508,10.1145/1015467.1015508,"In this paper, we develop simple models to study the performance of BitTorrent, a second generation peer-to-peer (P2P) application. We first present a simple fluid model and study the scalability, performance and efficiency of such a file-sharing mechanism. We then consider the built-in incentive mechanism of BitTorrent and study its effect on network performance. We also provide numerical results based on both simulations and real traces obtained from the Internet.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",367–378,12,"game theory, peer-to-peer networks, fluid model","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1671,article,"Yalagandula, Praveen and Dahlin, Mike",A Scalable Distributed Information Management System,2004,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1030194.1015509,10.1145/1030194.1015509,"We present a Scalable Distributed Information Management System (SDIMS) that aggregates information about large-scale networked systems and that can serve as a basic building block for a broad range of large-scale distributed applications by providing detailed views of nearby information and summary views of global information. To serve as a basic building block, a SDIMS should have four properties: scalability to many nodes and attributes, flexibility to accommodate a broad range of applications, administrative isolation for security and availability, and robustness to node and network failures. We design, implement and evaluate a SDIMS that (1) leverages Distributed Hash Tables (DHT) to create scalable aggregation trees, (2) provides flexibility through a simple API that lets applications control propagation of reads and writes, (3) provides administrative isolation through simple extensions to current DHT algorithms, and (4) achieves robustness to node and network reconfigurations through lazy reaggregation, on-demand reaggregation, and tunable spatial replication. Through extensive simulations and micro-benchmark experiments, we observe that our system is an order of magnitude more scalable than existing approaches, achieves isolation properties at the cost of modestly increased read latency in comparison to flat DHTs, and gracefully handles failures.",,379–390,12,"information management system, networked system monitoring, distributed hash tables",,,October 2004,34,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1672,inproceedings,"Yalagandula, Praveen and Dahlin, Mike",A Scalable Distributed Information Management System,2004,1581138628,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/1015467.1015509,10.1145/1015467.1015509,"We present a Scalable Distributed Information Management System (SDIMS) that aggregates information about large-scale networked systems and that can serve as a basic building block for a broad range of large-scale distributed applications by providing detailed views of nearby information and summary views of global information. To serve as a basic building block, a SDIMS should have four properties: scalability to many nodes and attributes, flexibility to accommodate a broad range of applications, administrative isolation for security and availability, and robustness to node and network failures. We design, implement and evaluate a SDIMS that (1) leverages Distributed Hash Tables (DHT) to create scalable aggregation trees, (2) provides flexibility through a simple API that lets applications control propagation of reads and writes, (3) provides administrative isolation through simple extensions to current DHT algorithms, and (4) achieves robustness to node and network reconfigurations through lazy reaggregation, on-demand reaggregation, and tunable spatial replication. Through extensive simulations and micro-benchmark experiments, we observe that our system is an order of magnitude more scalable than existing approaches, achieves isolation properties at the cost of modestly increased read latency in comparison to flat DHTs, and gracefully handles failures.","Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",379–390,12,"distributed hash tables, information management system, networked system monitoring","Portland, Oregon, USA",SIGCOMM '04,,,,,,
1673,inproceedings,"Wetherall, David",Session Details: Position Papers,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244075,10.1145/3244075,,"Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Karlsruhe, Germany",SIGCOMM '03,,,,,,
1674,inproceedings,"Clark, David D. and Partridge, Craig and Ramming, J. Christopher and Wroclawski, John T.",A Knowledge Plane for the Internet,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863957,10.1145/863955.863957,"We propose a new objective for network research: to build a fundamentally different sort of network that can assemble itself given high level instructions, reassemble itself as requirements change, automatically discover when something goes wrong, and automatically fix a detected problem or explain why it cannot do so.We further argue that to achieve this goal, it is not sufficient to improve incrementally on the techniques and algorithms we know today. Instead, we propose a new construct, the Knowledge Plane, a pervasive system within the network that builds and maintains high-level models of what the network is supposed to do, in order to provide services and advice to other elements of the network. The knowledge plane is novel in its reliance on the tools of AI and cognitive systems. We argue that cognitive techniques, rather than traditional algorithmic approaches, are best suited to meeting the uncertainties and complexity of our objective.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",3–10,8,"knowledge plane, cognition, network applications, network configuration","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1675,inproceedings,"Nakao, Akihiro and Peterson, Larry and Bavier, Andy",A Routing Underlay for Overlay Networks,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863958,10.1145/863955.863958,"We argue that designing overlay services to independently probe the Internet--with the goal of making informed application-specific routing decisions--is an untenable strategy. Instead, we propose a shared routing underlay that overlay services query. We posit that this underlay must adhere to two high-level principles. First, it must take cost (in terms of network probes) into account. Second, it must be layered so that specialized routing services can be built from a set of basic primitives. These principles lead to an underlay design where lower layers expose large-scale, coarse-grained static information already collected by the network, and upper layers perform more frequent probes over a narrow set of nodes. This paper proposes a set of primitive operations and three library routing services that can be built on top of them, and describes how such libraries could be useful to overlay services.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",11–18,8,"routing, overlay networks, infrastructure","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1676,inproceedings,"Gupta, Maruti and Singh, Suresh",Greening of the Internet,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863959,10.1145/863955.863959,"In this paper we examine the somewhat controversial subject of energy consumption of networking devices in the Internet, motivated by data collected by the U.S. Department of Commerce. We discuss the impact on network protocols of saving energy by putting network interfaces and other router & switch components to sleep. Using sample packet traces, we first show that it is indeed reasonable to do this and then we discuss the changes that may need to be made to current Internet protocols to support a more aggressive strategy for sleeping. Since this is a position paper, we do not present results but rather suggest interesting directions for core networking research. The impact of saving energy is huge, particularly in the developing world where energy is a precious resource whose scarcity hinders widespread Internet deployment.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",19–26,8,"energy, protocols, internet","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1677,inproceedings,"Fall, Kevin",A Delay-Tolerant Network Architecture for Challenged Internets,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863960,10.1145/863955.863960,"The highly successful architecture and protocols of today's Internet may operate poorly in environments characterized by very long delay paths and frequent network partitions. These problems are exacerbated by end nodes with limited power or memory resources. Often deployed in mobile and extreme environments lacking continuous connectivity, many such networks have their own specialized protocols, and do not utilize IP. To achieve interoperability between them, we propose a network architecture and application interface structured around optionally-reliable asynchronous message forwarding, with limited expectations of end-to-end connectivity and node resources. The architecture operates as an overlay above the transport layers of the networks it interconnects, and provides key services such as in-network data storage and retransmission, interoperable naming, authenticated forwarding and a coarse-grained class of service.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",27–34,8,,"Karlsruhe, Germany",SIGCOMM '03,,,,,,
1678,inproceedings,"Govindan, Ramesh",Session Details: Routing,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244076,10.1145/3244076,,"Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Karlsruhe, Germany",SIGCOMM '03,,,,,,
1679,inproceedings,"Basu, Anindya and Lin, Alvin and Ramanathan, Sharad",Routing Using Potentials: A Dynamic Traffic-Aware Routing Algorithm,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863962,10.1145/863955.863962,"We present a routing paradigm called PBR that utilizes steepest gradient search methods to route data packets. More specifically, the PBR paradigm assigns scalar potentials to network elements and forwards packets in the direction of maximum positive force. We show that the family of PBR schemes are loop free and that the standard shortest path routing algorithms are a special case of the PBR paradigm. We then show how to design a potential function that accounts for traffic conditions at a node. The resulting routing algorithm routes around congested areas while preserving the key desirable properties of IP routing mechanisms including hop-by-hop routing, local route computations and statistical multiplexing. Our simulations using the ns simulator indicate that the traffic aware routing algorithm shows significant improvements in end-to-end delay and jitter when compared to standard shortest path routing algorithms. The simulations also indicate that our algorithm does not incur too much control overheads and is fairly stable even when traffic conditions are dynamic.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",37–48,12,"potential, gradient, traffic aware, routing, congestion, steepest","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1680,inproceedings,"Sobrinho, Jo\~{a",Network Routing with Path Vector Protocols: Theory and Applications,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863963,10.1145/863955.863963,"Path vector protocols are currently in the limelight, mainly because the inter-domain routing protocol of the Internet, BGP (Border Gateway Protocol), belongs to this class. In this paper, we cast the operation of path vector protocols into a broad algebraic framework and relate the convergence of the protocol, and the characteristics of the paths to which it converges, with the monotonicity and isotonicity properties of its path compositional operation. Here, monotonicity means that the weight of a path cannot decrease when it is extended, and isotonicity means that the relationship between the weights of any two paths with the same origin is preserved when both are extended to the same node. We show that path vector protocols can be made to converge for every network if and only if the algebra is monotone, and that the resulting paths selected by the nodes are optimal if and only if the algebra is isotone as well.Many practical conclusions can be drawn from instances of the generic algebra. For performance-oriented routing, typical in intra-domain routing, we conclude that path vector protocols can be made to converge to widest or widest-shortest paths, but that the composite metric of IGRP (Interior Gateway Protocol), for example, does not guarantee convergence to optimal paths. For policy-based routing, typical in inter-domain routing, we formulate existing guidelines as instances of the generic algebra and we propose new ones. We also show how a particular instance of the algebra yields a sufficient condition for signaling correctness of internal BGP.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",49–60,12,"path vector protocols, algebra, border gateway protocol, BGP","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1681,inproceedings,"Griffin, Timothy G. and Jaggard, Aaron D. and Ramachandran, Vijay",Design Principles of Policy Languages for Path Vector Protocols,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863964,10.1145/863955.863964,"BGP is unique among IP-routing protocols in that routing is determined using semantically rich routing policies. However, this expressiveness has come with hidden risks. The interaction of locally defined routing policies can lead to unexpected global routing anomalies, which can be very difficult to identify and correct in the decentralized and competitive Internet environment. These risks increase as the complexity of local policies increase, which is precisely the current trend. BGP policy languages have evolved in a rather organic fashion with little effort to avoid policy-interaction problems. We believe that researchers should start to consider how to emphdesign policy languages for path-vector protocols that avoid such risks and yet retain other desirable features. We take a few steps in this direction by identifying the important dimensions of this design space and characterizing some of the inherent design trade-offs. We attempt to do this in a general way that is not constrained by the details of BGP.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",61–72,12,"interdomain routing, stable paths problem (SPP), path-vector protocols, routing-policy languages, border gateway protocol (BGP)","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1682,inproceedings,"Savage, Stefania",Session Details: Denial-of-Service,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244077,10.1145/3244077,,"Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Karlsruhe, Germany",SIGCOMM '03,,,,,,
1683,inproceedings,"Kuzmanovic, Aleksandar and Knightly, Edward W.",Low-Rate TCP-Targeted Denial of Service Attacks: The Shrew vs. the Mice and Elephants,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863966,10.1145/863955.863966,"Denial of Service attacks are presenting an increasing threat to the global inter-networking infrastructure. While TCP's congestion control algorithm is highly robust to diverse network conditions, its implicit assumption of end-system cooperation results in a well-known vulnerability to attack by high-rate non-responsive flows. In this paper, we investigate a class of low-rate denial of service attacks which, unlike high-rate attacks, are difficult for routers and counter-DoS mechanisms to detect. Using a combination of analytical modeling, simulations, and Internet experiments, we show that maliciously chosen low-rate DoS traffic patterns that exploit TCP's retransmission time-out mechanism can throttle TCP flows to a small fraction of their ideal rate while eluding detection. Moreover, as such attacks exploit protocol homogeneity, we study fundamental limits of the ability of a class of randomized time-out mechanisms to thwart such low-rate DoS attacks.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",75–86,12,"TCP, denial of service, retransmission timeout","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1684,inproceedings,"Gorinsky, Sergey and Jain, Sugat and Vin, Harrick and Zhang, Yongguang",Robustness to Inflated Subscription in Multicast Congestion Control,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863967,10.1145/863955.863967,"Group subscription is a useful mechanism for multicast congestion control: RLM, RLC, FLID-DL, and WEBRC form a promising line of multi-group protocols where receivers provide no feedback to the sender but control congestion via group membership regulation. Unfortunately, the group subscription mechanism also offers receivers an opportunity to elicit self-beneficial bandwidth allocations. In particular, a misbehaving receiver can ignoreguidelines for group subscription and choose an unfairly high subscription level in a multi-group multicast session. This poses a serious threat to fairness of bandwidth allocation. In this paper, we present the first solution for the problem of inflated subscription. Our design guards access to multicast groups with dynamic keys and consists of two independent components: DELTA (Distribution of ELigibility To Access) -- a novel method for in-band distribution of group keys to receivers that are eligible to access the groups according to the congestion control protocol, and SIGMA (Secure Internet Group Management Architecture) -- a generic architecture for key-based group access at edge routers.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",87–98,12,"congestion control, misbehaving receivers, fair bandwidth allocation, multicast, robustness","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1685,inproceedings,"Hussain, Alefiya and Heidemann, John and Papadopoulos, Christos",A Framework for Classifying Denial of Service Attacks,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863968,10.1145/863955.863968,"Launching a denial of service (DoS) attack is trivial, but detection and response is a painfully slow and often a manual process. Automatic classification of attacks as single- or multi-source can help focus a response, but current packet-header-based approaches are susceptible to spoofing. This paper introduces a framework for classifying DoS attacks based on header content, and novel techniques such as transient ramp-up behavior and spectral analysis. Although headers are easily forged, we show that characteristics of attack ramp-up and attack spectrum are more difficult to spoof. To evaluate our framework we monitored access links of a regional ISP detecting 80 live attacks. Header analysis identified the number of attackers in 67 attacks, while the remaining 13 attacks were classified based on ramp-up and spectral analysis. We validate our results through monitoring at a second site, controlled experiments, and simulation. We use experiments and simulation to understand the underlying reasons for the characteristics observed. In addition to helping understand attack dynamics, classification mechanisms such as ours are important for the development of realistic models of DoS traffic, can be packaged as an automated tool to aid in rapid response to attacks, and can also be used to estimate the level of DoS activity on the Internet.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",99–110,12,"denial of service attacks, time series analysis, measurement, security","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1686,inproceedings,"Seshan, Srinivasan",Session Details: Measurement,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244078,10.1145/3244078,,"Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Karlsruhe, Germany",SIGCOMM '03,,,,,,
1687,inproceedings,"Spring, Neil and Mahajan, Ratul and Anderson, Thomas",The Causes of Path Inflation,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863970,10.1145/863955.863970,"Researchers have shown that the Internet exhibits path inflation -- end-to-end paths can be significantly longer than necessary. We present a trace-driven study of 65 ISPs that characterizes the root causes of path inflation, namely topology and routing policy choices within an ISP, between pairs of ISPs, and across the global Internet. To do so, we develop and validate novel techniques to infer intra-domain and peering policies from end-to-end measurements. We provide the first measured characterization of ISP peering policies. In addition to ""early-exit,"" we observe a significant degree of helpful non-early-exit, load-balancing, and other policies in use between peers. We find that traffic engineering (the explicit addition of policy constraints on top of topology constraints) is widespread in both intra- and inter-domain routing. However, intra-domain traffic engineering has minimal impact on path inflation, while peering policies and inter-domain routing lead to significant inflation. We argue that the underlying cause of inter-domain path inflation is the lack of BGP policy controls to provide convenient engineering of good paths across ISPs.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",113–124,12,,"Karlsruhe, Germany",SIGCOMM '03,,,,,,
1688,inproceedings,"Narayan, Harsha and Govindan, Ramesh and Varghese, George",The Impact of Address Allocation and Routing on the Structure and Implementation of Routing Tables,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863971,10.1145/863955.863971,"The recent growth in the size of the routing table has led to an interest in quantitatively understanding both the causes (eg multihoming) as well as the effects (eg impact on router lookup implementations) of such routing table growth. In this paper, we describe a new model called ARAM that defines the structure of routing tables of any given size. Unlike simpler empirical models that work backwards from effects (eg current prefix length distributions), ARAM approximately models the causes of table growth (allocation by registries, assignment by ISPs, multihoming and load balancing). We show that ARAM models with high fidelity three abstract measures (prefix distribution, prefix depth, and number of nodes in the tree) of the shape of the prefix tree --- as validated against 20 snapshots of backbone routing tables from 1997 to the present. We then use ARAM for evaluating the scalability of IP lookup schemes, and studying the effects of multihoming and load balancing on their scaling behavior. Our results indicate that algorithmic solutions based on multibit tries will provide more prefixes per chip than TCAMs (as table sizes scale toward a million) unless TCAMs can be engineered to use 8 transistors per cell. By contrast, many of today's SRAM-based TCAMs use 14-16 transistors per cell.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",125–136,12,"IP lookups, routing tables, modeling","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1689,inproceedings,"Estan, Cristian and Savage, Stefan and Varghese, George",Automatically Inferring Patterns of Resource Consumption in Network Traffic,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863972,10.1145/863955.863972,"The Internet service model emphasizes flexibility -- any node can send any type of traffic at any time. While this design has allowed new applications and usage models to flourish, it also makes the job of network management significantly more challenging. This paper describes a new method of traffic characterization that automatically groups traffic into minimal clusters of conspicuous consumption. Rather than providing a static analysis specialized to capture flows, applications, or network-to-network traffic matrices, our approach dynamically produces hybrid traffic definitions that match the underlying usage. For example, rather than report five hundred small flows, or the amount of TCP traffic to port 80, or the ""top ten hosts"", our method might reveal that a certain percent of traffic was used by TCP connections between AOL clients and a particular group of Web servers. Similarly, our technique can be used to automatically classify new traffic patterns, such as network worms or peer-to-peer applications, without knowing the structure of such traffic a priori. We describe a series of algorithms for constructing these traffic clusters and minimizing their representation. In addition, we describe the design of our prototype system, AutoFocus and our experiences using it to discover the dominant and unusual modes of usage on several different production networks.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",137–148,12,"traffic measurement, data mining, network monitoring","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1690,inproceedings,"Zegura, Ellen",Session Details: Overlays,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244079,10.1145/3244079,,"Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Karlsruhe, Germany",SIGCOMM '03,,,,,,
1691,inproceedings,"Qiu, Lili and Yang, Yang Richard and Zhang, Yin and Shenker, Scott",On Selfish Routing in Internet-like Environments,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863974,10.1145/863955.863974,"A recent trend in routing research is to avoid inefficiencies in network-level routing by allowing hosts to either choose routes themselves (e.g., source routing) or use overlay routing networks (e.g., Detour or RON). Such approaches result in selfish routing, because routing decisions are no longer based on system-wide criteria but are instead designed to optimize host-based or overlay-based metrics. A series of theoretical results showing that selfish routing can result in suboptimal system behavior have cast doubts on this approach. In this paper, we use a game-theoretic approach to investigate the performance of selfish routing in Internet-like environments. We focus on intra-domain network environments and use realistic topologies and traffic demands in our simulations. We show that in contrast to theoretical worst cases, selfish routing achieves close to optimal average latency in such environments. However, such performance benefit comes at the expense of significantly increased congestion on certain links. Moreover, the adaptive nature of selfish overlays can significantly reduce the effectiveness of traffic engineering by making network traffic less predictable.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",151–162,12,"relaxation, traffic equilibrium, overlay, selfish routing, game theory, traffic engineering, optimization","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1692,inproceedings,"Carzaniga, Antonio and Wolf, Alexander L.",Forwarding in a Content-Based Network,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863975,10.1145/863955.863975,"This paper presents an algorithm for content-based forwarding, an essential function in content-based networking. Unlike in traditional address-based unicast or multicast networks, where messages are given explicit destination addresses, the movement of messages through a content-based network is driven by predicates applied to the content of the messages. Forwarding in such a network amounts to evaluating the predicates stored in a router's forwarding table in order to decide to which neighbor routers the message should be sent. We are interested in finding a forwarding algorithm that can make this decision as quickly as possible in situations where there are numerous, complex predicates and high volumes of messages. We present such an algorithm and give the results of studies evaluating its performance.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",163–174,12,"content-based network, publish/subscribe, forwarding, matching, overlay","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1693,inproceedings,"Tang, Chunqiang and Xu, Zhichen and Dwarkadas, Sandhya",Peer-to-Peer Information Retrieval Using Self-Organizing Semantic Overlay Networks,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863976,10.1145/863955.863976,"Content-based full-text search is a challenging problem in Peer-to-Peer (P2P) systems. Traditional approaches have either been centralized or use flooding to ensure accuracy of the results returned.In this paper, we present pSearch, a decentralized non-flooding P2P information retrieval system. pSearch distributes document indices through the P2P network based on document semantics generated by Latent Semantic Indexing (LSI). The search cost (in terms of different nodes searched and data transmitted) for a given query is thereby reduced, since the indices of semantically related documents are likely to be co located in the network.We also describe techniques that help distribute the indices more evenly across the nodes, and further reduce the number of nodes accessed using appropriate index distribution as well as using index samples and recently processed queries to guide the search.Experiments show that pSearch can achieve performance comparable to centralized information retrieval systems by searching only a small number of nodes. For a system with 128,000 nodes and 528,543 documents (from news, magazines, etc.), pSearch searches only 19 nodes and transmits only 95.5KB data during the search, whereas the top 15 documents returned by pSearch and LSI have a 91.7% intersection.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",175–186,12,"overlay network, information retrieval, peer-to-peer system","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1694,inproceedings,"Partridge, Craig",Session Details: Fowarding,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244080,10.1145/3244080,,"Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Karlsruhe, Germany",SIGCOMM '03,,,,,,
1695,inproceedings,"Keslassy, Isaac and Chuang, Shang-Tse and Yu, Kyoungsik and Miller, David and Horowitz, Mark and Solgaard, Olav and McKeown, Nick",Scaling Internet Routers Using Optics,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863978,10.1145/863955.863978,"Routers built around a single-stage crossbar and a centralized scheduler do not scale, and (in practice) do not provide the throughput guarantees that network operators need to make efficient use of their expensive long-haul links. In this paper we consider how optics can be used to scale capacity and reduce power in a router. We start with the promising load-balanced switch architecture proposed by C-S. Chang. This approach eliminates the scheduler, is scalable, and guarantees 100% throughput for a broad class of traffic. But several problems need to be solved to make this architecture practical: (1) Packets can be mis-sequenced, (2) Pathological periodic traffic patterns can make throughput arbitrarily small, (3) The architecture requires a rapidly configuring switch fabric, and (4) It does not work when linecards are missing or have failed. In this paper we solve each problem in turn, and describe new architectures that include our solutions. We motivate our work by designing a 100Tb/s packet-switched router arranged as 640 linecards, each operating at 160Gb/s. We describe two different implementations based on technology available within the next three years.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",189–200,12,"internet router, load-balancing, packet-switch","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1696,inproceedings,"Dharmapurikar, Sarang and Krishnamurthy, Praveen and Taylor, David E.",Longest Prefix Matching Using Bloom Filters,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863979,10.1145/863955.863979,"We introduce the first algorithm that we are aware of to employ Bloom filters for Longest Prefix Matching (LPM). The algorithm performs parallel queries on Bloom filters, an efficient data structure for membership queries, in order to determine address prefix membership in sets of prefixes sorted by prefix length. We show that use of this algorithm for Internet Protocol (IP) routing lookups results in a search engine providing better performance and scalability than TCAM-based approaches. The key feature of our technique is that the performance, as determined by the number of dependent memory accesses per lookup, can be held constant for longer address lengths or additional unique address prefix lengths in the forwarding table given that memory resources scale linearly with the number of prefixes in the forwarding table.Our approach is equally attractive for Internet Protocol Version 6 (IPv6) which uses 128-bit destination addresses, four times longer than IPv4. We present a basic version of our approach along with optimizations leveraging previous advances in LPM algorithms. We also report results of performance simulations of our system using snapshots of IPv4 BGP tables and extend the results to IPv6. Using less than 2Mb of embedded RAM and a commodity SRAM device, our technique achieves average performance of one hash probe per lookup and a worst case of two hash probes and one array access per lookup.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",201–212,12,"IP lookup, forwarding, longest prefix matching","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1697,inproceedings,"Singh, Sumeet and Baboescu, Florin and Varghese, George and Wang, Jia",Packet Classification Using Multidimensional Cutting,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863980,10.1145/863955.863980,"This paper introduces a classification algorithm called phHyperCuts. Like the previously best known algorithm, HiCuts, HyperCuts is based on a decision tree structure. Unlike HiCuts, however, in which each node in the decision tree represents a hyperplane, each node in the HyperCuts decision tree represents a k--dimensional hypercube. Using this extra degree of freedom and a new set of heuristics to find optimal hypercubes for a given amount of storage, HyperCuts can provide an order of magnitude improvement over existing classification algorithms. HyperCuts uses 2 to 10 times less memory than HiCuts optimized for memory, while the worst case search time of HyperCuts is 50--500% better than that of HiCuts optimized for speed. Compared with another recent scheme, EGT-PC, HyperCuts uses 1.8--7 times less memory space while the worst case search time is up to 5 times smaller. More importantly, unlike EGT-PC, HyperCuts can be fully pipelined to provide one classification result every packet arrival time, and also allows fast updates.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",213–224,12,"packet classification, QoS, firewalls","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1698,inproceedings,"Sventek, Joe",Session Details: Miscellany,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244081,10.1145/3244081,,"Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Karlsruhe, Germany",SIGCOMM '03,,,,,,
1699,inproceedings,"Elliott, Chip and Pearson, David and Troxel, Gregory",Quantum Cryptography in Practice,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863982,10.1145/863955.863982,"BBN, Harvard, and Boston University are building the DARPA Quantum Network, the world's first network that delivers end-to-end network security via high-speed Quantum Key Distribution, and testing that Network against sophisticated eavesdropping attacks. The first network link has been up and steadily operational in our laboratory since December 2002. It provides a Virtual Private Network between private enclaves, with user traffic protected by a weak-coherent implementation of quantum cryptography. This prototype is suitable for deployment in metro-size areas via standard telecom (dark) fiber. In this paper, we introduce quantum cryptography, discuss its relation to modern secure networks, and describe its unusual physical layer, its specialized quantum cryptographic protocol suite (quite interesting in its own right), and our extensions to IPsec to integrate it with quantum cryptography.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",227–238,12,"quantum key distribution, cryptographic protocols, key agreement protocols, error correction, quantum cryptography, secure networks, privacy amplification, IPsec","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1700,inproceedings,"Ramabhadran, Sriram and Pasquale, Joseph",Stratified Round Robin: A Low Complexity Packet Scheduler with Bandwidth Fairness and Bounded Delay,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863983,10.1145/863955.863983,"Fair queuing is a well-studied problem in modern computer networks. However, there remains a gap between scheduling algorithms that have provably good performance, and those that are feasible and practical to implement in high speed routers. In this paper, we propose a novel packet scheduler called Stratified Round Robin, which has low complexity, and is amenable to a simple hardware implementation. Stratified Robin Robin exhibits good fairness and delay properties that are demonstrated through both analytical results and simulations. In particular, it provides a single packet delay bound that is independent of the number of flows. This property is unique to Stratified Round Robin among all other schedulers of comparable complexity.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",239–250,12,"quality of service, fair queueing, output scheduling","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1701,inproceedings,"Ji, Ping and Ge, Zihui and Kurose, Jim and Towsley, Don",A Comparison of Hard-State and Soft-State Signaling Protocols,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863984,10.1145/863955.863984,"One of the key infrastructure components in all telecommunication networks, ranging from the telephone network, to VC-oriented data networks, to the Internet, is its signaling system. Two broad approaches towards signaling can be identified: so-called hard-state and soft-state approaches. Despite the fundamental importance of signaling, our understanding of these approaches - their pros and cons and the circumstances in which they might best be employed - is mostly anecdotal (and occasionally religious). In this paper, we compare and contrast a variety of signaling approaches ranging from a ""pure"" soft state, to soft-state approaches augmented with explicit state removal and/or reliable signaling, to a ""pure"" hard state approach. We develop an analytic model that allows us to quantify state inconsistency in single- and multiple-hop signaling scenarios, and the ""cost"" (both in terms of signaling overhead, and application-specific costs resulting from state inconsistency) associated with a given signaling approach and its parameters (e.g., state refresh and removal timers). Among the class of soft-state approaches, we find that a soft-state approach coupled with explicit removal substantially improves the degree of state consistency while introducing little additional signaling message overhead. The addition of reliable explicit setup/update/removal allows the soft-state approach to achieve comparable (and sometimes better) consistency than that of the hard-state approach.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",251–262,12,"signaling, hard-state, soft-state","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1702,inproceedings,"Katabi, Dina",Session Details: Queue Management,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244082,10.1145/3244082,,"Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Karlsruhe, Germany",SIGCOMM '03,,,,,,
1703,inproceedings,"Le, Long and Aikat, Jay and Jeffay, Kevin and Smith, F. Donelson",The Effects of Active Queue Management on Web Performance,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863986,10.1145/863955.863986,"We present an empirical study of the effects of active queue management (AQM) on the distribution of response times experienced by a population of web users. Three prominent AQM schemes are considered: the Proportional Integrator (PI) controller, the Random Exponential Marking (REM) controller, and Adaptive Random Early Detection (ARED). The effects of these AQM schemes were studied alone and in combination with Explicit Congestion Notification (ECN). Our major results are: For offered loads up to 80% of bottleneck link capacity, no AQM scheme provides better response times than simple drop-tail FIFO queue management.For loads of 90% of link capacity or greater when ECN is not used, PI results in a modest improvement over drop-tail and the other AQM schemes.With ECN, both PI and REM provide significant response time improvement at offered loads above 90% of link capacity. Moreover, at a load of 90% PI and REM with ECN provide response times competitive to that achieved on an unloaded network.ARED with recommended parameter settings consistently resulted in the poorest response times which was unimproved by the addition of ECN.We conclude that without ECN there is little end-user performance gain to be realized by employing the AQM designs studied here. However, with ECN, response times can be significantly improved. In addition it appears likely that provider links may be operated at near saturation levels without significant degradation in user-perceived performance.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",265–276,12,"web performance, congestion control, active queue management","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1704,inproceedings,"Heying, Zhang and Baohong, Liu and Wenhua, Dou",Design of a Robust Active Queue Management Algorithm Based on Feedback Compensation,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863987,10.1145/863955.863987,"Active Queue Management (AQM) is a very active research area in networking. The main objective of an AQM mechanism is to provide low delay and low loss service in best-effort service networks. In this paper we propose a new AQM algorithm based on the feedback compensation technique in control theory. The algorithm is called Proportional Integral based series compensation, and Position feedback compensation (PIP). By choosing the appropriate feedback compensation element and its parameters, the properties of the corrected system become dependent, to a great extent, on the series and feedback compensatory elements. Thus, PIP can eliminate the error incurred by the inaccuracy in the linear system model as well as eliminate the sensitivity to the changes in system parameters like load level, propagation delay, etc. The performance of PIP is compared to PI using ns simulations. From the experiments and analysis we conclude that PIP is more responsive to and robust under time-varying network conditions than PI.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",277–285,9,"feedback compensation, active queue management, robustness, queue length, congestion control","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1705,inproceedings,"Jamjoom, Hani and Shin, Kang G.",Persistent Dropping: An Efficient Control of Traffic Aggregates,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863988,10.1145/863955.863988,"Flash crowd events (FCEs) present a real threat to the stability of routers and end-servers. Such events are characterized by a large and sustained spike in client arrival rates, usually to the point of service failure. Traditional rate-based drop policies, such as Random Early Drop (RED), become ineffective in such situations since clients tend to be persistent, in the sense that they make multiple retransmission attempts before aborting their connection. As it is built into TCP's congestion control, this persistence is very widespread, making it a major stumbling block to providing responsive aggregate traffic controls. This paper focuses on analyzing and building a coherent model of the effects of client persistence on the controllability of aggregate traffic. Based on this model, we propose a new drop strategy called persistent dropping to regulate the arrival of SYN packets and achieves three important goals: (1) it allows routers and end-servers to quickly converge to their control targets without sacrificing fairness, (2) it minimizes the portion of client delay that is attributed to the applied controls, and (3) it is both easily implementable and computationally tractable. Using a real implementation of this controller in the Linux kernel, we demonstrate its efficacy, up to 60% delay reduction for drop probabilities less than 0.5.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",287–298,12,"modeling, queue management, flash crowd events, optimization","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1706,inproceedings,"Diot, Christophe",Session Details: Traffic Engineering,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244083,10.1145/3244083,,"Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Karlsruhe, Germany",SIGCOMM '03,,,,,,
1707,inproceedings,"Zhang, Yin and Roughan, Matthew and Lund, Carsten and Donoho, David",An Information-Theoretic Approach to Traffic Matrix Estimation,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863990,10.1145/863955.863990,"Traffic matrices are required inputs for many IP network management tasks: for instance, capacity planning, traffic engineering and network reliability analysis. However, it is difficult to measure these matrices directly, and so there has been recent interest in inferring traffic matrices from link measurements and other more easily measured data. Typically, this inference problem is ill-posed, as it involves significantly more unknowns than data. Experience in many scientific and engineering fields has shown that it is essential to approach such ill-posed problems via ""regularization"". This paper presents a new approach to traffic matrix estimation using a regularization based on ""entropy penalization"". Our solution chooses the traffic matrix consistent with the measured data that is information-theoretically closest to a model in which source/destination pairs are stochastically independent. We use fast algorithms based on modern convex optimization theory to solve for our traffic matrices. We evaluate the algorithm with real backbone traffic and routing data, and demonstrate that it is fast, accurate, robust, and flexible.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",301–312,12,"mutual information, regularization, traffic engineering, SNMP, information theory, traffic matrix estimation, minimum","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1708,inproceedings,"Applegate, David and Cohen, Edith",Making Intra-Domain Routing Robust to Changing and Uncertain Traffic Demands: Understanding Fundamental Tradeoffs,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863991,10.1145/863955.863991,"Intra-domain traffic engineering can significantly enhance the performance of large IP backbone networks. Two important components of traffic engineering are understanding the traffic demandsand configuring the routing protocols. These two components are inter-linked, as it is widely believed that an accurate view of traffic is important for optimizing the configuration of routing protocols and through that, the utilization of the network.This basic premise, however, never seems to have been quantified --How important is accurate knowledge of traffic demands for obtaining good utilization of the network? Since traffic demand values are dynamic and illusive, is it possible to obtain a routing that is ""robust"" to variations in demands? Armed with enhanced recent algorithmic tools we explore these questions on a diverse collection of ISP networks. We arrive at a surprising conclusion: it is possible to obtain a robust routing that guarantees a nearly optimal utilization with a fairly limited knowledge of the applicable traffic demands.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",313–324,12,"TM estimation, demand-oblivious routing, routing","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1709,inproceedings,"Duffield, Nick and Lund, Carsten and Thorup, Mikkel",Estimating Flow Distributions from Sampled Flow Statistics,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863992,10.1145/863955.863992,"Passive traffic measurement increasingly employs sampling at the packet level. Many high-end routers form flow statistics from a sampled substream of packets. Sampling is necessary in order to control the consumption of resources by the measurement operations. However, knowledge of the statistics of flows in the unsampled stream remains useful, for understanding both characteristics of source traffic, and consumption of resources in the network.This paper provide methods that use flow statistics formed from sampled packet stream to infer the absolute frequencies of lengths of flows in the unsampled stream. A key part of our work is inferring the numbers and lengths of flows of original traffic that evaded sampling altogether. We achieve this through statistical inference, and by exploiting protocol level detail reported in flow records. The method has applications to detection and characterization of network attacks: we show how to estimate, from sampled flow statistics, the number of compromised hosts that are sending attack traffic past the measurement point. We also investigate the impact on our results of different implementations of packet sampling.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",325–336,12,"packet sampling, maximum likelihood estimation, IP flows","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1710,inproceedings,"Voelker, Geoff",Session Details: Measurement,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244084,10.1145/3244084,,"Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Karlsruhe, Germany",SIGCOMM '03,,,,,,
1711,inproceedings,"Pang, Ruoming and Paxson, Vern",A High-Level Programming Environment for Packet Trace Anonymization and Transformation,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863994,10.1145/863955.863994,"Packet traces of operational Internet traffic are invaluable to network research, but public sharing of such traces is severely limited by the need to first remove all sensitive information. Current trace anonymization technology leaves only the packet headers intact, completely stripping the contents; to our knowledge, there are no publicly available traces of any significant size that contain packet payloads. We describe a new approach to transform and anonymize packet traces. Our tool provides high-level language support for packet transformation, allowing the user to write short policy scripts to express sophisticated trace transformations. The resulting scripts can anonymize both packet headers and payloads, and can perform application-level transformations such as editing HTTP or SMTP headers, replacing the content of Web items with MD5 hashes, or altering filenames or reply codes that match given patterns. We discuss the critical issue of verifying that anonymizations are both correctly applied and correctly specified, and experiences with anonymizing FTP traces from the Lawrence Berkeley National Laboratory for public release.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",339–351,13,"anonymization, privacy, transformation, packet trace, network intrusion detection, internet, measurement","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1712,inproceedings,"Akella, Aditya and Maggs, Bruce and Seshan, Srinivasan and Shaikh, Anees and Sitaraman, Ramesh",A Measurement-Based Analysis of Multihoming,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863995,10.1145/863955.863995,"Multihoming has traditionally been employed by stub networks to enhance the reliability of their network connectivity. With the advent of commercial ""intelligent route control"" products, stubs now leverage multihoming to improve performance. Although multihoming is widely used for reliability and, increasingly for performance, not much is known about the tangible benefits that multihoming can offer, or how these benefits can be fully exploited. In this paper, we aim to quantify the extent to which multihomed networks can leverage performance and reliability benefits from connections to multiple providers. We use data collected from servers belonging to the Akamai content distribution network to evaluate performance benefits from two distinct perspectives of multihoming: high-volume content-providers which transmit large volumes of data to many distributed clients, and enterprises which primarily receive data from the network. In both cases, we find that multihoming can improve performance significantly and that not choosing the right set of providers could result in a performance penalty as high as 40%. We also find evidence of diminishing returns in performance when more than four providers are considered for multihoming. In addition, using a large collection of measurements, we provide an analysis of the reliability benefits of multihoming. Finally, we provide guidelines on how multihomed networks can choose ISPs, and discuss practical strategies of using multiple upstream connections to achieve optimal performance benefits.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",353–364,12,,"Karlsruhe, Germany",SIGCOMM '03,,,,,,
1713,inproceedings,"Mao, Zhuoqing Morley and Rexford, Jennifer and Wang, Jia and Katz, Randy H.",Towards an Accurate AS-Level Traceroute Tool,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863996,10.1145/863955.863996,"Traceroute is widely used to detect routing problems, characterize end-to-end paths, and discover the Internet topology. Providing an accurate list of the Autonomous Systems (ASes) along the forwarding path would make traceroute even more valuable to researchers and network operators. However, conventional approaches to mapping traceroute hops to AS numbers are not accurate enough. Address registries are often incomplete and out-of-date. BGP routing tables provide a better IP-to-AS mapping, though this approach has significant limitations as well. Based on our extensive measurements, about 10% of the traceroute paths have one or more hops that do not map to a unique AS number, and around 15% of the traceroute AS paths have an AS loop. In addition, some traceroute AS paths have extra or missing AS hops due to Internet eXchange Points, sibling ASes managed by the same institution, and ASes that do not advertise routes to their infrastructure. Using the BGP tables as a starting point, we propose techniques for improving the IP-to-AS mapping as an important step toward an AS-level traceroute tool. Our algorithms draw on analysis of traceroute probes, reverse DNS lookups, BGP routing tables, and BGP update messages collected from multiple locations. We also discuss how the improved IP-to-AS mapping allows us to home in on cases where the BGP and traceroute AS paths differ for legitimate reasons.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",365–378,14,"border gateway protocol, AS-level path, network measurements, internet topology","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1714,inproceedings,"Rowstron, Antony",Session Details: Peer-to-Peer,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/3244085,10.1145/3244085,,"Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",,,,"Karlsruhe, Germany",SIGCOMM '03,,,,,,
1715,inproceedings,"Gummadi, K. and Gummadi, R. and Gribble, S. and Ratnasamy, S. and Shenker, S. and Stoica, I.",The Impact of DHT Routing Geometry on Resilience and Proximity,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863998,10.1145/863955.863998,"The various proposed DHT routing algorithms embody several different underlying routing geometries. These geometries include hypercubes, rings, tree-like structures, and butterfly networks. In this paper we focus on how these basic geometric approaches affect the resilience and proximity properties of DHTs. One factor that distinguishes these geometries is the degree of flexibility they provide in the selection of neighbors and routes. Flexibility is an important factor in achieving good static resilience and effective proximity neighbor and route selection. Our basic finding is that, despite our initial preference for more complex geometries, the ring geometry allows the greatest flexibility, and hence achieves the best resilience and proximity performance.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",381–394,14,"routing geometry, DHT, flexibility","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1716,inproceedings,"Loguinov, Dmitri and Kumar, Anuj and Rai, Vivek and Ganesh, Sai",Graph-Theoretic Analysis of Structured Peer-to-Peer Systems: Routing Distances and Fault Resilience,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.863999,10.1145/863955.863999,"This paper examines graph-theoretic properties of existing peer-to-peer architectures and proposes a new infrastructure based on optimal diameter de Bruijn graphs. Since generalized de Bruijn graphs possess very short average routing distances and high resilience to node failure, they are well suited for structured peer-to-peer networks. Using the example of Chord, CAN, and de Bruijn, we first study routing performance, graph expansion, and clustering properties of each graph. We then examine bisection width, path overlap, and several other properties that affect routing and resilience of peer-to-peer networks. Having confirmed that de Bruijn graphs offer the best diameter and highest connectivity among the existing peer-to-peer structures, we offer a very simple incremental building process that preserves optimal properties of de Bruijn graphs under uniform user joins/departures. We call the combined peer-to-peer architecture ODRI -- Optimal Diameter Routing Infrastructure.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",395–406,12,"DHT, peer-to-peer, modeling, de Bruijn, graph theory","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1717,inproceedings,"Chawathe, Yatin and Ratnasamy, Sylvia and Breslau, Lee and Lanham, Nick and Shenker, Scott",Making Gnutella-like P2P Systems Scalable,2003,1581137354,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/863955.864000,10.1145/863955.864000,"Napster pioneered the idea of peer-to-peer file sharing, and supported it with a centralized file search facility. Subsequent P2P systems like Gnutella adopted decentralized search algorithms. However, Gnutella's notoriously poor scaling led some to propose distributed hash table solutions to the wide-area file search problem. Contrary to that trend, we advocate retaining Gnutella's simplicity while proposing new mechanisms that greatly improve its scalability. Building upon prior research [1, 12, 22], we propose several modifications to Gnutella's design that dynamically adapt the overlay topology and the search algorithms in order to accommodate the natural heterogeneity present in most peer-to-peer systems. We test our design through simulations and the results show three to five orders of magnitude improvement in total system capacity. We also report on a prototype implementation and its deployment on a testbed.","Proceedings of the 2003 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",407–418,12,"distributed hash tables, peer-to-peer, Gnutella","Karlsruhe, Germany",SIGCOMM '03,,,,,,
1718,inproceedings,"Mahajan, Ratul and Wetherall, David and Anderson, Tom",Understanding BGP Misconfiguration,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633027,10.1145/633025.633027,"It is well-known that simple, accidental BGP configuration errors can disrupt Internet connectivity. Yet little is known about the frequency of misconfiguration or its causes, except for the few spectacular incidents of widespread outages. In this paper, we present the first quantitative study of BGP misconfiguration. Over a three week period, we analyzed routing table advertisements from 23 vantage points across the Internet backbone to detect incidents of misconfiguration. For each incident we polled the ISP operators involved to verify whether it was a misconfiguration, and to learn the cause of the incident. We also actively probed the Internet to determine the impact of misconfiguration on connectivity.Surprisingly, we find that configuration errors are pervasive, with 200-1200 prefixes (0.2-1.0% of the BGP table size) suffering from misconfiguration each day. Close to 3 in 4 of all new prefix advertisements were results of misconfiguration. Fortunately, the connectivity seen by end users is surprisingly robust to misconfigurations. While misconfigurations can substantially increase the update load on routers, only one in twenty five affects connectivity. While the causes of misconfiguration are diverse, we argue that most could be prevented through better router design.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",3–16,14,,"Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1719,article,"Mahajan, Ratul and Wetherall, David and Anderson, Tom",Understanding BGP Misconfiguration,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633027,10.1145/964725.633027,"It is well-known that simple, accidental BGP configuration errors can disrupt Internet connectivity. Yet little is known about the frequency of misconfiguration or its causes, except for the few spectacular incidents of widespread outages. In this paper, we present the first quantitative study of BGP misconfiguration. Over a three week period, we analyzed routing table advertisements from 23 vantage points across the Internet backbone to detect incidents of misconfiguration. For each incident we polled the ISP operators involved to verify whether it was a misconfiguration, and to learn the cause of the incident. We also actively probed the Internet to determine the impact of misconfiguration on connectivity.Surprisingly, we find that configuration errors are pervasive, with 200-1200 prefixes (0.2-1.0% of the BGP table size) suffering from misconfiguration each day. Close to 3 in 4 of all new prefix advertisements were results of misconfiguration. Fortunately, the connectivity seen by end users is surprisingly robust to misconfigurations. While misconfigurations can substantially increase the update load on routers, only one in twenty five affects connectivity. While the causes of misconfiguration are diverse, we argue that most could be prevented through better router design.",,3–16,14,,,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1720,inproceedings,"Griffin, Timothy G. and Wilfong, Gordon",On the Correctness of IBGP Configuration,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633028,10.1145/633025.633028,"The Border Gateway Protocol (BGP) has two distinct modes of operation. External BGP (EBGP) exchanges reachability information between autonomous systems, while Internal BGP (IBGP) exchanges external reachability information within an autonomous system. We study several routing anomalies that are unique to IBGP because, unlike EBGP, forwarding paths and signaling paths are not always symmetric. In particular, we focus on anomalies that can cause the protocol to diverge, and those that can cause a router's chosen forwarding path to an egress point to be deflected by another router on that path. Deflections can greatly complicate the debugging of routing problems, and in the worst case multiple deflections can combine to form persistent forwarding loops. We define a correct IBGP configuration to be one that is anomaly free for every possible set of routes sent by neighboring autonomous systems. We show that determination of IBGP configuration correctness is NP-hard. However, we give simple sufficient conditions on network configurations that guarantee correctness.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",17–29,13,"border gateway protocol, BGP, BGP congfiguration, internal BGP","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1721,article,"Griffin, Timothy G. and Wilfong, Gordon",On the Correctness of IBGP Configuration,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633028,10.1145/964725.633028,"The Border Gateway Protocol (BGP) has two distinct modes of operation. External BGP (EBGP) exchanges reachability information between autonomous systems, while Internal BGP (IBGP) exchanges external reachability information within an autonomous system. We study several routing anomalies that are unique to IBGP because, unlike EBGP, forwarding paths and signaling paths are not always symmetric. In particular, we focus on anomalies that can cause the protocol to diverge, and those that can cause a router's chosen forwarding path to an egress point to be deflected by another router on that path. Deflections can greatly complicate the debugging of routing problems, and in the worst case multiple deflections can combine to form persistent forwarding loops. We define a correct IBGP configuration to be one that is anomaly free for every possible set of routes sent by neighboring autonomous systems. We show that determination of IBGP configuration correctness is NP-hard. However, we give simple sufficient conditions on network configurations that guarantee correctness.",,17–29,13,"border gateway protocol, internal BGP, BGP, BGP congfiguration",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1722,inproceedings,"Maennel, Olaf and Feldmann, Anja",Realistic BGP Traffic for Test Labs,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633029,10.1145/633025.633029,"This paper examines the possibility of generating realistic routing tables of arbitrary size along with realistic BGP updates of arbitrary frequencies via an automated tool deployable in a small-scale test lab. Such a tool provides the necessary foundations to study such questions as: the limits of BGP scalability, the reasons behind routing instability, and the extent to which routing instability influences the forwarding performance of a router.We find that the answer is affirmative. In this paper we identify important characteristics/metrics of routing tables and updates which provide the foundation of the proposed BGP workload model. Based on the insights of an extensive characterization of BGP traffic according to such metrics as prefix length distributions, fanout, amount of nesting of routing table prefixes, AS path length, number and times between BGP update bursts and number and times between BGP session resets, etc., we introduce our prototype tool, rtg. rtg realizes the workload model and is capable of generating realistic BGP traffic. Through its flexibility and parameterization rtg enables us to study the sensibilities of test systems in a repeatable and consistent manner while still providing the possibility of capturing the different characteristics from different vantage points in the network.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",31–44,14,"workload, BGP","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1723,article,"Maennel, Olaf and Feldmann, Anja",Realistic BGP Traffic for Test Labs,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633029,10.1145/964725.633029,"This paper examines the possibility of generating realistic routing tables of arbitrary size along with realistic BGP updates of arbitrary frequencies via an automated tool deployable in a small-scale test lab. Such a tool provides the necessary foundations to study such questions as: the limits of BGP scalability, the reasons behind routing instability, and the extent to which routing instability influences the forwarding performance of a router.We find that the answer is affirmative. In this paper we identify important characteristics/metrics of routing tables and updates which provide the foundation of the proposed BGP workload model. Based on the insights of an extensive characterization of BGP traffic according to such metrics as prefix length distributions, fanout, amount of nesting of routing table prefixes, AS path length, number and times between BGP update bursts and number and times between BGP session resets, etc., we introduce our prototype tool, rtg. rtg realizes the workload model and is capable of generating realistic BGP traffic. Through its flexibility and parameterization rtg enables us to study the sensibilities of test systems in a repeatable and consistent manner while still providing the possibility of capturing the different characteristics from different vantage points in the network.",,31–44,14,"BGP, workload",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1724,inproceedings,"Byers, John and Considine, Jeffrey and Mitzenmacher, Michael and Rost, Stanislav",Informed Content Delivery across Adaptive Overlay Networks,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633031,10.1145/633025.633031,"Overlay networks have emerged as a powerful and highly flexible method for delivering content. We study how to optimize throughput of large transfers across richly connected, adaptive overlay networks, focusing on the potential of collaborative transfers between peers to supplement ongoing downloads. First, we make the case for an erasure-resilient encoding of the content. Using the digital fountain encoding approach, end-hosts can efficiently reconstruct the original content of size $n$ from a subset of any $n$ symbols drawn from a large universe of encoded symbols. Such an approach affords reliability and a substantial degree of application-level flexibility, as it seamlessly accommodates connection migration and parallel transfers while providing resilience to packet loss. However, since the sets of encoded symbols acquired by peers during downloads may overlap substantially, care must be taken to enable them to collaborate effectively. Our main contribution is a collection of useful algorithmic tools for efficient estimation, summarization, and approximate reconciliation of sets of symbols between pairs of collaborating peers, all of which keep message complexity and computation to a minimum. Through simulations and experiments on a prototype implementation, we demonstrate the performance benefits of our informed content delivery mechanisms and how they complement existing overlay network architectures.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",47–60,14,"min-wise summary, overlay, erasure correcting code, Bloom filter, collaboration, digital fountain, reconciliation, peer-to-peer, content delivery","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1725,article,"Byers, John and Considine, Jeffrey and Mitzenmacher, Michael and Rost, Stanislav",Informed Content Delivery across Adaptive Overlay Networks,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633031,10.1145/964725.633031,"Overlay networks have emerged as a powerful and highly flexible method for delivering content. We study how to optimize throughput of large transfers across richly connected, adaptive overlay networks, focusing on the potential of collaborative transfers between peers to supplement ongoing downloads. First, we make the case for an erasure-resilient encoding of the content. Using the digital fountain encoding approach, end-hosts can efficiently reconstruct the original content of size $n$ from a subset of any $n$ symbols drawn from a large universe of encoded symbols. Such an approach affords reliability and a substantial degree of application-level flexibility, as it seamlessly accommodates connection migration and parallel transfers while providing resilience to packet loss. However, since the sets of encoded symbols acquired by peers during downloads may overlap substantially, care must be taken to enable them to collaborate effectively. Our main contribution is a collection of useful algorithmic tools for efficient estimation, summarization, and approximate reconciliation of sets of symbols between pairs of collaborating peers, all of which keep message complexity and computation to a minimum. Through simulations and experiments on a prototype implementation, we demonstrate the performance benefits of our informed content delivery mechanisms and how they complement existing overlay network architectures.",,47–60,14,"digital fountain, content delivery, reconciliation, Bloom filter, overlay, collaboration, peer-to-peer, erasure correcting code, min-wise summary",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1726,inproceedings,"Keromytis, Angelos D. and Misra, Vishal and Rubenstein, Dan",SOS: Secure Overlay Services,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633032,10.1145/633025.633032,"Denial of service (DoS) attacks continue to threaten the reliability of networking systems. Previous approaches for protecting networks from DoS attacks are reactive in that they wait for an attack to be launched before taking appropriate measures to protect the network. This leaves the door open for other attacks that use more sophisticated methods to mask their traffic.We propose an architecture called Secure Overlay Services (SOS) that proactively prevents DoS attacks, geared toward supporting Emergency Services or similar types of communication. The architecture is constructed using a combination of secure overlay tunneling, routing via consistent hashing, and filtering. We reduce the probability of successful attacks by (i) performing intensive filtering near protected network edges, pushing the attack point perimeter into the core of the network, where high-speed routers can handle the volume of attack traffic, and (ii) introducing randomness and anonymity into the architecture, making it difficult for an attacker to target nodes along the path to a specific SOS-protected destination.Using simple analytical models, we evaluate the likelihood that an attacker can successfully launch a DoS attack against an SOS-protected network. Our analysis demonstrates that such an architecture reduces the likelihood of a successful attack to minuscule levels.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",61–72,12,"network security, overlay networks, denial of service attacks","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1727,article,"Keromytis, Angelos D. and Misra, Vishal and Rubenstein, Dan",SOS: Secure Overlay Services,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633032,10.1145/964725.633032,"Denial of service (DoS) attacks continue to threaten the reliability of networking systems. Previous approaches for protecting networks from DoS attacks are reactive in that they wait for an attack to be launched before taking appropriate measures to protect the network. This leaves the door open for other attacks that use more sophisticated methods to mask their traffic.We propose an architecture called Secure Overlay Services (SOS) that proactively prevents DoS attacks, geared toward supporting Emergency Services or similar types of communication. The architecture is constructed using a combination of secure overlay tunneling, routing via consistent hashing, and filtering. We reduce the probability of successful attacks by (i) performing intensive filtering near protected network edges, pushing the attack point perimeter into the core of the network, where high-speed routers can handle the volume of attack traffic, and (ii) introducing randomness and anonymity into the architecture, making it difficult for an attacker to target nodes along the path to a specific SOS-protected destination.Using simple analytical models, we evaluate the likelihood that an attacker can successfully launch a DoS attack against an SOS-protected network. Our analysis demonstrates that such an architecture reduces the likelihood of a successful attack to minuscule levels.",,61–72,12,"network security, overlay networks, denial of service attacks",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1728,inproceedings,"Stoica, Ion and Adkins, Daniel and Zhuang, Shelley and Shenker, Scott and Surana, Sonesh",Internet Indirection Infrastructure,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633033,10.1145/633025.633033,"Attempts to generalize the Internet's point-to-point communication abstraction to provide services like multicast, anycast, and mobility have faced challenging technical problems and deployment barriers. To ease the deployment of such services, this paper proposes an overlay-based Internet Indirection Infrastructure ( I3) that offers a rendezvous-based communication abstraction. Instead of explicitly sending a packet to a destination, each packet is associated with an identifier; this identifier is then used by the receiver to obtain delivery of the packet. This level of indirection decouples the act of sending from the act of receiving, and allows I3 to efficiently support a wide variety of fundamental communication services. To demonstrate the feasibility of this approach, we have designed and built a prototype based on the Chord lookup protocol.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",73–86,14,"scalable, abstraction, indirection, architecture, internet","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1729,article,"Stoica, Ion and Adkins, Daniel and Zhuang, Shelley and Shenker, Scott and Surana, Sonesh",Internet Indirection Infrastructure,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633033,10.1145/964725.633033,"Attempts to generalize the Internet's point-to-point communication abstraction to provide services like multicast, anycast, and mobility have faced challenging technical problems and deployment barriers. To ease the deployment of such services, this paper proposes an overlay-based Internet Indirection Infrastructure ( I3) that offers a rendezvous-based communication abstraction. Instead of explicitly sending a packet to a destination, each packet is associated with an identifier; this identifier is then used by the receiver to obtain delivery of the packet. This level of indirection decouples the act of sending from the act of receiving, and allows I3 to efficiently support a wide variety of fundamental communication services. To demonstrate the feasibility of this approach, we have designed and built a prototype based on the Chord lookup protocol.",,73–86,14,"abstraction, architecture, scalable, indirection, internet",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1730,inproceedings,"Katabi, Dina and Handley, Mark and Rohrs, Charlie",Congestion Control for High Bandwidth-Delay Product Networks,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633035,10.1145/633025.633035,"Theory and experiments show that as the per-flow product of bandwidth and latency increases, TCP becomes inefficient and prone to instability, regardless of the queuing scheme. This failing becomes increasingly important as the Internet evolves to incorporate very high-bandwidth optical links and more large-delay satellite links.To address this problem, we develop a novel approach to Internet congestion control that outperforms TCP in conventional environments, and remains efficient, fair, scalable, and stable as the bandwidth-delay product increases. This new eXplicit Control Protocol, XCP, generalizes the Explicit Congestion Notification proposal (ECN). In addition, XCP introduces the new concept of decoupling utilization control from fairness control. This allows a more flexible and analytically tractable protocol design and opens new avenues for service differentiation.Using a control theory framework, we model XCP and demonstrate it is stable and efficient regardless of the link capacity, the round trip delay, and the number of sources. Extensive packet-level simulations show that XCP outperforms TCP in both conventional and high bandwidth-delay environments. Further, XCP achieves fair bandwidth allocation, high utilization, small standing queue size, and near-zero packet drops, with both steady and highly varying traffic. Additionally, the new protocol does not maintain any per-flow state in routers and requires few CPU cycles per packet, which makes it implementable in high-speed routers.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",89–102,14,"high-speed networks, large bandwidth-delay product, congestion control","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1731,article,"Katabi, Dina and Handley, Mark and Rohrs, Charlie",Congestion Control for High Bandwidth-Delay Product Networks,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633035,10.1145/964725.633035,"Theory and experiments show that as the per-flow product of bandwidth and latency increases, TCP becomes inefficient and prone to instability, regardless of the queuing scheme. This failing becomes increasingly important as the Internet evolves to incorporate very high-bandwidth optical links and more large-delay satellite links.To address this problem, we develop a novel approach to Internet congestion control that outperforms TCP in conventional environments, and remains efficient, fair, scalable, and stable as the bandwidth-delay product increases. This new eXplicit Control Protocol, XCP, generalizes the Explicit Congestion Notification proposal (ECN). In addition, XCP introduces the new concept of decoupling utilization control from fairness control. This allows a more flexible and analytically tractable protocol design and opens new avenues for service differentiation.Using a control theory framework, we model XCP and demonstrate it is stable and efficient regardless of the link capacity, the round trip delay, and the number of sources. Extensive packet-level simulations show that XCP outperforms TCP in both conventional and high bandwidth-delay environments. Further, XCP achieves fair bandwidth allocation, high utilization, small standing queue size, and near-zero packet drops, with both steady and highly varying traffic. Additionally, the new protocol does not maintain any per-flow state in routers and requires few CPU cycles per packet, which makes it implementable in high-speed routers.",,89–102,14,"congestion control, high-speed networks, large bandwidth-delay product",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1732,inproceedings,"VojnoviΕ, Milan and Le Boudec, Jean-Yves",On the Long-Run Behavior of Equation-Based Rate Control,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633036,10.1145/633025.633036,"We consider unicast equation-based rate control, where a source estimates the loss event ratio $p$, and, primarily at loss events, adjusts its send rate to $f(p)$. Function $f$ is assumed to represent the loss-throughput relation that TCP would experience. When no loss occurs, the rate may also be increased according to some additional mechanism. We assume that the loss event interval estimator is non-biased. If the loss process is deterministic, the control is TCP-friendly in the long-run, i.e, the average throughput does not exceed that of TCP. If, in contrast, losses are random, it is a priori not clear whether this holds, due to the non-linearity of $f$, and a phenomenon similar to Feller's paradox. Our goal is to identify the key factors that drive whether, and how far, the control is TCP friendly (in the long run). As TCP and our source may experience different loss event intervals, we distinguish between TCP-friendliness and conservativeness (throughput does not exceed $f(p)$). We give a representation of the long term throughput, and derive that conservativeness is primarily influenced by various convexity properties of $f$, the variability of loss events, and the correlation structure of the loss process. In many cases, these factors lead to conservativeness, but we show reasonable experiments where the control is clearly non-conservative. However, our analysis also suggests that our source should experience a higher loss event ratio than TCP, which would make non-TCP friendliness less likely. Our findings provide guidelines that help understand when an equation base control is indeed TCP-friendly in the long-run, and in some cases, excessively so. The effects of round trip time and its variations are not included in this study.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",103–116,14,"equation-based rate control, congestion control, TCP-friendly, point processes, palm calculus, internet","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1733,article,"VojnoviΕ, Milan and Le Boudec, Jean-Yves",On the Long-Run Behavior of Equation-Based Rate Control,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633036,10.1145/964725.633036,"We consider unicast equation-based rate control, where a source estimates the loss event ratio $p$, and, primarily at loss events, adjusts its send rate to $f(p)$. Function $f$ is assumed to represent the loss-throughput relation that TCP would experience. When no loss occurs, the rate may also be increased according to some additional mechanism. We assume that the loss event interval estimator is non-biased. If the loss process is deterministic, the control is TCP-friendly in the long-run, i.e, the average throughput does not exceed that of TCP. If, in contrast, losses are random, it is a priori not clear whether this holds, due to the non-linearity of $f$, and a phenomenon similar to Feller's paradox. Our goal is to identify the key factors that drive whether, and how far, the control is TCP friendly (in the long run). As TCP and our source may experience different loss event intervals, we distinguish between TCP-friendliness and conservativeness (throughput does not exceed $f(p)$). We give a representation of the long term throughput, and derive that conservativeness is primarily influenced by various convexity properties of $f$, the variability of loss events, and the correlation structure of the loss process. In many cases, these factors lead to conservativeness, but we show reasonable experiments where the control is clearly non-conservative. However, our analysis also suggests that our source should experience a higher loss event ratio than TCP, which would make non-TCP friendliness less likely. Our findings provide guidelines that help understand when an equation base control is indeed TCP-friendly in the long-run, and in some cases, excessively so. The effects of round trip time and its variations are not included in this study.",,103–116,14,"internet, TCP-friendly, point processes, equation-based rate control, congestion control, palm calculus",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1734,inproceedings,"Akella, Aditya and Seshan, Srinivasan and Karp, Richard and Shenker, Scott and Papadimitriou, Christos",Selfish Behavior and Stability of the Internet: A Game-Theoretic Analysis of TCP,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633037,10.1145/633025.633037,"For years, the conventional wisdom [7, 22] has been that the continued stability of the Internet depends on the widespread deployment of ""socially responsible"" congestion control. In this paper, we seek to answer the following fundamental question: If network end-points behaved in a selfish manner, would the stability of the Internet be endangered?.We evaluate the impact of greedy end-point behavior through a game-theoretic analysis of TCP. In this ""TCP Game"" each flowattempts to maximize the throughput it achieves by modifying its congestion control behavior. We use a combination of analysis and simulation to determine the Nash Equilibrium of this game. Our question then reduces to whether the network operates efficiently at these Nash equilibria.Our findings are twofold. First, in more traditional environments -- where end-points use TCP Reno-style loss recovery and routers use drop-tail queues -- the Nash Equilibria are reasonably efficient. However, when endpoints use more recent variations of TCP (e.g., SACK) and routers employ either RED or drop-tail queues, the Nash equilibria are very inefficient. This suggests that the Internet of the past could remain stable in the face of greedy end-user behavior, but the Internet of today is vulnerable to such behavior. Second, we find that restoring the efficiency of the Nash equilibria in these settings does not require heavy-weight packet scheduling techniques (e.g., Fair Queuing) but instead can be done with a very simple stateless mechanism based on CHOKe [21].","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",117–130,14,,"Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1735,article,"Akella, Aditya and Seshan, Srinivasan and Karp, Richard and Shenker, Scott and Papadimitriou, Christos",Selfish Behavior and Stability of the Internet: A Game-Theoretic Analysis of TCP,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633037,10.1145/964725.633037,"For years, the conventional wisdom [7, 22] has been that the continued stability of the Internet depends on the widespread deployment of ""socially responsible"" congestion control. In this paper, we seek to answer the following fundamental question: If network end-points behaved in a selfish manner, would the stability of the Internet be endangered?.We evaluate the impact of greedy end-point behavior through a game-theoretic analysis of TCP. In this ""TCP Game"" each flowattempts to maximize the throughput it achieves by modifying its congestion control behavior. We use a combination of analysis and simulation to determine the Nash Equilibrium of this game. Our question then reduces to whether the network operates efficiently at these Nash equilibria.Our findings are twofold. First, in more traditional environments -- where end-points use TCP Reno-style loss recovery and routers use drop-tail queues -- the Nash Equilibria are reasonably efficient. However, when endpoints use more recent variations of TCP (e.g., SACK) and routers employ either RED or drop-tail queues, the Nash equilibria are very inefficient. This suggests that the Internet of the past could remain stable in the face of greedy end-user behavior, but the Internet of today is vulnerable to such behavior. Second, we find that restoring the efficiency of the Nash equilibria in these settings does not require heavy-weight packet scheduling techniques (e.g., Fair Queuing) but instead can be done with a very simple stateless mechanism based on CHOKe [21].",,117–130,14,,,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1736,inproceedings,"Spring, Neil and Mahajan, Ratul and Wetherall, David",Measuring ISP Topologies with Rocketfuel,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633039,10.1145/633025.633039,"To date, realistic ISP topologies have not been accessible to the research community, leaving work that depends on topology on an uncertain footing. In this paper, we present new Internet mapping techniques that have enabled us to directly measure router-level ISP topologies. Our techniques reduce the number of required traces compared to a brute-force, all-to-all approach by three orders of magnitude without a significant loss in accuracy. They include the use of BGP routing tables to focus the measurements, exploiting properties of IP routing to eliminate redundant measurements, better alias resolution, and the use of DNS to divide each map into POPs and backbone. We collect maps from ten diverse ISPs using our techniques, and find that our maps are substantially more complete than those of earlier Internet mapping efforts. We also report on properties of these maps, including the size of POPs, distribution of router outdegree, and the inter-domain peering structure. As part of this work, we release our maps to the community.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",133–145,13,,"Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1737,article,"Spring, Neil and Mahajan, Ratul and Wetherall, David",Measuring ISP Topologies with Rocketfuel,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633039,10.1145/964725.633039,"To date, realistic ISP topologies have not been accessible to the research community, leaving work that depends on topology on an uncertain footing. In this paper, we present new Internet mapping techniques that have enabled us to directly measure router-level ISP topologies. Our techniques reduce the number of required traces compared to a brute-force, all-to-all approach by three orders of magnitude without a significant loss in accuracy. They include the use of BGP routing tables to focus the measurements, exploiting properties of IP routing to eliminate redundant measurements, better alias resolution, and the use of DNS to divide each map into POPs and backbone. We collect maps from ten diverse ISPs using our techniques, and find that our maps are substantially more complete than those of earlier Internet mapping efforts. We also report on properties of these maps, including the size of POPs, distribution of router outdegree, and the inter-domain peering structure. As part of this work, we release our maps to the community.",,133–145,13,,,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1738,inproceedings,"Tangmunarunkit, Hongsuda and Govindan, Ramesh and Jamin, Sugih and Shenker, Scott and Willinger, Walter",Network Topology Generators: Degree-Based vs. Structural,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633040,10.1145/633025.633040,"Following the long-held belief that the Internet is hierarchical, the network topology generators most widely used by the Internet research community, Transit-Stub and Tiers, create networks with a deliberately hierarchical structure. However, in 1999 a seminal paper by Faloutsos et al. revealed that the Internet's degree distribution is a power-law. Because the degree distributions produced by the Transit-Stub and Tiers generators are not power-laws, the research community has largely dismissed them as inadequate and proposed new network generators that attempt to generate graphs with power-law degree distributions.Contrary to much of the current literature on network topology generators, this paper starts with the assumption that it is more important for network generators to accurately model the large-scale structure of the Internet (such as its hierarchical structure) than to faithfully imitate its local properties (such as the degree distribution). The purpose of this paper is to determine, using various topology metrics, which network generators better represent this large-scale structure. We find, much to our surprise, that network generators based on the degree distribution more accurately capture the large-scale structure of measured topologies. We then seek an explanation for this result by examining the nature of hierarchy in the Internet more closely; we find that degree-based generators produce a form of hierarchy that closely resembles the loosely hierarchical nature of the Internet.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",147–159,13,"large-scale structure, topology metrics, hierarchy, structural generators, degree-based generators, topology generators, network topology, topology characterization","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1739,article,"Tangmunarunkit, Hongsuda and Govindan, Ramesh and Jamin, Sugih and Shenker, Scott and Willinger, Walter",Network Topology Generators: Degree-Based vs. Structural,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633040,10.1145/964725.633040,"Following the long-held belief that the Internet is hierarchical, the network topology generators most widely used by the Internet research community, Transit-Stub and Tiers, create networks with a deliberately hierarchical structure. However, in 1999 a seminal paper by Faloutsos et al. revealed that the Internet's degree distribution is a power-law. Because the degree distributions produced by the Transit-Stub and Tiers generators are not power-laws, the research community has largely dismissed them as inadequate and proposed new network generators that attempt to generate graphs with power-law degree distributions.Contrary to much of the current literature on network topology generators, this paper starts with the assumption that it is more important for network generators to accurately model the large-scale structure of the Internet (such as its hierarchical structure) than to faithfully imitate its local properties (such as the degree distribution). The purpose of this paper is to determine, using various topology metrics, which network generators better represent this large-scale structure. We find, much to our surprise, that network generators based on the degree distribution more accurately capture the large-scale structure of measured topologies. We then seek an explanation for this result by examining the nature of hierarchy in the Internet more closely; we find that degree-based generators produce a form of hierarchy that closely resembles the loosely hierarchical nature of the Internet.",,147–159,13,"large-scale structure, topology characterization, degree-based generators, structural generators, topology metrics, topology generators, network topology, hierarchy",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1740,inproceedings,"Medina, A. and Taft, N. and Salamatian, K. and Bhattacharyya, S. and Diot, C.",Traffic Matrix Estimation: Existing Techniques and New Directions,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633041,10.1145/633025.633041,"Very few techniques have been proposed for estimating traffic matrices in the context of Internet traffic. Our work on POP-to-POP traffic matrices (TM) makes two contributions. The primary contribution is the outcome of a detailed comparative evaluation of the three existing techniques. We evaluate these methods with respect to the estimation errors yielded, sensitivity to prior information required and sensitivity to the statistical assumptions they make. We study the impact of characteristics such as path length and the amount of link sharing on the estimation errors. Using actual data from a Tier-1 backbone, we assess the validity of the typical assumptions needed by the TM estimation techniques. The secondary contribution of our work is the proposal of a new direction for TM estimation based on using choice models to model POP fanouts. These models allow us to overcome some of the problems of existing methods because they can incorporate additional data and information about POPs and they enable us to make a fundamentally different kind of modeling assumption. We validate this approach by illustrating that our modeling assumption matches actual Internet data well. Using two initial simple models we provide a proof of concept showing that the incorporation of knowledge of POP features (such as total incoming bytes, number of customers, etc.) can reduce estimation errors. Our proposed approach can be used in conjunction with existing or future methods in that it can be used to generate good priors that serve as inputs to statistical inference techniques.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",161–174,14,"traffic matrix estimation, statistical inference","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1741,article,"Medina, A. and Taft, N. and Salamatian, K. and Bhattacharyya, S. and Diot, C.",Traffic Matrix Estimation: Existing Techniques and New Directions,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633041,10.1145/964725.633041,"Very few techniques have been proposed for estimating traffic matrices in the context of Internet traffic. Our work on POP-to-POP traffic matrices (TM) makes two contributions. The primary contribution is the outcome of a detailed comparative evaluation of the three existing techniques. We evaluate these methods with respect to the estimation errors yielded, sensitivity to prior information required and sensitivity to the statistical assumptions they make. We study the impact of characteristics such as path length and the amount of link sharing on the estimation errors. Using actual data from a Tier-1 backbone, we assess the validity of the typical assumptions needed by the TM estimation techniques. The secondary contribution of our work is the proposal of a new direction for TM estimation based on using choice models to model POP fanouts. These models allow us to overcome some of the problems of existing methods because they can incorporate additional data and information about POPs and they enable us to make a fundamentally different kind of modeling assumption. We validate this approach by illustrating that our modeling assumption matches actual Internet data well. Using two initial simple models we provide a proof of concept showing that the incorporation of knowledge of POP features (such as total incoming bytes, number of customers, etc.) can reduce estimation errors. Our proposed approach can be used in conjunction with existing or future methods in that it can be used to generate good priors that serve as inputs to statistical inference techniques.",,161–174,14,"traffic matrix estimation, statistical inference",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1742,inproceedings,"Cohen, Edith and Shenker, Scott",Replication Strategies in Unstructured Peer-to-Peer Networks,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633043,10.1145/633025.633043,"The Peer-to-Peer (P2P) architectures that are most prevalent in today's Internet are decentralized and unstructured. Search is blind in that it is independent of the query and is thus not more effective than probing randomly chosen peers. One technique to improve the effectiveness of blind search is to proactively replicate data. We evaluate and compare different replication strategies and reveal interesting structure: Two very common but very different replication strategies - uniform and proportional - yield the same average performance on successful queries, and are in fact worse than any replication strategy which lies between them. The optimal strategy lies between the two and can be achieved by simple distributed algorithms. These fundamental results o.er a new understanding of replication and show that currently deployed replication strategies are far from optimal and that optimal replication is attainable by protocols that resemble existing ones in simplicity and operation.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",177–190,14,"replication, random search, peer-to-peer","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1743,article,"Cohen, Edith and Shenker, Scott",Replication Strategies in Unstructured Peer-to-Peer Networks,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633043,10.1145/964725.633043,"The Peer-to-Peer (P2P) architectures that are most prevalent in today's Internet are decentralized and unstructured. Search is blind in that it is independent of the query and is thus not more effective than probing randomly chosen peers. One technique to improve the effectiveness of blind search is to proactively replicate data. We evaluate and compare different replication strategies and reveal interesting structure: Two very common but very different replication strategies - uniform and proportional - yield the same average performance on successful queries, and are in fact worse than any replication strategy which lies between them. The optimal strategy lies between the two and can be achieved by simple distributed algorithms. These fundamental results o.er a new understanding of replication and show that currently deployed replication strategies are far from optimal and that optimal replication is attainable by protocols that resemble existing ones in simplicity and operation.",,177–190,14,"random search, replication, peer-to-peer",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1744,inproceedings,"Luby, Michael and Goyal, Vivek K. and Skaria, Simon and Horn, Gavin B.",Wave and Equation Based Rate Control Using Multicast Round Trip Time,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633044,10.1145/633025.633044,"This paper introduces Wave and Equation Based Rate Control (WEBRC), the first multiple rate multicast congestion control protocol to be equation based. The equation-based approach enforces fairness to TCP with the benefit that fluctuations in the flow rate are small in comparison to TCP.This paper also introduces the multicast round trip time (MRTT), a multicast analogue of the unicast round trip time (RTT). The MRTT is fundamental to the equation-based protocol that each receiver uses to adjust its reception rate. Each receiver independently measures its own MRTT without placing any added messaging burden on the receiver, the sender or the intermediate network elements. Benefits provided by the MRTT include those that the RTT provides to TCP, e.g., reduced reception rates in reaction to buffer filling and fair sharing of bottleneck links. In addition, the use of MRTT is shown to synchronize and equalize the reception rates of proximate receivers and to cause reception rates to increase as the density of receivers increases.Another innovation of WEBRC is the idea of transmitting data with waves: the transmission rate on a channel is periodic, with an exponentially decreasing form during an active period followed by a quiescent period. Benefits of using waves include insensitivity to large IGMP leave latency; a frequency of joins and leaves by each receiver that is small and independent of the receiver reception rate; the use of a small number of multicast channels; fine-grained control over the receiver reception rate; and minimal, at times nonexistent, losses due to buffer overflow.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",191–204,14,"TCP-friendliness, multiple-rate, multicast, congestion control","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1745,article,"Luby, Michael and Goyal, Vivek K. and Skaria, Simon and Horn, Gavin B.",Wave and Equation Based Rate Control Using Multicast Round Trip Time,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633044,10.1145/964725.633044,"This paper introduces Wave and Equation Based Rate Control (WEBRC), the first multiple rate multicast congestion control protocol to be equation based. The equation-based approach enforces fairness to TCP with the benefit that fluctuations in the flow rate are small in comparison to TCP.This paper also introduces the multicast round trip time (MRTT), a multicast analogue of the unicast round trip time (RTT). The MRTT is fundamental to the equation-based protocol that each receiver uses to adjust its reception rate. Each receiver independently measures its own MRTT without placing any added messaging burden on the receiver, the sender or the intermediate network elements. Benefits provided by the MRTT include those that the RTT provides to TCP, e.g., reduced reception rates in reaction to buffer filling and fair sharing of bottleneck links. In addition, the use of MRTT is shown to synchronize and equalize the reception rates of proximate receivers and to cause reception rates to increase as the density of receivers increases.Another innovation of WEBRC is the idea of transmitting data with waves: the transmission rate on a channel is periodic, with an exponentially decreasing form during an active period followed by a quiescent period. Benefits of using waves include insensitivity to large IGMP leave latency; a frequency of joins and leaves by each receiver that is small and independent of the receiver reception rate; the use of a small number of multicast channels; fine-grained control over the receiver reception rate; and minimal, at times nonexistent, losses due to buffer overflow.",,191–204,14,"multiple-rate, multicast, congestion control, TCP-friendliness",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1746,inproceedings,"Banerjee, Suman and Bhattacharjee, Bobby and Kommareddy, Christopher",Scalable Application Layer Multicast,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633045,10.1145/633025.633045,"We describe a new scalable application-layer multicast protocol, specifically designed for low-bandwidth, data streaming applications with large receiver sets. Our scheme is based upon a hierarchical clustering of the application-layer multicast peers and can support a number of different data delivery trees with desirable properties.We present extensive simulations of both our protocol and the Narada application-layer multicast protocol over Internet-like topologies. Our results show that for groups of size 32 or more, our protocol has lower link stress (by about 25%), improved or similar end-to-end latencies and similar failure recovery properties. More importantly, it is able to achieve these results by using orders of magnitude lower control traffic.Finally, we present results from our wide-area testbed in which we experimented with 32-100 member groups distributed over 8 different sites. In our experiments, average group members established and maintained low-latency paths and incurred a maximum packet loss rate of less than 1% as members randomly joined and left the multicast group. The average control overhead during our experiments was less than 1 Kbps for groups of size 100.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",205–217,13,"scalability, application layer multicast, hierarchy, overlay networks, peer-to-peer systems","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1747,article,"Banerjee, Suman and Bhattacharjee, Bobby and Kommareddy, Christopher",Scalable Application Layer Multicast,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633045,10.1145/964725.633045,"We describe a new scalable application-layer multicast protocol, specifically designed for low-bandwidth, data streaming applications with large receiver sets. Our scheme is based upon a hierarchical clustering of the application-layer multicast peers and can support a number of different data delivery trees with desirable properties.We present extensive simulations of both our protocol and the Narada application-layer multicast protocol over Internet-like topologies. Our results show that for groups of size 32 or more, our protocol has lower link stress (by about 25%), improved or similar end-to-end latencies and similar failure recovery properties. More importantly, it is able to achieve these results by using orders of magnitude lower control traffic.Finally, we present results from our wide-area testbed in which we experimented with 32-100 member groups distributed over 8 different sites. In our experiments, average group members established and maintained low-latency paths and incurred a maximum packet loss rate of less than 1% as members randomly joined and left the multicast group. The average control overhead during our experiments was less than 1 Kbps for groups of size 100.",,205–217,13,"scalability, application layer multicast, peer-to-peer systems, hierarchy, overlay networks",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1748,inproceedings,"Mao, Zhuoqing Morley and Govindan, Ramesh and Varghese, George and Katz, Randy H.",Route Flap Damping Exacerbates Internet Routing Convergence,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633047,10.1145/633025.633047,"Route flap damping is considered to be a widely deployed mechanism in core routers that limits the widespread propagation of unstable BGP routing information. Originally designed to suppress route changes caused by link flaps, flap damping attempts to distinguish persistently unstable routes from routes that occasionally fail. It is considered to be a major contributor to the stability of the Internet routing system.We show in this paper that, surprisingly, route flap damping can significantly exacerbate the convergence times of relatively stable routes. For example, a route to a prefix that is withdrawn exactly once and re-announced can be suppressed for up to an hour (using the current RIPE recommended damping parameters). We show that such abnormal behavior fundamentally arises from the interaction of flap damping with BGP path exploration during route withdrawal. We study this interaction using a simple analytical model and understand the impact of various BGP parameters on its occurrence using simulations. Finally, we outline a preliminary proposal to modify route flap damping scheme that removes the undesired interaction in all the topologies we studied. .","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",221–233,13,"border gateway protocol, routing convergence, routing dynamics, BGP, interdomain routing protocol, route flap damping","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1749,article,"Mao, Zhuoqing Morley and Govindan, Ramesh and Varghese, George and Katz, Randy H.",Route Flap Damping Exacerbates Internet Routing Convergence,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633047,10.1145/964725.633047,"Route flap damping is considered to be a widely deployed mechanism in core routers that limits the widespread propagation of unstable BGP routing information. Originally designed to suppress route changes caused by link flaps, flap damping attempts to distinguish persistently unstable routes from routes that occasionally fail. It is considered to be a major contributor to the stability of the Internet routing system.We show in this paper that, surprisingly, route flap damping can significantly exacerbate the convergence times of relatively stable routes. For example, a route to a prefix that is withdrawn exactly once and re-announced can be suppressed for up to an hour (using the current RIPE recommended damping parameters). We show that such abnormal behavior fundamentally arises from the interaction of flap damping with BGP path exploration during route withdrawal. We study this interaction using a simple analytical model and understand the impact of various BGP parameters on its occurrence using simulations. Finally, we outline a preliminary proposal to modify route flap damping scheme that removes the undesired interaction in all the topologies we studied. .",,221–233,13,"routing dynamics, interdomain routing protocol, BGP, route flap damping, border gateway protocol, routing convergence",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1750,inproceedings,"Basu, Anindya and Ong, Chih-Hao Luke and Rasala, April and Shepherd, F. Bruce and Wilfong, Gordon",Route Oscillations in I-BGP with Route Reflection,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633048,10.1145/633025.633048,"We study the route oscillation problem [16, 19] in the Internal Border Gateway Protocol (I-BGP)[18] when route reflection is used. We propose a formal model of I-BGP and use it to show that even deciding whether an I-BGP configuration with route reflection can converge is an NP-Complete problem. We then propose a modification to I-BGP and show that route reflection cannot cause the modified protocol to diverge. Moreover, we show that the modified protocol converges to the same stable routing configuration regardless of the order in which messages are sent or received.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",235–247,13,"route reflection, route oscillations, I-BGP, stability","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1751,article,"Basu, Anindya and Ong, Chih-Hao Luke and Rasala, April and Shepherd, F. Bruce and Wilfong, Gordon",Route Oscillations in I-BGP with Route Reflection,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633048,10.1145/964725.633048,"We study the route oscillation problem [16, 19] in the Internal Border Gateway Protocol (I-BGP)[18] when route reflection is used. We propose a formal model of I-BGP and use it to show that even deciding whether an I-BGP configuration with route reflection can converge is an NP-Complete problem. We then propose a modification to I-BGP and show that route reflection cannot cause the modified protocol to diverge. Moreover, we show that the modified protocol converges to the same stable routing configuration regardless of the order in which messages are sent or received.",,235–247,13,"route reflection, route oscillations, I-BGP, stability",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1752,inproceedings,"Iyer, Sundar and Zhang, Rui and McKeown, Nick",Routers with a Single Stage of Buffering,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633050,10.1145/633025.633050,"Most high performance routers today use combined input and output queueing (CIOQ). The CIOQ router is also frequently used as an abstract model for routers: at one extreme is input queueing, at the other extreme is output queueing, and in-between there is a continuum of performance as the speedup is increased from 1 to N (where N is the number of linecards). The model includes architectures in which a switch fabric is sandwiched between two stages of buffering. There is a rich and growing theory for CIOQ routers, including algorithms, throughput results and conditions under which delays can be guaranteed. But there is a broad class of architectures that are not captured by the CIOQ model, including routers with centralized shared memory, and load-balanced routers. In this paper we propose an abstract model called Single-Buffered (SB) routers that includes these architectures. We describe a method called Constraint Sets to analyze a number of SB router architectures. The model helped identify previously unstudied architectures, in particular the Distributed Shared Memory router. Although commercially deployed, its performance is not widely known. We find conditions under which it can emulate an ideal shared memory router, and believe it to be a promising architecture. Questions remain about its complexity, but we find that the memory bandwidth, and potentially the power consumption of the router is lower than for a CIOQ router.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",251–264,14,"buffers, constraint sets, routers, switching","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1753,article,"Iyer, Sundar and Zhang, Rui and McKeown, Nick",Routers with a Single Stage of Buffering,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633050,10.1145/964725.633050,"Most high performance routers today use combined input and output queueing (CIOQ). The CIOQ router is also frequently used as an abstract model for routers: at one extreme is input queueing, at the other extreme is output queueing, and in-between there is a continuum of performance as the speedup is increased from 1 to N (where N is the number of linecards). The model includes architectures in which a switch fabric is sandwiched between two stages of buffering. There is a rich and growing theory for CIOQ routers, including algorithms, throughput results and conditions under which delays can be guaranteed. But there is a broad class of architectures that are not captured by the CIOQ model, including routers with centralized shared memory, and load-balanced routers. In this paper we propose an abstract model called Single-Buffered (SB) routers that includes these architectures. We describe a method called Constraint Sets to analyze a number of SB router architectures. The model helped identify previously unstudied architectures, in particular the Distributed Shared Memory router. Although commercially deployed, its performance is not widely known. We find conditions under which it can emulate an ideal shared memory router, and believe it to be a promising architecture. Questions remain about its complexity, but we find that the memory bandwidth, and potentially the power consumption of the router is lower than for a CIOQ router.",,251–264,14,"constraint sets, switching, buffers, routers",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1754,inproceedings,"Calvert, Kenneth L. and Griffioen, James and Wen, Su",Lightweight Network Support for Scalable End-to-End Services,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633051,10.1145/633025.633051,"Some end-to-end network services benefit greatly from network support in terms of utility and scalability. However, when such support is provided through service-specific mechanisms, the proliferation of one-off solutions tend to decrease the robustness of the network over time. Programmable routers, on the other hand, offer generic support for a variety of end-to-end services, but face a different set of challenges with respect to performance, scalability, security, and robustness. Ideally, router-based support for end-to-end services should exhibit the kind of generality, simplicity, scalability, and performance that made the Internet Protocol (IP) so successful. In this paper we present a router-based building block called ephemeral state processing (ESP), which is designed to have IP-like characteristics. ESP allows packets to create and manipulate small amounts of temporary state at routers via short, predefined computations. We discuss the issues involved in the design of such a service and describe three broad classes of problems for which ESP enables robust solutions. We also present performance measurements from a network-processor-based implementation.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",265–278,14,"ephemeral state, end-to-end services, programmable network, router achitecture","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1755,article,"Calvert, Kenneth L. and Griffioen, James and Wen, Su",Lightweight Network Support for Scalable End-to-End Services,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633051,10.1145/964725.633051,"Some end-to-end network services benefit greatly from network support in terms of utility and scalability. However, when such support is provided through service-specific mechanisms, the proliferation of one-off solutions tend to decrease the robustness of the network over time. Programmable routers, on the other hand, offer generic support for a variety of end-to-end services, but face a different set of challenges with respect to performance, scalability, security, and robustness. Ideally, router-based support for end-to-end services should exhibit the kind of generality, simplicity, scalability, and performance that made the Internet Protocol (IP) so successful. In this paper we present a router-based building block called ephemeral state processing (ESP), which is designed to have IP-like characteristics. ESP allows packets to create and manipulate small amounts of temporary state at routers via short, predefined computations. We discuss the issues involved in the design of such a service and describe three broad classes of problems for which ESP enables robust solutions. We also present performance measurements from a network-processor-based implementation.",,265–278,14,"programmable network, router achitecture, end-to-end services, ephemeral state",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1756,inproceedings,"Xu, Jun and Lipton, Richard J.",On Fundamental Tradeoffs between Delay Bounds and Computational Complexity in Packet Scheduling Algorithms,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633052,10.1145/633025.633052,"In this work, we clarify, extend and solve an open problem concerning the computational complexity for packet scheduling algorithms to achieve tight end-to-end delay bounds. We first focus on the difference between the time a packet finishes service in a scheduling algorithm and its virtual finish time under a GPS (General Processor Sharing) scheduler, called GPS-relative delay. We prove that, under a slightly restrictive but reasonable computational model, the lower bound computational complexity of any scheduling algorithm that guarantees O(1) GPS-relative delay bound is Ω (log2 n) (widely believed as a ""folklore theorem"" but never proved). We also discover that, surprisingly, the complexity lower bound remains the same even if the delay bound is relaxed to O(na) for 0‹a⋵1. This implies that the delay-complexity tradeoff curve is ""flat"" in the ""interval"" [O(1), O(n)). We later extend both complexity results (for O(1) or O(na) delay) to a much stronger computational model. Finally, we show that the same complexity lower bounds are conditionally applicable to guaranteeing tight end-to-end delay bounds. This is done by untangling the relationship between the GPS-relative delay bound and the end-to-end delay bound.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",279–292,14,"delay bound, decision tree, quality of service, computational complexity, packet scheduling","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1757,article,"Xu, Jun and Lipton, Richard J.",On Fundamental Tradeoffs between Delay Bounds and Computational Complexity in Packet Scheduling Algorithms,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633052,10.1145/964725.633052,"In this work, we clarify, extend and solve an open problem concerning the computational complexity for packet scheduling algorithms to achieve tight end-to-end delay bounds. We first focus on the difference between the time a packet finishes service in a scheduling algorithm and its virtual finish time under a GPS (General Processor Sharing) scheduler, called GPS-relative delay. We prove that, under a slightly restrictive but reasonable computational model, the lower bound computational complexity of any scheduling algorithm that guarantees O(1) GPS-relative delay bound is Ω (log2 n) (widely believed as a ""folklore theorem"" but never proved). We also discover that, surprisingly, the complexity lower bound remains the same even if the delay bound is relaxed to O(na) for 0‹a⋵1. This implies that the delay-complexity tradeoff curve is ""flat"" in the ""interval"" [O(1), O(n)). We later extend both complexity results (for O(1) or O(na) delay) to a much stronger computational model. Finally, we show that the same complexity lower bounds are conditionally applicable to guaranteeing tight end-to-end delay bounds. This is done by untangling the relationship between the GPS-relative delay bound and the end-to-end delay bound.",,279–292,14,"computational complexity, quality of service, packet scheduling, delay bound, decision tree",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1758,inproceedings,"Jain, Manish and Dovrolis, Constantinos","End-to-End Available Bandwidth: Measurement Methodology, Dynamics, and Relation with TCP Throughput",2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633054,10.1145/633025.633054,"The available bandwidth (avail-bw) in a network path is of major importance in congestion control, streaming applications, QoS verification, server selection, and overlay networks. We describe an end-to-end methodology, called Self-Loading Periodic Streams (SLoPS), for measuring avail-bw. The basic idea in SLoPS is that the one-way delays of a periodic packet stream show an increasing trend when the stream's rate is higher than the avail-bw. We implemented SLoPS in a tool called pathload. The accuracy of the tool has been evaluated with both simulations and experiments over real-world Internet paths. Pathload is non-intrusive, meaning that it does not cause significant increases in the network utilization, delays, or losses. We used pathload to evaluate the variability ('dynamics') of the avail-bw in some paths that cross USA and Europe. The avail-bw becomes significantly more variable in heavily utilized paths, as well as in paths with limited capacity (probably due to a lower degree of statistical multiplexing). We finally examine the relation between avail-bw and TCP throughput. A persistent TCP connection can be used to roughly measure the avail-bw in a path, but TCP saturates the path, and increases significantly the path delays and jitter.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",295–308,14,"packet pair dispersion, network capacity, bulk transfer capacity, bottleneck bandwidth, active probing","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1759,article,"Jain, Manish and Dovrolis, Constantinos","End-to-End Available Bandwidth: Measurement Methodology, Dynamics, and Relation with TCP Throughput",2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633054,10.1145/964725.633054,"The available bandwidth (avail-bw) in a network path is of major importance in congestion control, streaming applications, QoS verification, server selection, and overlay networks. We describe an end-to-end methodology, called Self-Loading Periodic Streams (SLoPS), for measuring avail-bw. The basic idea in SLoPS is that the one-way delays of a periodic packet stream show an increasing trend when the stream's rate is higher than the avail-bw. We implemented SLoPS in a tool called pathload. The accuracy of the tool has been evaluated with both simulations and experiments over real-world Internet paths. Pathload is non-intrusive, meaning that it does not cause significant increases in the network utilization, delays, or losses. We used pathload to evaluate the variability ('dynamics') of the avail-bw in some paths that cross USA and Europe. The avail-bw becomes significantly more variable in heavily utilized paths, as well as in paths with limited capacity (probably due to a lower degree of statistical multiplexing). We finally examine the relation between avail-bw and TCP throughput. A persistent TCP connection can be used to roughly measure the avail-bw in a path, but TCP saturates the path, and increases significantly the path delays and jitter.",,295–308,14,"bottleneck bandwidth, network capacity, packet pair dispersion, active probing, bulk transfer capacity",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1760,inproceedings,"Zhang, Yin and Breslau, Lee and Paxson, Vern and Shenker, Scott",On the Characteristics and Origins of Internet Flow Rates,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633055,10.1145/633025.633055,"This paper considers the distribution of the rates at which flows transmit data, and the causes of these rates. First, using packet level traces from several Internet links, and summary flow statistics from an ISP backbone, we examine Internet flow rates and the relationship between the rate and other flow characteristics such as size and duration. We find, as have others, that while the distribution of flow rates is skewed, it is not as highly skewed as the distribution of flow sizes. We also find that for large flows the size and rate are highly correlated. Second, we attempt to determine the cause of the rates at which flows transmit data by developing a tool, T-RAT, to analyze packet-level TCP dynamics. In our traces, the most frequent causes appear to be network congestion and receiver window limits.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",309–322,14,"TCP, flow rates, network measurement","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1761,article,"Zhang, Yin and Breslau, Lee and Paxson, Vern and Shenker, Scott",On the Characteristics and Origins of Internet Flow Rates,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633055,10.1145/964725.633055,"This paper considers the distribution of the rates at which flows transmit data, and the causes of these rates. First, using packet level traces from several Internet links, and summary flow statistics from an ISP backbone, we examine Internet flow rates and the relationship between the rate and other flow characteristics such as size and duration. We find, as have others, that while the distribution of flow rates is skewed, it is not as highly skewed as the distribution of flow sizes. We also find that for large flows the size and rate are highly correlated. Second, we attempt to determine the cause of the rates at which flows transmit data by developing a tool, T-RAT, to analyze packet-level TCP dynamics. In our traces, the most frequent causes appear to be network congestion and receiver window limits.",,309–322,14,"flow rates, network measurement, TCP",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1762,inproceedings,"Estan, Cristian and Varghese, George",New Directions in Traffic Measurement and Accounting,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633056,10.1145/633025.633056,"Accurate network traffic measurement is required for accounting, bandwidth provisioning and detecting DoS attacks. These applications see the traffic as a collection of flows they need to measure. As link speeds and the number of flows increase, keeping a counter for each flow is too expensive (using SRAM) or slow (using DRAM). The current state-of-the-art methods (Cisco's sampled NetFlow) which log periodically sampled packets are slow, inaccurate and resource-intensive. Previous work showed that at different granularities a small number of ""heavy hitters"" accounts for a large share of traffic. Our paper introduces a paradigm shift for measurement by concentrating only on large flows --- those above some threshold such as 0.1% of the link capacity.We propose two novel and scalable algorithms for identifying the large flows: sample and hold and multistage filters, which take a constant number of memory references per packet and use a small amount of memory. If $M$ is the available memory, we show analytically that the errors of our new algorithms are proportional to $1/M$; by contrast, the error of an algorithm based on classical sampling is proportional to $1/sqrtM$, thus providing much less accuracy for the same amount of memory. We also describe further optimizations such as early removal and conservative update that further improve the accuracy of our algorithms, as measured on real traffic traces, by an order of magnitude. Our schemes allow a new form of accounting called threshold accounting in which only flows above a threshold are charged by usage while the rest are charged a fixed fee. Threshold accounting generalizes usage-based and duration based pricing.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",323–336,14,"on-line algorithms, network traffic measurement, scalability, identifying large flows, usage based accounting","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1763,article,"Estan, Cristian and Varghese, George",New Directions in Traffic Measurement and Accounting,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633056,10.1145/964725.633056,"Accurate network traffic measurement is required for accounting, bandwidth provisioning and detecting DoS attacks. These applications see the traffic as a collection of flows they need to measure. As link speeds and the number of flows increase, keeping a counter for each flow is too expensive (using SRAM) or slow (using DRAM). The current state-of-the-art methods (Cisco's sampled NetFlow) which log periodically sampled packets are slow, inaccurate and resource-intensive. Previous work showed that at different granularities a small number of ""heavy hitters"" accounts for a large share of traffic. Our paper introduces a paradigm shift for measurement by concentrating only on large flows --- those above some threshold such as 0.1% of the link capacity.We propose two novel and scalable algorithms for identifying the large flows: sample and hold and multistage filters, which take a constant number of memory references per packet and use a small amount of memory. If $M$ is the available memory, we show analytically that the errors of our new algorithms are proportional to $1/M$; by contrast, the error of an algorithm based on classical sampling is proportional to $1/sqrtM$, thus providing much less accuracy for the same amount of memory. We also describe further optimizations such as early removal and conservative update that further improve the accuracy of our algorithms, as measured on real traffic traces, by an order of magnitude. Our schemes allow a new form of accounting called threshold accounting in which only flows above a threshold are charged by usage while the rest are charged a fixed fee. Threshold accounting generalizes usage-based and duration based pricing.",,323–336,14,"scalability, usage based accounting, on-line algorithms, network traffic measurement, identifying large flows",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1764,inproceedings,"Beck, Micah and Moore, Terry and Plank, James S.",An End-to-End Approach to Globally Scalable Network Storage,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633058,10.1145/633025.633058,"This paper discusses the application of end-to-end design principles, which are characteristic of the architecture of the Internet, to network storage. While putting storage into the network fabric may seem to contradict end-to-end arguments, we try to show not only that there is no contradiction, but also that adherence to such an approach is the key to achieving true scalability of shared network storage. After discussing end-to-end arguments with respect to several properties of network storage, we describe the Internet Backplane Protocol and the exNode, which are tools that have been designed to create a network storage substrate that adheres to these principles. The name for this approach is Logistical Networking, and we believe its use is fundamental to the future of truly scalable communication.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",339–346,8,"end-to-end design, exNode, logistical networking, network storage, store and forward network, wide area storage, scalability, internet backplane protocol, asynchronous communications, IBP","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1765,article,"Beck, Micah and Moore, Terry and Plank, James S.",An End-to-End Approach to Globally Scalable Network Storage,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633058,10.1145/964725.633058,"This paper discusses the application of end-to-end design principles, which are characteristic of the architecture of the Internet, to network storage. While putting storage into the network fabric may seem to contradict end-to-end arguments, we try to show not only that there is no contradiction, but also that adherence to such an approach is the key to achieving true scalability of shared network storage. After discussing end-to-end arguments with respect to several properties of network storage, we describe the Internet Backplane Protocol and the exNode, which are tools that have been designed to create a network storage substrate that adheres to these principles. The name for this approach is Logistical Networking, and we believe its use is fundamental to the future of truly scalable communication.",,339–346,8,"scalability, IBP, network storage, internet backplane protocol, wide area storage, logistical networking, exNode, end-to-end design, asynchronous communications, store and forward network",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1766,inproceedings,"Clark, David D. and Wroclawski, John and Sollins, Karen R. and Braden, Robert",Tussle in Cyberspace: Defining Tomorrow's Internet,2002,158113570X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/633025.633059,10.1145/633025.633059,"The architecture of the Internet is based on a number of principles, including the self-describing datagram packet, the end to end arguments, diversity in technology and global addressing. As the Internet has moved from a research curiosity to a recognized component of mainstream society, new requirements have emerged that suggest new design principles, and perhaps suggest that we revisit some old ones. This paper explores one important reality that surrounds the Internet today: different stakeholders that are part of the Internet milieu have interests that may be adverse to each other, and these parties each vie to favor their particular interests. We call this process ""the tussle"". Our position is that accommodating this tussle is crucial to the evolution of the network's technical architecture. We discuss some examples of tussle, and offer some technical design principles that take it into account.","Proceedings of the 2002 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",347–356,10,"principles, competition, network architecture, tussle, economics, trust, design","Pittsburgh, Pennsylvania, USA",SIGCOMM '02,,,,,,
1767,article,"Clark, David D. and Wroclawski, John and Sollins, Karen R. and Braden, Robert",Tussle in Cyberspace: Defining Tomorrow's Internet,2002,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964725.633059,10.1145/964725.633059,"The architecture of the Internet is based on a number of principles, including the self-describing datagram packet, the end to end arguments, diversity in technology and global addressing. As the Internet has moved from a research curiosity to a recognized component of mainstream society, new requirements have emerged that suggest new design principles, and perhaps suggest that we revisit some old ones. This paper explores one important reality that surrounds the Internet today: different stakeholders that are part of the Internet milieu have interests that may be adverse to each other, and these parties each vie to favor their particular interests. We call this process ""the tussle"". Our position is that accommodating this tussle is crucial to the evolution of the network's technical architecture. We discuss some examples of tussle, and offer some technical design principles that take it into account.",,347–356,10,"competition, design, trust, tussle, network architecture, principles, economics",,,October 2002,32,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1768,inproceedings,"Snoeren, Alex C. and Partridge, Craig and Sanchez, Luis A. and Jones, Christine E. and Tchakountio, Fabrice and Kent, Stephen T. and Strayer, W. Timothy",Hash-Based IP Traceback,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383060,10.1145/383059.383060,"The design of the IP protocol makes it difficult to reliably identify the originator of an IP packet. Even in the absence of any deliberate attempt to disguise a packet's origin, wide-spread packet forwarding techniques such as NAT and encapsulation may obscure the packet's true source. Techniques have been developed to determine the source of large packet flows, but, to date, no system has been presented to track individual packets in an efficient, scalable fashion.We present a hash-based technique for IP traceback that generates audit trails for traffic within the network, and can trace the origin of a single IP packet delivered by the network in the recent past. We demonstrate that the system is effective, space-efficient (requiring approximately 0.5% of the link capacity per unit time in storage), and implementable in current or next-generation routing hardware. We present both analytic and simulation results showing the system's effectiveness.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",3–14,12,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1769,article,"Snoeren, Alex C. and Partridge, Craig and Sanchez, Luis A. and Jones, Christine E. and Tchakountio, Fabrice and Kent, Stephen T. and Strayer, W. Timothy",Hash-Based IP Traceback,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383060,10.1145/964723.383060,"The design of the IP protocol makes it difficult to reliably identify the originator of an IP packet. Even in the absence of any deliberate attempt to disguise a packet's origin, wide-spread packet forwarding techniques such as NAT and encapsulation may obscure the packet's true source. Techniques have been developed to determine the source of large packet flows, but, to date, no system has been presented to track individual packets in an efficient, scalable fashion.We present a hash-based technique for IP traceback that generates audit trails for traffic within the network, and can trace the origin of a single IP packet delivered by the network in the recent past. We demonstrate that the system is effective, space-efficient (requiring approximately 0.5% of the link capacity per unit time in storage), and implementable in current or next-generation routing hardware. We present both analytic and simulation results showing the system's effectiveness.",,3–14,12,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1770,inproceedings,"Park, Kihong and Lee, Heejo",On the Effectiveness of Route-Based Packet Filtering for Distributed DoS Attack Prevention in Power-Law Internets,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383061,10.1145/383059.383061,"Denial of service (DoS) attack on the Internet has become a pressing problem. In this paper, we describe and evaluate route-based distributed packet filtering (DPF), a novel approach to distributed DoS (DDoS) attack prevention. We show that DPF achieves proactiveness and scalability, and we show that there is an intimate relationship between the effectiveness of DPF at mitigating DDoS attack and power-law network topology.The salient features of this work are two-fold. First, we show that DPF is able to proactively filter out a significant fraction of spoofed packet flows and prevent attack packets from reaching their targets in the first place. The IP flows that cannot be proactively curtailed are extremely sparse so that their origin can be localized---i.e., IP traceback---to within a small, constant number of candidate sites. We show that the two proactive and reactive performance effects can be achieved by implementing route-based filtering on less than 20% of Internet autonomous system (AS) sites. Second, we show that the two complementary performance measures are dependent on the properties of the underlying AS graph. In particular, we show that the power-law structure of Internet AS topology leads to connectivity properties which are crucial in facilitating the observed performance effects.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",15–26,12,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1771,article,"Park, Kihong and Lee, Heejo",On the Effectiveness of Route-Based Packet Filtering for Distributed DoS Attack Prevention in Power-Law Internets,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383061,10.1145/964723.383061,"Denial of service (DoS) attack on the Internet has become a pressing problem. In this paper, we describe and evaluate route-based distributed packet filtering (DPF), a novel approach to distributed DoS (DDoS) attack prevention. We show that DPF achieves proactiveness and scalability, and we show that there is an intimate relationship between the effectiveness of DPF at mitigating DDoS attack and power-law network topology.The salient features of this work are two-fold. First, we show that DPF is able to proactively filter out a significant fraction of spoofed packet flows and prevent attack packets from reaching their targets in the first place. The IP flows that cannot be proactively curtailed are extremely sparse so that their origin can be localized---i.e., IP traceback---to within a small, constant number of candidate sites. We show that the two proactive and reactive performance effects can be achieved by implementing route-based filtering on less than 20% of Internet autonomous system (AS) sites. Second, we show that the two complementary performance measures are dependent on the properties of the underlying AS graph. In particular, we show that the power-law structure of Internet AS topology leads to connectivity properties which are crucial in facilitating the observed performance effects.",,15–26,12,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1772,inproceedings,"Yang, Yang Richard and Li, X. Steve and Zhang, X. Brian and Lam, Simon S.",Reliable Group Rekeying: A Performance Analysis,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383062,10.1145/383059.383062,"In secure group communications, users of a group share a common group key. A key server sends the group key to authorized new users as well as performs group rekeying for group users whenever the key changes. In this paper, we investigate scalability issues of reliable group rekeying, and provide a performance analysis of our group key management system (called keygem) based upon the use of key trees. Instead of rekeying after each join or leave, we use periodic batch rekeying to improve scalability and alleviate out-of-sync problems among rekey messages as well as between rekey and data messages. Our analyses show that batch rekeying can achieve large performance gains. We then investigate reliable multicast of rekey messages using proactive FEC. We observe that rekey transport has an eventual reliability and a soft real-time requirement, and that the rekey workload has a sparseness property, that is, each group user only needs to receive a small fraction of the packets that carry a rekey message sent by the key server. We also investigate tradeoffs between server and receiver bandwidth requirements versus group rekey interval, and show how to determine the maximum number of group users a key server can support.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",27–38,12,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1773,article,"Yang, Yang Richard and Li, X. Steve and Zhang, X. Brian and Lam, Simon S.",Reliable Group Rekeying: A Performance Analysis,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383062,10.1145/964723.383062,"In secure group communications, users of a group share a common group key. A key server sends the group key to authorized new users as well as performs group rekeying for group users whenever the key changes. In this paper, we investigate scalability issues of reliable group rekeying, and provide a performance analysis of our group key management system (called keygem) based upon the use of key trees. Instead of rekeying after each join or leave, we use periodic batch rekeying to improve scalability and alleviate out-of-sync problems among rekey messages as well as between rekey and data messages. Our analyses show that batch rekeying can achieve large performance gains. We then investigate reliable multicast of rekey messages using proactive FEC. We observe that rekey transport has an eventual reliability and a soft real-time requirement, and that the rekey workload has a sparseness property, that is, each group user only needs to receive a small fraction of the packets that carry a rekey message sent by the key server. We also investigate tradeoffs between server and receiver bandwidth requirements versus group rekey interval, and show how to determine the maximum number of group users a key server can support.",,27–38,12,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1774,inproceedings,"Cohen, Edith and Kaplan, Haim",Aging through Cascaded Caches: Performance Issues in the Distribution of Web Content,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383063,10.1145/383059.383063,"The Web is a distributed system, where data is stored and disseminated from both origin servers and caches. Origin servers provide the most up-to-date copy whereas caches store and serve copies that had been cached for a while. Origin servers do not maintain per-client state, and weak-consistency of cached copies is maintained by the origin server attaching to each copy an expiration time. Typically, the lifetime-duration of an object is fixed, and as a result, a copy fetched directly from its origin server has maximum time-to-live (TTL) whereas a copy obtained through a cache has a shorter TTL since its age (elapsed time since fetched from the origin) is deducted from its lifetime duration. Thus, a cache that is served from a cache would incur a higher miss-rate than a cache served from origin servers. Similarly, a high-level cache would receive more requests from the same client population than an origin server would have received. As Web caches are often served from other caches (e.g., proxy and reverse-proxy caches), age emerges as a performance factor. Guided by a formal model and analysis, we use different inter-request time distributions and trace-based simulations to explore the effect of age for different cache settings and configurations. We also evaluate the effectiveness of frequent pre-term refreshes by higher-level caches as a means to decrease client misses. Beyond Web content distribution, our conclusions generally apply to systems of caches deploying expiration-based consistency.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",41–53,13,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1775,article,"Cohen, Edith and Kaplan, Haim",Aging through Cascaded Caches: Performance Issues in the Distribution of Web Content,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383063,10.1145/964723.383063,"The Web is a distributed system, where data is stored and disseminated from both origin servers and caches. Origin servers provide the most up-to-date copy whereas caches store and serve copies that had been cached for a while. Origin servers do not maintain per-client state, and weak-consistency of cached copies is maintained by the origin server attaching to each copy an expiration time. Typically, the lifetime-duration of an object is fixed, and as a result, a copy fetched directly from its origin server has maximum time-to-live (TTL) whereas a copy obtained through a cache has a shorter TTL since its age (elapsed time since fetched from the origin) is deducted from its lifetime duration. Thus, a cache that is served from a cache would incur a higher miss-rate than a cache served from origin servers. Similarly, a high-level cache would receive more requests from the same client population than an origin server would have received. As Web caches are often served from other caches (e.g., proxy and reverse-proxy caches), age emerges as a performance factor. Guided by a formal model and analysis, we use different inter-request time distributions and trace-based simulations to explore the effect of age for different cache settings and configurations. We also evaluate the effectiveness of frequent pre-term refreshes by higher-level caches as a means to decrease client misses. Beyond Web content distribution, our conclusions generally apply to systems of caches deploying expiration-based consistency.",,41–53,13,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1776,inproceedings,"Chu, Yang and Rao, Sanjay and Seshan, Srinivasan and Zhang, Hui",Enabling Conferencing Applications on the Internet Using an Overlay Muilticast Architecture,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383064,10.1145/383059.383064,"In response to the serious scalability and deployment concerns with IP Multicast, we and other researchers have advocated an alternate architecture for supporting group communication applications over the Internet where all multicast functionality is pushed to the edge. We refer to such an architecture as End System Multicast. While End System Multicast has several potential advantages, a key concern is the performance penalty associated with such a design. While preliminary simulation results conducted in static environments are promising, they have yet to consider the challenging performance requirements of real world applications in a dynamic and heterogeneous Internet environment.In this paper, we explore how Internet environments and application requirements can influence End System Multicast design. We explore these issues in the context of audio and video conferencing: an important class of applications with stringent performance requirements. We conduct an extensive evaluation study of schemes for constructing overlay networks on a wide-area test-bed of about twenty hosts distributed around the Internet. Our results demonstrate that it is important to adapt to both latency and bandwidth while constructing overlays optimized for conferencing applications. Further, when relatively simple techniques are incorporated into current self-organizing protocols to enable dynamic adaptation to latency and bandwidth, the performance benefits are significant. Our results indicate that End System Multicast is a promising architecture for enabling performance-demanding conferencing applications in a dynamic and heterogeneous Internet environment.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",55–67,13,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1777,article,"Chu, Yang and Rao, Sanjay and Seshan, Srinivasan and Zhang, Hui",Enabling Conferencing Applications on the Internet Using an Overlay Muilticast Architecture,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383064,10.1145/964723.383064,"In response to the serious scalability and deployment concerns with IP Multicast, we and other researchers have advocated an alternate architecture for supporting group communication applications over the Internet where all multicast functionality is pushed to the edge. We refer to such an architecture as End System Multicast. While End System Multicast has several potential advantages, a key concern is the performance penalty associated with such a design. While preliminary simulation results conducted in static environments are promising, they have yet to consider the challenging performance requirements of real world applications in a dynamic and heterogeneous Internet environment.In this paper, we explore how Internet environments and application requirements can influence End System Multicast design. We explore these issues in the context of audio and video conferencing: an important class of applications with stringent performance requirements. We conduct an extensive evaluation study of schemes for constructing overlay networks on a wide-area test-bed of about twenty hosts distributed around the Internet. Our results demonstrate that it is important to adapt to both latency and bandwidth while constructing overlays optimized for conferencing applications. Further, when relatively simple techniques are incorporated into current self-organizing protocols to enable dynamic adaptation to latency and bandwidth, the performance benefits are significant. Our results indicate that End System Multicast is a promising architecture for enabling performance-demanding conferencing applications in a dynamic and heterogeneous Internet environment.",,55–67,13,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1778,inproceedings,"Francis, Paul and Gummadi, Ramakrishna",IPNL: A NAT-Extended Internet Architecture,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383065,10.1145/383059.383065,"This paper presents and analyzes IPNL (for IP Next Layer), a NAT-extended Internet protocol architecture designed to scalably solve the address depletion problem of IPv4. A NAT-extended architecture is one where only hosts and NAT boxes are modified. IPv4 routers and support protocols remain untouched. IPNL attempts to maintain all of the original characteristics of IPv4, most notably address prefix location independence. IPNL provides true site isolation (no renumbering), and allows sites to be multi-homed without polluting the default-free routing zone with per-site prefixes. We discuss IPNL's architectural benefits and drawbacks, and show that it comes acceptably close to achieving its goals.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",69–80,12,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1779,article,"Francis, Paul and Gummadi, Ramakrishna",IPNL: A NAT-Extended Internet Architecture,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383065,10.1145/964723.383065,"This paper presents and analyzes IPNL (for IP Next Layer), a NAT-extended Internet protocol architecture designed to scalably solve the address depletion problem of IPv4. A NAT-extended architecture is one where only hosts and NAT boxes are modified. IPv4 routers and support protocols remain untouched. IPNL attempts to maintain all of the original characteristics of IPv4, most notably address prefix location independence. IPNL provides true site isolation (no renumbering), and allows sites to be multi-homed without polluting the default-free routing zone with per-site prefixes. We discuss IPNL's architectural benefits and drawbacks, and show that it comes acceptably close to achieving its goals.",,69–80,12,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1780,inproceedings,"Ashmawi, Wael and Guerin, Roch and Wolf, Stephen and Pinson, Margaret",On the Impact of Policing and Rate Guarantees in DiffServ Networks: A Video Streaming Application Perspective,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383066,10.1145/383059.383066,"Over the past few years, there have been a number of proposals aimed at introducing different levels of service in the Internet. One of the more recent proposals is the Differentiated Services (Diff-Serv) architecture, and in this paper we explore how the policing actions and associated rate guarantees provided by the Expedited Forwarding (EF) translate into perceived benefits for applications that are the presumed users of such enhancements. Specifically, we focus on video streaming applications that arguably have relatively strong service quality requirements, and which should, therefore, stand to benefit from the availability of some form of enhanced service. Our goal is to gain a better understanding of the relation that exists between application level quality measures and the selection of the network level parameters that govern the delivery of the guarantees that an EF based service would provide. Our investigation, which is experimental in nature, relies on a number of standard streaming video servers and clients that have been modified and instrumented to allow quantification of the perceived quality of the received video stream. Quality assessments are performed using a Video Quality Measurement tool based on the ANSI objective quality standard. Measurements were made over both a local Diff-Serv testbed and across the QBone, a QoS enabled segment of the Internet2 infrastructure. The paper reports and analyzes the results of those measurements.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",83–95,13,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1781,article,"Ashmawi, Wael and Guerin, Roch and Wolf, Stephen and Pinson, Margaret",On the Impact of Policing and Rate Guarantees in DiffServ Networks: A Video Streaming Application Perspective,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383066,10.1145/964723.383066,"Over the past few years, there have been a number of proposals aimed at introducing different levels of service in the Internet. One of the more recent proposals is the Differentiated Services (Diff-Serv) architecture, and in this paper we explore how the policing actions and associated rate guarantees provided by the Expedited Forwarding (EF) translate into perceived benefits for applications that are the presumed users of such enhancements. Specifically, we focus on video streaming applications that arguably have relatively strong service quality requirements, and which should, therefore, stand to benefit from the availability of some form of enhanced service. Our goal is to gain a better understanding of the relation that exists between application level quality measures and the selection of the network level parameters that govern the delivery of the guarantees that an EF based service would provide. Our investigation, which is experimental in nature, relies on a number of standard streaming video servers and clients that have been modified and instrumented to allow quantification of the perceived quality of the received video stream. Quality assessments are performed using a Video Quality Measurement tool based on the ANSI objective quality standard. Measurements were made over both a local Diff-Serv testbed and across the QBone, a QoS enabled segment of the Internet2 infrastructure. The paper reports and analyzes the results of those measurements.",,83–95,13,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1782,inproceedings,"Mahanti, Anirban and Eager, Derek L. and Vernon, Mary K. and Sundaram-Stukel, David",Scalable On-Demand Media Streaming with Packet Loss Recovery,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383067,10.1145/383059.383067,"Inspired by recent techniques for reliable bulk data distribution, this paper develops scalable protocols for reliable on-demand delivery of streaming media. Models are developed that quantify the best possible scalability for given client characteristics. The results of the models are used to guide the design and assess the performance of the proposed streaming techniques. The new protocols, RPB and RBS, are relatively simple to implement and achieve nearly the best possible scalability and efficiency for a given set of client characteristics and desirable/feasible media quality.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",97–108,12,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1783,article,"Mahanti, Anirban and Eager, Derek L. and Vernon, Mary K. and Sundaram-Stukel, David",Scalable On-Demand Media Streaming with Packet Loss Recovery,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383067,10.1145/964723.383067,"Inspired by recent techniques for reliable bulk data distribution, this paper develops scalable protocols for reliable on-demand delivery of streaming media. Models are developed that quantify the best possible scalability for given client characteristics. The results of the models are used to guide the design and assess the performance of the proposed streaming techniques. The new protocols, RPB and RBS, are relatively simple to implement and achieve nearly the best possible scalability and efficiency for a given set of client characteristics and desirable/feasible media quality.",,97–108,12,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1784,inproceedings,"Fred, S. Ben and Bonald, T. and Proutiere, A. and R\'{e",Statistical Bandwidth Sharing: A Study of Congestion at Flow Level,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383068,10.1145/383059.383068,"In this paper we study the statistics of the realized throughput of elastic document transfers, accounting for the way network bandwidth is shared dynamically between the randomly varying number of concurrent flows. We first discuss the way TCP realizes statistical bandwidth sharing, illustrating essential properties by means of packet level simulations. Mathematical flow level models based on the theory of stochastic networks are then proposed to explain the observed behavior. A notable result is that first order performance (e.g., mean throughput) is insensitive with respect both to the flow size distribution and the flow arrival process, as long as ""sessions"" arrive according to a Poisson process. Perceived performance is shown to depend most significantly on whether demand at flow level is less than or greater than available capacity. The models provide a key to understanding the effectiveness of techniques for congestion management and service differentiation.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",111–122,12,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1785,article,"Fred, S. Ben and Bonald, T. and Proutiere, A. and R\'{e",Statistical Bandwidth Sharing: A Study of Congestion at Flow Level,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383068,10.1145/964723.383068,"In this paper we study the statistics of the realized throughput of elastic document transfers, accounting for the way network bandwidth is shared dynamically between the randomly varying number of concurrent flows. We first discuss the way TCP realizes statistical bandwidth sharing, illustrating essential properties by means of packet level simulations. Mathematical flow level models based on the theory of stochastic networks are then proposed to explain the observed behavior. A notable result is that first order performance (e.g., mean throughput) is insensitive with respect both to the flow size distribution and the flow arrival process, as long as ""sessions"" arrive according to a Poisson process. Perceived performance is shown to depend most significantly on whether demand at flow level is less than or greater than available capacity. The models provide a key to understanding the effectiveness of techniques for congestion management and service differentiation.",,111–122,12,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1786,inproceedings,"Kunniyur, Srisankar and Srikant, R.",Analysis and Design of an Adaptive Virtual Queue (AVQ) Algorithm for Active Queue Management,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383069,10.1145/383059.383069,"Virtual Queue-based marking schemes have been recently proposed for AQM (Active Queue Management) in Internet routers. We consider a particular scheme, which we call the Adaptive Virtual Queue (AVQ), and study its following properties: stability in the presence of feedback delays, its ability to maintain small queue lengths and its robustness in the presence of extremely short flows (the so-called web mice). Using a mathematical tool motivated by the earlier work of Hollot et al, we present a simple rule to design the parameters of the AVQ algorithm. We then compare its performance through simulation with several well-known AQM schemes such as RED, REM, PI controller and a non-adaptive virtual queue algorithm. With a view towards implementation, we show that AVQ can be implemented as a simple token bucket using only a few lines of code.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",123–134,12,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1787,article,"Kunniyur, Srisankar and Srikant, R.",Analysis and Design of an Adaptive Virtual Queue (AVQ) Algorithm for Active Queue Management,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383069,10.1145/964723.383069,"Virtual Queue-based marking schemes have been recently proposed for AQM (Active Queue Management) in Internet routers. We consider a particular scheme, which we call the Adaptive Virtual Queue (AVQ), and study its following properties: stability in the presence of feedback delays, its ability to maintain small queue lengths and its robustness in the presence of extremely short flows (the so-called web mice). Using a mathematical tool motivated by the earlier work of Hollot et al, we present a simple rule to design the parameters of the AVQ algorithm. We then compare its performance through simulation with several well-known AQM schemes such as RED, REM, PI controller and a non-adaptive virtual queue algorithm. With a view towards implementation, we show that AVQ can be implemented as a simple token bucket using only a few lines of code.",,123–134,12,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1788,inproceedings,"Kumar, Amit and Rastogi, Rajeev and Silberschatz, Avi and Yener, Bulent",Algorithms for Provisioning Virtual Private Networks in the Hose Model,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383070,10.1145/383059.383070,"Virtual Private Networks (VPNs) provide customers with predictable and secure network connections over a shared network. The recently proposed hose model for VPNs allows for greater flexibility since it permits traffic to and from a hose endpoint to be arbitrarily distributed to other endpoints. In this paper, we develop novel algorithms for provisioning VPNs in the hose model. We connect VPN endpoints using a tree structure and our algorithms attempt to optimize the total bandwidth reserved on edges of the VPN tree. We show that even for the simple scenario in which network links are assumed to have infinite capacity, the general problem of computing the optimal VPN tree is NP hard. Fortunately, for the special case when the ingress and egress bandwidths for each VPN endpoint are equal, we can devise an algorithm for computing the optimal tree whose time complexity is O (mn), where m and n are the number of links and nodes in the network, respectively. We present a novel integer programming formulation for the general VPN tree computation problem (that is, when ingress and egress bandwidths of VPN endpoints are arbitrary) and develop an algorithm that is based on the primal-dual method. Our experimental results with synthetic network graphs indicate that the VPN trees constructed by our proposed algorithms dramatically reduce bandwidth requirements (in many instances, by more than a factor of 2) compared to scenarios in which Steiner trees are employed to connect VPN endpoints.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",135–146,12,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1789,article,"Kumar, Amit and Rastogi, Rajeev and Silberschatz, Avi and Yener, Bulent",Algorithms for Provisioning Virtual Private Networks in the Hose Model,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383070,10.1145/964723.383070,"Virtual Private Networks (VPNs) provide customers with predictable and secure network connections over a shared network. The recently proposed hose model for VPNs allows for greater flexibility since it permits traffic to and from a hose endpoint to be arbitrarily distributed to other endpoints. In this paper, we develop novel algorithms for provisioning VPNs in the hose model. We connect VPN endpoints using a tree structure and our algorithms attempt to optimize the total bandwidth reserved on edges of the VPN tree. We show that even for the simple scenario in which network links are assumed to have infinite capacity, the general problem of computing the optimal VPN tree is NP hard. Fortunately, for the special case when the ingress and egress bandwidths for each VPN endpoint are equal, we can devise an algorithm for computing the optimal tree whose time complexity is O (mn), where m and n are the number of links and nodes in the network, respectively. We present a novel integer programming formulation for the general VPN tree computation problem (that is, when ingress and egress bandwidths of VPN endpoints are arbitrary) and develop an algorithm that is based on the primal-dual method. Our experimental results with synthetic network graphs indicate that the VPN trees constructed by our proposed algorithms dramatically reduce bandwidth requirements (in many instances, by more than a factor of 2) compared to scenarios in which Steiner trees are employed to connect VPN endpoints.",,135–146,12,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1790,inproceedings,"Stoica, Ion and Morris, Robert and Karger, David and Kaashoek, M. Frans and Balakrishnan, Hari",Chord: A Scalable Peer-to-Peer Lookup Service for Internet Applications,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383071,10.1145/383059.383071,"A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",149–160,12,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1791,article,"Stoica, Ion and Morris, Robert and Karger, David and Kaashoek, M. Frans and Balakrishnan, Hari",Chord: A Scalable Peer-to-Peer Lookup Service for Internet Applications,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383071,10.1145/964723.383071,"A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.",,149–160,12,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1792,inproceedings,"Ratnasamy, Sylvia and Francis, Paul and Handley, Mark and Karp, Richard and Shenker, Scott",A Scalable Content-Addressable Network,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383072,10.1145/383059.383072,"Hash tables - which map ""keys"" onto ""values"" - are an essential building block in modern software systems. We believe a similar functionality would be equally valuable to large distributed systems. In this paper, we introduce the concept of a Content-Addressable Network (CAN) as a distributed infrastructure that provides hash table-like functionality on Internet-like scales. The CAN is scalable, fault-tolerant and completely self-organizing, and we demonstrate its scalability, robustness and low-latency properties through simulation.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",161–172,12,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1793,article,"Ratnasamy, Sylvia and Francis, Paul and Handley, Mark and Karp, Richard and Shenker, Scott",A Scalable Content-Addressable Network,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383072,10.1145/964723.383072,"Hash tables - which map ""keys"" onto ""values"" - are an essential building block in modern software systems. We believe a similar functionality would be equally valuable to large distributed systems. In this paper, we introduce the concept of a Content-Addressable Network (CAN) as a distributed infrastructure that provides hash table-like functionality on Internet-like scales. The CAN is scalable, fault-tolerant and completely self-organizing, and we demonstrate its scalability, robustness and low-latency properties through simulation.",,161–172,12,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1794,inproceedings,"Padmanabhan, Venkata N. and Subramanian, Lakshminarayanan",An Investigation  of Geographic Mapping Techniques for Internet Hosts,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383073,10.1145/383059.383073,"In this paper, we ask whether it is possible to build an IP address to geographic location mapping service for Internet hosts. Such a service would enable a large and interesting class of location-aware applications. This is a challenging problem because an IP address does not inherently contain an indication of location.We present and evaluate three distinct techniques, collectively referred to as IP2Geo, for determining the geographic location of Internet hosts. The first technique, Geo Track, infers location based on the DNS names of the target host or other nearby network nodes. The second technique, GeoPing, uses network delay measurements from geographically distributed locations to deduce the coordinates of the target host. The third technique, GeoCluster, combines partial (and possibly inaccurate) host-to-location mapping information and BGP prefix information to infer the location of the target host. Using extensive and varied data sets, we evaluate the performance of these techniques and identify fundamental challenges in deducing geographic location from the IP address of an Internet host.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",173–185,13,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1795,article,"Padmanabhan, Venkata N. and Subramanian, Lakshminarayanan",An Investigation  of Geographic Mapping Techniques for Internet Hosts,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383073,10.1145/964723.383073,"In this paper, we ask whether it is possible to build an IP address to geographic location mapping service for Internet hosts. Such a service would enable a large and interesting class of location-aware applications. This is a challenging problem because an IP address does not inherently contain an indication of location.We present and evaluate three distinct techniques, collectively referred to as IP2Geo, for determining the geographic location of Internet hosts. The first technique, Geo Track, infers location based on the DNS names of the target host or other nearby network nodes. The second technique, GeoPing, uses network delay measurements from geographically distributed locations to deduce the coordinates of the target host. The third technique, GeoCluster, combines partial (and possibly inaccurate) host-to-location mapping information and BGP prefix information to infer the location of the target host. Using extensive and varied data sets, we evaluate the performance of these techniques and identify fundamental challenges in deducing geographic location from the IP address of an Internet host.",,173–185,13,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1796,inproceedings,"Blanquer, Josep M. and \""{O",Fair Queuing for Aggregated Multiple Links,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383074,10.1145/383059.383074,"Provisioning of a shared server with guarantees is an important scheduling task that has led to significant work in a number of areas including link scheduling. Fair Queuing algorithms provide a method for proportionally sharing a single server among competing flows, however, they do not address the problem of sharing multiple servers. Multiserver systems arise in a number of applications including link aggregation, multiprocessors and multi-path storage I/O. In this paper we introduce a new service discipline for multi-server systems that provides guarantees for competing flows. We prove that this new service discipline is a close approximation of the idealized Generalized Processor Sharing (GPS) discipline. We calculate its maximum packet delay and service discrepancy with respect to GPS. We also discuss its relevance to several applications, in particular, Ethernet link aggregation.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",189–197,9,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1797,article,"Blanquer, Josep M. and \""{O",Fair Queuing for Aggregated Multiple Links,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383074,10.1145/964723.383074,"Provisioning of a shared server with guarantees is an important scheduling task that has led to significant work in a number of areas including link scheduling. Fair Queuing algorithms provide a method for proportionally sharing a single server among competing flows, however, they do not address the problem of sharing multiple servers. Multiserver systems arise in a number of applications including link aggregation, multiprocessors and multi-path storage I/O. In this paper we introduce a new service discipline for multi-server systems that provides guarantees for competing flows. We prove that this new service discipline is a close approximation of the idealized Generalized Processor Sharing (GPS) discipline. We calculate its maximum packet delay and service discrepancy with respect to GPS. We also discuss its relevance to several applications, in particular, Ethernet link aggregation.",,189–197,9,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1798,inproceedings,"Baboescu, Florin and Varghese, George",Scalable Packet Classification,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383075,10.1145/383059.383075,"Packet classification is important for applications such as firewalls, intrusion detection, and differentiated services. Existing algorithms for packet classification reported in the literature scale poorly in either time or space as filter databases grow in size. Hardware solutions such as TCAMs do not scale to large classifiers. However, even for large classifiers (say 100,000 rules), any packet is likely to match a few (say 10) rules. Our paper seeks to exploit this observation to produce a scalable packet classification scheme called Aggregated Bit Vector (ABV). Our paper takes the bit vector search algorithm (BV) described in [11] (which takes linear time) and adds two new ideas, recursive aggregation of bit maps and filter rearrangement, to create ABV (which can take logarithmic time for many databases). We show that ABV outperforms BV by an order of magnitude using simulations on both industrial firewall databases and synthetically generated databases.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",199–210,12,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1799,article,"Baboescu, Florin and Varghese, George",Scalable Packet Classification,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383075,10.1145/964723.383075,"Packet classification is important for applications such as firewalls, intrusion detection, and differentiated services. Existing algorithms for packet classification reported in the literature scale poorly in either time or space as filter databases grow in size. Hardware solutions such as TCAMs do not scale to large classifiers. However, even for large classifiers (say 100,000 rules), any packet is likely to match a few (say 10) rules. Our paper seeks to exploit this observation to produce a scalable packet classification scheme called Aggregated Bit Vector (ABV). Our paper takes the bit vector search algorithm (BV) described in [11] (which takes linear time) and adds two new ideas, recursive aggregation of bit maps and filter rearrangement, to create ABV (which can take logarithmic time for many databases). We show that ABV outperforms BV by an order of magnitude using simulations on both industrial firewall databases and synthetically generated databases.",,199–210,12,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1800,inproceedings,"Chuanxiong, Guo",SRR: An O(1) Time Complexity Packet Scheduler for Flows in Multi-Service Packet Networks,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383076,10.1145/383059.383076,"In this paper, we present a novel fair queueing scheme, which we call Smoothed Round Robin (SRR). Ordinary round robin schedulers are well known for their burstiness in the scheduling output. In order to overcome this problem, SRR codes the weights of the flows into binary vectors to form a Weight Matrix, then uses a Weight Spread Sequence (WSS), which is specially designed to distribute the output more evenly, to schedule packets by scanning the Weight Matrix. By using the WSS and the Weight Matrix, SRR can emulate the Generalized Processor Sharing (GPS) well. It possesses better short-term fairness and schedule delay properties in comparison with various round robin schedulers. At the same time, it preserves O(1) time complexity by avoiding the time-stamp maintenance employed in various Fair Queueing schedulers. Simulation and implementation experiments show that SRR can provide good average end-to-end delay for soft real-time services. SRR can also be implemented in high-speed networks to provide QoS for its simplicity and low time complexity.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",211–222,12,"fair queueing, high speed networks, QoS, complexity, end-to-end delay, packet scheduler","San Diego, California, USA",SIGCOMM '01,,,,,,
1801,article,"Chuanxiong, Guo",SRR: An O(1) Time Complexity Packet Scheduler for Flows in Multi-Service Packet Networks,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383076,10.1145/964723.383076,"In this paper, we present a novel fair queueing scheme, which we call Smoothed Round Robin (SRR). Ordinary round robin schedulers are well known for their burstiness in the scheduling output. In order to overcome this problem, SRR codes the weights of the flows into binary vectors to form a Weight Matrix, then uses a Weight Spread Sequence (WSS), which is specially designed to distribute the output more evenly, to schedule packets by scanning the Weight Matrix. By using the WSS and the Weight Matrix, SRR can emulate the Generalized Processor Sharing (GPS) well. It possesses better short-term fairness and schedule delay properties in comparison with various round robin schedulers. At the same time, it preserves O(1) time complexity by avoiding the time-stamp maintenance employed in various Fair Queueing schedulers. Simulation and implementation experiments show that SRR can provide good average end-to-end delay for soft real-time services. SRR can also be implemented in high-speed networks to provide QoS for its simplicity and low time complexity.",,211–222,12,"QoS, end-to-end delay, high speed networks, fair queueing, complexity, packet scheduler",,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1802,inproceedings,"Basu, Anindya and Riecke, Jon",Stability Issues in OSPF Routing,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383077,10.1145/383059.383077,"We study the stability of the OSPF protocol under steady state and perturbed conditions. We look at three indicators of stability, namely, (a) network convergence times, (b) routing load on processors, and (c) the number of route flaps. We study these statistics under three different scenarios: (a) on networks that deploy OSPF with TE extensions, (b) on networks that use subsecond HELLO timers, and (c) on networks that use alternative strategies for refreshing link-state information. Our results are based on a very detailed simulation of a real ISP network with 292 nodes and 765 links.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",225–236,12,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1803,article,"Basu, Anindya and Riecke, Jon",Stability Issues in OSPF Routing,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383077,10.1145/964723.383077,"We study the stability of the OSPF protocol under steady state and perturbed conditions. We look at three indicators of stability, namely, (a) network convergence times, (b) routing load on processors, and (c) the number of route flaps. We study these statistics under three different scenarios: (a) on networks that deploy OSPF with TE extensions, (b) on networks that use subsecond HELLO timers, and (c) on networks that use alternative strategies for refreshing link-state information. Our results are based on a very detailed simulation of a real ISP network with 292 nodes and 765 links.",,225–236,12,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1804,inproceedings,"Lowekamp, Bruce and O'Hallaron, David and Gross, Thomas",Topology Discovery for Large Ethernet Networks,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383078,10.1145/383059.383078,"Accurate network topology information is important for both network management and application performance prediction. Most topology discovery research has focused on wide-area networks and examined topology only at the IP router level, ignoring the need for LAN topology information. Recent work has demonstrated that bridged Ethernet topology can be determined using standard SNMP MIBs; however, these algorithms require each bridge to learn about all other bridges in the network. Our approach to Ethernet topology discovery can determine the connection between a pair of the bridges that share forwarding entries for only three hosts. This minimal knowledge requirement significantly expands the size of the network that can be discovered. We have implemented the new algorithm, and it has accurately determined the topology of several different networks using a variety of hardware and network configurations. Our implementation requires access to only one endpoint to perform the queries needed for topology discovery.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",237–248,12,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1805,article,"Lowekamp, Bruce and O'Hallaron, David and Gross, Thomas",Topology Discovery for Large Ethernet Networks,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383078,10.1145/964723.383078,"Accurate network topology information is important for both network management and application performance prediction. Most topology discovery research has focused on wide-area networks and examined topology only at the IP router level, ignoring the need for LAN topology information. Recent work has demonstrated that bridged Ethernet topology can be determined using standard SNMP MIBs; however, these algorithms require each bridge to learn about all other bridges in the network. Our approach to Ethernet topology discovery can determine the connection between a pair of the bridges that share forwarding entries for only three hosts. This minimal knowledge requirement significantly expands the size of the network that can be discovered. We have implemented the new algorithm, and it has accurately determined the topology of several different networks using a variety of hardware and network configurations. Our implementation requires access to only one endpoint to perform the queries needed for topology discovery.",,237–248,12,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1806,inproceedings,"Costa, Lu Henrique M. K. and Fdida, Serge and Duarte, Otto",Hop by Hop Multicast Routing Protocol,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383079,10.1145/383059.383079,"IP Multicast is facing a slow take-off although it is a hotly debated topic since more than a decade. Many reasons are responsible for this status. Hence, the Internet is likely to be organized with both unicast and multicast enabled networks. Thus, it is of utmost importance to design protocols that allow the progressive deployment of the multicast service by supporting unicast clouds. This paper proposes HBH (Hop-By-Hop multicast routing protocol). HBH adopts the source-specific channel abstraction to simplify address allocation and implements data distribution using recursive unicast trees, which allow the transparent support of unicast-only routers. Additionally, HBH is original because its tree construction algorithm takes into account the unicast routing asymmetries. As most multicast routing protocols rely on the unicast infrastructure, these asymmetries impact the structure of the multicast trees. We show through simulation that HBH outperforms other multicast routing protocols in terms of the delay experienced by the receivers and the bandwidth consumption of the multicast trees.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",249–259,11,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1807,article,"Costa, Lu Henrique M. K. and Fdida, Serge and Duarte, Otto",Hop by Hop Multicast Routing Protocol,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383079,10.1145/964723.383079,"IP Multicast is facing a slow take-off although it is a hotly debated topic since more than a decade. Many reasons are responsible for this status. Hence, the Internet is likely to be organized with both unicast and multicast enabled networks. Thus, it is of utmost importance to design protocols that allow the progressive deployment of the multicast service by supporting unicast clouds. This paper proposes HBH (Hop-By-Hop multicast routing protocol). HBH adopts the source-specific channel abstraction to simplify address allocation and implements data distribution using recursive unicast trees, which allow the transparent support of unicast-only routers. Additionally, HBH is original because its tree construction algorithm takes into account the unicast routing asymmetries. As most multicast routing protocols rely on the unicast infrastructure, these asymmetries impact the structure of the multicast trees. We show through simulation that HBH outperforms other multicast routing protocols in terms of the delay experienced by the receivers and the bandwidth consumption of the multicast trees.",,249–259,11,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1808,inproceedings,"Bansal, Deepak and Balakrishnan, Hari and Floyd, Sally and Shenker, Scott",Dynamic Behavior of Slowly-Responsive Congestion Control Algorithms,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383080,10.1145/383059.383080,"The recently developed notion of TCP-compatibility has led to a number of proposals for alternative congestion control algorithms whose long-term throughput as a function of a steady-state loss rate is similar to that of TCP. Motivated by the needs of some streaming and multicast applications, these algorithms seem poised to take the current TCP-dominated Internet to an Internet where many congestion control algorithms co-exist. An important characteristic of these alternative algorithms is that they are slowly-responsive, refraining from reacting as drastically as TCP to a single packet loss.However, the TCP-compatibility criteria explored so far in the literature considers only the static condition of a fixed loss rate. This paper investigates the behavior of slowly-responsive, TCP-compatible congestion control algorithms under more realistic dynamic network conditions, addressing the fundamental question of whether these algorithms are safe to deploy in the public Internet. We study persistent loss rates, long- and short-term fairness properties, bottleneck link utilization, and smoothness of transmission rates.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",263–274,12,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1809,article,"Bansal, Deepak and Balakrishnan, Hari and Floyd, Sally and Shenker, Scott",Dynamic Behavior of Slowly-Responsive Congestion Control Algorithms,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383080,10.1145/964723.383080,"The recently developed notion of TCP-compatibility has led to a number of proposals for alternative congestion control algorithms whose long-term throughput as a function of a steady-state loss rate is similar to that of TCP. Motivated by the needs of some streaming and multicast applications, these algorithms seem poised to take the current TCP-dominated Internet to an Internet where many congestion control algorithms co-exist. An important characteristic of these alternative algorithms is that they are slowly-responsive, refraining from reacting as drastically as TCP to a single packet loss.However, the TCP-compatibility criteria explored so far in the literature considers only the static condition of a fixed loss rate. This paper investigates the behavior of slowly-responsive, TCP-compatible congestion control algorithms under more realistic dynamic network conditions, addressing the fundamental question of whether these algorithms are safe to deploy in the public Internet. We study persistent loss rates, long- and short-term fairness properties, bottleneck link utilization, and smoothness of transmission rates.",,263–274,12,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1810,inproceedings,"Widmer, J\""{o",Extending Equation-Based Congestion Control to Multicast Applications,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383081,10.1145/383059.383081,"In this paper we introduce TFMCC, an equation-based multicast congestion control mechanism that extends the TCP-friendly TFRC protocol from the unicast to the multicast domain. The key challenges in the design of TFMCC lie in scalable round-trip time measurements, appropriate feedback suppression, and in ensuring that feedback delays in the control loop do not adversely affect fairness towards competing flows. A major contribution is the feedback mechanism, the key component of end-to-end multicast congestion control schemes. We improve upon the well-known approach of using exponentially weighted random timers by biasing feedback in favor of low-rate receivers while still preventing a response implosion. We evaluate the design using simulation, and demonstrate that TFMCC is both TCP-friendly and scales well to multicast groups with thousands of receivers. We also investigate TFMCC's weaknesses and scaling limits to provide guidance as to application domains for which it is well suited.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",275–285,11,"multicast, feedback, TCP-friendliness, single-rate, suppression, congestion control","San Diego, California, USA",SIGCOMM '01,,,,,,
1811,article,"Widmer, J\""{o",Extending Equation-Based Congestion Control to Multicast Applications,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383081,10.1145/964723.383081,"In this paper we introduce TFMCC, an equation-based multicast congestion control mechanism that extends the TCP-friendly TFRC protocol from the unicast to the multicast domain. The key challenges in the design of TFMCC lie in scalable round-trip time measurements, appropriate feedback suppression, and in ensuring that feedback delays in the control loop do not adversely affect fairness towards competing flows. A major contribution is the feedback mechanism, the key component of end-to-end multicast congestion control schemes. We improve upon the well-known approach of using exponentially weighted random timers by biasing feedback in favor of low-rate receivers while still preventing a response implosion. We evaluate the design using simulation, and demonstrate that TFMCC is both TCP-friendly and scales well to multicast groups with thousands of receivers. We also investigate TFMCC's weaknesses and scaling limits to provide guidance as to application domains for which it is well suited.",,275–285,11,"suppression, congestion control, feedback, multicast, TCP-friendliness, single-rate",,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1812,inproceedings,"Pahdye, Jitendra and Floyd, Sally",On Inferring TCP Behavior,2001,1581134118,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/383059.383083,10.1145/383059.383083,"Most of the traffic in today's Internet is controlled by the Transmission Control Protocol (TCP). Hence, the performance of TCP has a significant impact on the performance of the overall Internet. TCP is a complex protocol with many user-configurable parameters and a range of different implementations. In addition, research continues to produce new developments in congestion control mechanisms and TCP options, and it is useful to trace the deployment of these new mechanisms in the Internet. As a final concern, the stability and fairness of the current Internet relies on the voluntary use of congestion control mechanisms by end hosts. Therefore it is important to test TCP implementations for conformant end-to-end congestion control. Since web traffic forms the majority of the TCP traffic, TCP implementations in today's web servers are of particular interest. We have developed a tool called TCP Behavior Inference Tool (TBIT) to characterize the TCP behavior of a remote web server. In this paper, we describe TBIT, and present results about the TCP behaviors of major web servers, obtained using this tool. We also describe the use of TBIT to detect bugs and non-compliance in TCP implementations deployed in public web servers.","Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications",287–298,12,,"San Diego, California, USA",SIGCOMM '01,,,,,,
1813,article,"Pahdye, Jitendra and Floyd, Sally",On Inferring TCP Behavior,2001,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/964723.383083,10.1145/964723.383083,"Most of the traffic in today's Internet is controlled by the Transmission Control Protocol (TCP). Hence, the performance of TCP has a significant impact on the performance of the overall Internet. TCP is a complex protocol with many user-configurable parameters and a range of different implementations. In addition, research continues to produce new developments in congestion control mechanisms and TCP options, and it is useful to trace the deployment of these new mechanisms in the Internet. As a final concern, the stability and fairness of the current Internet relies on the voluntary use of congestion control mechanisms by end hosts. Therefore it is important to test TCP implementations for conformant end-to-end congestion control. Since web traffic forms the majority of the TCP traffic, TCP implementations in today's web servers are of particular interest. We have developed a tool called TCP Behavior Inference Tool (TBIT) to characterize the TCP behavior of a remote web server. In this paper, we describe TBIT, and present results about the TCP behaviors of major web servers, obtained using this tool. We also describe the use of TBIT to detect bugs and non-compliance in TCP implementations deployed in public web servers.",,287–298,12,,,,October 2001,31,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1814,inproceedings,"Danthine, Andr\'{e",Keynote Speech,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347382,10.1145/347059.347382,,"Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",0.11,,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1815,article,"Danthine, Andr\'{e",Keynote Speech,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347382,10.1145/347057.347382,,,0.11,,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1816,inproceedings,"Katabi, Dina and Wroclawski, John",A Framework for Scalable Global IP-Anycast (GIA),2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347388,10.1145/347059.347388,"This paper proposes GIA, a scalable architecture for global IP-anycast. Existing designs for providing IP-anycast must either globally distribute routes to individual anycast groups, or confine each anycast group to a pre-configured topological region. The first approach does not scale because of excessive growth in the routing tables, whereas the second one severely limits the utility of the service. Our design scales by dividing inter-domain anycast routing into two components. The first component builds inexpensive default anycast routes that consume no bandwidth or storage space. The second component, controlled by the edge domains, generates enhanced anycast routes that are customized according to the beneficiary domain's interests. We evaluate the performance of our design using simulation, and prove its practicality by implementing it in the Multi-threaded Routing Toolkit.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",3–15,13,"routing, scalable, Internet, architecture, anycast","Stockholm, Sweden",SIGCOMM '00,,,,,,
1817,article,"Katabi, Dina and Wroclawski, John",A Framework for Scalable Global IP-Anycast (GIA),2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347388,10.1145/347057.347388,"This paper proposes GIA, a scalable architecture for global IP-anycast. Existing designs for providing IP-anycast must either globally distribute routes to individual anycast groups, or confine each anycast group to a pre-configured topological region. The first approach does not scale because of excessive growth in the routing tables, whereas the second one severely limits the utility of the service. Our design scales by dividing inter-domain anycast routing into two components. The first component builds inexpensive default anycast routes that consume no bandwidth or storage space. The second component, controlled by the edge domains, generates enhanced anycast routes that are customized according to the beneficiary domain's interests. We evaluate the performance of our design using simulation, and prove its practicality by implementing it in the Multi-threaded Routing Toolkit.",,3–15,13,"scalable, routing, anycast, architecture, Internet",,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1818,inproceedings,"Rizzo, Luigi",Pgmcc: A TCP-Friendly Single-Rate Multicast Congestion Control Scheme,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347390,10.1145/347059.347390,"We present a single rate multicast congestion control scheme (pgmcc) which is TCP-friendly and achieves scalability, stability and fast response to variations in network conditions. pgmcc is suitable for both non-reliable and reliable data transfers; it uses a window-based TCP-like controller based on positive ACKs and run between the sender and a group's representative, the acker. The innovative part of pgmcc is a fast and low-overhead procedure to select (and track changes of) the acker, which permits us to consider the acker as a moving receiver rather than a changing one. As such, the scheme is robust to measurement errors, and supports fast response to changes in the receiver set and/or network conditions. The scheme has been implemented in the PGM protocol, and the paper presents a number of experimental results on its performance.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",17–28,12,"multicast, TCP, fairness, congestion control","Stockholm, Sweden",SIGCOMM '00,,,,,,
1819,article,"Rizzo, Luigi",Pgmcc: A TCP-Friendly Single-Rate Multicast Congestion Control Scheme,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347390,10.1145/347057.347390,"We present a single rate multicast congestion control scheme (pgmcc) which is TCP-friendly and achieves scalability, stability and fast response to variations in network conditions. pgmcc is suitable for both non-reliable and reliable data transfers; it uses a window-based TCP-like controller based on positive ACKs and run between the sender and a group's representative, the acker. The innovative part of pgmcc is a fast and low-overhead procedure to select (and track changes of) the acker, which permits us to consider the acker as a moving receiver rather than a changing one. As such, the scheme is robust to measurement errors, and supports fast response to changes in the receiver set and/or network conditions. The scheme has been implemented in the PGM protocol, and the paper presents a number of experimental results on its performance.",,17–28,12,"fairness, multicast, congestion control, TCP",,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1820,inproceedings,"Reddy, Anoop and Govindan, Ramesh and Estrin, Deborah",Fault Isolation in Multicast Trees,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347395,10.1145/347059.347395,"Fault isolation has received little attention in the Internet research literature. We take a step towards addressing this deficiency, exploring robust and scalable techniques by which multicast receivers can (in some cases, approximately) locate the on-tree router responsible for a route change, or the link responsible for significant packet loss. A common property of our techniques is that receivers with overlapped paths coordinate to share the responsibility of monitoring paths to the source. Our techniques assume no additional path monitoring capability other than that provided by multicast traceroute (mtrace).","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",29–40,12,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1821,article,"Reddy, Anoop and Govindan, Ramesh and Estrin, Deborah",Fault Isolation in Multicast Trees,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347395,10.1145/347057.347395,"Fault isolation has received little attention in the Internet research literature. We take a step towards addressing this deficiency, exploring robust and scalable techniques by which multicast receivers can (in some cases, approximately) locate the on-tree router responsible for a route change, or the link responsible for significant packet loss. A common property of our techniques is that receivers with overlapped paths coordinate to share the responsibility of monitoring paths to the source. Our techniques assume no additional path monitoring capability other than that provided by multicast traceroute (mtrace).",,29–40,12,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1822,inproceedings,"Floyd, Sally and Handley, Mark and Padhye, Jitendra and Widmer, J\""{o",Equation-Based Congestion Control for Unicast Applications,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347397,10.1145/347059.347397,"This paper proposes a mechanism for equation-based congestion control for unicast traffic. Most best-effort traffic in the current Internet is well-served by the dominant transport protocol, TCP. However, traffic such as best-effort unicast streaming multimedia could find use for a TCP-friendly congestion control mechanism that refrains from reducing the sending rate in half in response to a single packet drop. With our mechanism, the sender explicitly adjusts its sending rate as a function of the measured rate of loss events, where a loss event consists of one or more packets dropped within a single round-trip time. We use both simulations and experiments over the Internet to explore performance.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",43–56,14,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1823,article,"Floyd, Sally and Handley, Mark and Padhye, Jitendra and Widmer, J\""{o",Equation-Based Congestion Control for Unicast Applications,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347397,10.1145/347057.347397,"This paper proposes a mechanism for equation-based congestion control for unicast traffic. Most best-effort traffic in the current Internet is well-served by the dominant transport protocol, TCP. However, traffic such as best-effort unicast streaming multimedia could find use for a TCP-friendly congestion control mechanism that refrains from reducing the sending rate in half in response to a single packet drop. With our mechanism, the sender explicitly adjusts its sending rate as a function of the measured rate of loss events, where a loss event consists of one or more packets dropped within a single round-trip time. We use both simulations and experiments over the Internet to explore performance.",,43–56,14,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1824,inproceedings,"Breslau, Lee and Knightly, Edward W. and Shenker, Scott and Stoica, Ion and Zhang, Hui",Endpoint Admission Control: Architectural Issues and Performance,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347400,10.1145/347059.347400,"The traditional approach to implementing admission control, as exemplified by the Integrated Services proposal in the IETF, uses a signalling protocol to establish reservations at all routers along the path. While providing excellent quality-of-service, this approach has limited scalability because it requires routers to keep per-flow state and to process per-flow reservation messages. In an attempt to implement admission control without these scalability problems, several recent papers have proposed various forms of endpoint admission control. In these designs, the hosts (the endpoints) probe the network to detect the level of congestion; the host admits the flow only if the detected level of congestion is sufficiently low. This paper is devoted to the study of endpoint admission control. We first consider several architectural issues that guide (and constrain) the design of such systems. We then use simulations to evaluate the performance of endpoint admission control in various settings. The modest performance degradation between traditional router-based admission control and endpoint admission control suggests that a real-time service based on endpoint probing may be viable.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",57–69,13,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1825,article,"Breslau, Lee and Knightly, Edward W. and Shenker, Scott and Stoica, Ion and Zhang, Hui",Endpoint Admission Control: Architectural Issues and Performance,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347400,10.1145/347057.347400,"The traditional approach to implementing admission control, as exemplified by the Integrated Services proposal in the IETF, uses a signalling protocol to establish reservations at all routers along the path. While providing excellent quality-of-service, this approach has limited scalability because it requires routers to keep per-flow state and to process per-flow reservation messages. In an attempt to implement admission control without these scalability problems, several recent papers have proposed various forms of endpoint admission control. In these designs, the hosts (the endpoints) probe the network to detect the level of congestion; the host admits the flow only if the detected level of congestion is sufficiently low. This paper is devoted to the study of endpoint admission control. We first consider several architectural issues that guide (and constrain) the design of such systems. We then use simulations to evaluate the performance of endpoint admission control in various settings. The modest performance degradation between traditional router-based admission control and endpoint admission control suggests that a real-time service based on endpoint probing may be viable.",,57–69,13,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1826,inproceedings,"Zhang, Zhi-Li and Duan, Zhenhai and Gao, Lixin and Hou, Yiwei Thomas",Decoupling QoS Control from Core Routers: A Novel Bandwidth Broker Architecture for Scalable Support of Guaranteed Services,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347403,10.1145/347059.347403,"For scalable support of guaranteed services that decouples the QoS control plane from the packet forwarding plane. More specifically, under this architecture, core routers do not maintain any QoS reservation states, whether per-flow or aggregate. Instead, QoS reservation states are stored at and managed by bandwidth broker(s). There are several advantages of such a bandwidth broker architecture. Among others, it relieves core routers of QoS control functions such as admission control and QoS state management, and thus enables a network service provider to introduce new (guaranteed) services without necessarily requiring software/hardware upgrades at core routers. Furthermore, it allows us to design efficient admission control algorithms without incurring any overhead at core routers. The proposed bandwidth broker architecture is designed based on a core stateless virtual time reference system developed in [20].","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",71–83,13,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1827,article,"Zhang, Zhi-Li and Duan, Zhenhai and Gao, Lixin and Hou, Yiwei Thomas",Decoupling QoS Control from Core Routers: A Novel Bandwidth Broker Architecture for Scalable Support of Guaranteed Services,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347403,10.1145/347057.347403,"For scalable support of guaranteed services that decouples the QoS control plane from the packet forwarding plane. More specifically, under this architecture, core routers do not maintain any QoS reservation states, whether per-flow or aggregate. Instead, QoS reservation states are stored at and managed by bandwidth broker(s). There are several advantages of such a bandwidth broker architecture. Among others, it relieves core routers of QoS control functions such as admission control and QoS state management, and thus enables a network service provider to introduce new (guaranteed) services without necessarily requiring software/hardware upgrades at core routers. Furthermore, it allows us to design efficient admission control algorithms without incurring any overhead at core routers. The proposed bandwidth broker architecture is designed based on a core stateless virtual time reference system developed in [20].",,71–83,13,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1828,inproceedings,"Spring, Neil T. and Wetherall, David",A Protocol-Independent Technique for Eliminating Redundant Network Traffic,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347408,10.1145/347059.347408,"We present a technique for identifying repetitive information transfers and use it to analyze the redundancy of network traffic. Our insight is that dynamic content, streaming media and other traffic that is not caught by today's Web caches is nonetheless likely to derive from similar information. We have therefore adapted similarity detection techniques to the problem of designing a system to eliminate redundant transfers. We identify repeated byte ranges between packets to avoid retransmitting the redundant data.We find a high level of redundancy and are able to detect repetition that Web proxy caches are not. In our traces, after Web proxy caching has been applied, an additional 39% of the original volume of Web traffic is found to be redundant. Moreover, because our technique makes no assumptions about HTTP protocol syntax or caching semantics, it provides immediate benefits for other types of content, such as streaming media, FTP traffic, news and mail.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",87–95,9,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1829,article,"Spring, Neil T. and Wetherall, David",A Protocol-Independent Technique for Eliminating Redundant Network Traffic,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347408,10.1145/347057.347408,"We present a technique for identifying repetitive information transfers and use it to analyze the redundancy of network traffic. Our insight is that dynamic content, streaming media and other traffic that is not caught by today's Web caches is nonetheless likely to derive from similar information. We have therefore adapted similarity detection techniques to the problem of designing a system to eliminate redundant transfers. We identify repeated byte ranges between packets to avoid retransmitting the redundant data.We find a high level of redundancy and are able to detect repetition that Web proxy caches are not. In our traces, after Web proxy caching has been applied, an additional 39% of the original volume of Web traffic is found to be redundant. Moreover, because our technique makes no assumptions about HTTP protocol syntax or caching semantics, it provides immediate benefits for other types of content, such as streaming media, FTP traffic, news and mail.",,87–95,9,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1830,inproceedings,"Krishnamurthy, Balachander and Wang, Jia",On Network-Aware Clustering of Web Clients,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347412,10.1145/347059.347412,"Being able to identify the groups of clients that are responsible for a significant portion of a Web site's requests can be helpful to both the Web site and the clients. In a Web application, it is beneficial to move content closer to groups of clients that are responsible for large subsets of requests to an origin server. We introduce clusters---a grouping of clients that are close together topologically and likely to be under common administrative control. We identify clusters using a ``network-aware"" method, based on information available from BGP routing table snapshots.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",97–110,14,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1831,article,"Krishnamurthy, Balachander and Wang, Jia",On Network-Aware Clustering of Web Clients,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347412,10.1145/347057.347412,"Being able to identify the groups of clients that are responsible for a significant portion of a Web site's requests can be helpful to both the Web site and the clients. In a Web application, it is beneficial to move content closer to groups of clients that are responsible for large subsets of requests to an origin server. We introduce clusters---a grouping of clients that are close together topologically and likely to be under common administrative control. We identify clusters using a ``network-aware"" method, based on information available from BGP routing table snapshots.",,97–110,14,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1832,inproceedings,"Padmanabhan, Venkata N. and Qiu, Lili",The Content and Access Dynamics of a Busy Web Site: Findings and Implications,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347413,10.1145/347059.347413,"In this paper, we study the dynamics of the MSNBC news site, one of the busiest Web sites in the Internet today. Unlike many other efforts that have analyzed client accesses as seen by proxies, we focus on the server end. We analyze the dynamics of both the server content and client accesses made to the server. The former considers the content creation and modification process while the latter considers page popularity and locality in client accesses. Some of our key results are: (a) files tend to change little when they are modified, (b) a small set of files tends to get modified repeatedly, (c) file popularity follows a Zipf-like distribution with a parameter &agr that is much larger than reported in previous, proxy-based studies, and (d) there is significant temporal stability in file popularity but not much stability in the domains from which clients access the popular content. We discuss the implications of these findings for techniques such as Web caching (including cache consistency algorithms), and prefetching or server-based ``push'' of Web content.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",111–123,13,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1833,article,"Padmanabhan, Venkata N. and Qiu, Lili",The Content and Access Dynamics of a Busy Web Site: Findings and Implications,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347413,10.1145/347057.347413,"In this paper, we study the dynamics of the MSNBC news site, one of the busiest Web sites in the Internet today. Unlike many other efforts that have analyzed client accesses as seen by proxies, we focus on the server end. We analyze the dynamics of both the server content and client accesses made to the server. The former considers the content creation and modification process while the latter considers page popularity and locality in client accesses. Some of our key results are: (a) files tend to change little when they are modified, (b) a small set of files tends to get modified repeatedly, (c) file popularity follows a Zipf-like distribution with a parameter &agr that is much larger than reported in previous, proxy-based studies, and (d) there is significant temporal stability in file popularity but not much stability in the domains from which clients access the popular content. We discuss the implications of these findings for techniques such as Web caching (including cache consistency algorithms), and prefetching or server-based ``push'' of Web content.",,111–123,13,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1834,inproceedings,"Barford, Paul and Crovella, Mark",Critical Path Analysis of TCP Transactions,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347416,10.1145/347059.347416,"Improving the performance of data transfers in the Internet (such as Web transfers) requires a detailed understanding of when and how delays are introduced. Unfortunately, the complexity of data transfers like those using HTTP is great enough that identifying the precise causes of delays is difficult. In this paper we describe a method for pinpointing where delays are introduced into applications like HTTP by using critical path analysis. By constructing and profiling the critical path, it is possible to determine what fraction of total transfer latency is due to packet propagation, network variation (e.g., queuing at routers or route fluctuation), packet losses, and delays at the server and at the client. We have implemented our technique in a tool called tcpeval that automates critical path analysis for Web transactions. We show that our analysis method is robust enough to analyze traces taken for two different TCP implementations (Linux and FreeBSD). To demonstrate the utility of our approach, we present the results of critical path analysis for a set of Web transactions taken over 14 days under a variety of server and network conditions. The results show that critical path analysis can shed considerable light on the causes of delays in Web transfers, and can expose subtleties in the behavior of the entire end-to-end system.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",127–138,12,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1835,article,"Barford, Paul and Crovella, Mark",Critical Path Analysis of TCP Transactions,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347416,10.1145/347057.347416,"Improving the performance of data transfers in the Internet (such as Web transfers) requires a detailed understanding of when and how delays are introduced. Unfortunately, the complexity of data transfers like those using HTTP is great enough that identifying the precise causes of delays is difficult. In this paper we describe a method for pinpointing where delays are introduced into applications like HTTP by using critical path analysis. By constructing and profiling the critical path, it is possible to determine what fraction of total transfer latency is due to packet propagation, network variation (e.g., queuing at routers or route fluctuation), packet losses, and delays at the server and at the client. We have implemented our technique in a tool called tcpeval that automates critical path analysis for Web transactions. We show that our analysis method is robust enough to analyze traces taken for two different TCP implementations (Linux and FreeBSD). To demonstrate the utility of our approach, we present the results of critical path analysis for a set of Web transactions taken over 14 days under a variety of server and network conditions. The results show that critical path analysis can shed considerable light on the causes of delays in Web transfers, and can expose subtleties in the behavior of the entire end-to-end system.",,127–138,12,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1836,inproceedings,"Christiansen, Mikkel and Jeffay, Kevin and Ott, David and Smith, F. Donelson",Tuning RED for Web Traffic,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347418,10.1145/347059.347418,"We study the effects of RED on the performance of Web browsing with a novel aspect of our work being the use of a user-centric measure of performance - response time for HTTP request-response pairs. We empirically evaluate RED across a range of parameter settings and offered loads. Our results show that: (1) contrary to expectations, compared to a FIFO queue, RED has a minimal effect on HTTP response times for offered loads up to 90% of link ca?pacity, (2) response times at loads in this range are not substantially effected by RED pa?rameters, (3) between 90% and 100% load, RED can be carefully tuned to yield performance somewhat superior to FIFO, however, response times are quite sensitive to the actual RED pa?rameter values selected, and (4) in such heavily congested networks, RED parameters that provide the best link utilization produce poorer response times. We conclude that for links carrying only web traf?fic, RED queue management appears to provide no clear advantage over tail-drop FIFO for end-user response times.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",139–150,12,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1837,article,"Christiansen, Mikkel and Jeffay, Kevin and Ott, David and Smith, F. Donelson",Tuning RED for Web Traffic,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347418,10.1145/347057.347418,"We study the effects of RED on the performance of Web browsing with a novel aspect of our work being the use of a user-centric measure of performance - response time for HTTP request-response pairs. We empirically evaluate RED across a range of parameter settings and offered loads. Our results show that: (1) contrary to expectations, compared to a FIFO queue, RED has a minimal effect on HTTP response times for offered loads up to 90% of link ca?pacity, (2) response times at loads in this range are not substantially effected by RED pa?rameters, (3) between 90% and 100% load, RED can be carefully tuned to yield performance somewhat superior to FIFO, however, response times are quite sensitive to the actual RED pa?rameter values selected, and (4) in such heavily congested networks, RED parameters that provide the best link utilization produce poorer response times. We conclude that for links carrying only web traf?fic, RED queue management appears to provide no clear advantage over tail-drop FIFO for end-user response times.",,139–150,12,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1838,inproceedings,"Misra, Vishal and Gong, Wei-Bo and Towsley, Don",Fluid-Based Analysis of a Network of AQM Routers Supporting TCP Flows with an Application to RED,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347421,10.1145/347059.347421,"In this paper we use jump process driven Stochastic Differential Equations to model the interactions of a set of TCP flows and Active Queue Management routers in a network setting. We show how the SDEs can be transformed into a set of Ordinary Differential Equations which can be easily solved numerically. Our solution methodology scales well to a large number of flows. As an application, we model and solve a system where RED is the AQM policy. Our results show excellent agreement with those of similar networks simulated using the well known ns simulator. Our model enables us to get an in-depth understanding of the RED algorithm. Using the tools developed in this paper, we present a critical analysis of the RED algorithm. We explain the role played by the RED configuration parameters on the behavior of the algorithm in a network. We point out a flaw in the RED averaging mechanism which we believe is a cause of tuning problems for RED. We believe this modeling/solution methodology has a great potential in analyzing and understanding various network congestion control algorithms.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",151–160,10,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1839,article,"Misra, Vishal and Gong, Wei-Bo and Towsley, Don",Fluid-Based Analysis of a Network of AQM Routers Supporting TCP Flows with an Application to RED,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347421,10.1145/347057.347421,"In this paper we use jump process driven Stochastic Differential Equations to model the interactions of a set of TCP flows and Active Queue Management routers in a network setting. We show how the SDEs can be transformed into a set of Ordinary Differential Equations which can be easily solved numerically. Our solution methodology scales well to a large number of flows. As an application, we model and solve a system where RED is the AQM policy. Our results show excellent agreement with those of similar networks simulated using the well known ns simulator. Our model enables us to get an in-depth understanding of the RED algorithm. Using the tools developed in this paper, we present a critical analysis of the RED algorithm. We explain the role played by the RED configuration parameters on the behavior of the algorithm in a network. We point out a flaw in the RED averaging mechanism which we believe is a cause of tuning problems for RED. We believe this modeling/solution methodology has a great potential in analyzing and understanding various network congestion control algorithms.",,151–160,10,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1840,inproceedings,"Shaikh, Aman and Varma, Anujan and Kalampoukas, Lampros and Dube, Rohit",Routing Stability in Congested Networks: Experimentation and Analysis,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347426,10.1145/347059.347426,"Loss of the routing protocol messages due to network congestion can cause peering session failures in routers, leading to route flaps and routing instabilities. We study the effects of traffic overload on routing protocols by quantifying the stability and robustness properties of two common Internet routing protocols, OSPF and BGP, when the routing control traffic is not isolated from data traffic. We develop analytical models to quantify the effect of congestion on the robustness of OSPF and BGP as a function of the traffic overload factor, queueing delays, and packet sizes. We perform extensive measurements in an experimental network of routers to validate the analytical results. Subsequently we use the analytical framework to investigate the effect of factors that are difficult to incorporate into an experimental setup, such as a wide range of link propagation delays and packet dropping policies. Our results show that increased queueing and propagation delays adversely affect BGP's resilience to congestion, in spite of its use of a reliable transport protocol. Our findings demonstrate the importance of selective treatment of routing protocol messages from other traffic, by using scheduling and utilizing buffer management policies in the routers, to achieve stable and robust network operation.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",163–174,12,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1841,article,"Shaikh, Aman and Varma, Anujan and Kalampoukas, Lampros and Dube, Rohit",Routing Stability in Congested Networks: Experimentation and Analysis,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347426,10.1145/347057.347426,"Loss of the routing protocol messages due to network congestion can cause peering session failures in routers, leading to route flaps and routing instabilities. We study the effects of traffic overload on routing protocols by quantifying the stability and robustness properties of two common Internet routing protocols, OSPF and BGP, when the routing control traffic is not isolated from data traffic. We develop analytical models to quantify the effect of congestion on the robustness of OSPF and BGP as a function of the traffic overload factor, queueing delays, and packet sizes. We perform extensive measurements in an experimental network of routers to validate the analytical results. Subsequently we use the analytical framework to investigate the effect of factors that are difficult to incorporate into an experimental setup, such as a wide range of link propagation delays and packet dropping policies. Our results show that increased queueing and propagation delays adversely affect BGP's resilience to congestion, in spite of its use of a reliable transport protocol. Our findings demonstrate the importance of selective treatment of routing protocol messages from other traffic, by using scheduling and utilizing buffer management policies in the routers, to achieve stable and robust network operation.",,163–174,12,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1842,inproceedings,"Labovitz, Craig and Ahuja, Abha and Bose, Abhijit and Jahanian, Farnam",Delayed Internet Routing Convergence,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347428,10.1145/347059.347428,"This paper examines the latency in Internet path failure, failover and repair due to the convergence properties of inter-domain routing. Unlike switches in the public telephony network which exhibit failover on the order of milliseconds, our experimental measurements show that inter-domain routers in the packet switched Internet may take tens of minutes to reach a consistent view of the network topology after a fault. These delays stem from temporary routing table oscillations formed during the operation of the BGP path selection process on Internet backbone routers. During these periods of delayed convergence, we show that end-to-end Internet paths will experience intermittent loss of connectivity, as well as increased packet loss and latency. We present a two-year study of Internet routing convergence through the experimental instrumentation of key portions of the Internet infrastructure, including both passive data collection and fault-injection machines at major Internet exchange points. Based on data from the injection and measurement of several hundred thousand inter-domain routing faults, we describe several unexpected properties of convergence and show that the measured upper bound on Internet inter-domain routing convergence delay is an order of magnitude slower than previously thought. Our analysis also shows that the upper theoretic computational bound on the number of router states and control messages exchanged during the process of BGP convergence is factorial with respect to the number of autonomous systems in the Internet. Finally, we demonstrate that much of the observed convergence delay stems from specific router vendor implementation decisions and ambiguity in the BGP specification.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",175–187,13,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1843,article,"Labovitz, Craig and Ahuja, Abha and Bose, Abhijit and Jahanian, Farnam",Delayed Internet Routing Convergence,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347428,10.1145/347057.347428,"This paper examines the latency in Internet path failure, failover and repair due to the convergence properties of inter-domain routing. Unlike switches in the public telephony network which exhibit failover on the order of milliseconds, our experimental measurements show that inter-domain routers in the packet switched Internet may take tens of minutes to reach a consistent view of the network topology after a fault. These delays stem from temporary routing table oscillations formed during the operation of the BGP path selection process on Internet backbone routers. During these periods of delayed convergence, we show that end-to-end Internet paths will experience intermittent loss of connectivity, as well as increased packet loss and latency. We present a two-year study of Internet routing convergence through the experimental instrumentation of key portions of the Internet infrastructure, including both passive data collection and fault-injection machines at major Internet exchange points. Based on data from the injection and measurement of several hundred thousand inter-domain routing faults, we describe several unexpected properties of convergence and show that the measured upper bound on Internet inter-domain routing convergence delay is an order of magnitude slower than previously thought. Our analysis also shows that the upper theoretic computational bound on the number of router states and control messages exchanged during the process of BGP convergence is factorial with respect to the number of autonomous systems in the Internet. Finally, we demonstrate that much of the observed convergence delay stems from specific router vendor implementation decisions and ambiguity in the BGP specification.",,175–187,13,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1844,inproceedings,"Partridge, Craig and Snoeren, Alex C. and Strayer, W. Timothy and Schwartz, Beverly and Condell, Matthew and Casti\~{n",FIRE: Flexible Intra-AS Routing Environment,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347544,10.1145/347059.347544,"Current routing protocols are monolithic, specifying the algorithm used to construct forwarding tables, the metric used by the algorithm (generally some form of hop-count), and the protocol used to distribute these metrics as an integrated package. The Flexible Intra-AS Routing Environment (FIRE) is a link-state, intra-domain routing protocol that decouples these components. FIRE supports run-time-pro- grammable algorithms and metrics over a secure link-state distribution protocol. By allowing the network operator to dynamically reprogram both the information being advertised and the routing algorithm used to construct forwarding tables in Java, FIRE enables the development and deployment of novel routing algorithms without the need for a new protocol to distribute state. FIRE supports multiple concurrent routing algorithms and metrics, each constructing separate forwarding tables. By using operator-specified packet filters, separate classes of traffic are routed using completely different routing algorithms, all supported by a single routing protocol.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",191–203,13,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1845,article,"Partridge, Craig and Snoeren, Alex C. and Strayer, W. Timothy and Schwartz, Beverly and Condell, Matthew and Casti\~{n",FIRE: Flexible Intra-AS Routing Environment,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347544,10.1145/347057.347544,"Current routing protocols are monolithic, specifying the algorithm used to construct forwarding tables, the metric used by the algorithm (generally some form of hop-count), and the protocol used to distribute these metrics as an integrated package. The Flexible Intra-AS Routing Environment (FIRE) is a link-state, intra-domain routing protocol that decouples these components. FIRE supports run-time-pro- grammable algorithms and metrics over a secure link-state distribution protocol. By allowing the network operator to dynamically reprogram both the information being advertised and the routing algorithm used to construct forwarding tables in Java, FIRE enables the development and deployment of novel routing algorithms without the need for a new protocol to distribute state. FIRE supports multiple concurrent routing algorithms and metrics, each constructing separate forwarding tables. By using operator-specified packet filters, separate classes of traffic are routed using completely different routing algorithms, all supported by a single routing protocol.",,191–203,13,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1846,inproceedings,"Rodeheffer, Thomas L. and Thekkath, Chandramohan A. and Anderson, Darrell C.",SmartBridge: A Scalable Bridge Architecture,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347546,10.1145/347059.347546,"As the number of hosts attached to a network increases beyond what can be connected by a single local area network (LAN), forwarding packets between hosts on different LANs becomes an issue. Two common solutions to the forwarding problem are IP routing and spanning tree bridging. IP routing scales well, but imposes the administrative burden of managing subnets and assigning addresses. Spanning tree bridging, in contrast, requires no administration, but often does not perform well in a large network, because too much traffic must detour toward the root of the spanning tree, wasting link bandwidth.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",205–216,12,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1847,article,"Rodeheffer, Thomas L. and Thekkath, Chandramohan A. and Anderson, Darrell C.",SmartBridge: A Scalable Bridge Architecture,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347546,10.1145/347057.347546,"As the number of hosts attached to a network increases beyond what can be connected by a single local area network (LAN), forwarding packets between hosts on different LANs becomes an issue. Two common solutions to the forwarding problem are IP routing and spanning tree bridging. IP routing scales well, but imposes the administrative burden of managing subnets and assigning addresses. Spanning tree bridging, in contrast, requires no administration, but often does not perform well in a large network, because too much traffic must detour toward the root of the spanning tree, wasting link bandwidth.",,205–216,12,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1848,inproceedings,"Baccelli, Fran\c{c",TCP is Max-plus Linear and What It Tells Us on Its Throughput,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347548,10.1145/347059.347548,"We give a representation of the packet-level dynamical behavior of the Reno and Tahoe variants of TCP over a single end-to-end connection. This representation allows one to consider the case when the connection involves a network made of several, possibly heterogeneous, deterministic or random routers in series. It is shown that the key features of the protocol and of the network can be expressed via a linear dynamical system in the so called max-plus algebra. This opens new ways of both analytical evaluation and fast simulation based on products of matrices in this algebra. This also leads to closed form formulas for the throughput allowed by TCP under natural assumptions on the behavior of the routers and on the detection of losses and timeouts; these new formulas are shown to refine those obtained from earlier models which either assume that the network could be reduced to a single bottleneck router and/or approximate the packets by a fluid.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",219–230,12,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1849,article,"Baccelli, Fran\c{c",TCP is Max-plus Linear and What It Tells Us on Its Throughput,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347548,10.1145/347057.347548,"We give a representation of the packet-level dynamical behavior of the Reno and Tahoe variants of TCP over a single end-to-end connection. This representation allows one to consider the case when the connection involves a network made of several, possibly heterogeneous, deterministic or random routers in series. It is shown that the key features of the protocol and of the network can be expressed via a linear dynamical system in the so called max-plus algebra. This opens new ways of both analytical evaluation and fast simulation based on products of matrices in this algebra. This also leads to closed form formulas for the throughput allowed by TCP under natural assumptions on the behavior of the routers and on the detection of losses and timeouts; these new formulas are shown to refine those obtained from earlier models which either assume that the network could be reduced to a single bottleneck router and/or approximate the packets by a fluid.",,219–230,12,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1850,inproceedings,"Altman, Eitan and Avrachenkov, Konstantin and Barakat, Chadi",A Stochastic Model of TCP/IP with Stationary Random Losses,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347549,10.1145/347059.347549,"We present a technique for identifying repetitive information transfers and use it to analyze the redundancy of network traffic. Our insight is that dynamic content, streaming media and other traffic that is not caught by today's Web caches is nonetheless likely to derive from similar information. We have therefore adapted similarity detection techniques to the problem of designing a system to eliminate redundant transfers. We identify repeated byte ranges between packets to avoid retransmitting the redundant data.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",231–242,12,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1851,article,"Altman, Eitan and Avrachenkov, Konstantin and Barakat, Chadi",A Stochastic Model of TCP/IP with Stationary Random Losses,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347549,10.1145/347057.347549,"We present a technique for identifying repetitive information transfers and use it to analyze the redundancy of network traffic. Our insight is that dynamic content, streaming media and other traffic that is not caught by today's Web caches is nonetheless likely to derive from similar information. We have therefore adapted similarity detection techniques to the problem of designing a system to eliminate redundant transfers. We identify repeated byte ranges between packets to avoid retransmitting the redundant data.",,231–242,12,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1852,inproceedings,"Veres, A. and Moln\'{a",On the Propagation of Long-Range Dependence in the Internet,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347551,10.1145/347059.347551,"This paper analyzes how TCP congestion control can propagate self-similarity between distant areas of the Internet. This property of TCP is due to its congestion control algorithm, which adapts to self-similar fluctuations on several timescales. The mechanisms and limitations of this propagation are investigated, and it is demonstrated that if a TCP connection shares a bottleneck link with a self-similar background traffic flow, it propagates the correlation structure of the background traffic flow above a characteristic timescale. The cut-off timescale depends on the end-to-end path properties, e.g., round-trip time and average window size. It is also demonstrated that even short TCP connections can propagate long-range correlations effectively. Our analysis reveals that if congestion periods in a connection's hops are long-range dependent, then the end-user perceived end-to-end traffic is also long-range dependent and it is characterized by the largest Hurst exponent. Furthermore, it is shown that self-similarity of one TCP stream can be passed on to other TCP streams that it is multiplexed with. These mechanisms complement the widespread scaling phenomena reported in a number of recent papers. Our arguments are supported with a combination of analytic techniques, simulations and statistical analyses of real Internet traffic measurements.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",243–254,12,"long-range dependence, TCP adaptivity, self-similarity, TCP congestion control","Stockholm, Sweden",SIGCOMM '00,,,,,,
1853,article,"Veres, A. and Moln\'{a",On the Propagation of Long-Range Dependence in the Internet,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347551,10.1145/347057.347551,"This paper analyzes how TCP congestion control can propagate self-similarity between distant areas of the Internet. This property of TCP is due to its congestion control algorithm, which adapts to self-similar fluctuations on several timescales. The mechanisms and limitations of this propagation are investigated, and it is demonstrated that if a TCP connection shares a bottleneck link with a self-similar background traffic flow, it propagates the correlation structure of the background traffic flow above a characteristic timescale. The cut-off timescale depends on the end-to-end path properties, e.g., round-trip time and average window size. It is also demonstrated that even short TCP connections can propagate long-range correlations effectively. Our analysis reveals that if congestion periods in a connection's hops are long-range dependent, then the end-user perceived end-to-end traffic is also long-range dependent and it is characterized by the largest Hurst exponent. Furthermore, it is shown that self-similarity of one TCP stream can be passed on to other TCP streams that it is multiplexed with. These mechanisms complement the widespread scaling phenomena reported in a number of recent papers. Our arguments are supported with a combination of analytic techniques, simulations and statistical analyses of real Internet traffic measurements.",,243–254,12,"long-range dependence, TCP congestion control, self-similarity, TCP adaptivity",,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1854,inproceedings,"Feldmann, Anja and Greenberg, Albert and Lund, Carsten and Reingold, Nick and Rexford, Jennifer and True, Fred",Deriving Traffic Demands for Operational IP Networks: Methodology and Experience,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347554,10.1145/347059.347554,"Engineering a large IP backbone network without an accurate, network-wide view of the traffic demands is challenging. Shifts in user behavior, changes in routing policies, and failures of network elements can result in significant (and sudden) fluctuations in load. In this paper, we present a model of traffic demands to support traffic engineering and performance debugging of large Internet Service Provider networks. By defining a traffic demand as a volume of load originating from an ingress link and destined to a set of egress links, we can capture and predict how routing affects the traffic traveling between domains. To infer the traffic demands, we propose a measurement methodology that combines flow-level measurements collected at all ingress links with reachability information about all egress links. We discuss how to cope with situations where practical considerations limit the amount and quality of the necessary data. Specifically, we show how to infer interdomain traffic demands using measurements collected at a smaller number of edge links --- the peering links connecting to neighboring providers. We report on our experiences in deriving the traffic demands in the AT&T IP Backbone, by collecting, validating, and joining very large and diverse sets of usage, configuration, and routing data over extended periods of time. The paper concludes with a preliminary analysis of the observed dynamics of the traffic demands and a discussion of the practical implications for traffic engineering.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",257–270,14,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1855,article,"Feldmann, Anja and Greenberg, Albert and Lund, Carsten and Reingold, Nick and Rexford, Jennifer and True, Fred",Deriving Traffic Demands for Operational IP Networks: Methodology and Experience,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347554,10.1145/347057.347554,"Engineering a large IP backbone network without an accurate, network-wide view of the traffic demands is challenging. Shifts in user behavior, changes in routing policies, and failures of network elements can result in significant (and sudden) fluctuations in load. In this paper, we present a model of traffic demands to support traffic engineering and performance debugging of large Internet Service Provider networks. By defining a traffic demand as a volume of load originating from an ingress link and destined to a set of egress links, we can capture and predict how routing affects the traffic traveling between domains. To infer the traffic demands, we propose a measurement methodology that combines flow-level measurements collected at all ingress links with reachability information about all egress links. We discuss how to cope with situations where practical considerations limit the amount and quality of the necessary data. Specifically, we show how to infer interdomain traffic demands using measurements collected at a smaller number of edge links --- the peering links connecting to neighboring providers. We report on our experiences in deriving the traffic demands in the AT&T IP Backbone, by collecting, validating, and joining very large and diverse sets of usage, configuration, and routing data over extended periods of time. The paper concludes with a preliminary analysis of the observed dynamics of the traffic demands and a discussion of the practical implications for traffic engineering.",,257–270,14,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1856,inproceedings,"Duffield, N. G. and Grossglauser, M.",Trajectory Sampling for Direct Traffic Observation,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347555,10.1145/347059.347555,"Traffic measurement is a critical component for the control and engineering of communication networks. We argue that traffic measurement should make it possible to obtain the spatial flow of traffic through the domain, i.e., the paths followed by packets between any ingress and egress point of the domain. Most resource allocation and capacity planning tasks can benefit from such information. Also, traffic measurements should be obtained without a routing model and without knowledge of network state. This allows the traffic measurement process to be resilient to network failures and state uncertainty.We propose a method that allows the direct inference of traffic flows through a domain by observing the trajectories of a subset of all packets traversing the network. The key advantages of the method are that (i) it does not rely on routing state, (ii) its implementation cost is small, and (iii) the measurement reporting traffic is modest and can be controlled precisely. The key idea of the method is to sample packets based on a hash function computed over the packet content. Using the same hash function will yield the same sample set of packets in the entire domain, and enables us to reconstruct packet trajectories.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",271–282,12,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1857,article,"Duffield, N. G. and Grossglauser, M.",Trajectory Sampling for Direct Traffic Observation,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347555,10.1145/347057.347555,"Traffic measurement is a critical component for the control and engineering of communication networks. We argue that traffic measurement should make it possible to obtain the spatial flow of traffic through the domain, i.e., the paths followed by packets between any ingress and egress point of the domain. Most resource allocation and capacity planning tasks can benefit from such information. Also, traffic measurements should be obtained without a routing model and without knowledge of network state. This allows the traffic measurement process to be resilient to network failures and state uncertainty.We propose a method that allows the direct inference of traffic flows through a domain by observing the trajectories of a subset of all packets traversing the network. The key advantages of the method are that (i) it does not rely on routing state, (ii) its implementation cost is small, and (iii) the measurement reporting traffic is modest and can be controlled precisely. The key idea of the method is to sample packets based on a hash function computed over the packet content. Using the same hash function will yield the same sample set of packets in the entire domain, and enables us to reconstruct packet trajectories.",,271–282,12,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1858,inproceedings,"Lai, Kevin and Baker, Mary",Measuring Link Bandwidths Using a Deterministic Model of Packet Delay,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347557,10.1145/347059.347557,"We describe a deterministic model of packet delay and use it to derive both the packet pair [2] property of FIFO-queueing networks and a new technique packet tailgating) for actively measuring link bandwidths. Compared to previously known techniques, packet tailgating usually consumes less network bandwidth, does not rely on consistent behavior of routers handling ICMP packets, and does not rely on timely delivery of acknowledgments.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",283–294,12,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1859,article,"Lai, Kevin and Baker, Mary",Measuring Link Bandwidths Using a Deterministic Model of Packet Delay,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347557,10.1145/347057.347557,"We describe a deterministic model of packet delay and use it to derive both the packet pair [2] property of FIFO-queueing networks and a new technique packet tailgating) for actively measuring link bandwidths. Compared to previously known techniques, packet tailgating usually consumes less network bandwidth, does not rely on consistent behavior of routers handling ICMP packets, and does not rely on timely delivery of acknowledgments.",,283–294,12,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1860,inproceedings,"Savage, Stefan and Wetherall, David and Karlin, Anna and Anderson, Tom",Practical Network Support for IP Traceback,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347560,10.1145/347059.347560,"This paper describes a technique for tracing anonymous packet flooding attacks in the Internet back towards their source. This work is motivated by the increased frequency and sophistication of denial-of-service attacks and by the difficulty in tracing packets with incorrect, or ``spoofed'', source addresses. In this paper we describe a general purpose traceback mechanism based on probabilistic packet marking in the network. Our approach allows a victim to identify the network path(s) traversed by attack traffic without requiring interactive operational support from Internet Service Providers (ISPs). Moreover, this traceback can be performed ``post-mortem'' -- after an attack has completed. We present an implementation of this technology that is incrementally deployable, (mostly) backwards compatible and can be efficiently implemented using conventional technology.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",295–306,12,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1861,article,"Savage, Stefan and Wetherall, David and Karlin, Anna and Anderson, Tom",Practical Network Support for IP Traceback,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347560,10.1145/347057.347560,"This paper describes a technique for tracing anonymous packet flooding attacks in the Internet back towards their source. This work is motivated by the increased frequency and sophistication of denial-of-service attacks and by the difficulty in tracing packets with incorrect, or ``spoofed'', source addresses. In this paper we describe a general purpose traceback mechanism based on probabilistic packet marking in the network. Our approach allows a victim to identify the network path(s) traversed by attack traffic without requiring interactive operational support from Internet Service Providers (ISPs). Moreover, this traceback can be performed ``post-mortem'' -- after an attack has completed. We present an implementation of this technology that is incrementally deployable, (mostly) backwards compatible and can be efficiently implemented using conventional technology.",,295–306,12,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1862,inproceedings,"Stone, Jonathan and Partridge, Craig",When the CRC and TCP Checksum Disagree,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347561,10.1145/347059.347561,"Traces of Internet packets from the past two years show that between 1 packet in 1,100 and 1 packet in 32,000 fails the TCP checksum, even on links where link-level CRCs should catch all but 1 in 4 billion errors. For certain situations, the rate of checksum failures can be even higher: in one hour-long test we observed a checksum failure of 1 packet in 400. We investigate why so many errors are observed, when link-level CRCs should catch nearly all of them.We have collected nearly 500,000 packets which failed the TCP or UDP or IP checksum. This dataset shows the Internet has a wide variety of error sources which can not be detected by link-level checks. We describe analysis tools that have identified nearly 100 different error patterns. Categorizing packet errors, we can infer likely causes which explain roughly half the observed errors. The causes span the entire spectrum of a network stack, from memory errors to bugs in TCP.After an analysis we conclude that the checksum will fail to detect errors for roughly 1 in 16 million to 10 billion packets. From our analysis of the cause of errors, we propose simple changes to several protocols which will decrease the rate of undetected error. Even so, the highly non-random distribution of errors strongly suggests some applications should employ application-level checksums or equivalents.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",309–319,11,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1863,article,"Stone, Jonathan and Partridge, Craig",When the CRC and TCP Checksum Disagree,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347561,10.1145/347057.347561,"Traces of Internet packets from the past two years show that between 1 packet in 1,100 and 1 packet in 32,000 fails the TCP checksum, even on links where link-level CRCs should catch all but 1 in 4 billion errors. For certain situations, the rate of checksum failures can be even higher: in one hour-long test we observed a checksum failure of 1 packet in 400. We investigate why so many errors are observed, when link-level CRCs should catch nearly all of them.We have collected nearly 500,000 packets which failed the TCP or UDP or IP checksum. This dataset shows the Internet has a wide variety of error sources which can not be detected by link-level checks. We describe analysis tools that have identified nearly 100 different error patterns. Categorizing packet errors, we can infer likely causes which explain roughly half the observed errors. The causes span the entire spectrum of a network stack, from memory errors to bugs in TCP.After an analysis we conclude that the checksum will fail to detect errors for roughly 1 in 16 million to 10 billion packets. From our analysis of the cause of errors, we propose simple changes to several protocols which will decrease the rate of undetected error. Even so, the highly non-random distribution of errors strongly suggests some applications should employ application-level checksums or equivalents.",,309–319,11,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1864,inproceedings,"McCann, Peter J. and Chandra, Satish",Packet Types: Abstract Specification of Network Protocol Messages,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347563,10.1145/347059.347563,"In writing networking code, one is often faced with the task of interpreting a raw buffer according to a standardized packet format. This is needed, for example, when monitoring network traffic for specific kinds of packets, or when unmarshaling an incoming packet for protocol processing. In such cases, a programmer typically writes C code that understands the grammar of a packet and that also performs any necessary byte-order and alignment adjustments. Because of the complexity of certain protocol formats, and because of the low-level of programming involved, writing such code is usually a cumbersome and error-prone process. Furthermore, code written in this style loses the domain-specific information, viz. the packet format, in its details, making it difficult to maintain.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",321–333,13,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1865,article,"McCann, Peter J. and Chandra, Satish",Packet Types: Abstract Specification of Network Protocol Messages,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347563,10.1145/347057.347563,"In writing networking code, one is often faced with the task of interpreting a raw buffer according to a standardized packet format. This is needed, for example, when monitoring network traffic for specific kinds of packets, or when unmarshaling an incoming packet for protocol processing. In such cases, a programmer typically writes C code that understands the grammar of a packet and that also performs any necessary byte-order and alignment adjustments. Because of the complexity of certain protocol formats, and because of the low-level of programming involved, writing such code is usually a cumbersome and error-prone process. Furthermore, code written in this style loses the domain-specific information, viz. the packet format, in its details, making it difficult to maintain.",,321–333,13,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1866,inproceedings,"Sikka, Sandeep and Varghese, George",Memory-Efficient State Lookups with Fast Updates,2000,1581132239,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347059.347565,10.1145/347059.347565,"Routers must do a best matching prefix lookup for every packet; solutions for Gigabit speeds are well known. As Internet link speeds higher, we seek a scalable solution whose speed scales with memory speeds while allowing large prefix databases. In this paper we show that providing such a solution requires careful attention to memory allocation and pipelining. This is because fast lookups require on-chip or off-chip SRAM which is limited by either expense or manufacturing process. We show that doing so while providing guarantees on the number of prefixes supported requires new algorithms and the breaking down of traditional abstraction boundaries between hardware and software. We introduce new problem-specific memory allocators that have provable memory utilization guarantees that can reach 100%; this is contrast to all standard allocators that can only guarantee 20% utilization when the requests can come in the range [1 ... 32]. An optimal version of our algorithm requires a new (but feasible) SRAM memory design that allows shifted access in addition to normal word access. Our techniques generalize to other IP lookup schemes and to other state lookups besides prefix lookup.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",335–347,13,,"Stockholm, Sweden",SIGCOMM '00,,,,,,
1867,article,"Sikka, Sandeep and Varghese, George",Memory-Efficient State Lookups with Fast Updates,2000,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/347057.347565,10.1145/347057.347565,"Routers must do a best matching prefix lookup for every packet; solutions for Gigabit speeds are well known. As Internet link speeds higher, we seek a scalable solution whose speed scales with memory speeds while allowing large prefix databases. In this paper we show that providing such a solution requires careful attention to memory allocation and pipelining. This is because fast lookups require on-chip or off-chip SRAM which is limited by either expense or manufacturing process. We show that doing so while providing guarantees on the number of prefixes supported requires new algorithms and the breaking down of traditional abstraction boundaries between hardware and software. We introduce new problem-specific memory allocators that have provable memory utilization guarantees that can reach 100%; this is contrast to all standard allocators that can only guarantee 20% utilization when the requests can come in the range [1 ... 32]. An optimal version of our algorithm requires a new (but feasible) SRAM memory design that allows shifted access in addition to normal word access. Our techniques generalize to other IP lookup schemes and to other state lookups besides prefix lookup.",,335–347,13,,,,October 2000,30,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1868,inproceedings,"Kohler, Eddie and Kaashoek, M. Frans and Montgomery, David R.",A Readable TCP in the Prolac Protocol Language,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316200,10.1145/316188.316200,"Prolac is a new statically-typed, object-oriented language for network protocol implementation. It is designed for readability, extensibility, and real-world implementation; most previous protocol languages, in contrast, have been based on hard-to-implement theoretical models and have focused on verification. We present a working Prolac TCP implementation directly derived from 4.4BSD. Our implementation is modular---protocol processing is logically divided into minimally-interacting pieces; readable---Prolac encourages top-down structure and naming intermediate computations; and extensible---subclassing cleanly separates protocol extensions like delayed acknowledgements and slow start. The Prolac compiler uses simple global analysis to remove expensive language features like dynamic dispatch, resulting in end-to-end performance comparable to an unmodified Linux 2.0 TCP.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",3–13,11,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1869,article,"Kohler, Eddie and Kaashoek, M. Frans and Montgomery, David R.",A Readable TCP in the Prolac Protocol Language,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316200,10.1145/316194.316200,"Prolac is a new statically-typed, object-oriented language for network protocol implementation. It is designed for readability, extensibility, and real-world implementation; most previous protocol languages, in contrast, have been based on hard-to-implement theoretical models and have focused on verification. We present a working Prolac TCP implementation directly derived from 4.4BSD. Our implementation is modular---protocol processing is logically divided into minimally-interacting pieces; readable---Prolac encourages top-down structure and naming intermediate computations; and extensible---subclassing cleanly separates protocol extensions like delayed acknowledgements and slow start. The Prolac compiler uses simple global analysis to remove expensive language features like dynamic dispatch, resulting in end-to-end performance comparable to an unmodified Linux 2.0 TCP.",,3–13,11,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1870,inproceedings,"Raman, Suchitra and McCanne, Steven","A Model, Analysis, and Protocol Framework for Soft State-Based Communication",1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316202,10.1145/316188.316202,"""Soft state"" is an often cited yet vague concept in network protocol design in which two or more network entities intercommunicate in a loosely coupled, often anonymous fashion. Researchers often define this concept operationally (if at all) rather than analytically: a source of soft state transmits periodic ""refresh messages"" over a (lossy) communication channel to one or more receivers that maintain a copy of that state, which in turn ""expires"" if the periodic updates cease. Though a number of crucial Internet protocol building blocks are rooted in soft state-based designs --- e.g., RSVP refresh messages, PIM membership updates, various routing protocol updates, RTCP control messages, directory services like SAP, and so forth --- controversy is building as to whether the performance overhead of soft state refresh messages justify their qualitative benefit of enhanced system ""robustness"". We believe that this controversy has risen not from fundamental performance tradeoffs but rather from our lack of a comprehensive understanding of soft state. To better understand these tradeoffs, we propose herein a formal model for soft state communication based on a probabilistic delivery model with relaxed reliability. Using this model, we conduct queueing analysis and simulation to characterize the data consistency and performance tradeoffs under a range of workloads and network loss rates. We then extend our model with feedback and show, through simulation, that adding feedback dramatically improves data consistency (by up to 55%) without increasing network resource consumption. Our model not only provides a foundation for understanding soft state, but also induces a new fundamental transport protocol based on probabilistic delivery. Toward this end, we sketch our design of the ""Soft State Transport Protocol"" (SSTP), which enjoys the robustness of soft state while retaining the performance benefit of hard state protocols like TCP through its judicious use of feedback.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",15–25,11,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1871,article,"Raman, Suchitra and McCanne, Steven","A Model, Analysis, and Protocol Framework for Soft State-Based Communication",1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316202,10.1145/316194.316202,"""Soft state"" is an often cited yet vague concept in network protocol design in which two or more network entities intercommunicate in a loosely coupled, often anonymous fashion. Researchers often define this concept operationally (if at all) rather than analytically: a source of soft state transmits periodic ""refresh messages"" over a (lossy) communication channel to one or more receivers that maintain a copy of that state, which in turn ""expires"" if the periodic updates cease. Though a number of crucial Internet protocol building blocks are rooted in soft state-based designs --- e.g., RSVP refresh messages, PIM membership updates, various routing protocol updates, RTCP control messages, directory services like SAP, and so forth --- controversy is building as to whether the performance overhead of soft state refresh messages justify their qualitative benefit of enhanced system ""robustness"". We believe that this controversy has risen not from fundamental performance tradeoffs but rather from our lack of a comprehensive understanding of soft state. To better understand these tradeoffs, we propose herein a formal model for soft state communication based on a probabilistic delivery model with relaxed reliability. Using this model, we conduct queueing analysis and simulation to characterize the data consistency and performance tradeoffs under a range of workloads and network loss rates. We then extend our model with feedback and show, through simulation, that adding feedback dramatically improves data consistency (by up to 55%) without increasing network resource consumption. Our model not only provides a foundation for understanding soft state, but also induces a new fundamental transport protocol based on probabilistic delivery. Toward this end, we sketch our design of the ""Soft State Transport Protocol"" (SSTP), which enjoys the robustness of soft state while retaining the performance benefit of hard state protocols like TCP through its judicious use of feedback.",,15–25,11,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1872,inproceedings,"Rubenstein, Dan and Kurose, Jim and Towsley, Don",The Impact of Multicast Layering on Network Fairness,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316203,10.1145/316188.316203,"Many definitions of fairness for multicast networks assume that sessions are single-rate, requiring that each multicast session transmits data to all of its receivers at the same rate. These definitions do not account for multi-rate approaches, such as layering, that permit receiving rates within a session to be chosen independently. We identify four desirable fairness properties for multicast networks, derived from properties that hold within the max-min fair allocations of unicast networks. We extend the definition of multicast max-min fairness to networks that contain multi-rate sessions, and show that all four fairness properties hold in a multi-rate max-min fair allocation, but need not hold in a single-rate max-min fair allocation. We then show that multi-rate max-min fair rate allocations can be achieved via intra-session coordinated joins and leaves of multicast groups. However, in the absence of coordination, the resulting max-min fair rate allocation uses link bandwidth inefficiently, and does not exhibit some of the desirable fairness properties. We evaluate this inefficiency for several layered multi-rate congestion control schemes, and find that, in a protocol where the sender coordinates joins, this inefficiency has minimal impact on desirable fairness properties. Our results indicate that sender-coordinated layered protocols show promise for achieving desirable fairness properties for allocations in large-scale multicast networks.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",27–38,12,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1873,article,"Rubenstein, Dan and Kurose, Jim and Towsley, Don",The Impact of Multicast Layering on Network Fairness,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316203,10.1145/316194.316203,"Many definitions of fairness for multicast networks assume that sessions are single-rate, requiring that each multicast session transmits data to all of its receivers at the same rate. These definitions do not account for multi-rate approaches, such as layering, that permit receiving rates within a session to be chosen independently. We identify four desirable fairness properties for multicast networks, derived from properties that hold within the max-min fair allocations of unicast networks. We extend the definition of multicast max-min fairness to networks that contain multi-rate sessions, and show that all four fairness properties hold in a multi-rate max-min fair allocation, but need not hold in a single-rate max-min fair allocation. We then show that multi-rate max-min fair rate allocations can be achieved via intra-session coordinated joins and leaves of multicast groups. However, in the absence of coordination, the resulting max-min fair rate allocation uses link bandwidth inefficiently, and does not exhibit some of the desirable fairness properties. We evaluate this inefficiency for several layered multi-rate congestion control schemes, and find that, in a protocol where the sender coordinates joins, this inefficiency has minimal impact on desirable fairness properties. Our results indicate that sender-coordinated layered protocols show promise for achieving desirable fairness properties for allocations in large-scale multicast networks.",,27–38,12,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1874,inproceedings,"Phillips, Graham and Shenker, Scott and Tangmunarunkit, Hongsuda",Scaling of Multicast Trees: Comments on the Chuang-Sirbu Scaling Law,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316205,10.1145/316188.316205,"One of the many benefits of multicast, when compared to traditional unicast, is that multicast reduces the overall network load. While the importance of multicast is beyond dispute, there have been surprisingly few attempts to quantify multicast's reduction in overall network load. The only substantial and quantitative effort we are aware of is that of Chuang and Sirbu [3]. They calculate the number of links L in a multicast delivery tree connecting a random source to m random and distinct network sites; extensive simulations over a range of networks suggest that L(m) ∝ m0.8. In this paper we examine the function L(m) in more detail and derive the asymptotic form for L(m) in k-ary trees. These results suggest one possible explanation for the universality of the Chuang-Sirbu scaling behavior.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",41–51,11,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1875,article,"Phillips, Graham and Shenker, Scott and Tangmunarunkit, Hongsuda",Scaling of Multicast Trees: Comments on the Chuang-Sirbu Scaling Law,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316205,10.1145/316194.316205,"One of the many benefits of multicast, when compared to traditional unicast, is that multicast reduces the overall network load. While the importance of multicast is beyond dispute, there have been surprisingly few attempts to quantify multicast's reduction in overall network load. The only substantial and quantitative effort we are aware of is that of Chuang and Sirbu [3]. They calculate the number of links L in a multicast delivery tree connecting a random source to m random and distinct network sites; extensive simulations over a range of networks suggest that L(m) ∝ m0.8. In this paper we examine the function L(m) in more detail and derive the asymptotic form for L(m) in k-ary trees. These results suggest one possible explanation for the universality of the Chuang-Sirbu scaling behavior.",,41–51,11,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1876,inproceedings,"Shields, Clay and Garcia-Luna-Aceves, J. J.",KHIP—a Scalable Protocol for Secure Multicast Routing,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316206,10.1145/316188.316206,"We present Keyed HIP (KHIP), a secure, hierarchical multicast routing protocol. We show that other shared-tree multicast routing protocols are subject to attacks against the multicast routing infrastructure that can isolate receivers or domains or introduce loops into the structure of the multicast routing tree. KHIP changes the multicast routing model so that only trusted members are able to join the multicast tree. This protects the multicast routing against attacks that could form branches to unauthorized receivers, prevents replay attacks and limits the effects of flooding attacks. Untrusted routers that are present on the path between trusted routers cannot change the routing and can mount no denial-of-service attack stronger than simply dropping control messages. KHIP also provides a simple mechanism for distributing data encryption keys while adding little overhead to the protocol.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",53–64,12,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1877,article,"Shields, Clay and Garcia-Luna-Aceves, J. J.",KHIP—a Scalable Protocol for Secure Multicast Routing,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316206,10.1145/316194.316206,"We present Keyed HIP (KHIP), a secure, hierarchical multicast routing protocol. We show that other shared-tree multicast routing protocols are subject to attacks against the multicast routing infrastructure that can isolate receivers or domains or introduce loops into the structure of the multicast routing tree. KHIP changes the multicast routing model so that only trusted members are able to join the multicast tree. This protects the multicast routing against attacks that could form branches to unauthorized receivers, prevents replay attacks and limits the effects of flooding attacks. Untrusted routers that are present on the path between trusted routers cannot change the routing and can mount no denial-of-service attack stronger than simply dropping control messages. KHIP also provides a simple mechanism for distributing data encryption keys while adding little overhead to the protocol.",,53–64,12,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1878,inproceedings,"Holbrook, Hugh W. and Cheriton, David R.",IP Multicast Channels: EXPRESS Support for Large-Scale Single-Source Applications,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316207,10.1145/316188.316207,"In the IP multicast model, a set of hosts can be aggregated into a group of hosts with one address, to which any host can send. However, Internet TV, distance learning, file distribution and other emerging large-scale multicast applications strain the current realization of this model, which lacks a basis for charging, lacks access control, and is difficult to scale.This paper proposes an extension to IP multicast to support the channel model of multicast and describes a specific realization called EXPlicitly REquested Single-Source (EXPRESS) multicast. In this model, a multicast channel has exactly one explicitly designated source, and zero or more channel subscribers. A single protocol supports both channel subscription and efficient collection of channel information such as subscriber count. We argue that EXPRESS addresses the aforementioned problems, justifying this multicast service model in the Internet.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",65–78,14,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1879,article,"Holbrook, Hugh W. and Cheriton, David R.",IP Multicast Channels: EXPRESS Support for Large-Scale Single-Source Applications,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316207,10.1145/316194.316207,"In the IP multicast model, a set of hosts can be aggregated into a group of hosts with one address, to which any host can send. However, Internet TV, distance learning, file distribution and other emerging large-scale multicast applications strain the current realization of this model, which lacks a basis for charging, lacks access control, and is difficult to scale.This paper proposes an extension to IP multicast to support the channel model of multicast and describes a specific realization called EXPlicitly REquested Single-Source (EXPRESS) multicast. In this model, a multicast channel has exactly one explicitly designated source, and zero or more channel subscribers. A single protocol supports both channel subscription and efficient collection of channel information such as subscriber count. We argue that EXPRESS addresses the aforementioned problems, justifying this multicast service model in the Internet.",,65–78,14,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1880,inproceedings,"Stoica, Ion and Zhang, Hui",Providing Guaranteed Services without per Flow Management,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316208,10.1145/316188.316208,"Existing approaches for providing guaranteed services require routers to manage per flow states and perform per flow operations [9, 21]. Such a stateful network architecture is less scalable and robust than stateless network architectures like the original IP and the recently proposed Diffserv [3]. However, services provided with current stateless solutions, Diffserv included, have lower flexibility, utilization, and/or assurance level as compared to the services that can be provided with per flow mechanisms.In this paper, we propose techniques that do not require per flow management (either control or data planes) at core routers, but can implement guaranteed services with levels of flexibility, utilization, and assurance similar to those that can be provided with per flow mechanisms. In this way we can simultaneously achieve high quality of service, high scalability and robustness. The key technique we use is called Dynamic Packet State (DPS), which provides a lightweight and robust mechanism for routers to coordinate actions and implement distributed algorithms. We present an implementation of the proposed algorithms that has minimum incompatibility with IPv4.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",81–94,14,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1881,article,"Stoica, Ion and Zhang, Hui",Providing Guaranteed Services without per Flow Management,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316208,10.1145/316194.316208,"Existing approaches for providing guaranteed services require routers to manage per flow states and perform per flow operations [9, 21]. Such a stateful network architecture is less scalable and robust than stateless network architectures like the original IP and the recently proposed Diffserv [3]. However, services provided with current stateless solutions, Diffserv included, have lower flexibility, utilization, and/or assurance level as compared to the services that can be provided with per flow mechanisms.In this paper, we propose techniques that do not require per flow management (either control or data planes) at core routers, but can implement guaranteed services with levels of flexibility, utilization, and assurance similar to those that can be provided with per flow mechanisms. In this way we can simultaneously achieve high quality of service, high scalability and robustness. The key technique we use is called Dynamic Packet State (DPS), which provides a lightweight and robust mechanism for routers to coordinate actions and implement distributed algorithms. We present an implementation of the proposed algorithms that has minimum incompatibility with IPv4.",,81–94,14,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1882,inproceedings,"Duffield, N. G. and Goyal, Pawan and Greenberg, Albert and Mishra, Partho and Ramakrishnan, K. K. and van der Merive, Jacobus E.",A Flexible Model for Resource Management in Virtual Private Networks,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316209,10.1145/316188.316209,"As IP technologies providing both tremendous capacity and the ability to establish dynamic secure associations between endpoints emerge, Virtual Private Networks (VPNs) are going through dramatic growth. The number of endpoints per VPN is growing and the communication pattern between endpoints is becoming increasingly hard to forecast. Consequently, users are demanding dependable, dynamic connectivity between endpoints, with the network expected to accommodate any traffic matrix, as long as the traffic to the endpoints does not overwhelm the rates of the respective ingress and egress links. We propose a new service interface, termed a hose, to provide the appropriate performance abstraction. A hose is characterized by the aggregate traffic to and from one endpoint in the VPN to the set of other endpoints in the VPN, and by an associated performance guarantee.Hoses provide important advantages to a VPN customer: (i) flexibility to send traffic to a set of endpoints without having to specify the detailed traffic matrix, and (ii) reduction in the size of access links through multiplexing gains obtained from the natural aggregation of the flows between endpoints. As compared with the conventional point to point (or customer-pipe) model for managing QoS, hoses provide reduction in the state information a customer must maintain. On the other hand, hoses would appear to increase the complexity of the already difficult problem of resource management to support QoS. To manage network resources in the face of this increased uncertainty, we consider both conventional statistical multiplexing techniques, and a new resizing technique based on online measurements.To study these performance issues, we run trace driven simulations, using traffic derived from AT&T's voice network, and from a large corporate data network. From the customer's perspective, we find that aggregation of traffic at the hose level provides significant multiplexing gains. From the provider's perspective, we find that the statistical multiplexing and resizing techniques deal effectively with uncertainties about the traffic, providing significant gains over the conventional alternative of a mesh of statically sized customer-pipes between endpoints.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",95–108,14,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1883,article,"Duffield, N. G. and Goyal, Pawan and Greenberg, Albert and Mishra, Partho and Ramakrishnan, K. K. and van der Merive, Jacobus E.",A Flexible Model for Resource Management in Virtual Private Networks,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316209,10.1145/316194.316209,"As IP technologies providing both tremendous capacity and the ability to establish dynamic secure associations between endpoints emerge, Virtual Private Networks (VPNs) are going through dramatic growth. The number of endpoints per VPN is growing and the communication pattern between endpoints is becoming increasingly hard to forecast. Consequently, users are demanding dependable, dynamic connectivity between endpoints, with the network expected to accommodate any traffic matrix, as long as the traffic to the endpoints does not overwhelm the rates of the respective ingress and egress links. We propose a new service interface, termed a hose, to provide the appropriate performance abstraction. A hose is characterized by the aggregate traffic to and from one endpoint in the VPN to the set of other endpoints in the VPN, and by an associated performance guarantee.Hoses provide important advantages to a VPN customer: (i) flexibility to send traffic to a set of endpoints without having to specify the detailed traffic matrix, and (ii) reduction in the size of access links through multiplexing gains obtained from the natural aggregation of the flows between endpoints. As compared with the conventional point to point (or customer-pipe) model for managing QoS, hoses provide reduction in the state information a customer must maintain. On the other hand, hoses would appear to increase the complexity of the already difficult problem of resource management to support QoS. To manage network resources in the face of this increased uncertainty, we consider both conventional statistical multiplexing techniques, and a new resizing technique based on online measurements.To study these performance issues, we run trace driven simulations, using traffic derived from AT&T's voice network, and from a large corporate data network. From the customer's perspective, we find that aggregation of traffic at the hose level provides significant multiplexing gains. From the provider's perspective, we find that the statistical multiplexing and resizing techniques deal effectively with uncertainties about the traffic, providing significant gains over the conventional alternative of a mesh of statically sized customer-pipes between endpoints.",,95–108,14,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1884,inproceedings,"Dovrolis, Constantinos and Stiliadis, Dimitrios and Ramanathan, Parameswaran",Proportional Differentiated Services: Delay Differentiation and Packet Scheduling,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316211,10.1145/316188.316211,"Internet applications and users have very diverse service expectations, making the current same-service-to-all model inadequate and limiting. In the relative differentiated services approach, the network traffic is grouped in a small number of service classes which are ordered based on their packet forwarding quality, in terms of per-hop metrics for the queueing delays and packet losses. The users and applications, in this context, can adaptivelychoose the class that best meets their quality and pricing constraints, based on the assurance that higher classes will be better, or at least no worse, than lower classes. In this work, we propose the proportional differentiation model as a way to refine and quantify this basic premise of relative differentiated services. The proportional differentiation model aims to provide the network operator with the 'tuning knobs' for adjusting the quality spacing between classes, independent of the class loads; this cannot be achieved with other relative differentiation models, such as strict prioritization or capacity differentiation. We apply the proportional model on queueing-delay differentiation only, leaving the problem of coupled delay and loss differentiation for future work. We discuss the dynamics of the proportional delay differentiation model and state the conditions under which it is feasible. Then, we identify and evaluate (using simulations) two packet schedulers that approximate the proportional differentiation model in heavy-load conditions, even in short timescales. Finally, we demonstrate that such per-hop and class-based mechanisms can provide consistent end-to-end differentiation to individual flows from different classes, independently of the network path and flow characteristics.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",109–120,12,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1885,article,"Dovrolis, Constantinos and Stiliadis, Dimitrios and Ramanathan, Parameswaran",Proportional Differentiated Services: Delay Differentiation and Packet Scheduling,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316211,10.1145/316194.316211,"Internet applications and users have very diverse service expectations, making the current same-service-to-all model inadequate and limiting. In the relative differentiated services approach, the network traffic is grouped in a small number of service classes which are ordered based on their packet forwarding quality, in terms of per-hop metrics for the queueing delays and packet losses. The users and applications, in this context, can adaptivelychoose the class that best meets their quality and pricing constraints, based on the assurance that higher classes will be better, or at least no worse, than lower classes. In this work, we propose the proportional differentiation model as a way to refine and quantify this basic premise of relative differentiated services. The proportional differentiation model aims to provide the network operator with the 'tuning knobs' for adjusting the quality spacing between classes, independent of the class loads; this cannot be achieved with other relative differentiation models, such as strict prioritization or capacity differentiation. We apply the proportional model on queueing-delay differentiation only, leaving the problem of coupled delay and loss differentiation for future work. We discuss the dynamics of the proportional delay differentiation model and state the conditions under which it is feasible. Then, we identify and evaluate (using simulations) two packet schedulers that approximate the proportional differentiation model in heavy-load conditions, even in short timescales. Finally, we demonstrate that such per-hop and class-based mechanisms can provide consistent end-to-end differentiation to individual flows from different classes, independently of the network path and flow characteristics.",,109–120,12,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1886,inproceedings,"Begel, Andrew and McCanne, Steven and Graham, Susan L.",BPF+: Exploiting Global Data-Flow Optimization in a Generalized Packet Filter Architecture,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316214,10.1145/316188.316214,"A packet filter is a programmable selection criterion for classifying or selecting packets from a packet stream in a generic, reusable fashion. Previous work on packet filters falls roughly into two categories, namely those efforts that investigate flexible and extensible filter abstractions but sacrifice performance, and those that focus on low-level, optimized filtering representations but sacrifice flexibility. Applications like network monitoring and intrusion detection, however, require both high-level expressiveness and raw performance. In this paper, we propose a fully general packet filter framework that affords both a high degree of flexibility and good performance. In our framework, a packet filter is expressed in a high-level language that is compiled into a highly efficient native implementation. The optimization phase of the compiler uses a flowgraph set relation called edge dominators and the novel application of an optimization technique that we call ""redundant predicate elimination,"" in which we interleave partial redundancy elimination, predicate assertion propagation, and flowgraph edge elimination to carry out the filter predicate optimization. Our resulting packet-filtering framework, which we call BPF+, derives from the BSD packet filter (BPF), and includes a filter program translator, a byte code optimizer, a byte code safety verifier to allow code to migrate across protection boundaries, and a just-in-time assembler to convert byte codes to efficient native code. Despite the high degree of flexibility afforded by our generalized framework, our performance measurements show that our system achieves performance comparable to state-of-the-art packet filter architectures and better than hand-coded filters written in C.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",123–134,12,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1887,article,"Begel, Andrew and McCanne, Steven and Graham, Susan L.",BPF+: Exploiting Global Data-Flow Optimization in a Generalized Packet Filter Architecture,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316214,10.1145/316194.316214,"A packet filter is a programmable selection criterion for classifying or selecting packets from a packet stream in a generic, reusable fashion. Previous work on packet filters falls roughly into two categories, namely those efforts that investigate flexible and extensible filter abstractions but sacrifice performance, and those that focus on low-level, optimized filtering representations but sacrifice flexibility. Applications like network monitoring and intrusion detection, however, require both high-level expressiveness and raw performance. In this paper, we propose a fully general packet filter framework that affords both a high degree of flexibility and good performance. In our framework, a packet filter is expressed in a high-level language that is compiled into a highly efficient native implementation. The optimization phase of the compiler uses a flowgraph set relation called edge dominators and the novel application of an optimization technique that we call ""redundant predicate elimination,"" in which we interleave partial redundancy elimination, predicate assertion propagation, and flowgraph edge elimination to carry out the filter predicate optimization. Our resulting packet-filtering framework, which we call BPF+, derives from the BSD packet filter (BPF), and includes a filter program translator, a byte code optimizer, a byte code safety verifier to allow code to migrate across protection boundaries, and a just-in-time assembler to convert byte codes to efficient native code. Despite the high degree of flexibility afforded by our generalized framework, our performance measurements show that our system achieves performance comparable to state-of-the-art packet filter architectures and better than hand-coded filters written in C.",,123–134,12,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1888,inproceedings,"Srinivasan, V. and Suri, S. and Varghese, G.",Packet Classification Using Tuple Space Search,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316216,10.1145/316188.316216,"Routers must perform packet classification at high speeds to efficiently implement functions such as firewalls and QoS routing. Packet classification requires matching each packet against a database of filters (or rules), and forwarding the packet according to the highest priority filter. Existing filter schemes with fast lookup time do not scale to large filter databases. Other more scalable schemes work for 2-dimensional filters, but their lookup times degrade quickly with each additional dimension. While there exist good hardware solutions, our new schemes are geared towards software implementation.We introduce a generic packet classification algorithm, called Tuple Space Search (TSS). Because real databases typically use only a small number of distinct field lengths, by mapping filters to tuples even a simple linear search of the tuple space can provide significant speedup over naive linear search over the filters. Each tuple is maintained as a hash table that can be searched in one memory access. We then introduce techniques for further refining the search of the tuple space, and demonstrate their effectiveness on some firewall databases. For example, a real database of 278 filters had a tuple space of 41 which our algorithm prunes to 11 tuples. Even as we increased the filter database size from 1K to 100K (using a random two-dimensional filter generation model), the number of tuples grew from 53 to only 186, and the pruned tuples only grew from 1 to 4. Our Pruned Tuple Space search is also the only scheme known to us that allows fast updates and fast search times. We also show a lower bound on the general tuple space search problem, and describe an optimal algorithm, called Rectangle Search, for two-dimensional filters.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",135–146,12,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1889,article,"Srinivasan, V. and Suri, S. and Varghese, G.",Packet Classification Using Tuple Space Search,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316216,10.1145/316194.316216,"Routers must perform packet classification at high speeds to efficiently implement functions such as firewalls and QoS routing. Packet classification requires matching each packet against a database of filters (or rules), and forwarding the packet according to the highest priority filter. Existing filter schemes with fast lookup time do not scale to large filter databases. Other more scalable schemes work for 2-dimensional filters, but their lookup times degrade quickly with each additional dimension. While there exist good hardware solutions, our new schemes are geared towards software implementation.We introduce a generic packet classification algorithm, called Tuple Space Search (TSS). Because real databases typically use only a small number of distinct field lengths, by mapping filters to tuples even a simple linear search of the tuple space can provide significant speedup over naive linear search over the filters. Each tuple is maintained as a hash table that can be searched in one memory access. We then introduce techniques for further refining the search of the tuple space, and demonstrate their effectiveness on some firewall databases. For example, a real database of 278 filters had a tuple space of 41 which our algorithm prunes to 11 tuples. Even as we increased the filter database size from 1K to 100K (using a random two-dimensional filter generation model), the number of tuples grew from 53 to only 186, and the pruned tuples only grew from 1 to 4. Our Pruned Tuple Space search is also the only scheme known to us that allows fast updates and fast search times. We also show a lower bound on the general tuple space search problem, and describe an optimal algorithm, called Rectangle Search, for two-dimensional filters.",,135–146,12,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1890,inproceedings,"Gupta, Pankaj and McKeown, Nick",Packet Classification on Multiple Fields,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316217,10.1145/316188.316217,"Routers classify packets to determine which flow they belong to, and to decide what service they should receive. Classification may, in general, be based on an arbitrary number of fields in the packet header. Performing classification quickly on an arbitrary number of fields is known to be difficult, and has poor worst-case performance. In this paper, we consider a number of classifiers taken from real networks. We find that the classifiers contain considerable structure and redundancy that can be exploited by the classification algorithm. In particular, we find that a simple multi-stage classification algorithm, called RFC (recursive flow classification), can classify 30 million packets per second in pipelined hardware, or one million packets per second in software.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",147–160,14,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1891,article,"Gupta, Pankaj and McKeown, Nick",Packet Classification on Multiple Fields,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316217,10.1145/316194.316217,"Routers classify packets to determine which flow they belong to, and to decide what service they should receive. Classification may, in general, be based on an arbitrary number of fields in the packet header. Performing classification quickly on an arbitrary number of fields is known to be difficult, and has poor worst-case performance. In this paper, we consider a number of classifiers taken from real networks. We find that the classifiers contain considerable structure and redundancy that can be exploited by the classification algorithm. In particular, we find that a simple multi-stage classification algorithm, called RFC (recursive flow classification), can classify 30 million packets per second in pipelined hardware, or one million packets per second in software.",,147–160,14,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1892,inproceedings,"Yu, Haobo and Breslau, Lee and Shenker, Scott",A Scalable Web Cache Consistency Architecture,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316219,10.1145/316188.316219,"The rapid increase in web usage has led to dramatically increased loads on the network infrastructure and on individual web servers. To ameliorate these mounting burdens, there has been much recent interest in web caching architectures and algorithms. Web caching reduces network load, server load, and the latency of responses. However, web caching has the disadvantage that the pages returned to clients by caches may be stale, in that they may not be consistent with the version currently on the server. In this paper we describe a scalable web cache consistency architecture that provides fairly tight bounds on the staleness of pages. Our architecture borrows heavily from the literature, and can best be described as an invalidation approach made scalable by using a caching hierarchy and application-level multicast routing to convey the invalidations. We evaluate this design with calculations and simulations, and compare it to several other approaches.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",163–174,12,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1893,article,"Yu, Haobo and Breslau, Lee and Shenker, Scott",A Scalable Web Cache Consistency Architecture,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316219,10.1145/316194.316219,"The rapid increase in web usage has led to dramatically increased loads on the network infrastructure and on individual web servers. To ameliorate these mounting burdens, there has been much recent interest in web caching architectures and algorithms. Web caching reduces network load, server load, and the latency of responses. However, web caching has the disadvantage that the pages returned to clients by caches may be stale, in that they may not be consistent with the version currently on the server. In this paper we describe a scalable web cache consistency architecture that provides fairly tight bounds on the staleness of pages. Our architecture borrows heavily from the literature, and can best be described as an invalidation approach made scalable by using a caching hierarchy and application-level multicast routing to convey the invalidations. We evaluate this design with calculations and simulations, and compare it to several other approaches.",,163–174,12,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1894,inproceedings,"Balakrishnan, Hari and Rahul, Hariharan S. and Seshan, Srinivasan",An Integrated Congestion Management Architecture for Internet Hosts,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316220,10.1145/316188.316220,"This paper presents a novel framework for managing network congestion from an end-to-end perspective. Our work is motivated by trends in traffic patterns that threaten the long-term stability of the Internet. These trends include the use of multiple independent concurrent flows by Web applications and the increasing use of transport protocols and applications that do not adapt to congestion. We present an end-system architecture centered around a Congestion Manager (CM) that ensures proper congestion behavior and allows applications to easily adapt to network congestion. Our framework integrates congestion management across all applications and transport protocols. The CM maintains congestion parameters and exposes an API to enable applications to learn about network characteristics, pass information to the CM, and schedule data transmissions. Internally, it uses a window-based control algorithm, a scheduler to regulate transmissions, and a lightweight protocol to elicit feedback from receivers.We describe how TCP and an adaptive real-time streaming audio application can be implemented using the CM. Our simulation results show that an ensemble of concurrent TCP connections can effectively share bandwidth and obtain consistent performance, without adversely affecting other network flows. Our results also show that the CM enables audio applications to adapt to congestion conditions without having to perform congestion control or bandwidth probing on their own. We conclude that the CM provides a useful and pragmatic framework for building adaptive Internet applications.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",175–187,13,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1895,article,"Balakrishnan, Hari and Rahul, Hariharan S. and Seshan, Srinivasan",An Integrated Congestion Management Architecture for Internet Hosts,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316220,10.1145/316194.316220,"This paper presents a novel framework for managing network congestion from an end-to-end perspective. Our work is motivated by trends in traffic patterns that threaten the long-term stability of the Internet. These trends include the use of multiple independent concurrent flows by Web applications and the increasing use of transport protocols and applications that do not adapt to congestion. We present an end-system architecture centered around a Congestion Manager (CM) that ensures proper congestion behavior and allows applications to easily adapt to network congestion. Our framework integrates congestion management across all applications and transport protocols. The CM maintains congestion parameters and exposes an API to enable applications to learn about network characteristics, pass information to the CM, and schedule data transmissions. Internally, it uses a window-based control algorithm, a scheduler to regulate transmissions, and a lightweight protocol to elicit feedback from receivers.We describe how TCP and an adaptive real-time streaming audio application can be implemented using the CM. Our simulation results show that an ensemble of concurrent TCP connections can effectively share bandwidth and obtain consistent performance, without adversely affecting other network flows. Our results also show that the CM enables audio applications to adapt to congestion conditions without having to perform congestion control or bandwidth probing on their own. We conclude that the CM provides a useful and pragmatic framework for building adaptive Internet applications.",,175–187,13,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1896,inproceedings,"Rejaie, Reza and Handley, Mark and Estrin, Deborah",Quality Adaptation for Congestion Controlled Video Playback over the Internet,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316222,10.1145/316188.316222,"Streaming audio and video applications are becoming increasingly popular on the Internet, and the lack of effective congestion control in such applications is now a cause for significant concern. The problem is one of adapting the compression without requiring video-servers to re-encode the data, and fitting the resulting stream into the rapidly varying available bandwidth. At the same time, rapid fluctuations in quality will be disturbing to the users and should be avoided.In this paper we present a mechanism for using layered video in the context of unicast congestion control. This quality adaptation mechanism adds and drops layers of the video stream to perform long-term coarse-grain adaptation, while using a TCP-friendly congestion control mechanism to react to congestion on very short timescales. The mismatches between the two timescales are absorbed using buffering at the receiver. We present an efficient-scheme for the distribution of buffering among the active layers. Our scheme allows the server to trade short-term improvement for long-term smoothing of quality. We discuss the issues involved in implementing and tuning such a mechanism, and present our simulation results.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",189–200,12,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1897,article,"Rejaie, Reza and Handley, Mark and Estrin, Deborah",Quality Adaptation for Congestion Controlled Video Playback over the Internet,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316222,10.1145/316194.316222,"Streaming audio and video applications are becoming increasingly popular on the Internet, and the lack of effective congestion control in such applications is now a cause for significant concern. The problem is one of adapting the compression without requiring video-servers to re-encode the data, and fitting the resulting stream into the rapidly varying available bandwidth. At the same time, rapid fluctuations in quality will be disturbing to the users and should be avoided.In this paper we present a mechanism for using layered video in the context of unicast congestion control. This quality adaptation mechanism adds and drops layers of the video stream to perform long-term coarse-grain adaptation, while using a TCP-friendly congestion control mechanism to react to congestion on very short timescales. The mismatches between the two timescales are absorbed using buffering at the receiver. We present an efficient-scheme for the distribution of buffering among the active layers. Our scheme allows the server to trade short-term improvement for long-term smoothing of quality. We discuss the issues involved in implementing and tuning such a mechanism, and present our simulation results.",,189–200,12,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1898,inproceedings,"Bremler-Barr, Anat and Afek, Yehuda and Har-Peled, Sariel",Routing with a Clue,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316223,10.1145/316188.316223,"We suggest a new simple forwarding technique to speed-up IP destination address lookup. The technique is a natural extension of IP, requires 5 bits in the IP header (IPv4, 7 in IPv6) and performs IP lookup nearly as fast as IP/Tag-switching but with a smaller memory requirement and a much simpler protocol. The basic idea is that each router adds a ""clue"" to each packet, telling its downstream router where it ended the IP lookup. Since the forwarding tables of neighboring routers are similar, the clue either directly determines the best prefix match for the downstream router, or provides the downstream router with a good point to start its IP lookup. The new scheme thus prevents repeated computations and distributes the lookup process across the routers along the packet path. Each router starts the lookup computation at the point its up-stream neighbor has finished. Furthermore, the new scheme is easily assimilated into heterogeneous IP networks, does not require routers coordination, and requires no setup time. Even a flow of one packet enjoys the benefits of the scheme without any additional overhead. The speedup we achieve is about 10 times faster than current standard techniques. In a sense this paper shows that the current routers employed in the Internet are clue-less; Namely, it is possible to speedup the IP-lookup by an order of magnitude without any major changes to the existing protocols.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",203–214,12,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1899,article,"Bremler-Barr, Anat and Afek, Yehuda and Har-Peled, Sariel",Routing with a Clue,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316223,10.1145/316194.316223,"We suggest a new simple forwarding technique to speed-up IP destination address lookup. The technique is a natural extension of IP, requires 5 bits in the IP header (IPv4, 7 in IPv6) and performs IP lookup nearly as fast as IP/Tag-switching but with a smaller memory requirement and a much simpler protocol. The basic idea is that each router adds a ""clue"" to each packet, telling its downstream router where it ended the IP lookup. Since the forwarding tables of neighboring routers are similar, the clue either directly determines the best prefix match for the downstream router, or provides the downstream router with a good point to start its IP lookup. The new scheme thus prevents repeated computations and distributes the lookup process across the routers along the packet path. Each router starts the lookup computation at the point its up-stream neighbor has finished. Furthermore, the new scheme is easily assimilated into heterogeneous IP networks, does not require routers coordination, and requires no setup time. Even a flow of one packet enjoys the benefits of the scheme without any additional overhead. The speedup we achieve is about 10 times faster than current standard techniques. In a sense this paper shows that the current routers employed in the Internet are clue-less; Namely, it is possible to speedup the IP-lookup by an order of magnitude without any major changes to the existing protocols.",,203–214,12,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1900,inproceedings,"Shaikh, Anees and Rexford, Jennifer and Shin, Kang G.",Load-Sensitive Routing of Long-Lived IP Flows,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316225,10.1145/316188.316225,"Internet service providers face a daunting challenge in provisioning network resources, due to the rapid growth of the Internet and wide fluctuations in the underlying traffic patterns. The ability of dynamic routing to circumvent congested links and improve application performance makes it a valuable traffic engineering tool. However, deployment of load-sensitive routing is hampered by the overheads imposed by link-state update propagation, path selection, and signaling. Under reasonable protocol and computational overheads, traditional approaches to load-sensitive routing of IP traffic are ineffective, and can introduce significant route flapping, since paths are selected based on out-of-date link-state information. Although stability is improved by performing load-sensitive routing at the flow level, flapping still occurs, because most IP flows have a short duration relative to the desired frequency of link-state updates. To address the efficiency and stability challenges of load-sensitive routing, we introduce a new hybrid approach that performs dynamic routing of long-lived flows, while forwarding short-lived flows on static preprovisioned paths. By relating the detection of long-lived flows to the timescale of link-state update messages in the routing protocol, route stability is considerably improved. Through simulation experiments using a one-week ISP packet trace, we show that our hybrid approach significantly outperforms traditional static and dynamic routing schemes, by reacting to fluctuations in network load without introducing route flapping.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",215–226,12,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1901,article,"Shaikh, Anees and Rexford, Jennifer and Shin, Kang G.",Load-Sensitive Routing of Long-Lived IP Flows,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316225,10.1145/316194.316225,"Internet service providers face a daunting challenge in provisioning network resources, due to the rapid growth of the Internet and wide fluctuations in the underlying traffic patterns. The ability of dynamic routing to circumvent congested links and improve application performance makes it a valuable traffic engineering tool. However, deployment of load-sensitive routing is hampered by the overheads imposed by link-state update propagation, path selection, and signaling. Under reasonable protocol and computational overheads, traditional approaches to load-sensitive routing of IP traffic are ineffective, and can introduce significant route flapping, since paths are selected based on out-of-date link-state information. Although stability is improved by performing load-sensitive routing at the flow level, flapping still occurs, because most IP flows have a short duration relative to the desired frequency of link-state updates. To address the efficiency and stability challenges of load-sensitive routing, we introduce a new hybrid approach that performs dynamic routing of long-lived flows, while forwarding short-lived flows on static preprovisioned paths. By relating the detection of long-lived flows to the timescale of link-state update messages in the routing protocol, route stability is considerably improved. Through simulation experiments using a one-week ISP packet trace, we show that our hybrid approach significantly outperforms traditional static and dynamic routing schemes, by reacting to fluctuations in network load without introducing route flapping.",,215–226,12,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1902,inproceedings,"Vutukury, Srinivas and Garcia-Luna-Aceves, J. J.",A Simple Approximation to Minimum-Delay Routing,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316227,10.1145/316188.316227,"The conventional approach to routing in computer networks consists of using a heuristic to compute a single shortest path from a source to a destination. Single-path routing is very responsive to topological and link-cost changes; however, except under light traffic loads, the delays obtained with this type of routing are far from optimal. Furthermore, if link costs are associated with delays, single-path routing exhibits oscillatory behavior and becomes unstable as traffic loads increase. On the other hand, minimum-delay routing approaches can minimize delays only when traffic is stationary or very slowly changing.We present a ""near-optimal"" routing framework that offers delays comparable to those of optimal routing and that is as flexible and responsive as single-path routing protocols proposed to date. First, an approximation to the Gallager's minimum-delay routing problem is derived, and then algorithms that implement the approximation scheme are presented and verified. We introduce the first routing algorithm based on link-state information that provides multiple paths of unequal cost to each destination that are loop-free at every instant. We show through simulations that the delays obtained in our framework are comparable to those obtained using the Gallager's minimum-delay routing. Also, we show that our framework renders far smaller delays and makes better use of resources than traditional single-path routing.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",227–238,12,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1903,article,"Vutukury, Srinivas and Garcia-Luna-Aceves, J. J.",A Simple Approximation to Minimum-Delay Routing,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316227,10.1145/316194.316227,"The conventional approach to routing in computer networks consists of using a heuristic to compute a single shortest path from a source to a destination. Single-path routing is very responsive to topological and link-cost changes; however, except under light traffic loads, the delays obtained with this type of routing are far from optimal. Furthermore, if link costs are associated with delays, single-path routing exhibits oscillatory behavior and becomes unstable as traffic loads increase. On the other hand, minimum-delay routing approaches can minimize delays only when traffic is stationary or very slowly changing.We present a ""near-optimal"" routing framework that offers delays comparable to those of optimal routing and that is as flexible and responsive as single-path routing protocols proposed to date. First, an approximation to the Gallager's minimum-delay routing problem is derived, and then algorithms that implement the approximation scheme are presented and verified. We introduce the first routing algorithm based on link-state information that provides multiple paths of unequal cost to each destination that are loop-free at every instant. We show through simulations that the delays obtained in our framework are comparable to those obtained using the Gallager's minimum-delay routing. Also, we show that our framework renders far smaller delays and makes better use of resources than traditional single-path routing.",,227–238,12,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1904,inproceedings,"Downey, Allen B.",Using Pathchar to Estimate Internet Link Characteristics,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316228,10.1145/316188.316228,"We evaluate pathchar, a tool that infers the characteristics of links along an Internet path (latency, bandwidth, queue delays). Looking at two example paths, we identify circumstances where pathchar is likely to succeed, and develop techniques to improve the accuracy of pathchar's estimates and reduce the time it takes to generate them. The most successful of these techniques is a form of adaptive data collection that reduces the number of measurements pathchar needs by more than 90% for some links.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",241–250,10,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1905,article,"Downey, Allen B.",Using Pathchar to Estimate Internet Link Characteristics,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316228,10.1145/316194.316228,"We evaluate pathchar, a tool that infers the characteristics of links along an Internet path (latency, bandwidth, queue delays). Looking at two example paths, we identify circumstances where pathchar is likely to succeed, and develop techniques to improve the accuracy of pathchar's estimates and reduce the time it takes to generate them. The most successful of these techniques is a form of adaptive data collection that reduces the number of measurements pathchar needs by more than 90% for some links.",,241–250,10,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1906,inproceedings,"Faloutsos, Michalis and Faloutsos, Petros and Faloutsos, Christos",On Power-Law Relationships of the Internet Topology,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316229,10.1145/316188.316229,"Despite the apparent randomness of the Internet, we discover some surprisingly simple power-laws of the Internet topology. These power-laws hold for three snapshots of the Internet, between November 1997 and December 1998, despite a 45% growth of its size during that period. We show that our power-laws fit the real data very well resulting in correlation coefficients of 96% or higher.Our observations provide a novel perspective of the structure of the Internet. The power-laws describe concisely skewed distributions of graph properties such as the node outdegree. In addition, these power-laws can be used to estimate important parameters such as the average neighborhood size, and facilitate the design and the performance analysis of protocols. Furthermore, we can use them to generate and select realistic topologies for simulation purposes.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",251–262,12,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1907,article,"Faloutsos, Michalis and Faloutsos, Petros and Faloutsos, Christos",On Power-Law Relationships of the Internet Topology,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316229,10.1145/316194.316229,"Despite the apparent randomness of the Internet, we discover some surprisingly simple power-laws of the Internet topology. These power-laws hold for three snapshots of the Internet, between November 1997 and December 1998, despite a 45% growth of its size during that period. We show that our power-laws fit the real data very well resulting in correlation coefficients of 96% or higher.Our observations provide a novel perspective of the structure of the Internet. The power-laws describe concisely skewed distributions of graph properties such as the node outdegree. In addition, these power-laws can be used to estimate important parameters such as the average neighborhood size, and facilitate the design and the performance analysis of protocols. Furthermore, we can use them to generate and select realistic topologies for simulation purposes.",,251–262,12,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1908,inproceedings,"Allman, Mark and Paxson, Vern",On Estimating End-to-End Network Path Properties,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316230,10.1145/316188.316230,"The more information about current network conditions available to a transport protocol, the more efficiently it can use the network to transfer its data. In networks such as the Internet, the transport protocol must often form its own estimates of network properties based on measurements performed by the connection endpoints. We consider two basic transport estimation problems: determining the setting of the retransmission timer (RTO) for a reliable protocol, and estimating the bandwidth available to a connection as it begins. We look at both of these problems in the context of TCP, using a large TCP measurement set [Pax97b] for trace-driven simulations. For RTO estimation, we evaluate a number of different algorithms, finding that the performance of the estimators is dominated by their minimum values, and to a lesser extent, the timer granularity, while being virtually unaffected by how often round-trip time measurements are made or the settings of the parameters in the exponentially-weighted moving average estimators commonly used. For bandwidth estimation, we explore techniques previously sketched in the literature [Hoe96, AD98] and find that in practice they perform less well than anticipated. We then develop a receiver-side algorithm that performs significantly better.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",263–274,12,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1909,article,"Allman, Mark and Paxson, Vern",On Estimating End-to-End Network Path Properties,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316230,10.1145/316194.316230,"The more information about current network conditions available to a transport protocol, the more efficiently it can use the network to transfer its data. In networks such as the Internet, the transport protocol must often form its own estimates of network properties based on measurements performed by the connection endpoints. We consider two basic transport estimation problems: determining the setting of the retransmission timer (RTO) for a reliable protocol, and estimating the bandwidth available to a connection as it begins. We look at both of these problems in the context of TCP, using a large TCP measurement set [Pax97b] for trace-driven simulations. For RTO estimation, we evaluate a number of different algorithms, finding that the performance of the estimators is dominated by their minimum values, and to a lesser extent, the timer granularity, while being virtually unaffected by how often round-trip time measurements are made or the settings of the parameters in the exponentially-weighted moving average estimators commonly used. For bandwidth estimation, we explore techniques previously sketched in the literature [Hoe96, AD98] and find that in practice they perform less well than anticipated. We then develop a receiver-side algorithm that performs significantly better.",,263–274,12,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1910,inproceedings,"Griffin, Timothy G. and Wilfong, Gordon",An Analysis of BGP Convergence Properties,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316231,10.1145/316188.316231,The Border Gateway Protocol (BGP) is the de facto inter-domain routing protocol used to exchange reachability information between Autonomous Systems in the global Internet. BGP is a path-vector protocol that allows each Autonomous System to override distance-based metrics with policy-based metrics when choosing best routes. Varadhan et al. [18] have shown that it is possible for a group of Autonomous Systems to independently define BGP policies that together lead to BGP protocol oscillations that never converge on a stable routing. One approach to addressing this problem is based on static analysis of routing policies to determine if they are safe. We explore the worst-case complexity for convergence-oriented static analysis of BGP routing policies. We present an abstract model of BGP and use it to define several global sanity conditions on routing policies that are related to BGP convergence/divergence. For each condition we show that the complexity of statically checking it is either NP-complete or NP-hard.,"Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",277–288,12,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1911,article,"Griffin, Timothy G. and Wilfong, Gordon",An Analysis of BGP Convergence Properties,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316231,10.1145/316194.316231,The Border Gateway Protocol (BGP) is the de facto inter-domain routing protocol used to exchange reachability information between Autonomous Systems in the global Internet. BGP is a path-vector protocol that allows each Autonomous System to override distance-based metrics with policy-based metrics when choosing best routes. Varadhan et al. [18] have shown that it is possible for a group of Autonomous Systems to independently define BGP policies that together lead to BGP protocol oscillations that never converge on a stable routing. One approach to addressing this problem is based on static analysis of routing policies to determine if they are safe. We explore the worst-case complexity for convergence-oriented static analysis of BGP routing policies. We present an abstract model of BGP and use it to define several global sanity conditions on routing policies that are related to BGP convergence/divergence. For each condition we show that the complexity of statically checking it is either NP-complete or NP-hard.,,277–288,12,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1912,inproceedings,"Savage, Stefan and Collins, Andy and Hoffman, Eric and Snell, John and Anderson, Thomas",The End-to-End Effects of Internet Path Selection,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316233,10.1145/316188.316233,"The path taken by a packet traveling across the Internet depends on a large number of factors, including routing protocols and per-network routing policies. The impact of these factors on the end-to-end performance experienced by users is poorly understood. In this paper, we conduct a measurement-based study comparing the performance seen using the ""default"" path taken in the Internet with the potential performance available using some alternate path. Our study uses five distinct datasets containing measurements of ""path quality"", such as round-trip time, loss rate, and bandwidth, taken between pairs of geographically diverse Internet hosts. We construct the set of potential alternate paths by composing these measurements to form new synthetic paths. We find that in 30-80% of the cases, there is an alternate path with significantly superior quality. We argue that the overall result is robust and we explore two hypotheses for explaining it.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",289–299,11,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1913,article,"Savage, Stefan and Collins, Andy and Hoffman, Eric and Snell, John and Anderson, Thomas",The End-to-End Effects of Internet Path Selection,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316233,10.1145/316194.316233,"The path taken by a packet traveling across the Internet depends on a large number of factors, including routing protocols and per-network routing policies. The impact of these factors on the end-to-end performance experienced by users is poorly understood. In this paper, we conduct a measurement-based study comparing the performance seen using the ""default"" path taken in the Internet with the potential performance available using some alternate path. Our study uses five distinct datasets containing measurements of ""path quality"", such as round-trip time, loss rate, and bandwidth, taken between pairs of geographically diverse Internet hosts. We construct the set of potential alternate paths by composing these measurements to form new synthetic paths. We find that in 30-80% of the cases, there is an alternate path with significantly superior quality. We argue that the overall result is robust and we explore two hypotheses for explaining it.",,289–299,11,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1914,inproceedings,"Feldmann, Anja and Gilbert, Anna C. and Huang, Polly and Willinger, Walter",Dynamics of IP Traffic: A Study of the Role of Variability and the Impact of Control,1999,1581131356,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316188.316235,10.1145/316188.316235,"Using the ns-2-simulator to experiment with different aspects of user- or session-behaviors and network configurations and focusing on the qualitative aspects of a wavelet-based scaling analysis, we present a systematic investigation into how and why variability and feedback-control contribute to the intriguing scaling properties observed in actual Internet traces (as our benchmark data, we use measured Internet traffic from an ISP). We illustrate how variability of both user aspects and network environments (i) causes self-similar scaling behavior over large time scales, (ii) determines a more or less pronounced change in scaling behavior around a specific time scale, and (iii) sets the stage for the emergence of surprisingly rich scaling dynamics over small time scales; i.e., multifractal scaling. Moreover, our scaling analyses indicate whether or not open-loop controls such as UDP or closed-loop controls such as TCP impact the local or small-scale behavior of the traffic and how they contribute to the observed multifractal nature of measured Internet traffic. In fact, our findings suggest an initial physical explanation for why measured Internet traffic over small time scales is highly complex and suggest novel ways for detecting and identifying, for example, performance bottlenecks.This paper focuses on the qualitative aspects of a wavelet-based scaling analysis rather than on the quantitative use for which it was originally designed. We demonstrate how the presented techniques can be used for analyzing a wide range of different kinds of network-related measurements in ways that were not previously feasible. We show that scaling analysis has the ability to extract relevant information about the time-scale dynamics of Internet traffic, thereby, we hope, making these techniques available to a larger segment of the networking research community.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",301–313,13,,"Cambridge, Massachusetts, USA",SIGCOMM '99,,,,,,
1915,article,"Feldmann, Anja and Gilbert, Anna C. and Huang, Polly and Willinger, Walter",Dynamics of IP Traffic: A Study of the Role of Variability and the Impact of Control,1999,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/316194.316235,10.1145/316194.316235,"Using the ns-2-simulator to experiment with different aspects of user- or session-behaviors and network configurations and focusing on the qualitative aspects of a wavelet-based scaling analysis, we present a systematic investigation into how and why variability and feedback-control contribute to the intriguing scaling properties observed in actual Internet traces (as our benchmark data, we use measured Internet traffic from an ISP). We illustrate how variability of both user aspects and network environments (i) causes self-similar scaling behavior over large time scales, (ii) determines a more or less pronounced change in scaling behavior around a specific time scale, and (iii) sets the stage for the emergence of surprisingly rich scaling dynamics over small time scales; i.e., multifractal scaling. Moreover, our scaling analyses indicate whether or not open-loop controls such as UDP or closed-loop controls such as TCP impact the local or small-scale behavior of the traffic and how they contribute to the observed multifractal nature of measured Internet traffic. In fact, our findings suggest an initial physical explanation for why measured Internet traffic over small time scales is highly complex and suggest novel ways for detecting and identifying, for example, performance bottlenecks.This paper focuses on the qualitative aspects of a wavelet-based scaling analysis rather than on the quantitative use for which it was originally designed. We demonstrate how the presented techniques can be used for analyzing a wide range of different kinds of network-related measurements in ways that were not previously feasible. We show that scaling analysis has the ability to extract relevant information about the time-scale dynamics of Internet traffic, thereby, we hope, making these techniques available to a larger segment of the networking research community.",,301–313,13,,,,Oct. 1999,29,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
1916,inproceedings,"Breslau, Lee and Shenker, Scott",Best-Effort versus Reservations: A Simple Comparative Analysis,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285248,10.1145/285237.285248,"Using a simple analytical model, this paper addresses the following question: Should the Internet retain its best-effort-only architecture, or should it adopt one that is reservation-capable? We characterize the differences between reservation-capable and best-effort-only networks in terms of application performance and total welfare. Our analysis does not yield a definitive answer to the question we pose, since it would necessarily depend on unknowable factors such as the future cost of network bandwidth and the nature of the future traffic load. However, our model does reveal some interesting phenomena. First, in some circumstances, the amount of incremental bandwidth needed to make a best-effort-only network perform as well as a reservation capable one diverges as capacity increases. Second, in some circumstances reservation-capable networks retain significant advantages over best-effort-only networks, no matter how cheap bandwidth becomes. Lastly, we find bounds on the maximum performance advantage a reservation-capable network can achieve over best-effort architectures.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",3–16,14,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1917,article,"Breslau, Lee and Shenker, Scott",Best-Effort versus Reservations: A Simple Comparative Analysis,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285248,10.1145/285243.285248,"Using a simple analytical model, this paper addresses the following question: Should the Internet retain its best-effort-only architecture, or should it adopt one that is reservation-capable? We characterize the differences between reservation-capable and best-effort-only networks in terms of application performance and total welfare. Our analysis does not yield a definitive answer to the question we pose, since it would necessarily depend on unknowable factors such as the future cost of network bandwidth and the nature of the future traffic load. However, our model does reveal some interesting phenomena. First, in some circumstances, the amount of incremental bandwidth needed to make a best-effort-only network perform as well as a reservation capable one diverges as capacity increases. Second, in some circumstances reservation-capable networks retain significant advantages over best-effort-only networks, no matter how cheap bandwidth becomes. Lastly, we find bounds on the maximum performance advantage a reservation-capable network can achieve over best-effort architectures.",,3–16,14,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1918,inproceedings,"Apostolopoulos, George and Gu\'{e",Quality of Service Based Routing: A Performance Perspective,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285251,10.1145/285237.285251,"Recent studies provide evidence that Quality of Service (QoS) routing can provide increased network utilization compared to routing that is not sensitive to QoS requirements of traffic. However, there are still strong concerns about the increased cost of QoS routing, both in terms of more complex and frequent computations and increased routing protocol overhead. The main goals of this paper are to study these two cost components, and propose solutions that achieve good routing performance with reduced processing cost. First, we identify the parameters that determine the protocol traffic overhead, namely (a) policy for triggering updates, (b) sensitivity of this policy, and (c) clamp down timers that limit the rate of updates. Using simulation, we study the relative significance of these factors and investigate the relationship between routing performance and the amount of update traffic. In addition, we explore a range of design options to reduce the processing cost of QoS routing algorithms, and study their effect on routing performance. Based on the conclusions of these studies, we develop extensions to the basic QoS routing, that can achieve good routing performance with limited update generation rates. The paper also addresses the impact on the results of a number of secondary factors such as topology, high level admission control, and characteristics of network traffic.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",17–28,12,"performance evaluation, path pre-computation, QoS routing, link state routing","Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1919,article,"Apostolopoulos, George and Gu\'{e",Quality of Service Based Routing: A Performance Perspective,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285251,10.1145/285243.285251,"Recent studies provide evidence that Quality of Service (QoS) routing can provide increased network utilization compared to routing that is not sensitive to QoS requirements of traffic. However, there are still strong concerns about the increased cost of QoS routing, both in terms of more complex and frequent computations and increased routing protocol overhead. The main goals of this paper are to study these two cost components, and propose solutions that achieve good routing performance with reduced processing cost. First, we identify the parameters that determine the protocol traffic overhead, namely (a) policy for triggering updates, (b) sensitivity of this policy, and (c) clamp down timers that limit the rate of updates. Using simulation, we study the relative significance of these factors and investigate the relationship between routing performance and the amount of update traffic. In addition, we explore a range of design options to reduce the processing cost of QoS routing algorithms, and study their effect on routing performance. Based on the conclusions of these studies, we develop extensions to the basic QoS routing, that can achieve good routing performance with limited update generation rates. The paper also addresses the impact on the results of a number of secondary factors such as topology, high level admission control, and characteristics of network traffic.",,17–28,12,"QoS routing, performance evaluation, path pre-computation, link state routing",,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1920,inproceedings,Gu\'{e,Scalable QoS Provision through Buffer Management,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285254,10.1145/285237.285254,"In recent years, a number of link scheduling algorithms have been proposed that greatly improve upon traditional FIFO scheduling in being able to assure rate and delay bounds for individual sessions. However, they cannot be easily deployed in a backbone environment with thousands of sessions, as their complexity increases with the number of sessions. In this paper, we propose and analyze an approach that uses a simple buffer management scheme to provide rate guarantees to individual flows (or to a set of flows) multiplexed into a common FIFO queue. We establish the buffer allocation requirements to achieve these rate guarantees and study the trade-off between the achievable link utilization and the buffer size required with the proposed scheme. The aspect of fair access to excess bandwidth is also addressed, and its mapping onto a buffer allocation rule is investigated. Numerical examples are provided that illustrate the performance of the proposed schemes. Finally, a scalable architecture for QoS provisioning is presented that integrates the proposed buffer management scheme with WFQ scheduling that uses a small number of queues.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",29–40,12,"buffer management, scheduling, fairness, rate guarantees, sharing","Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1921,article,Gu\'{e,Scalable QoS Provision through Buffer Management,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285254,10.1145/285243.285254,"In recent years, a number of link scheduling algorithms have been proposed that greatly improve upon traditional FIFO scheduling in being able to assure rate and delay bounds for individual sessions. However, they cannot be easily deployed in a backbone environment with thousands of sessions, as their complexity increases with the number of sessions. In this paper, we propose and analyze an approach that uses a simple buffer management scheme to provide rate guarantees to individual flows (or to a set of flows) multiplexed into a common FIFO queue. We establish the buffer allocation requirements to achieve these rate guarantees and study the trade-off between the achievable link utilization and the buffer size required with the proposed scheme. The aspect of fair access to excess bandwidth is also addressed, and its mapping onto a buffer allocation rule is investigated. Numerical examples are provided that illustrate the performance of the proposed schemes. Finally, a scalable architecture for QoS provisioning is presented that integrates the proposed buffer management scheme with WFQ scheduling that uses a small number of queues.",,29–40,12,"rate guarantees, buffer management, scheduling, sharing, fairness",,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1922,inproceedings,"Feldmann, A. and Gilbert, A. C. and Willinger, W.",Data Networks as Cascades: Investigating the Multifractal Nature of Internet WAN Traffic,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285256,10.1145/285237.285256,"In apparent contrast to the well-documented self-similar (i.e., monofractal) scaling behavior of measured LAN traffic, recent studies have suggested that measured TCP/IP and ATM WAN traffic exhibits more complex scaling behavior, consistent with multifractals. To bring multifractals into the realm of networking, this paper provides a simple construction based on cascades (also known as multiplicative processes) that is motivated by the protocol hierarchy of IP data networks. The cascade framework allows for a plausible physical explanation of the observed multifractal scaling behavior of data traffic and suggests that the underlying multiplicative structure is a traffic invariant for WAN traffic that co-exists with self-similarity. In particular, cascades allow us to refine the previously observed self-similar nature of data traffic to account for local irregularities in WAN traffic that are typically associated with networking mechanisms operating on small time scales, such as TCP flow control.To validate our approach, we show that recent measurements of Internet WAN traffic from both an ISP and a corporate environment are consistent with the proposed cascade paradigm and hence with multifractality. We rely on wavelet-based time-scale analysis techniques to visualize and to infer the scaling behavior of the traces, both globally and locally. We also discuss and illustrate with some examples how this cascade-based approach to describing data network traffic suggests novel ways for dealing with networking problems and helps in building intuition and physical understanding about the possible implications of multifractality on issues related to network performance analysis.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",42–55,14,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1923,article,"Feldmann, A. and Gilbert, A. C. and Willinger, W.",Data Networks as Cascades: Investigating the Multifractal Nature of Internet WAN Traffic,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285256,10.1145/285243.285256,"In apparent contrast to the well-documented self-similar (i.e., monofractal) scaling behavior of measured LAN traffic, recent studies have suggested that measured TCP/IP and ATM WAN traffic exhibits more complex scaling behavior, consistent with multifractals. To bring multifractals into the realm of networking, this paper provides a simple construction based on cascades (also known as multiplicative processes) that is motivated by the protocol hierarchy of IP data networks. The cascade framework allows for a plausible physical explanation of the observed multifractal scaling behavior of data traffic and suggests that the underlying multiplicative structure is a traffic invariant for WAN traffic that co-exists with self-similarity. In particular, cascades allow us to refine the previously observed self-similar nature of data traffic to account for local irregularities in WAN traffic that are typically associated with networking mechanisms operating on small time scales, such as TCP flow control.To validate our approach, we show that recent measurements of Internet WAN traffic from both an ISP and a corporate environment are consistent with the proposed cascade paradigm and hence with multifractality. We rely on wavelet-based time-scale analysis techniques to visualize and to infer the scaling behavior of the traces, both globally and locally. We also discuss and illustrate with some examples how this cascade-based approach to describing data network traffic suggests novel ways for dealing with networking problems and helps in building intuition and physical understanding about the possible implications of multifractality on issues related to network performance analysis.",,42–55,14,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1924,inproceedings,"Byers, John W. and Luby, Michael and Mitzenmacher, Michael and Rege, Ashutosh",A Digital Fountain Approach to Reliable Distribution of Bulk Data,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285258,10.1145/285237.285258,"The proliferation of applications that must reliably distribute bulk data to a large number of autonomous clients motivates the design of new multicast and broadcast protocols. We describe an ideal, fully scalable protocol for these applications that we call a digital fountain. A digital fountain allows any number of heterogeneous clients to acquire bulk data with optimal efficiency at times of their choosing. Moreover, no feedback channels are needed to ensure reliable delivery, even in the face of high loss rates.We develop a protocol that closely approximates a digital fountain using a new class of erasure codes that for large block sizes are orders of magnitude faster than standard erasure codes. We provide performance measurements that demonstrate the feasibility of our approach and discuss the design, implementation and performance of an experimental system.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",56–67,12,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1925,article,"Byers, John W. and Luby, Michael and Mitzenmacher, Michael and Rege, Ashutosh",A Digital Fountain Approach to Reliable Distribution of Bulk Data,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285258,10.1145/285243.285258,"The proliferation of applications that must reliably distribute bulk data to a large number of autonomous clients motivates the design of new multicast and broadcast protocols. We describe an ideal, fully scalable protocol for these applications that we call a digital fountain. A digital fountain allows any number of heterogeneous clients to acquire bulk data with optimal efficiency at times of their choosing. Moreover, no feedback channels are needed to ensure reliable delivery, even in the face of high loss rates.We develop a protocol that closely approximates a digital fountain using a new class of erasure codes that for large block sizes are orders of magnitude faster than standard erasure codes. We provide performance measurements that demonstrate the feasibility of our approach and discuss the design, implementation and performance of an experimental system.",,56–67,12,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1926,inproceedings,"Wong, Chung Kei and Gouda, Mohamed and Lam, Simon S.",Secure Group Communications Using Key Graphs,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285260,10.1145/285237.285260,"Many emerging applications (e.g., teleconference, real-time information services, pay per view, distributed interactive simulation, and collaborative work) are based upon a group communications model, i.e., they require packet delivery from one or more authorized senders to a very large number of authorized receivers. As a result, securing group communications (i.e., providing confidentiality, integrity, and authenticity of messages delivered between group members) will become a critical networking issue.In this paper, we present a novel solution to the scalability problem of group/multicast key management. We formalize the notion of a secure group as a triple (U,K,R) where U denotes a set of users, K a set of keys held by the users, and R a user-key relation. We then introduce key graphs to specify secure groups. For a special class of key graphs, we present three strategies for securely distributing rekey messages after a join/leave, and specify protocols for joining and leaving a secure group. The rekeying strategies and join/leave protocols are implemented in a prototype group key server we have built. We present measurement results from experiments and discuss performance comparisons. We show that our group key management service, using any of the three rekeying strategies, is scalable to large groups with frequent joins and leaves. In particular, the average measured processing time per join/leave increases linearly with the logarithm of group size.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",68–79,12,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1927,article,"Wong, Chung Kei and Gouda, Mohamed and Lam, Simon S.",Secure Group Communications Using Key Graphs,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285260,10.1145/285243.285260,"Many emerging applications (e.g., teleconference, real-time information services, pay per view, distributed interactive simulation, and collaborative work) are based upon a group communications model, i.e., they require packet delivery from one or more authorized senders to a very large number of authorized receivers. As a result, securing group communications (i.e., providing confidentiality, integrity, and authenticity of messages delivered between group members) will become a critical networking issue.In this paper, we present a novel solution to the scalability problem of group/multicast key management. We formalize the notion of a secure group as a triple (U,K,R) where U denotes a set of users, K a set of keys held by the users, and R a user-key relation. We then introduce key graphs to specify secure groups. For a special class of key graphs, we present three strategies for securely distributing rekey messages after a join/leave, and specify protocols for joining and leaving a secure group. The rekeying strategies and join/leave protocols are implemented in a prototype group key server we have built. We present measurement results from experiments and discuss performance comparisons. We show that our group key management service, using any of the three rekeying strategies, is scalable to large groups with frequent joins and leaves. In particular, the average measured processing time per join/leave increases linearly with the logarithm of group size.",,68–79,12,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1928,inproceedings,"Wang, Huayan Amy and Schwartz, Mischa",Achieving Bounded Fairness for Multicast and TCP Traffic in the Internet,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285263,10.1145/285237.285263,"There is an urgent need for effective multicast congestion control algorithms which enable reasonably fair share of network resources between multicast and unicast TCP traffic under the current Internet infrastructure. In this paper, we propose a quantitative definition of a type of bounded fairness between multicast and unicast best-effort traffic, termed ""essentially fair"". We also propose a window-based Random Listening Algorithm (RLA) for multicast congestion control. The algorithm is proven to be essentially fair to TCP connections under a restricted topology with equal round-trip times and with phase effects eliminated. The algorithm is also fair to multiple multicast sessions. This paper provides the theoretical proofs and some simulation results to demonstrate that the RLA achieves good performance under various network topologies. These include the performance of a generalization of the RLA algorithm for topologies with different round-trip times.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",81–92,12,"flow and congestion control, RED and drop-tail gateways, phase effect, multicast, Internet","Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1929,article,"Wang, Huayan Amy and Schwartz, Mischa",Achieving Bounded Fairness for Multicast and TCP Traffic in the Internet,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285263,10.1145/285243.285263,"There is an urgent need for effective multicast congestion control algorithms which enable reasonably fair share of network resources between multicast and unicast TCP traffic under the current Internet infrastructure. In this paper, we propose a quantitative definition of a type of bounded fairness between multicast and unicast best-effort traffic, termed ""essentially fair"". We also propose a window-based Random Listening Algorithm (RLA) for multicast congestion control. The algorithm is proven to be essentially fair to TCP connections under a restricted topology with equal round-trip times and with phase effects eliminated. The algorithm is also fair to multiple multicast sessions. This paper provides the theoretical proofs and some simulation results to demonstrate that the RLA achieves good performance under various network topologies. These include the performance of a generalization of the RLA algorithm for topologies with different round-trip times.",,81–92,12,"Internet, multicast, phase effect, RED and drop-tail gateways, flow and congestion control",,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1930,inproceedings,"Kumar, Satish and Radoslavov, Pavlin and Thaler, David and Alaettino\u{g",The MASC/BGMP Architecture for Inter-Domain Multicast Routing,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285264,10.1145/285237.285264,"Multicast routing enables efficient data distribution to multiple recipients. However, existing work has concentrated on extending single-domain techniques to wide-area networks, rather than providing mechanisms to realize inter-domain multicast on a global scale in the Internet.We describe an architecture for inter-domain multicast routing that consists of two complementary protocols. The Multicast Address-Set Claim (MASC) protocol forms the basis for a hierarchical address allocation architecture. It dynamically allocates to domains multicast address ranges from which groups initiated in the domain get their multicast addresses. The Border-Gateway Multicast Protocol (BGMP), run by the border routers of a domain, constructs inter-domain bidirectional shared trees, while allowing any existing multicast routing protocol to be used within individual domains. The resulting shared tree for a group is rooted at the domain whose address range covers the group's address; this domain is typically the group initiator's domain. We demonstrate the feasibility and performance of these complementary protocols through simulation.This architecture, together with existing protocols operating within each domain, is intended as a framework in which to solve the problems facing the current multicast addressing and routing infrastructure.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",93–104,12,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1931,article,"Kumar, Satish and Radoslavov, Pavlin and Thaler, David and Alaettino\u{g",The MASC/BGMP Architecture for Inter-Domain Multicast Routing,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285264,10.1145/285243.285264,"Multicast routing enables efficient data distribution to multiple recipients. However, existing work has concentrated on extending single-domain techniques to wide-area networks, rather than providing mechanisms to realize inter-domain multicast on a global scale in the Internet.We describe an architecture for inter-domain multicast routing that consists of two complementary protocols. The Multicast Address-Set Claim (MASC) protocol forms the basis for a hierarchical address allocation architecture. It dynamically allocates to domains multicast address ranges from which groups initiated in the domain get their multicast addresses. The Border-Gateway Multicast Protocol (BGMP), run by the border routers of a domain, constructs inter-domain bidirectional shared trees, while allowing any existing multicast routing protocol to be used within individual domains. The resulting shared tree for a group is rooted at the domain whose address range covers the group's address; this domain is typically the group initiator's domain. We demonstrate the feasibility and performance of these complementary protocols through simulation.This architecture, together with existing protocols operating within each domain, is intended as a framework in which to solve the problems facing the current multicast addressing and routing infrastructure.",,93–104,12,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1932,inproceedings,"Handley, Mark",Session Directories and Scalable Internet Multicast Address Allocation,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285269,10.1145/285237.285269,"A multicast session directory is a mechanism by which users can discover the existence of multicast sessions. In the Mbone, session announcements have also served as multicast address reservations - a dual purpose that is efficient, but which may cause some side-affects as session directories scale.In this paper we examine the scaling of multicast address allocation when it is performed by such a multicast session directory. Despite our best efforts to make such an approach scale, this analysis ultimately reveals significant scaling problems, and suggests a new approach to multicast address allocation in the Internet environment.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",105–116,12,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1933,article,"Handley, Mark",Session Directories and Scalable Internet Multicast Address Allocation,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285269,10.1145/285243.285269,"A multicast session directory is a mechanism by which users can discover the existence of multicast sessions. In the Mbone, session announcements have also served as multicast address reservations - a dual purpose that is efficient, but which may cause some side-affects as session directories scale.In this paper we examine the scaling of multicast address allocation when it is performed by such a multicast session directory. Despite our best efforts to make such an approach scale, this analysis ultimately reveals significant scaling problems, and suggests a new approach to multicast address allocation in the Internet environment.",,105–116,12,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1934,inproceedings,"Stoica, Ion and Shenker, Scott and Zhang, Hui",Core-Stateless Fair Queueing: Achieving Approximately Fair Bandwidth Allocations in High Speed Networks,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285273,10.1145/285237.285273,"Router mechanisms designed to achieve fair bandwidth allocations, like Fair Queueing, have many desirable properties for congestion control in the Internet. However, such mechanisms usually need to maintain state, manage buffers, and/or perform packet scheduling on a per flow basis, and this complexity may prevent them from being cost-effectively implemented and widely deployed. In this paper, we propose an architecture that significantly reduces this implementation complexity yet still achieves approximately fair bandwidth allocations. We apply this approach to an island of routers --- that is, a contiguous region of the network --- and we distinguish between edge routers and core routers. Edge routers maintain per flow state; they estimate the incoming rate of each flow and insert a label into each packet header based on this estimate. Core routers maintain no per flow state; they use FIFO packet scheduling augmented by a probabilistic dropping algorithm that uses the packet labels and an estimate of the aggregate traffic at the router. We call the scheme Core-Stateless Fair Queueing. We present simulations and analysis on the performance of this approach, and discuss an alternate approach.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",118–130,13,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1935,article,"Stoica, Ion and Shenker, Scott and Zhang, Hui",Core-Stateless Fair Queueing: Achieving Approximately Fair Bandwidth Allocations in High Speed Networks,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285273,10.1145/285243.285273,"Router mechanisms designed to achieve fair bandwidth allocations, like Fair Queueing, have many desirable properties for congestion control in the Internet. However, such mechanisms usually need to maintain state, manage buffers, and/or perform packet scheduling on a per flow basis, and this complexity may prevent them from being cost-effectively implemented and widely deployed. In this paper, we propose an architecture that significantly reduces this implementation complexity yet still achieves approximately fair bandwidth allocations. We apply this approach to an island of routers --- that is, a contiguous region of the network --- and we distinguish between edge routers and core routers. Edge routers maintain per flow state; they estimate the incoming rate of each flow and insert a label into each packet header based on this estimate. Core routers maintain no per flow state; they use FIFO packet scheduling augmented by a probabilistic dropping algorithm that uses the packet labels and an estimate of the aggregate traffic at the router. We call the scheme Core-Stateless Fair Queueing. We present simulations and analysis on the performance of this approach, and discuss an alternate approach.",,118–130,13,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1936,inproceedings,"Bajaj, Sandeep and Breslau, Lee and Shenker, Scott",Uniform versus Priority Dropping for Layered Video,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285275,10.1145/285237.285275,"In this paper, we analyze the relative merits of uniform versus priority dropping for the transmission of layered video. We first present our original intuitions about these two approaches, and then investigate the issue more thoroughly through simulations and analysis in which we explicitly model the performance of layered video applications. We compare both their performance characteristics and incentive properties, and find that the performance benefit of priority dropping is smaller than we expected, while uniform dropping has worse incentive properties than we previously believed.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",131–143,13,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1937,article,"Bajaj, Sandeep and Breslau, Lee and Shenker, Scott",Uniform versus Priority Dropping for Layered Video,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285275,10.1145/285243.285275,"In this paper, we analyze the relative merits of uniform versus priority dropping for the transmission of layered video. We first present our original intuitions about these two approaches, and then investigate the issue more thoroughly through simulations and analysis in which we explicitly model the performance of layered video applications. We compare both their performance characteristics and incentive properties, and find that the performance benefit of priority dropping is smaller than we expected, while uniform dropping has worse incentive properties than we previously believed.",,131–143,13,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1938,inproceedings,"Faloutsos, Michalis and Banerjea, Anindo and Pankaj, Rajesh",QoSMIC: Quality of Service Sensitive Multicast Internet Protocol,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285276,10.1145/285237.285276,"In this paper, we present, QoSMIC, a multicast protocol for the Internet that supports QoS-sensitive routing, and minimizes the importance of a priori configuration decisions (such as core selection). The protocol is resource-efficient, robust, flexible, and scalable. In addition, our protocol is provably loop-free.Our protocol starts with a resources-saving tree (Shared Tree) and individual receivers switch to a QoS-competitive tree (Source-Based Tree) when necessary. In both trees, the new destination is able to choose the most promising among several paths. An innovation is that we use dynamic routing information without relying on a link state exchange protocol to provide it. Our protocol limits the effect of pre-configuration decisions drastically, by separating the management from the data transfer functions; administrative routers are not necessarily part of the tree. This separation increases the robustness, and flexibility of the protocol. Furthermore, QoSMIC is able to adapt dynamically to the conditions of the network.The QoSMIC protocol introduces several new ideas that make it more flexible than other protocols proposed to date. In fact, many of the other protocols, (such as YAM, PIMSM, BGMP, CBT) can be seen as special cases of QoSMIC. This paper presents the motivation behind, and the design of QoSMIC, and provides both analytical and experimental results to support our claims.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",144–153,10,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1939,article,"Faloutsos, Michalis and Banerjea, Anindo and Pankaj, Rajesh",QoSMIC: Quality of Service Sensitive Multicast Internet Protocol,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285276,10.1145/285243.285276,"In this paper, we present, QoSMIC, a multicast protocol for the Internet that supports QoS-sensitive routing, and minimizes the importance of a priori configuration decisions (such as core selection). The protocol is resource-efficient, robust, flexible, and scalable. In addition, our protocol is provably loop-free.Our protocol starts with a resources-saving tree (Shared Tree) and individual receivers switch to a QoS-competitive tree (Source-Based Tree) when necessary. In both trees, the new destination is able to choose the most promising among several paths. An innovation is that we use dynamic routing information without relying on a link state exchange protocol to provide it. Our protocol limits the effect of pre-configuration decisions drastically, by separating the management from the data transfer functions; administrative routers are not necessarily part of the tree. This separation increases the robustness, and flexibility of the protocol. Furthermore, QoSMIC is able to adapt dynamically to the conditions of the network.The QoSMIC protocol introduces several new ideas that make it more flexible than other protocols proposed to date. In fact, many of the other protocols, (such as YAM, PIMSM, BGMP, CBT) can be seen as special cases of QoSMIC. This paper presents the motivation behind, and the design of QoSMIC, and provides both analytical and experimental results to support our claims.",,144–153,10,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1940,inproceedings,"Choi, Sunghyun and Shin, Kang G.",Predictive and Adaptive Bandwidth Reservation for Hand-Offs in QoS-Sensitive Cellular Networks,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285278,10.1145/285237.285278,"How to control hand-off drops is a very important Quality-of-Service (QoS) issue in cellular networks. In order to keep the hand-off dropping probability below a pre-specified target value (thus providing a probabilistic QoS guarantee), we design and evaluate predictive and adaptive schemes for the bandwidth reservation for the existing connections' handoffs and the admission control of new connections.We first develop a method to estimate user mobility based on an aggregate history of hand-offs observed in each cell. This method is then used to predict (probabilistically) mobiles' directions and hand-off times in a cell. For each cell, the bandwidth to be reserved for hand-offs is calculated by estimating the total sum of fractional bandwidths of the expected hand-offs within a mobility-estimation time window. We also develop an algorithm that controls this window for efficient use of bandwidth and effective response to (1) time-varying traffic/mobility and (2) inaccuracy of mobility estimation. Three different admission-control schemes for new connection requests using this bandwidth reservation are proposed. Finally, we evaluate the performance of the proposed schemes to show that they meet our design goal and outperform the static reservation scheme under various scenarios.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",155–166,12,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1941,article,"Choi, Sunghyun and Shin, Kang G.",Predictive and Adaptive Bandwidth Reservation for Hand-Offs in QoS-Sensitive Cellular Networks,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285278,10.1145/285243.285278,"How to control hand-off drops is a very important Quality-of-Service (QoS) issue in cellular networks. In order to keep the hand-off dropping probability below a pre-specified target value (thus providing a probabilistic QoS guarantee), we design and evaluate predictive and adaptive schemes for the bandwidth reservation for the existing connections' handoffs and the admission control of new connections.We first develop a method to estimate user mobility based on an aggregate history of hand-offs observed in each cell. This method is then used to predict (probabilistically) mobiles' directions and hand-off times in a cell. For each cell, the bandwidth to be reserved for hand-offs is calculated by estimating the total sum of fractional bandwidths of the expected hand-offs within a mobility-estimation time window. We also develop an algorithm that controls this window for efficient use of bandwidth and effective response to (1) time-varying traffic/mobility and (2) inaccuracy of mobility estimation. Three different admission-control schemes for new connection requests using this bandwidth reservation are proposed. Finally, we evaluate the performance of the proposed schemes to show that they meet our design goal and outperform the static reservation scheme under various scenarios.",,155–166,12,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1942,inproceedings,"Haas, Zygmunt J. and Pearlman, Marc R.",The Performance of Query Control Schemes for the Zone Routing Protocol,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285279,10.1145/285237.285279,"In this paper, we study the performance of route query control mechanisms for the recently proposed Zone Routing Protocol (ZRP) for ad-hoc networks. The ZRP proactively maintains routing information for a local neighborhood (routing zone), while reactively acquiring routes to destinations beyond the routing zone. This hybrid routing approach has the potential to be more efficient in the generation of control traffic than traditional routing schemes. However, without proper query control techniques, the ZRP can actually produce more traffic than standard flooding protocols.Our proposed query control schemes exploit the structure of the routing zone to provide enhanced detection (Query Detection (QD1/QD2)), termination (Loop-back Termination (LT), Early Termination (ET)) and prevention (Selective Bordercasting (SBC)) of overlapping queries. We demonstrate how certain combinations of these techniques can be applied to single channel or multiple channel ad-hoc networks to improve both the delay and control traffic performance of the ZRP. Our query control mechanisms allow the ZRP to provide routes to all accessible network nodes with only a fraction of the control traffic generated by purely proactive distance vector and purely reactive flooding schemes, and with a response time as low as 10% of a flooding route query delay.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",167–177,11,"protocols, routing zone, routing, zone routing, ad-hoc networks","Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1943,article,"Haas, Zygmunt J. and Pearlman, Marc R.",The Performance of Query Control Schemes for the Zone Routing Protocol,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285279,10.1145/285243.285279,"In this paper, we study the performance of route query control mechanisms for the recently proposed Zone Routing Protocol (ZRP) for ad-hoc networks. The ZRP proactively maintains routing information for a local neighborhood (routing zone), while reactively acquiring routes to destinations beyond the routing zone. This hybrid routing approach has the potential to be more efficient in the generation of control traffic than traditional routing schemes. However, without proper query control techniques, the ZRP can actually produce more traffic than standard flooding protocols.Our proposed query control schemes exploit the structure of the routing zone to provide enhanced detection (Query Detection (QD1/QD2)), termination (Loop-back Termination (LT), Early Termination (ET)) and prevention (Selective Bordercasting (SBC)) of overlapping queries. We demonstrate how certain combinations of these techniques can be applied to single channel or multiple channel ad-hoc networks to improve both the delay and control traffic performance of the ZRP. Our query control mechanisms allow the ZRP to provide routes to all accessible network nodes with only a fraction of the control traffic generated by purely proactive distance vector and purely reactive flooding schemes, and with a response time as low as 10% of a flooding route query delay.",,167–177,11,"zone routing, protocols, ad-hoc networks, routing zone, routing",,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1944,inproceedings,"Amir, Elan and McCanne, Steven and Katz, Randy",An Active Service Framework and Its Application to Real-Time Multimedia Transcoding,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285281,10.1145/285237.285281,"Several recent proposals for an ""active networks"" architecture advocate the placement of user-defined computation within the network as a key mechanism to enable a wide range of new applications and protocols, including reliable multicast transports, mechanisms to foil denial of service attacks, intra-network real-time signal transcoding, and so forth. This laudable goal, however, creates a number of very difficult research problems, and although a number of pioneering research efforts in active networks have solved some of the preliminary small-scale problems, a large number of wide open problems remain. In this paper, we propose an alternative to active networks that addresses a restricted and more tractable subset of the active-networks design space. Our approach, which we (and others) call ""active services"", advocates the placement of user-defined computation within the network as with active networks, but unlike active networks preserves all of the routing and forwarding semantics of current Internet architecture by restricting the computation environment to the application layer. Because active services do not require changes to the Internet architecture, they can be deployed incrementally in today's Internet.We believe that many of the applications and protocols targeted by the active networks initiative can be solved with active services and, toward this end, we propose herein a specific architecture for an active service and develop one such service in detail --- the Media Gateway (MeGa) service --- that exploits this architecture. In defining our active service, we encountered six key problems --- service location, service control, service management, service attachment, service composition, and the definition of the service environment --- and have crafted solutions for these problems in the context of the MeGa service. To verify our design, we implemented and fielded MeGa on the UC Berkeley campus, where it has been used regularly for several months by real users who connect via ISDN to an ""on-line classroom"". Our initial experience indicates that our active services prototype provides a very flexible and programmable platform for intra-network computation that strikes a good balance between the flexibility of the active networks architecture and the practical constraints of incremental deployment in the current Internet.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",178–189,12,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1945,article,"Amir, Elan and McCanne, Steven and Katz, Randy",An Active Service Framework and Its Application to Real-Time Multimedia Transcoding,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285281,10.1145/285243.285281,"Several recent proposals for an ""active networks"" architecture advocate the placement of user-defined computation within the network as a key mechanism to enable a wide range of new applications and protocols, including reliable multicast transports, mechanisms to foil denial of service attacks, intra-network real-time signal transcoding, and so forth. This laudable goal, however, creates a number of very difficult research problems, and although a number of pioneering research efforts in active networks have solved some of the preliminary small-scale problems, a large number of wide open problems remain. In this paper, we propose an alternative to active networks that addresses a restricted and more tractable subset of the active-networks design space. Our approach, which we (and others) call ""active services"", advocates the placement of user-defined computation within the network as with active networks, but unlike active networks preserves all of the routing and forwarding semantics of current Internet architecture by restricting the computation environment to the application layer. Because active services do not require changes to the Internet architecture, they can be deployed incrementally in today's Internet.We believe that many of the applications and protocols targeted by the active networks initiative can be solved with active services and, toward this end, we propose herein a specific architecture for an active service and develop one such service in detail --- the Media Gateway (MeGa) service --- that exploits this architecture. In defining our active service, we encountered six key problems --- service location, service control, service management, service attachment, service composition, and the definition of the service environment --- and have crafted solutions for these problems in the context of the MeGa service. To verify our design, we implemented and fielded MeGa on the UC Berkeley campus, where it has been used regularly for several months by real users who connect via ISDN to an ""on-line classroom"". Our initial experience indicates that our active services prototype provides a very flexible and programmable platform for intra-network computation that strikes a good balance between the flexibility of the active networks architecture and the practical constraints of incremental deployment in the current Internet.",,178–189,12,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1946,inproceedings,"Srinivasan, V. and Varghese, G. and Suri, S. and Waldvogel, M.",Fast and Scalable Layer Four Switching,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285282,10.1145/285237.285282,"In Layer Four switching, the route and resources allocated to a packet are determined by the destination address as well as other header fields of the packet such as source address, TCP and UDP port numbers. Layer Four switching unifies firewall processing, RSVP style resource reservation filters, QoS Routing, and normal unicast and multicast forwarding into a single framework. In this framework, the forwarding database of a router consists of a potentially large number of filters on key header fields. A given packet header can match multiple filters, so each filter is given a cost, and the packet is forwarded using the least cost matching filter.In this paper, we describe two new algorithms for solving the least cost matching filter problem at high speeds. Our first algorithm is based on a grid-of-tries construction and works optimally for processing filters consisting of two prefix fields (such as destination-source filters) using linear space. Our second algorithm, cross-producting, provides fast lookup times for arbitrary filters but potentially requires large storage. We describe a combination scheme that combines the advantages of both schemes. The combination scheme can be optimized to handle pure destination prefix filters in 4 memory accesses, destination-source filters in 8 memory accesses worst case, and all other filters in 11 memory accesses in the typical case.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",191–202,12,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1947,article,"Srinivasan, V. and Varghese, G. and Suri, S. and Waldvogel, M.",Fast and Scalable Layer Four Switching,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285282,10.1145/285243.285282,"In Layer Four switching, the route and resources allocated to a packet are determined by the destination address as well as other header fields of the packet such as source address, TCP and UDP port numbers. Layer Four switching unifies firewall processing, RSVP style resource reservation filters, QoS Routing, and normal unicast and multicast forwarding into a single framework. In this framework, the forwarding database of a router consists of a potentially large number of filters on key header fields. A given packet header can match multiple filters, so each filter is given a cost, and the packet is forwarded using the least cost matching filter.In this paper, we describe two new algorithms for solving the least cost matching filter problem at high speeds. Our first algorithm is based on a grid-of-tries construction and works optimally for processing filters consisting of two prefix fields (such as destination-source filters) using linear space. Our second algorithm, cross-producting, provides fast lookup times for arbitrary filters but potentially requires large storage. We describe a combination scheme that combines the advantages of both schemes. The combination scheme can be optimized to handle pure destination prefix filters in 4 memory accesses, destination-source filters in 8 memory accesses worst case, and all other filters in 11 memory accesses in the typical case.",,191–202,12,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1948,inproceedings,"Lakshman, T. V. and Stiliadis, D.",High-Speed Policy-Based Packet Forwarding Using Efficient Multi-Dimensional Range Matching,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285283,10.1145/285237.285283,"The ability to provide differentiated services to users with widely varying requirements is becoming increasingly important, and Internet Service Providers would like to provide these differentiated services using the same shared network infrastructure. The key mechanism, that enables differentiation in a connectionless network, is the packet classification function that parses the headers of the packets, and after determining their context, classifies them based on administrative policies or real-time reservation decisions. Packet classification, however, is a complex operation that can become the bottleneck in routers that try to support gigabit link capacities. Hence, many proposals for differentiated services only require classification at lower speed edge routers and also avoid classification based on multiple fields in the packet header even if it might be advantageous to service providers. In this paper, we present new packet classification schemes that, with a worst-case and traffic-independent performance metric, can classify packets, by checking amongst a few thousand filtering rules, at rates of a million packets per second using range matches on more than 4 packet header fields. For a special case of classification in two dimensions, we present an algorithm that can handle more than 128K rules at these speeds in a traffic independent manner. We emphasize worst-case performance over average case performance because providing differentiated services requires intelligent queueing and scheduling of packets that precludes any significant queueing before the differentiating step (i.e., before packet classification). The presented filtering or classification schemes can be used to classify packets for security policy enforcement, applying resource management decisions, flow identification for RSVP reservations, multicast look-ups, and for source-destination and policy based routing. The scalability and performance of the algorithms have been demonstrated by implementation and testing in a prototype system.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",203–214,12,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1949,article,"Lakshman, T. V. and Stiliadis, D.",High-Speed Policy-Based Packet Forwarding Using Efficient Multi-Dimensional Range Matching,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285283,10.1145/285243.285283,"The ability to provide differentiated services to users with widely varying requirements is becoming increasingly important, and Internet Service Providers would like to provide these differentiated services using the same shared network infrastructure. The key mechanism, that enables differentiation in a connectionless network, is the packet classification function that parses the headers of the packets, and after determining their context, classifies them based on administrative policies or real-time reservation decisions. Packet classification, however, is a complex operation that can become the bottleneck in routers that try to support gigabit link capacities. Hence, many proposals for differentiated services only require classification at lower speed edge routers and also avoid classification based on multiple fields in the packet header even if it might be advantageous to service providers. In this paper, we present new packet classification schemes that, with a worst-case and traffic-independent performance metric, can classify packets, by checking amongst a few thousand filtering rules, at rates of a million packets per second using range matches on more than 4 packet header fields. For a special case of classification in two dimensions, we present an algorithm that can handle more than 128K rules at these speeds in a traffic independent manner. We emphasize worst-case performance over average case performance because providing differentiated services requires intelligent queueing and scheduling of packets that precludes any significant queueing before the differentiating step (i.e., before packet classification). The presented filtering or classification schemes can be used to classify packets for security policy enforcement, applying resource management decisions, flow identification for RSVP reservations, multicast look-ups, and for source-destination and policy based routing. The scalability and performance of the algorithms have been demonstrated by implementation and testing in a prototype system.",,203–214,12,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1950,inproceedings,"Malan, G. Robert and Jahanian, Farnam",An Extensible Probe Architecture for Network Protocol Performance Measurement,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285284,10.1145/285237.285284,"This paper describes the architecture and implementation of Windmill, a passive network protocol performance measurement tool. Windmill enables experimenters to measure a broad range of protocol performance metrics by both reconstructing application-level network protocols and exposing the underlying protocol layers' events. Windmill is split into three functional components: a dynamically compiled Windmill Protocol Filter (WPF), a set of abstract protocol modules, and an extensible experiment engine. To demonstrate Windmill's utility, the results from several experiments are presented. The first set of experiments suggests a possible cause for the correlation between Internet routing instability and network utilization. The second set of experiments highlights: Windmill's ability to act as a driver for a complementary active Internet measurement apparatus, its ability to perform online data reduction, and the non-intrusive measurement of a closed system.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",215–227,13,"passive measurement, protocol performance, online analysis, packet filter","Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1951,article,"Malan, G. Robert and Jahanian, Farnam",An Extensible Probe Architecture for Network Protocol Performance Measurement,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285284,10.1145/285243.285284,"This paper describes the architecture and implementation of Windmill, a passive network protocol performance measurement tool. Windmill enables experimenters to measure a broad range of protocol performance metrics by both reconstructing application-level network protocols and exposing the underlying protocol layers' events. Windmill is split into three functional components: a dynamically compiled Windmill Protocol Filter (WPF), a set of abstract protocol modules, and an extensible experiment engine. To demonstrate Windmill's utility, the results from several experiments are presented. The first set of experiments suggests a possible cause for the correlation between Internet routing instability and network utilization. The second set of experiments highlights: Windmill's ability to act as a driver for a complementary active Internet measurement apparatus, its ability to perform online data reduction, and the non-intrusive measurement of a closed system.",,215–227,13,"packet filter, passive measurement, protocol performance, online analysis",,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1952,inproceedings,"Decasper, Dan and Dittia, Zubin and Parulkar, Guru and Plattner, Bernhard",Router Plugins: A Software Architecture for next Generation Routers,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285285,10.1145/285237.285285,"Present day routers typically employ monolithic operating systems which are not easily upgradable and extensible. With the rapid rate of protocol development it is becoming increasingly important to dynamically upgrade router software in an incremental fashion. We have designed and implemented a high performance, modular, extended integrated services router software architecture in the NetBSD operating system kernel. This architecture allows code modules, called plugins, to be dynamically added and configured at run time. One of the novel features of our design is the ability to bind different plugins to individual flows; this allows for distinct plugin implementations to seamlessly coexist in the same runtime environment. High performance is achieved through a carefully designed modular architecture; an innovative packet classification algorithm that is both powerful and highly efficient; and by caching that exploits the flow-like characteristics of Internet traffic. Compared to a monolithic best-effort kernel, our implementation requires an average increase in packet processing overhead of only 8%, or 500 cycles/2.1ms per packet when running on a P6/233.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",229–240,12,"modular router architecture, router plugins, high performance integrated services routing","Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1953,article,"Decasper, Dan and Dittia, Zubin and Parulkar, Guru and Plattner, Bernhard",Router Plugins: A Software Architecture for next Generation Routers,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285285,10.1145/285243.285285,"Present day routers typically employ monolithic operating systems which are not easily upgradable and extensible. With the rapid rate of protocol development it is becoming increasingly important to dynamically upgrade router software in an incremental fashion. We have designed and implemented a high performance, modular, extended integrated services router software architecture in the NetBSD operating system kernel. This architecture allows code modules, called plugins, to be dynamically added and configured at run time. One of the novel features of our design is the ability to bind different plugins to individual flows; this allows for distinct plugin implementations to seamlessly coexist in the same runtime environment. High performance is achieved through a carefully designed modular architecture; an innovative packet classification algorithm that is both powerful and highly efficient; and by caching that exploits the flow-like characteristics of Internet traffic. Compared to a monolithic best-effort kernel, our implementation requires an average increase in packet processing overhead of only 8%, or 500 cycles/2.1ms per packet when running on a P6/233.",,229–240,12,"modular router architecture, router plugins, high performance integrated services routing",,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1954,inproceedings,"Cohen, Edith and Krishnamurthy, Balachander and Rexford, Jennifer",Improving End-to-End Performance of the Web Using Server Volumes and Proxy Filters,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285286,10.1145/285237.285286,"The rapid growth of the World Wide Web has caused serious performance degradation on the Internet. This paper offers an end-to-end approach to improving Web performance by collectively examining the Web components --- clients, proxies, servers, and the network. Our goal is to reduce user-perceived latency and the number of TCP connections, improve cache coherency and cache replacement, and enable prefetching of resources that are likely to be accessed in the near future. In our scheme, server response messages include piggybacked information customized to the requesting proxy. Our enhancement to the existing request-response protocol does not require per-proxy state at a server, and a very small amount of transient per-server state at the proxy, and can be implemented without changes to HTTP 1.1. The server groups related resources into volumes (based on access patterns and the file system's directory structure) and applies a proxy-generated filter (indicating the type of information of interest to the proxy) to tailor the piggyback information. We present efficient data structures for constructing server volumes and applying proxy filters, and a transparent way to perform volume maintenance and piggyback generation at a router along the path between the proxy and the server. We demonstrate the effectiveness of our end-to-end approach by evaluating various volume construction and filtering techniques across a collection of large client and server logs.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",241–253,13,"Web, prefetching, piggybacking, filters, coherency, caching, volumes","Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1955,article,"Cohen, Edith and Krishnamurthy, Balachander and Rexford, Jennifer",Improving End-to-End Performance of the Web Using Server Volumes and Proxy Filters,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285286,10.1145/285243.285286,"The rapid growth of the World Wide Web has caused serious performance degradation on the Internet. This paper offers an end-to-end approach to improving Web performance by collectively examining the Web components --- clients, proxies, servers, and the network. Our goal is to reduce user-perceived latency and the number of TCP connections, improve cache coherency and cache replacement, and enable prefetching of resources that are likely to be accessed in the near future. In our scheme, server response messages include piggybacked information customized to the requesting proxy. Our enhancement to the existing request-response protocol does not require per-proxy state at a server, and a very small amount of transient per-server state at the proxy, and can be implemented without changes to HTTP 1.1. The server groups related resources into volumes (based on access patterns and the file system's directory structure) and applies a proxy-generated filter (indicating the type of information of interest to the proxy) to tailor the piggyback information. We present efficient data structures for constructing server volumes and applying proxy filters, and a transparent way to perform volume maintenance and piggyback generation at a router along the path between the proxy and the server. We demonstrate the effectiveness of our end-to-end approach by evaluating various volume construction and filtering techniques across a collection of large client and server logs.",,241–253,13,"piggybacking, caching, prefetching, coherency, volumes, filters, Web",,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1956,inproceedings,"Fan, Li and Cao, Pei and Almeida, Jussara and Broder, Andrei Z.",Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285287,10.1145/285237.285287,"The sharing of caches among Web proxies is an important technique to reduce Web traffic and alleviate network bottlenecks. Nevertheless it is not widely deployed due to the overhead of existing protocols. In this paper we propose a new protocol called ""Summary Cache""; each proxy keeps a summary of the URLs of cached documents of each participating proxy and checks these summaries for potential hits before sending any queries. Two factors contribute to the low overhead: the summaries are updated only periodically, and the summary representations are economical --- as low as 8 bits per entry. Using trace-driven simulations and a prototype implementation, we show that compared to the existing Internet Cache Protocol (ICP), Summary Cache reduces the number of inter-cache messages by a factor of 25 to 60, reduces the bandwidth consumption by over 50%, and eliminates between 30% to 95% of the CPU overhead, while at the same time maintaining almost the same hit ratio as ICP. Hence Summary Cache enables cache sharing among a large number of proxies.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",254–265,12,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1957,article,"Fan, Li and Cao, Pei and Almeida, Jussara and Broder, Andrei Z.",Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285287,10.1145/285243.285287,"The sharing of caches among Web proxies is an important technique to reduce Web traffic and alleviate network bottlenecks. Nevertheless it is not widely deployed due to the overhead of existing protocols. In this paper we propose a new protocol called ""Summary Cache""; each proxy keeps a summary of the URLs of cached documents of each participating proxy and checks these summaries for potential hits before sending any queries. Two factors contribute to the low overhead: the summaries are updated only periodically, and the summary representations are economical --- as low as 8 bits per entry. Using trace-driven simulations and a prototype implementation, we show that compared to the existing Internet Cache Protocol (ICP), Summary Cache reduces the number of inter-cache messages by a factor of 25 to 60, reduces the bandwidth consumption by over 50%, and eliminates between 30% to 95% of the CPU overhead, while at the same time maintaining almost the same hit ratio as ICP. Hence Summary Cache enables cache sharing among a large number of proxies.",,254–265,12,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1958,inproceedings,"Cankaya, Hakki C. and Nair, V. S. S.",Accelerated Reliability Analysis for Self-Healing SONET Networks,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285288,10.1145/285237.285288,"Recently, a parametric State Reward Markov Model (SRMM/p) has been developed for the reliability and availability analysis of self-healing SONET mesh networks [2]. In this paper, we investigate the factors that affect the run-time complexity of the model presented in [2]. In order to accelerate the reliability and availability analysis, we present an approach that aggregates a set of states in the model based on 2-phase hypoexponential distribution. A comparison of the original and the reduced model, with respect to runtime complexity and accuracy, is carried out by applying the models for the analysis of few complex networks.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",268–277,10,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1959,article,"Cankaya, Hakki C. and Nair, V. S. S.",Accelerated Reliability Analysis for Self-Healing SONET Networks,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285288,10.1145/285243.285288,"Recently, a parametric State Reward Markov Model (SRMM/p) has been developed for the reliability and availability analysis of self-healing SONET mesh networks [2]. In this paper, we investigate the factors that affect the run-time complexity of the model presented in [2]. In order to accelerate the reliability and availability analysis, we present an approach that aggregates a set of states in the model based on 2-phase hypoexponential distribution. A comparison of the original and the reduced model, with respect to runtime complexity and accuracy, is carried out by applying the models for the analysis of few complex networks.",,268–277,10,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1960,inproceedings,"Kermode, Roger G.",Scoped Hybrid Automatic Repeat ReQuest with Forward Error Correction (SHARQFEC),1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285289,10.1145/285237.285289,"Reliable multicast protocols scale only as well as their ability to localize traffic. This is true for repair requests, repairs, and the session traffic that enables receivers to suppress extraneous requests and repairs. We propose a new reliable multicast traffic localization technique called Scoped Hybrid Automatic Repeat reQuest with Forward Error Correction (SHARQFEC). SHARQFEC operates in an end-to-end fashion and localizes traffic using a hierarchy of administratively scoped regions. Session traffic is further reduced through the use of a novel method for indirectly determining the distances between session members. For large sessions, this mechanism reduces the amount of session traffic by several orders of magnitude over non-scoped protocols such as Scalable Reliable Multicast (SRM). Forward Error Correction is selectively added to regions which are experiencing greater loss, thereby reducing the volume of repair traffic and recovery times. Receivers request additional repairs as necessary. Simulations show that SHARQFEC out performs both SRM and non-scoped hybrid Automatic Repeat reQuest / Forward Error Correction protocols. Assuming the widespread deployment of administrative scoping, SHARQFEC could conceivably provide scalable reliable delivery to tens of millions of receivers without huge increases in network bandwidth.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",278–289,12,"hierarchy, reliable, ARQ, administrative-scoping, FEC, multicast, scalable","Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1961,article,"Kermode, Roger G.",Scoped Hybrid Automatic Repeat ReQuest with Forward Error Correction (SHARQFEC),1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285289,10.1145/285243.285289,"Reliable multicast protocols scale only as well as their ability to localize traffic. This is true for repair requests, repairs, and the session traffic that enables receivers to suppress extraneous requests and repairs. We propose a new reliable multicast traffic localization technique called Scoped Hybrid Automatic Repeat reQuest with Forward Error Correction (SHARQFEC). SHARQFEC operates in an end-to-end fashion and localizes traffic using a hierarchy of administratively scoped regions. Session traffic is further reduced through the use of a novel method for indirectly determining the distances between session members. For large sessions, this mechanism reduces the amount of session traffic by several orders of magnitude over non-scoped protocols such as Scalable Reliable Multicast (SRM). Forward Error Correction is selectively added to regions which are experiencing greater loss, thereby reducing the volume of repair traffic and recovery times. Receivers request additional repairs as necessary. Simulations show that SHARQFEC out performs both SRM and non-scoped hybrid Automatic Repeat reQuest / Forward Error Correction protocols. Assuming the widespread deployment of administrative scoping, SHARQFEC could conceivably provide scalable reliable delivery to tens of millions of receivers without huge increases in network bandwidth.",,278–289,12,"hierarchy, FEC, scalable, multicast, administrative-scoping, ARQ, reliable",,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1962,inproceedings,"Rhee, Injong",Error Control Techniques for Interactive Low-Bit Rate Video Transmission over the Internet,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285290,10.1145/285237.285290,"A new retransmission-based error control technique is presented that does not incur any additional latency in frame playout times, and hence are suitable for interactive applications. It takes advantage of the motion prediction loop employed in most motion compensation-based codecs. By correcting errors in a reference frame caused by earlier packet loss, it prevents error propagation. The technique rearranges the temporal dependency of frames so that a displayed frame is referenced for the decoding of its succeeding dependent frames much later than its display time. Thus, the delay in repairing lost packets can be effectively masked out. The developed technique is combined with layered video coding to maintain consistently good video quality even under heavy packet loss. Through the results of extensive Internet experiments, the paper shows that layered coding can be very effective when combined with the retransmission-based error control technique for low-bit rate transmission over best effort networks where no network-level mechanism exists for protecting high priority data from packet loss.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",290–301,12,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1963,article,"Rhee, Injong",Error Control Techniques for Interactive Low-Bit Rate Video Transmission over the Internet,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285290,10.1145/285243.285290,"A new retransmission-based error control technique is presented that does not incur any additional latency in frame playout times, and hence are suitable for interactive applications. It takes advantage of the motion prediction loop employed in most motion compensation-based codecs. By correcting errors in a reference frame caused by earlier packet loss, it prevents error propagation. The technique rearranges the temporal dependency of frames so that a displayed frame is referenced for the decoding of its succeeding dependent frames much later than its display time. Thus, the delay in repairing lost packets can be effectively masked out. The developed technique is combined with layered video coding to maintain consistently good video quality even under heavy packet loss. Through the results of extensive Internet experiments, the paper shows that layered coding can be very effective when combined with the retransmission-based error control technique for low-bit rate transmission over best effort networks where no network-level mechanism exists for protecting high priority data from packet loss.",,290–301,12,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1964,inproceedings,"Padhye, Jitendra and Firoiu, Victor and Towsley, Don and Kurose, Jim",Modeling TCP Throughput: A Simple Model and Its Empirical Validation,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285291,10.1145/285237.285291,"In this paper we develop a simple analytic characterization of the steady state throughput, as a function of loss rate and round trip time for a bulk transfer TCP flow, i.e., a flow with an unlimited amount of data to send. Unlike the models in [6, 7, 10], our model captures not only the behavior of TCP's fast retransmit mechanism (which is also considered in [6, 7, 10]) but also the effect of TCP's timeout mechanism on throughput. Our measurements suggest that this latter behavior is important from a modeling perspective, as almost all of our TCP traces contained more time-out events than fast retransmit events. Our measurements demonstrate that our model is able to more accurately predict TCP throughput and is accurate over a wider range of loss rates.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",303–314,12,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1965,article,"Padhye, Jitendra and Firoiu, Victor and Towsley, Don and Kurose, Jim",Modeling TCP Throughput: A Simple Model and Its Empirical Validation,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285291,10.1145/285243.285291,"In this paper we develop a simple analytic characterization of the steady state throughput, as a function of loss rate and round trip time for a bulk transfer TCP flow, i.e., a flow with an unlimited amount of data to send. Unlike the models in [6, 7, 10], our model captures not only the behavior of TCP's fast retransmit mechanism (which is also considered in [6, 7, 10]) but also the effect of TCP's timeout mechanism on throughput. Our measurements suggest that this latter behavior is important from a modeling perspective, as almost all of our TCP traces contained more time-out events than fast retransmit events. Our measurements demonstrate that our model is able to more accurately predict TCP throughput and is accurate over a wider range of loss rates.",,303–314,12,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1966,inproceedings,"Semke, Jeffrey and Mahdavi, Jamshid and Mathis, Matthew",Automatic TCP Buffer Tuning,1998,1581130031,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285237.285292,10.1145/285237.285292,"With the growth of high performance networking, a single host may have simultaneous connections that vary in bandwidth by as many as six orders of magnitude. We identify requirements for an automatically-tuning TCP to achieve maximum throughput across all connections simultaneously within the resource limits of the sender. Our auto-tuning TCP implementation makes use of several existing technologies and adds dynamically adjusting socket buffers to achieve maximum transfer rates on each connection without manual configuration.Our implementation involved slight modifications to a BSD-based socket interface and TCP stack. With these modifications, we achieved drastic improvements in performance over large bandwidth delay paths compared to the default system configuration, and significant reductions in memory usage compared to hand-tuned connections, allowing servers to support at least twice as many simultaneous connections.","Proceedings of the ACM SIGCOMM '98 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",315–323,9,,"Vancouver, British Columbia, Canada",SIGCOMM '98,,,,,,
1967,article,"Semke, Jeffrey and Mahdavi, Jamshid and Mathis, Matthew",Automatic TCP Buffer Tuning,1998,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/285243.285292,10.1145/285243.285292,"With the growth of high performance networking, a single host may have simultaneous connections that vary in bandwidth by as many as six orders of magnitude. We identify requirements for an automatically-tuning TCP to achieve maximum throughput across all connections simultaneously within the resource limits of the sender. Our auto-tuning TCP implementation makes use of several existing technologies and adds dynamically adjusting socket buffers to achieve maximum transfer rates on each connection without manual configuration.Our implementation involved slight modifications to a BSD-based socket interface and TCP stack. With these modifications, we achieved drastic improvements in performance over large bandwidth delay paths compared to the default system configuration, and significant reductions in memory usage compared to hand-tuned connections, allowing servers to support at least twice as many simultaneous connections.",,315–323,9,,,,Oct. 1998,28,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1968,inproceedings,"Degermark, Mikael and Brodnik, Andrej and Carlsson, Svante and Pink, Stephen",Small Forwarding Tables for Fast Routing Lookups,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263133,10.1145/263105.263133,"For some time, the networking community has assumed that it is impossible to do IP routing lookups in software fast enough to support gigabit speeds. IP routing lookups must find the routing entry with the longest matching prefix, a task that has been thought to require hardware support at lookup frequencies of millions per second.We present a forwarding table data structure designed for quick routing lookups. Forwarding tables are small enough to fit in the cache of a conventional general purpose processor. With the table in cache, a 200 MHz Pentium Pro or a 333 MHz Alpha 21164 can perform a few million lookups per second. This means that it is feasible to do a full routing lookup for each IP packet at gigabit speeds without special hardware.The forwarding tables are very small, a large routing table with 40,000 routing entries can be compacted to a forwarding table of 150-160 Kbytes. A lookup typically requires less than 100 instructions on an Alpha, using eight memory references accessing a total of 14 bytes.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",3–14,12,,"Cannes, France",SIGCOMM '97,,,,,,
1969,article,"Degermark, Mikael and Brodnik, Andrej and Carlsson, Svante and Pink, Stephen",Small Forwarding Tables for Fast Routing Lookups,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263133,10.1145/263109.263133,"For some time, the networking community has assumed that it is impossible to do IP routing lookups in software fast enough to support gigabit speeds. IP routing lookups must find the routing entry with the longest matching prefix, a task that has been thought to require hardware support at lookup frequencies of millions per second.We present a forwarding table data structure designed for quick routing lookups. Forwarding tables are small enough to fit in the cache of a conventional general purpose processor. With the table in cache, a 200 MHz Pentium Pro or a 333 MHz Alpha 21164 can perform a few million lookups per second. This means that it is feasible to do a full routing lookup for each IP packet at gigabit speeds without special hardware.The forwarding tables are very small, a large routing table with 40,000 routing entries can be compacted to a forwarding table of 150-160 Kbytes. A lookup typically requires less than 100 instructions on an Alpha, using eight memory references accessing a total of 14 bytes.",,3–14,12,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1970,inproceedings,"Lin, Steven and McKeown, Nick",A Simulation Study of IP Switching,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263135,10.1145/263105.263135,"Recently there has been much interest in combining the speed of layer-2 switching with the features of layer-3 routing. This has been prompted by numerous proposals, including: IP Switching [1], Tag Switching [2], ARIS [3], CSR [4], and IP over ATM [5]. In this paper, we study IP Switching and evaluate the performance claims made by Newman et al in [1] and [6]. In particular, using ten network traces, we study how well IP Switching performs with traffic found in campus, corporate, and Internet Service Provider (ISP) environments. Our main finding is that IP Switching will lead to a high proportion of datagrams that are switched; over 75% in all of the environments we studied. We also investigate the effects that different flow classifiers and various timer values have on performance, and note that some choices can result in a large VC space requirement. Finally, we present recommendations for the flow classifier and timer values, as a function of the VC space of the switch and the network environment being served.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",15–24,10,,"Cannes, France",SIGCOMM '97,,,,,,
1971,article,"Lin, Steven and McKeown, Nick",A Simulation Study of IP Switching,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263135,10.1145/263109.263135,"Recently there has been much interest in combining the speed of layer-2 switching with the features of layer-3 routing. This has been prompted by numerous proposals, including: IP Switching [1], Tag Switching [2], ARIS [3], CSR [4], and IP over ATM [5]. In this paper, we study IP Switching and evaluate the performance claims made by Newman et al in [1] and [6]. In particular, using ten network traces, we study how well IP Switching performs with traffic found in campus, corporate, and Internet Service Provider (ISP) environments. Our main finding is that IP Switching will lead to a high proportion of datagrams that are switched; over 75% in all of the environments we studied. We also investigate the effects that different flow classifiers and various timer values have on performance, and note that some choices can result in a large VC space requirement. Finally, we present recommendations for the flow classifier and timer values, as a function of the VC space of the switch and the network environment being served.",,15–24,10,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1972,inproceedings,"Waldvogel, Marcel and Varghese, George and Turner, Jon and Plattner, Bernhard",Scalable High Speed IP Routing Lookups,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263136,10.1145/263105.263136,"Internet address lookup is a challenging problem because of increasing routing table sizes, increased traffic, higher speed links, and the migration to 128 bit IPv6 addresses. IP routing lookup requires computing the best matching prefix, for which standard solutions like hashing were believed to be inapplicable. The best existing solution we know of, BSD radix tries, scales badly as IP moves to 128 bit addresses. Our paper describes a new algorithm for best matching prefix using binary search on hash tables organized by prefix lengths. Our scheme scales very well as address and routing table sizes increase: independent of the table size, it requires a worst case time of log2(address bits) hash lookups. Thus only 5 hash lookups are needed for IPv4 and 7 for IPv6. We also introduce Mutating Binary Search and other optimizations that, for a typical IPv4 backbone router with over 33,000 entries, considerably reduce the average number of hashes to less than 2, of which one hash can be simplified to an indexed array access. We expect similar average case behavior for IPv6.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",25–36,12,,"Cannes, France",SIGCOMM '97,,,,,,
1973,article,"Waldvogel, Marcel and Varghese, George and Turner, Jon and Plattner, Bernhard",Scalable High Speed IP Routing Lookups,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263136,10.1145/263109.263136,"Internet address lookup is a challenging problem because of increasing routing table sizes, increased traffic, higher speed links, and the migration to 128 bit IPv6 addresses. IP routing lookup requires computing the best matching prefix, for which standard solutions like hashing were believed to be inapplicable. The best existing solution we know of, BSD radix tries, scales badly as IP moves to 128 bit addresses. Our paper describes a new algorithm for best matching prefix using binary search on hash tables organized by prefix lengths. Our scheme scales very well as address and routing table sizes increase: independent of the table size, it requires a worst case time of log2(address bits) hash lookups. Thus only 5 hash lookups are needed for IPv4 and 7 for IPv6. We also introduce Mutating Binary Search and other optimizations that, for a typical IPv4 backbone router with over 33,000 entries, considerably reduce the average number of hashes to less than 2, of which one hash can be simplified to an indexed array access. We expect similar average case behavior for IPv6.",,25–36,12,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1974,inproceedings,"Fullmer, Chane L. and Garcia-Luna-Aceves, J. J.",Solutions to Hidden Terminal Problems in Wireless Networks,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263137,10.1145/263105.263137,"The floor acquisition multiple access (FAMA) discipline is analyzed in networks with hidden terminals. According to FAMA, control of the channel (the floor) is assigned to at most one station in the network at any given time, and this station is guaranteed to be able to transmit one or more data packets to different destinations with no collisions. The FAMA protocols described consist of non-persistent carrier or packet sensing, plus a collision-avoidance dialogue between a source and the intended receiver of a packet. Sufficient conditions under which these protocols provide correct floor acquisition are presented and verified for networks with hidden terminals; it is shown that FAMA protocols must use carrier sensing to support correct floor acquisition. The throughput of FAMA protocols is analyzed for single-channel networks with hidden terminals; it is shown that carrier-sensing FAMA protocols perform better than ALOHA and CSMA protocols in the presence of hidden terminals.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",39–49,11,,"Cannes, France",SIGCOMM '97,,,,,,
1975,article,"Fullmer, Chane L. and Garcia-Luna-Aceves, J. J.",Solutions to Hidden Terminal Problems in Wireless Networks,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263137,10.1145/263109.263137,"The floor acquisition multiple access (FAMA) discipline is analyzed in networks with hidden terminals. According to FAMA, control of the channel (the floor) is assigned to at most one station in the network at any given time, and this station is guaranteed to be able to transmit one or more data packets to different destinations with no collisions. The FAMA protocols described consist of non-persistent carrier or packet sensing, plus a collision-avoidance dialogue between a source and the intended receiver of a packet. Sufficient conditions under which these protocols provide correct floor acquisition are presented and verified for networks with hidden terminals; it is shown that FAMA protocols must use carrier sensing to support correct floor acquisition. The throughput of FAMA protocols is analyzed for single-channel networks with hidden terminals; it is shown that carrier-sensing FAMA protocols perform better than ALOHA and CSMA protocols in the presence of hidden terminals.",,39–49,11,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1976,inproceedings,"Noble, Brian D. and Satyanarayanan, M. and Nguyen, Giao T. and Katz, Randy H.",Trace-Based Mobile Network Emulation,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263140,10.1145/263105.263140,"Subjecting a mobile computing system to wireless network conditions that are realistic yet reproducible is a challenging problem. In this paper, we describe a technique called trace modulation that re-creates the observed end-to-end characteristics of a real wireless network in a controlled and repeatable manner. Trace modulation is transparent to applications and accounts for all network traffic sent or received by the system under test. We present results that show that it is indeed capable of reproducing wireless network performance faithfully.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",51–61,11,,"Cannes, France",SIGCOMM '97,,,,,,
1977,article,"Noble, Brian D. and Satyanarayanan, M. and Nguyen, Giao T. and Katz, Randy H.",Trace-Based Mobile Network Emulation,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263140,10.1145/263109.263140,"Subjecting a mobile computing system to wireless network conditions that are realistic yet reproducible is a challenging problem. In this paper, we describe a technique called trace modulation that re-creates the observed end-to-end characteristics of a real wireless network in a controlled and repeatable manner. Trace modulation is transparent to applications and accounts for all network traffic sent or received by the system under test. We present results that show that it is indeed capable of reproducing wireless network performance faithfully.",,51–61,11,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1978,inproceedings,"Lu, Songwu and Bharghavan, Vaduvur and Srikant, Rayadurgam",Fair Scheduling in Wireless Packet Networks,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263141,10.1145/263105.263141,"Fair scheduling of delay and rate-sensitive packet flows over a wireless channel is not addressed effectively by most contemporary wireline fair scheduling algorithms because of two unique characteristics of wireless media: (a) bursty channel errors, and (b) location-dependent channel capacity and errors. Besides, in packet cellular networks, the base station typically performs the task of packet scheduling for both downlink and uplink flows in a cell; however a base station has only a limited knowledge of the arrival processes of uplink flows.In this paper, we propose a new model for wireless fair scheduling based on an adaptation of fluid fair queueing to handle location-dependent error bursts. We describe an ideal wireless fair scheduling algorithm which provides a packetized implementation of the fluid model while assuming full knowledge of the current channel conditions. For this algorithm, we derive the worst-case throughput and delay bounds. Finally, we describe a practical wireless scheduling algorithm which approximates the ideal algorithm. Through simulations, we show that the algorithm achieves the desirable properties identified in the wireless fluid fair queueing model.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",63–74,12,,"Cannes, France",SIGCOMM '97,,,,,,
1979,article,"Lu, Songwu and Bharghavan, Vaduvur and Srikant, Rayadurgam",Fair Scheduling in Wireless Packet Networks,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263141,10.1145/263109.263141,"Fair scheduling of delay and rate-sensitive packet flows over a wireless channel is not addressed effectively by most contemporary wireline fair scheduling algorithms because of two unique characteristics of wireless media: (a) bursty channel errors, and (b) location-dependent channel capacity and errors. Besides, in packet cellular networks, the base station typically performs the task of packet scheduling for both downlink and uplink flows in a cell; however a base station has only a limited knowledge of the arrival processes of uplink flows.In this paper, we propose a new model for wireless fair scheduling based on an adaptation of fluid fair queueing to handle location-dependent error bursts. We describe an ideal wireless fair scheduling algorithm which provides a packetized implementation of the fluid model while assuming full knowledge of the current channel conditions. For this algorithm, we derive the worst-case throughput and delay bounds. Finally, we describe a practical wireless scheduling algorithm which approximates the ideal algorithm. Through simulations, we show that the algorithm achieves the desirable properties identified in the wireless fluid fair queueing model.",,63–74,12,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1980,inproceedings,"Han, Seungjae and Shin, Kang G.",Fast Restoration of Real-Time Communication Service from Component Failures in Multi-Hop Networks,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263143,10.1145/263105.263143,"For many applications it is important to provide communication services with guaranteed timeliness and fault-tolerance at an acceptable level of overhead. In this paper, we present a scheme for restoring real-time channels, each with guaranteed timeliness, from component failures in multi-hop networks. To ensure fast/guaranteed recovery, backup channels are set up a priori in addition to each primary channel. That is, a dependable real-time connection consists of a primary channel and one or more backup channels. If a primary channel fails, one of its backup channels is activated to become a new primary channel. We describe a protocol which provides an integrated solution to the failure-recovery problem (i.e., channel switching, resource re-allocation, ...). We also present a resource sharing method that significantly reduces the overhead of backup channels. The simulation results show that good coverage (in recovering from failures) can be achieved with about 30% degradation in network utilization under a reasonable failure condition. Moreover, the fault-tolerance level of each dependable connection can be controlled, independently of other connections, to reflect its criticality.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",77–88,12,,"Cannes, France",SIGCOMM '97,,,,,,
1981,article,"Han, Seungjae and Shin, Kang G.",Fast Restoration of Real-Time Communication Service from Component Failures in Multi-Hop Networks,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263143,10.1145/263109.263143,"For many applications it is important to provide communication services with guaranteed timeliness and fault-tolerance at an acceptable level of overhead. In this paper, we present a scheme for restoring real-time channels, each with guaranteed timeliness, from component failures in multi-hop networks. To ensure fast/guaranteed recovery, backup channels are set up a priori in addition to each primary channel. That is, a dependable real-time connection consists of a primary channel and one or more backup channels. If a primary channel fails, one of its backup channels is activated to become a new primary channel. We describe a protocol which provides an integrated solution to the failure-recovery problem (i.e., channel switching, resource re-allocation, ...). We also present a resource sharing method that significantly reduces the overhead of backup channels. The simulation results show that good coverage (in recovering from failures) can be achieved with about 30% degradation in network utilization under a reasonable failure condition. Moreover, the fault-tolerance level of each dependable connection can be controlled, independently of other connections, to reflect its criticality.",,77–88,12,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1982,inproceedings,"Hua, Kien A. and Sheu, Simon",Skyscraper Broadcasting: A New Broadcasting Scheme for Metropolitan Video-on-Demand Systems,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263144,10.1145/263105.263144,"We investigate a novel multicast technique, called Skyscraper Broadcasting (SB), for video-on-demand applications. We discuss the data fragmentation technique, the broadcasting strategy, and the client design. We also show the correctness of our technique, and derive mathematical equations to analyze its storage requirement. To assess its performance, we compare it to the latest designs known as Pyramid Broadcasting (PB) and Permutation-Based Pyramid Broadcasting (PPB). Our study indicates that PB offers excellent access latency. However, it requires very large storage space and disk bandwidth at the receiving end. PPB is able to address these problems. However, this is accomplished at the expense of a larger access latency and more complex synchronization. With SB, we are able to achieve the low latency of PB while using only 20% of the buffer space required by PPB.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",89–100,12,,"Cannes, France",SIGCOMM '97,,,,,,
1983,article,"Hua, Kien A. and Sheu, Simon",Skyscraper Broadcasting: A New Broadcasting Scheme for Metropolitan Video-on-Demand Systems,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263144,10.1145/263109.263144,"We investigate a novel multicast technique, called Skyscraper Broadcasting (SB), for video-on-demand applications. We discuss the data fragmentation technique, the broadcasting strategy, and the client design. We also show the correctness of our technique, and derive mathematical equations to analyze its storage requirement. To assess its performance, we compare it to the latest designs known as Pyramid Broadcasting (PB) and Permutation-Based Pyramid Broadcasting (PPB). Our study indicates that PB offers excellent access latency. However, it requires very large storage space and disk bandwidth at the receiving end. PPB is able to address these problems. However, this is accomplished at the expense of a larger access latency and more complex synchronization. With SB, we are able to achieve the low latency of PB while using only 20% of the buffer space required by PPB.",,89–100,12,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1984,inproceedings,"Alexander, D. Scott and Shaw, Marianne and Nettles, Scott M. and Smith, Jonathan M.",Active Bridging,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263149,10.1145/263105.263149,"Active networks accelerate network evolution by permitting the network infrastructure to be programmable, on a per-user, per-packet, or other basis. This programmability must be balanced against the safety and security needs inherent in shared resources.This paper describes the design, implementation, and performance of a new type of network element, an Active Bridge. The active bridge can be reprogrammed ""on the fly"", with loadable modules called switchlets. To demonstrate the use of the active property, we incrementally extend what is initially a programmable buffered repeater with switchlets into a self-learning bridge, and then a bridge supporting spanning tree algorithms. To demonstrate the agility that active networking gives, we show how it is possible to upgrade a network from an ""old"" protocol to a ""new"" protocol on-the-fly. Moreover, we are able to take advantage of information unavailable to the implementors of either protocol to validate the new protocol and fall back to the old protocol if an error is detected. This shows that the Active Bridge can protect itself from some algorithmic failures in loadable modules.Our approach to safety and security favors static checking and prevention over dynamic checks when possible. We rely on strong type checking in the Caml language for the loadable module infrastructure, and achieve respectable performance. The prototype implementation on a Pentium-based HP Netserver LS running Linux with 100 Mbps Ethernet LANS achieves ttcp throughput of 16 Mbps between two PCs running Linux, compared with 76 Mbps unbridged. Measured frame rates are in the neighborhood of 1800 frames per second.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",101–111,11,,"Cannes, France",SIGCOMM '97,,,,,,
1985,article,"Alexander, D. Scott and Shaw, Marianne and Nettles, Scott M. and Smith, Jonathan M.",Active Bridging,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263149,10.1145/263109.263149,"Active networks accelerate network evolution by permitting the network infrastructure to be programmable, on a per-user, per-packet, or other basis. This programmability must be balanced against the safety and security needs inherent in shared resources.This paper describes the design, implementation, and performance of a new type of network element, an Active Bridge. The active bridge can be reprogrammed ""on the fly"", with loadable modules called switchlets. To demonstrate the use of the active property, we incrementally extend what is initially a programmable buffered repeater with switchlets into a self-learning bridge, and then a bridge supporting spanning tree algorithms. To demonstrate the agility that active networking gives, we show how it is possible to upgrade a network from an ""old"" protocol to a ""new"" protocol on-the-fly. Moreover, we are able to take advantage of information unavailable to the implementors of either protocol to validate the new protocol and fall back to the old protocol if an error is detected. This shows that the Active Bridge can protect itself from some algorithmic failures in loadable modules.Our approach to safety and security favors static checking and prevention over dynamic checks when possible. We rely on strong type checking in the Caml language for the loadable module infrastructure, and achieve respectable performance. The prototype implementation on a Pentium-based HP Netserver LS running Linux with 100 Mbps Ethernet LANS achieves ttcp throughput of 16 Mbps between two PCs running Linux, compared with 76 Mbps unbridged. Measured frame rates are in the neighborhood of 1800 frames per second.",,101–111,11,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1986,inproceedings,"Labovitz, Craig and Malan, G. Robert and Jahanian, Farnam",Internet Routing Instability,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263151,10.1145/263105.263151,"This paper examines the network inter-domain routing information exchanged between backbone service providers at the major U.S. public Internet exchange points. Internet routing instability, or the rapid fluctuation of network reachability information, is an important problem currently facing the Internet engineering community. High levels of network instability can lead to packet loss, increased network latency and time to convergence. At the extreme, high levels of routing instability have lead to the loss of internal connectivity in wide-area, national networks. In this paper, we describe several unexpected trends in routing instability, and examine a number of anomalies and pathologies observed in the exchange of inter-domain routing information. The analysis in this paper is based on data collected from BGP routing messages generated by border routers at five of the Internet core's public exchange points during a nine month period. We show that the volume of these routing updates is several orders of magnitude more than expected and that the majority of this routing information is redundant, or pathological. Furthermore, our analysis reveals several unexpected trends and ill-behaved systematic properties in Internet routing. We finally posit a number of explanations for these anomalies and evaluate their potential impact on the Internet infrastructure.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",115–126,12,,"Cannes, France",SIGCOMM '97,,,,,,
1987,article,"Labovitz, Craig and Malan, G. Robert and Jahanian, Farnam",Internet Routing Instability,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263151,10.1145/263109.263151,"This paper examines the network inter-domain routing information exchanged between backbone service providers at the major U.S. public Internet exchange points. Internet routing instability, or the rapid fluctuation of network reachability information, is an important problem currently facing the Internet engineering community. High levels of network instability can lead to packet loss, increased network latency and time to convergence. At the extreme, high levels of routing instability have lead to the loss of internal connectivity in wide-area, national networks. In this paper, we describe several unexpected trends in routing instability, and examine a number of anomalies and pathologies observed in the exchange of inter-domain routing information. The analysis in this paper is based on data collected from BGP routing messages generated by border routers at five of the Internet core's public exchange points during a nine month period. We show that the volume of these routing updates is several orders of magnitude more than expected and that the majority of this routing information is redundant, or pathological. Furthermore, our analysis reveals several unexpected trends and ill-behaved systematic properties in Internet routing. We finally posit a number of explanations for these anomalies and evaluate their potential impact on the Internet infrastructure.",,115–126,12,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1988,inproceedings,"Lin, Dong and Morris, Robert",Dynamics of Random Early Detection,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263154,10.1145/263105.263154,"In this paper we evaluate the effectiveness of Random Early Detection (RED) over traffic types categorized as non-adaptive, fragile and robust, according to their responses to congestion. We point out that RED allows unfair bandwidth sharing when a mixture of the three traffic types shares a link. This unfairness is caused by the fact that at any given time RED imposes the same loss rate on all flows, regardless of their bandwidths.We propose Fair Random Early Drop (FRED), a modified version of RED. FRED uses per-active-flow accounting to impose on each flow a loss rate that depends on the flow's buffer use.We show that FRED provides better protection than RED for adaptive (fragile and robust) flows. In addition, FRED is able to isolate non-adaptive greedy traffic more effectively. Finally, we present a ""two-packet-buffer"" gateway mechanism to support a large number of flows without incurring additional queueing delays inside the network. These improvements are demonstrated by simulations of TCP and UDP traffic.FRED does not make any assumptions about queueing architecture; it will work with a FIFO gateway. FRED's per-active-flow accounting uses memory in proportion to the total number of buffers used: a FRED gateway maintains state only for flows for which it has packets buffered, not for all flows that traverse the gateway.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",127–137,11,,"Cannes, France",SIGCOMM '97,,,,,,
1989,article,"Lin, Dong and Morris, Robert",Dynamics of Random Early Detection,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263154,10.1145/263109.263154,"In this paper we evaluate the effectiveness of Random Early Detection (RED) over traffic types categorized as non-adaptive, fragile and robust, according to their responses to congestion. We point out that RED allows unfair bandwidth sharing when a mixture of the three traffic types shares a link. This unfairness is caused by the fact that at any given time RED imposes the same loss rate on all flows, regardless of their bandwidths.We propose Fair Random Early Drop (FRED), a modified version of RED. FRED uses per-active-flow accounting to impose on each flow a loss rate that depends on the flow's buffer use.We show that FRED provides better protection than RED for adaptive (fragile and robust) flows. In addition, FRED is able to isolate non-adaptive greedy traffic more effectively. Finally, we present a ""two-packet-buffer"" gateway mechanism to support a large number of flows without incurring additional queueing delays inside the network. These improvements are demonstrated by simulations of TCP and UDP traffic.FRED does not make any assumptions about queueing architecture; it will work with a FIFO gateway. FRED's per-active-flow accounting uses memory in proportion to the total number of buffers used: a FRED gateway maintains state only for flows for which it has packets buffered, not for all flows that traverse the gateway.",,127–137,11,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1990,inproceedings,"Paxson, Vern",End-to-End Internet Packet Dynamics,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263155,10.1145/263105.263155,"We discuss findings from a large-scale study of Internet packet dynamics conducted by tracing 20,000 TCP bulk transfers between 35 Internet sites. Because we traced each 100 Kbyte transfer at both the sender and the receiver, the measurements allow us to distinguish between the end-to-end behaviors due to the different directions of the Internet paths, which often exhibit asymmetries. We characterize the prevalence of unusual network events such as out-of-order delivery and packet corruption; discuss a robust receiver-based algorithm for estimating ""bottleneck bandwidth"" that addresses deficiencies discovered in techniques based on ""packet pair""; investigate patterns of packet loss, finding that loss events are not well-modeled as independent and, furthermore, that the distribution of the duration of loss events exhibits infinite variance; and analyze variations in packet transit delays as indicators of congestion periods, finding that congestion periods also span a wide range of time scales.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",139–152,14,,"Cannes, France",SIGCOMM '97,,,,,,
1991,article,"Paxson, Vern",End-to-End Internet Packet Dynamics,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263155,10.1145/263109.263155,"We discuss findings from a large-scale study of Internet packet dynamics conducted by tracing 20,000 TCP bulk transfers between 35 Internet sites. Because we traced each 100 Kbyte transfer at both the sender and the receiver, the measurements allow us to distinguish between the end-to-end behaviors due to the different directions of the Internet paths, which often exhibit asymmetries. We characterize the prevalence of unusual network events such as out-of-order delivery and packet corruption; discuss a robust receiver-based algorithm for estimating ""bottleneck bandwidth"" that addresses deficiencies discovered in techniques based on ""packet pair""; investigate patterns of packet loss, finding that loss events are not well-modeled as independent and, furthermore, that the distribution of the duration of loss events exhibits infinite variance; and analyze variations in packet transit delays as indicators of congestion periods, finding that congestion periods also span a wide range of time scales.",,139–152,14,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1992,inproceedings,"Nielsen, Henrik Frystyk and Gettys, James and Baird-Smith, Anselm and Prud'hommeaux, Eric and Lie, H\r{a","Network Performance Effects of HTTP/1.1, CSS1, and PNG",1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263157,10.1145/263105.263157,"We describe our investigation of the effect of persistent connections, pipelining and link level document compression on our client and server HTTP implementations. A simple test setup is used to verify HTTP/1.1's design and understand HTTP/1.1 implementation strategies. We present TCP and real time performance data between the libwww robot [27] and both the W3C's Jigsaw [28] and Apache [29] HTTP servers using HTTP/1.0, HTTP/1.1 with persistent connections, HTTP/1.1 with pipelined requests, and HTTP/1.1 with pipelined requests and deflate data compression [22]. We also investigate whether the TCP Nagle algorithm has an effect on HTTP/1.1 performance. While somewhat artificial and possibly overstating the benefits of HTTP/1.1, we believe the tests and results approximate some common behavior seen in browsers. The results confirm that HTTP/1.1 is meeting its major design goals. Our experience has been that implementation details are very important to achieve all of the benefits of HTTP/1.1.For all our tests, a pipelined HTTP/1.1 implementation outperformed HTTP/1.0, even when the HTTP/1.0 implementation used multiple connections in parallel, under all network environments tested. The savings were at least a factor of two, and sometimes as much as a factor of ten, in terms of packets transmitted. Elapsed time improvement is less dramatic, and strongly depends on your network connection.Some data is presented showing further savings possible by changes in Web content, specifically by the use of CSS style sheets [10], and the more compact PNG [20] image representation, both recent recommendations of W3C. Time did not allow full end to end data collection on these cases. The results show that HTTP/1.1 and changes in Web content will have dramatic results in Internet and Web performance as HTTP/1.1 and related technologies deploy over the near future. Universal use of style sheets, even without deployment of HTTP/1.1, would cause a very significant reduction in network traffic.This paper does not investigate further performance and network savings enabled by the improved caching facilities provided by the HTTP/1.1 protocol, or by sophisticated use of range requests.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",155–166,12,,"Cannes, France",SIGCOMM '97,,,,,,
1993,article,"Nielsen, Henrik Frystyk and Gettys, James and Baird-Smith, Anselm and Prud'hommeaux, Eric and Lie, H\r{a","Network Performance Effects of HTTP/1.1, CSS1, and PNG",1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263157,10.1145/263109.263157,"We describe our investigation of the effect of persistent connections, pipelining and link level document compression on our client and server HTTP implementations. A simple test setup is used to verify HTTP/1.1's design and understand HTTP/1.1 implementation strategies. We present TCP and real time performance data between the libwww robot [27] and both the W3C's Jigsaw [28] and Apache [29] HTTP servers using HTTP/1.0, HTTP/1.1 with persistent connections, HTTP/1.1 with pipelined requests, and HTTP/1.1 with pipelined requests and deflate data compression [22]. We also investigate whether the TCP Nagle algorithm has an effect on HTTP/1.1 performance. While somewhat artificial and possibly overstating the benefits of HTTP/1.1, we believe the tests and results approximate some common behavior seen in browsers. The results confirm that HTTP/1.1 is meeting its major design goals. Our experience has been that implementation details are very important to achieve all of the benefits of HTTP/1.1.For all our tests, a pipelined HTTP/1.1 implementation outperformed HTTP/1.0, even when the HTTP/1.0 implementation used multiple connections in parallel, under all network environments tested. The savings were at least a factor of two, and sometimes as much as a factor of ten, in terms of packets transmitted. Elapsed time improvement is less dramatic, and strongly depends on your network connection.Some data is presented showing further savings possible by changes in Web content, specifically by the use of CSS style sheets [10], and the more compact PNG [20] image representation, both recent recommendations of W3C. Time did not allow full end to end data collection on these cases. The results show that HTTP/1.1 and changes in Web content will have dramatic results in Internet and Web performance as HTTP/1.1 and related technologies deploy over the near future. Universal use of style sheets, even without deployment of HTTP/1.1, would cause a very significant reduction in network traffic.This paper does not investigate further performance and network savings enabled by the improved caching facilities provided by the HTTP/1.1 protocol, or by sophisticated use of range requests.",,155–166,12,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1994,inproceedings,"Paxson, Vern",Automated Packet Trace Analysis of TCP Implementations,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263160,10.1145/263105.263160,"We describe tcpanaly, a tool for automatically analyzing a TCP implementation's behavior by inspecting packet traces of the TCP's activity. Doing so requires surmounting a number of hurdles, including detecting packet filter measurement errors, coping with ambiguities due to the distance between the measurement point and the TCP, and accommodating a surprisingly large range of behavior among different TCP implementations. We discuss why our efforts to develop a fully general tool failed, and detail a number of significant differences among 8 major TCP implementations, some of which, if ubiquitous, would devastate Internet performance. The most problematic TCPs were all independently written, suggesting that correct TCP implementation is fraught with difficulty. Consequently, it behooves the Internet community to develop testing programs and reference implementations.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",167–179,13,,"Cannes, France",SIGCOMM '97,,,,,,
1995,article,"Paxson, Vern",Automated Packet Trace Analysis of TCP Implementations,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263160,10.1145/263109.263160,"We describe tcpanaly, a tool for automatically analyzing a TCP implementation's behavior by inspecting packet traces of the TCP's activity. Doing so requires surmounting a number of hurdles, including detecting packet filter measurement errors, coping with ambiguities due to the distance between the measurement point and the TCP, and accommodating a surprisingly large range of behavior among different TCP implementations. We discuss why our efforts to develop a fully general tool failed, and detail a number of significant differences among 8 major TCP implementations, some of which, if ubiquitous, would devastate Internet performance. The most problematic TCPs were all independently written, suggesting that correct TCP implementation is fraught with difficulty. Consequently, it behooves the Internet community to develop testing programs and reference implementations.",,167–179,13,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1996,inproceedings,"Mogul, Jeffrey C. and Douglis, Fred and Feldmann, Anja and Krishnamurthy, Balachander",Potential Benefits of Delta Encoding and Data Compression for HTTP,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263162,10.1145/263105.263162,"Caching in the World Wide Web currently follows a naive model, which assumes that resources are referenced many times between changes. The model also provides no way to update a cache entry if a resource does change, except by transferring the resource's entire new value. Several previous papers have proposed updating cache entries by transferring only the differences, or ""delta,"" between the cached entry and the current value.In this paper, we make use of dynamic traces of the full contents of HTTP messages to quantify the potential benefits of delta-encoded responses. We show that delta encoding can provide remarkable improvements in response size and response delay for an important subset of HTTP content types. We also show the added benefit of data compression, and that the combination of delta encoding and data compression yields the best results.We propose specific extensions to the HTTP protocol for delta encoding and data compression. These extensions are compatible with existing implementations and specifications, yet allow efficient use of a variety of encoding techniques.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",181–194,14,,"Cannes, France",SIGCOMM '97,,,,,,
1997,article,"Mogul, Jeffrey C. and Douglis, Fred and Feldmann, Anja and Krishnamurthy, Balachander",Potential Benefits of Delta Encoding and Data Compression for HTTP,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263162,10.1145/263109.263162,"Caching in the World Wide Web currently follows a naive model, which assumes that resources are referenced many times between changes. The model also provides no way to update a cache entry if a resource does change, except by transferring the resource's entire new value. Several previous papers have proposed updating cache entries by transferring only the differences, or ""delta,"" between the cached entry and the current value.In this paper, we make use of dynamic traces of the full contents of HTTP messages to quantify the potential benefits of delta-encoded responses. We show that delta encoding can provide remarkable improvements in response size and response delay for an important subset of HTTP content types. We also show the added benefit of data compression, and that the combination of delta encoding and data compression yields the best results.We propose specific extensions to the HTTP protocol for delta encoding and data compression. These extensions are compatible with existing implementations and specifications, yet allow efficient use of a variety of encoding techniques.",,181–194,14,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
1998,inproceedings,"Handley, Mark and Crowcroft, Jon",Network Text Editor (NTE): A Scalable Shared Text Editor for the MBone,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263167,10.1145/263105.263167,"IP Multicast, Lightweight Sessions and Application Level Framing provide guidelines by which multimedia conferencing tools can be designed, but they do not provide specific solutions. In this paper, we use these design principles to guide the design of a multicast based shared editor, and examine the consequences of taking a loose consistency approach to achieve good performance in the face of network failures and losses.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",197–208,12,,"Cannes, France",SIGCOMM '97,,,,,,
1999,article,"Handley, Mark and Crowcroft, Jon",Network Text Editor (NTE): A Scalable Shared Text Editor for the MBone,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263167,10.1145/263109.263167,"IP Multicast, Lightweight Sessions and Application Level Framing provide guidelines by which multimedia conferencing tools can be designed, but they do not provide specific solutions. In this paper, we use these design principles to guide the design of a multicast based shared editor, and examine the consequences of taking a loose consistency approach to achieve good performance in the face of network failures and losses.",,197–208,12,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2000,inproceedings,"Cheshire, Stuart and Baker, Mary",Consistent Overhead Byte Stuffing,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263168,10.1145/263105.263168,"Byte stuffing is a process that transforms a sequence of data bytes that may contain 'illegal' or 'reserved' values into a potentially longer sequence that contains no occurrences of those values. The extra length is referred to in this paper as the overhead of the algorithm.To date, byte stuffing algorithms, such as those used by SLIP [RFC1055], PPP [RFC1662] and AX.25 [ARRL84], have been designed to incur low average overhead but have made little effort to minimize worst case overhead.Some increasingly popular network devices, however, care more about the worst case. For example, the transmission time for ISM-band packet radio transmitters is strictly limited by FCC regulation. To adhere to this regulation, the practice is to set the maximum packet size artificially low so that no packet, even after worst case overhead, can exceed the transmission time limit.This paper presents a new byte stuffing algorithm, called Consistent Overhead Byte Stuffing (COBS), that tightly bounds the worst case overhead. It guarantees in the worst case to add no more than one byte in 254 to any packet. Furthermore, the algorithm is computationally cheap, and its average overhead is very competitive with that of existing algorithms.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",209–220,12,,"Cannes, France",SIGCOMM '97,,,,,,
2001,article,"Cheshire, Stuart and Baker, Mary",Consistent Overhead Byte Stuffing,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263168,10.1145/263109.263168,"Byte stuffing is a process that transforms a sequence of data bytes that may contain 'illegal' or 'reserved' values into a potentially longer sequence that contains no occurrences of those values. The extra length is referred to in this paper as the overhead of the algorithm.To date, byte stuffing algorithms, such as those used by SLIP [RFC1055], PPP [RFC1662] and AX.25 [ARRL84], have been designed to incur low average overhead but have made little effort to minimize worst case overhead.Some increasingly popular network devices, however, care more about the worst case. For example, the transmission time for ISM-band packet radio transmitters is strictly limited by FCC regulation. To adhere to this regulation, the practice is to set the maximum packet size artificially low so that no packet, even after worst case overhead, can exceed the transmission time limit.This paper presents a new byte stuffing algorithm, called Consistent Overhead Byte Stuffing (COBS), that tightly bounds the worst case overhead. It guarantees in the worst case to add no more than one byte in 254 to any packet. Furthermore, the algorithm is computationally cheap, and its average overhead is very competitive with that of existing algorithms.",,209–220,12,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2002,inproceedings,"Mittra, Suvo and Woo, Thomas Y. C.",A Flow-Based Approach to Datagram Security,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263170,10.1145/263105.263170,"Datagram services provide a simple, flexible, robust, and scalable communication abstraction; their usefulness has been well demonstrated by the success of IP, UDP, and RPC. Yet, the overwhelming majority of network security protocols that have been proposed are geared towards connection-oriented communications. The few that do cater to datagram communications tend to either rely on long term host-pair keying or impose a session-oriented (i.e., requiring connection setup) semantics.Separately, the concept of flows has received a great deal of attention recently, especially in the context of routing and QoS. A flow characterizes a sequence of datagrams sharing some pre-defined attributes. In this paper, we advocate the use of flows as a basis for structuring secure datagram communications. We support this by proposing a novel protocol for datagram security based on flows. Our protocol achieves zero-message keying, thus preserving the connectionless nature of datagram, and makes use of soft state, thus providing the per-packet processing efficiency of session-oriented schemes. We have implemented an instantiation for IP in the 4.4BSD kernel, and we provide a description of our implementation along with performance results.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",221–234,14,,"Cannes, France",SIGCOMM '97,,,,,,
2003,article,"Mittra, Suvo and Woo, Thomas Y. C.",A Flow-Based Approach to Datagram Security,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263170,10.1145/263109.263170,"Datagram services provide a simple, flexible, robust, and scalable communication abstraction; their usefulness has been well demonstrated by the success of IP, UDP, and RPC. Yet, the overwhelming majority of network security protocols that have been proposed are geared towards connection-oriented communications. The few that do cater to datagram communications tend to either rely on long term host-pair keying or impose a session-oriented (i.e., requiring connection setup) semantics.Separately, the concept of flows has received a great deal of attention recently, especially in the context of routing and QoS. A flow characterizes a sequence of datagrams sharing some pre-defined attributes. In this paper, we advocate the use of flows as a basis for structuring secure datagram communications. We support this by proposing a novel protocol for datagram security based on flows. Our protocol achieves zero-message keying, thus preserving the connectionless nature of datagram, and makes use of soft state, thus providing the per-packet processing efficiency of session-oriented schemes. We have implemented an instantiation for IP in the 4.4BSD kernel, and we provide a description of our implementation along with performance results.",,221–234,14,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2004,inproceedings,"Grossglauser, Matthias and Tse, David",A Framework for Robust Measurement-Based Admission Control,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263173,10.1145/263105.263173,"Measurement-based Admission Control (MBAC) is an attractive mechanism to concurrently offer Quality of Service (QoS) to users, without requiring a-priori traffic specification and on-line policing. However, several aspects of such a system need to be clearly understood in order to devise robust MBAC schemes. Through a sequence of increasingly sophisticated stochastic models, we study the impact of parameter estimation errors, of flow arrival and departure dynamics, and of estimation memory on the performance of an MBAC system.We show that a certainty equivalence assumption, i.e., assuming that the measured parameters are the real ones, can grossly compromise the target performance of the system. We quantify the improvement in performance as a function of the memory size of the estimator and a more conservative choice of the certainty-equivalent parameters. Our results yield valuable new insight into the performance of MBAC schemes, and represent quantitative guidelines for the design of robust schemes.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",237–248,12,,"Cannes, France",SIGCOMM '97,,,,,,
2005,article,"Grossglauser, Matthias and Tse, David",A Framework for Robust Measurement-Based Admission Control,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263173,10.1145/263109.263173,"Measurement-based Admission Control (MBAC) is an attractive mechanism to concurrently offer Quality of Service (QoS) to users, without requiring a-priori traffic specification and on-line policing. However, several aspects of such a system need to be clearly understood in order to devise robust MBAC schemes. Through a sequence of increasingly sophisticated stochastic models, we study the impact of parameter estimation errors, of flow arrival and departure dynamics, and of estimation memory on the performance of an MBAC system.We show that a certainty equivalence assumption, i.e., assuming that the measured parameters are the real ones, can grossly compromise the target performance of the system. We quantify the improvement in performance as a function of the memory size of the estimator and a more conservative choice of the certainty-equivalent parameters. Our results yield valuable new insight into the performance of MBAC schemes, and represent quantitative guidelines for the design of robust schemes.",,237–248,12,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2006,inproceedings,"Stoica, Ion and Zhang, Hui and Ng, T. S. Eugene","A Hierarchical Fair Service Curve Algorithm for Link-Sharing, Real-Time and Priority Services",1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263175,10.1145/263105.263175,"In this paper, we study hierarchical resource management models and algorithms that support both link-sharing and guaranteed real-time services with decoupled delay (priority) and bandwidth allocation. We extend the service curve based QoS model, which defines both delay and bandwidth requirements of a class, to include fairness, which is important for the integration of real-time and hierarchical link-sharing services. The resulting Fair Service Curve link-sharing model formalizes the goals of link-sharing and real-time services and exposes the fundamental tradeoffs between these goals. In particular, with decoupled delay and band-width allocation, it is impossible to simultaneously provide guaranteed real-time service and achieve perfect link-sharing. We propose a novel scheduling algorithm called Hierarchical Fair Service Curve (H-FSC) that approximates the model closely and efficiently. The algorithm always guarantees the performance for leaf classes, thus ensures real-time services, while minimizing the discrepancy between the actual services provided to the interior classes and the services defined by the Fair Service Curve link-sharing model. We have implemented the H-FSC scheduler in the NetBSD environment. By performing simulation and measurement experiments, we evaluate the link-sharing and real-time performances of H-FSC, and determine the computation over-head.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",249–262,14,,"Cannes, France",SIGCOMM '97,,,,,,
2007,article,"Stoica, Ion and Zhang, Hui and Ng, T. S. Eugene","A Hierarchical Fair Service Curve Algorithm for Link-Sharing, Real-Time and Priority Services",1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263175,10.1145/263109.263175,"In this paper, we study hierarchical resource management models and algorithms that support both link-sharing and guaranteed real-time services with decoupled delay (priority) and bandwidth allocation. We extend the service curve based QoS model, which defines both delay and bandwidth requirements of a class, to include fairness, which is important for the integration of real-time and hierarchical link-sharing services. The resulting Fair Service Curve link-sharing model formalizes the goals of link-sharing and real-time services and exposes the fundamental tradeoffs between these goals. In particular, with decoupled delay and band-width allocation, it is impossible to simultaneously provide guaranteed real-time service and achieve perfect link-sharing. We propose a novel scheduling algorithm called Hierarchical Fair Service Curve (H-FSC) that approximates the model closely and efficiently. The algorithm always guarantees the performance for leaf classes, thus ensures real-time services, while minimizing the discrepancy between the actual services provided to the interior classes and the services defined by the Fair Service Curve link-sharing model. We have implemented the H-FSC scheduler in the NetBSD environment. By performing simulation and measurement experiments, we evaluate the link-sharing and real-time performances of H-FSC, and determine the computation over-head.",,249–262,14,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2008,inproceedings,"Kontovasilis, Kimon and Mitrou, Nikolas",Effective Bandwidths for a Class of Non Markovian Fluid Sources,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263177,10.1145/263105.263177,"This paper proves the existence of and explicitly determines effective bandwidths for a class of non Markovian fluid source models, featuring multiple data-transmission rates and arbitrary distributions for the times these rates are sustained. The investigated models cover considerably more traffic profiles than the usual Markovian counterparts and have reduced state-space requirements. The effective bandwidth, as a function of the asymptotic loss probability decay rate, is implicitly derivable by the requirement that the spectral radius of an appropriate nonnegative matrix be equal to unity. The effective bandwidth function is shown to be, either strictly increasing, or constant and equal to the mean rate. Sources of the second kind, which are characterized, generalize the notion of 'CBR' traffic. Furthermore, a study for the limiting effective bandwidth, towards a loss-less environment, is undertaken; it is shown that the limiting value may, under some fully identified restrictions on the source behavior, be less than the source's peak rate. Under those restrictions, a source may have reduced bandwidth requirements, even if it features a large peak rate.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",263–274,12,,"Cannes, France",SIGCOMM '97,,,,,,
2009,article,"Kontovasilis, Kimon and Mitrou, Nikolas",Effective Bandwidths for a Class of Non Markovian Fluid Sources,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263177,10.1145/263109.263177,"This paper proves the existence of and explicitly determines effective bandwidths for a class of non Markovian fluid source models, featuring multiple data-transmission rates and arbitrary distributions for the times these rates are sustained. The investigated models cover considerably more traffic profiles than the usual Markovian counterparts and have reduced state-space requirements. The effective bandwidth, as a function of the asymptotic loss probability decay rate, is implicitly derivable by the requirement that the spectral radius of an appropriate nonnegative matrix be equal to unity. The effective bandwidth function is shown to be, either strictly increasing, or constant and equal to the mean rate. Sources of the second kind, which are characterized, generalize the notion of 'CBR' traffic. Furthermore, a study for the limiting effective bandwidth, towards a loss-less environment, is undertaken; it is shown that the limiting value may, under some fully identified restrictions on the source behavior, be less than the source's peak rate. Under those restrictions, a source may have reduced bandwidth requirements, even if it features a large peak rate.",,263–274,12,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2010,inproceedings,"Mittra, Suvo",Iolus: A Framework for Scalable Secure Multicasting,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263179,10.1145/263105.263179,"As multicast applications are deployed for mainstream use, the need to secure multicast communications will become critical. Multicast, however, does not fit the point-to-point model of most network security protocols which were designed with unicast communications in mind. As we will show, securing multicast (or group) communications is fundamentally different from securing unicast (or paired) communications. In turn, these differences can result in scalability problems for many typical applications.In this paper, we examine and model the differences between unicast and multicast security and then propose Iolus: a novel framework for scalable secure multicasting. Protocols based on Iolus can be used to achieve a variety of security objectives and may be used either to directly secure multicast communications or to provide a separate group key management service to other ""security-aware"" applications. We describe the architecture and operation of Iolus in detail and also describe our experience with a protocol based on the Iolus framework.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",277–288,12,,"Cannes, France",SIGCOMM '97,,,,,,
2011,article,"Mittra, Suvo",Iolus: A Framework for Scalable Secure Multicasting,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263179,10.1145/263109.263179,"As multicast applications are deployed for mainstream use, the need to secure multicast communications will become critical. Multicast, however, does not fit the point-to-point model of most network security protocols which were designed with unicast communications in mind. As we will show, securing multicast (or group) communications is fundamentally different from securing unicast (or paired) communications. In turn, these differences can result in scalability problems for many typical applications.In this paper, we examine and model the differences between unicast and multicast security and then propose Iolus: a novel framework for scalable secure multicasting. Protocols based on Iolus can be used to achieve a variety of security objectives and may be used either to directly secure multicast communications or to provide a separate group key management service to other ""security-aware"" applications. We describe the architecture and operation of Iolus in detail and also describe our experience with a protocol based on the Iolus framework.",,277–288,12,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2012,inproceedings,"Nonnenmacher, J\""{o",Parity-Based Loss Recovery for Reliable Multicast Transmission,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263181,10.1145/263105.263181,"We investigate how FEC (Forward Error Correction) can be combined with ARQ (Automatic Repeat Request) to achieve scalable reliable multicast transmission. We consider the two scenarios where FEC is introduced as a transparent layer underneath a reliable multicast layer that uses ARQ, and where FEC and ARQ are both integrated into a single layer that uses the retransmission of parity data to recover from the loss of original data packets.To evaluate the performance improvements due to FEC, we consider different types of loss behaviors (spatially or temporally correlated loss, homogeneous or heterogeneous loss) and loss rates for up to 106 receivers. Our results show that introducing FEC as a layer below ARQ can improve multicast transmission efficiency and scalability and that there are substantial additional improvements when the two are integrated.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",289–300,12,,"Cannes, France",SIGCOMM '97,,,,,,
2013,article,"Nonnenmacher, J\""{o",Parity-Based Loss Recovery for Reliable Multicast Transmission,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263181,10.1145/263109.263181,"We investigate how FEC (Forward Error Correction) can be combined with ARQ (Automatic Repeat Request) to achieve scalable reliable multicast transmission. We consider the two scenarios where FEC is introduced as a transparent layer underneath a reliable multicast layer that uses ARQ, and where FEC and ARQ are both integrated into a single layer that uses the retransmission of parity data to recover from the loss of original data packets.To evaluate the performance improvements due to FEC, we consider different types of loss behaviors (spatially or temporally correlated loss, homogeneous or heterogeneous loss) and loss rates for up to 106 receivers. Our results show that introducing FEC as a layer below ARQ can improve multicast transmission efficiency and scalability and that there are substantial additional improvements when the two are integrated.",,289–300,12,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2014,inproceedings,"Ortiz, Zeydy and Rouskas, George N. and Perros, Harry G.",Scheduling of Multicast Traffic in Tunable-Receiver WDM Networks with Non-Negligible Tuning Latencies,1997,089791905X,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263105.263182,10.1145/263105.263182,"We consider the problem of supporting multipoint communication at the media access control (MAC) layer of broadcast-and-select WDM networks. We first show that bandwidth consumption and channel utilization arise as two conflicting objectives in the design of scheduling algorithms for multicast traffic in this environment. We then present a new technique for the transmission of multicast packets, based on the concept of a virtual receiver, a set of physical receivers which behave identically in terms of tuning. We also show that the number κ of virtual receivers naturally captures the tradeoff between channel utilization and bandwidth consumption. Our approach decouples the problem of determining the virtual receivers from the problem of scheduling packet transmissions, making it possible to employ existing scheduling algorithms that have been shown to successfully hide the effects of tuning latency. Consequently, we focus on the problem of optimally selecting the virtual receivers, and we prove that it is NP-complete. Finally, we present four heuristics of varying degrees of complexity for obtaining virtual receivers that provide a good balance between the two conflicting objectives.","Proceedings of the ACM SIGCOMM '97 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",301–310,10,,"Cannes, France",SIGCOMM '97,,,,,,
2015,article,"Ortiz, Zeydy and Rouskas, George N. and Perros, Harry G.",Scheduling of Multicast Traffic in Tunable-Receiver WDM Networks with Non-Negligible Tuning Latencies,1997,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/263109.263182,10.1145/263109.263182,"We consider the problem of supporting multipoint communication at the media access control (MAC) layer of broadcast-and-select WDM networks. We first show that bandwidth consumption and channel utilization arise as two conflicting objectives in the design of scheduling algorithms for multicast traffic in this environment. We then present a new technique for the transmission of multicast packets, based on the concept of a virtual receiver, a set of physical receivers which behave identically in terms of tuning. We also show that the number κ of virtual receivers naturally captures the tradeoff between channel utilization and bandwidth consumption. Our approach decouples the problem of determining the virtual receivers from the problem of scheduling packet transmissions, making it possible to employ existing scheduling algorithms that have been shown to successfully hide the effects of tuning latency. Consequently, we focus on the problem of optimally selecting the virtual receivers, and we prove that it is NP-complete. Finally, we present four heuristics of varying degrees of complexity for obtaining virtual receivers that provide a good balance between the two conflicting objectives.",,301–310,10,,,,Oct. 1997,27,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2016,inproceedings,"Ryu, Bong K. and Elwalid, Anwar",The Importance of Long-Range Dependence of VBR Video Traffic in ATM Traffic Engineering: Myths and Realities,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248158,10.1145/248156.248158,"There has been a growing concern about the potential impact of long-term correlations (second-order statistic) in variable-bit-rate (VBR) video traffic on ATM buffer dimensioning. Previous studies have shown that video traffic exhibits long-range dependence (LRD) (Hurst parameter large than 0.5). We investigate the practical implications of LRD in the context of realistic ATM traffic engineering by studying ATM multiplexers of VBR video sources over a range of desirable cell loss rates and buffer sizes (maximum delays). Using results based on large deviations theory, we introduce the notion of Critical Time Scale (CTS). For a given buffer size, link capacity, and the marginal distribution of frame size, the CTS of a VBR video source is defined as the number of frame correlations that contribute to the cell loss rate. In other words, second-order behavior at the time scale beyond the CTS does not significantly affect the network performance. We show that whether the video source model is Markov or has LRD, its CTS is finite, attains a small value for small buffer, and is a non-decreasing function of buffer size. Numerical results show that (i) even in the presence of LRD, long-term correlations do not have significant impact on the cell loss rate; and (ii) short-term correlations have dominant effect on cell loss rate, and therefore, well-designed Markov traffic models are effective for predicting Quality of Service (QOS) of LRD VBR video traffic. Therefore, we conclude that it is unnecessary to capture the long-term correlations of a real-time VBR video source under realistic ATM buffer dimensioning scenarios as far as the cell loss rates and maximum buffer delays are concerned.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",3–14,12,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2017,article,"Ryu, Bong K. and Elwalid, Anwar",The Importance of Long-Range Dependence of VBR Video Traffic in ATM Traffic Engineering: Myths and Realities,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248158,10.1145/248157.248158,"There has been a growing concern about the potential impact of long-term correlations (second-order statistic) in variable-bit-rate (VBR) video traffic on ATM buffer dimensioning. Previous studies have shown that video traffic exhibits long-range dependence (LRD) (Hurst parameter large than 0.5). We investigate the practical implications of LRD in the context of realistic ATM traffic engineering by studying ATM multiplexers of VBR video sources over a range of desirable cell loss rates and buffer sizes (maximum delays). Using results based on large deviations theory, we introduce the notion of Critical Time Scale (CTS). For a given buffer size, link capacity, and the marginal distribution of frame size, the CTS of a VBR video source is defined as the number of frame correlations that contribute to the cell loss rate. In other words, second-order behavior at the time scale beyond the CTS does not significantly affect the network performance. We show that whether the video source model is Markov or has LRD, its CTS is finite, attains a small value for small buffer, and is a non-decreasing function of buffer size. Numerical results show that (i) even in the presence of LRD, long-term correlations do not have significant impact on the cell loss rate; and (ii) short-term correlations have dominant effect on cell loss rate, and therefore, well-designed Markov traffic models are effective for predicting Quality of Service (QOS) of LRD VBR video traffic. Therefore, we conclude that it is unnecessary to capture the long-term correlations of a real-time VBR video source under realistic ATM buffer dimensioning scenarios as far as the cell loss rates and maximum buffer delays are concerned.",,3–14,12,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2018,inproceedings,"Grossglauser, M. and Bolot, J.-C.",On the Relevance of Long-Range Dependence in Network Traffic,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248159,10.1145/248156.248159,"There is mounting experimental evidence that network traffic processes exhibit ubiquitous properties of self-similarity and long range dependence (LRD), i.e. of correlations over a wide range of time scales. However, there is still considerable debate about how to model such processes and about their impact on network and application performance. In this paper, we argue that much recent modeling work has failed to consider the impact of two important parameters, namely the finite range of time scales of interest in performance evaluation and prediction problems, and the first-order statistics such as the marginal distribution of the process.We introduce and evaluate a model in which these parameters can be easily controlled. Specifically, our model is a modulated fluid traffic model in which the correlation function of the fluid rate is asymptotically second-order self-similar with given Hurst parameter, then drops to zero at a cutoff time lag. We develop a very efficient numerical procedure to evaluate the performance of the single server queue fed with the above fluid input process. We use this procedure to examine the fluid loss rate for a wide range of marginal distributions, Hurst parameters, cutoff lags, and buffer sizes.Our main results are as follows. First, we find that the amount of correlation that needs to be taken into account for performance evaluation depends not only on the correlation structure of the source traffic, but also on time scales specific to the system under study. For example, the time scale associated to a queueing system is a function of the maximum buffer size. Thus for finite buffer queues, we find that the impact on loss of the correlation in the arrival process becomes nil beyond a time scale we refer to as the correlation horizon. Second, we find that loss depends in a crucial way on the marginal distribution of the fluid rate process. Third, our results suggest that reducing loss by buffering is hard. We advocate the use of source traffic control and statistical multiplexing instead.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",15–24,10,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2019,article,"Grossglauser, M. and Bolot, J.-C.",On the Relevance of Long-Range Dependence in Network Traffic,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248159,10.1145/248157.248159,"There is mounting experimental evidence that network traffic processes exhibit ubiquitous properties of self-similarity and long range dependence (LRD), i.e. of correlations over a wide range of time scales. However, there is still considerable debate about how to model such processes and about their impact on network and application performance. In this paper, we argue that much recent modeling work has failed to consider the impact of two important parameters, namely the finite range of time scales of interest in performance evaluation and prediction problems, and the first-order statistics such as the marginal distribution of the process.We introduce and evaluate a model in which these parameters can be easily controlled. Specifically, our model is a modulated fluid traffic model in which the correlation function of the fluid rate is asymptotically second-order self-similar with given Hurst parameter, then drops to zero at a cutoff time lag. We develop a very efficient numerical procedure to evaluate the performance of the single server queue fed with the above fluid input process. We use this procedure to examine the fluid loss rate for a wide range of marginal distributions, Hurst parameters, cutoff lags, and buffer sizes.Our main results are as follows. First, we find that the amount of correlation that needs to be taken into account for performance evaluation depends not only on the correlation structure of the source traffic, but also on time scales specific to the system under study. For example, the time scale associated to a queueing system is a function of the maximum buffer size. Thus for finite buffer queues, we find that the impact on loss of the correlation in the arrival process becomes nil beyond a time scale we refer to as the correlation horizon. Second, we find that loss depends in a crucial way on the marginal distribution of the fluid rate process. Third, our results suggest that reducing loss by buffering is hard. We advocate the use of source traffic control and statistical multiplexing instead.",,15–24,10,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2020,inproceedings,"Paxson, Vern",End-to-End Routing Behavior in the Internet,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248160,10.1145/248156.248160,"The large-scale behavior of routing in the Internet has gone virtually without any formal study, the exception being Chinoy's analysis of the dynamics of Internet routing information [Ch93]. We report on an analysis of 40,000 end-to-end route measurements conducted using repeated ""traceroutes"" between 37 Internet sites. We analyze the routing behavior for pathological conditions, routing stability, and routing symmetry. For pathologies, we characterize the prevalence of routing loops, erroneous routing, infrastructure failures, and temporary outages. We find that the likelihood of encountering a major routing pathology more than doubled between the end of 1994 and the end of 1995, rising from 1.5% to 3.4%. For routing stability, we define two separate types of stability, ""prevalence"" meaning the overall likelihood that a particular route is encountered, and ""persistence,"" the likelihood that a route remains unchanged over a long period of time. We find that Internet paths are heavily dominated by a single prevalent route, but that the time periods over which routes persist show wide variation, ranging from seconds up to days. About 2/3's of the Internet paths had routes persisting for either days or weeks. For routing symmetry, we look at the likelihood that a path through the Internet visits at least one different city in the two directions. At the end of 1995, this was the case half the time, and at least one different autonomous system was visited 30% of the time.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",25–38,14,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2021,article,"Paxson, Vern",End-to-End Routing Behavior in the Internet,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248160,10.1145/248157.248160,"The large-scale behavior of routing in the Internet has gone virtually without any formal study, the exception being Chinoy's analysis of the dynamics of Internet routing information [Ch93]. We report on an analysis of 40,000 end-to-end route measurements conducted using repeated ""traceroutes"" between 37 Internet sites. We analyze the routing behavior for pathological conditions, routing stability, and routing symmetry. For pathologies, we characterize the prevalence of routing loops, erroneous routing, infrastructure failures, and temporary outages. We find that the likelihood of encountering a major routing pathology more than doubled between the end of 1994 and the end of 1995, rising from 1.5% to 3.4%. For routing stability, we define two separate types of stability, ""prevalence"" meaning the overall likelihood that a particular route is encountered, and ""persistence,"" the likelihood that a route remains unchanged over a long period of time. We find that Internet paths are heavily dominated by a single prevalent route, but that the time periods over which routes persist show wide variation, ranging from seconds up to days. About 2/3's of the Internet paths had routes persisting for either days or weeks. For routing symmetry, we look at the likelihood that a path through the Internet visits at least one different city in the two directions. At the end of 1995, this was the case half the time, and at least one different autonomous system was visited 30% of the time.",,25–38,14,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2022,inproceedings,"Wallach, Deborah A. and Engler, Dawson R. and Kaashoek, M. Frans",ASHs: Application-Specific Handlers for High-Performance Messaging,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248161,10.1145/248156.248161,"Application-specific safe message handlers (ASHs) are designed to provide applications with hardware-level network performance. ASHs are user-written code fragments that safely and efficiently execute in the kernel in response to message arrival. ASHs can direct message transfers (thereby eliminating copies) and send messages (thereby reducing send-response latency). In addition, the ASH system provides support for dynamic integrated layer processing (thereby eliminating duplicate message traversals) and dynamic protocol composition (thereby supporting modularity). ASHs provide this high degree of flexibility while still providing network performance as good as, or (if they exploit application-specific knowledge) even better than, hard-wired in-kernel implementations. A combination of user-level microbenchmarks and end-to-end system measurements using TCP demonstrate the benefits of the ASH system.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",40–52,13,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2023,article,"Wallach, Deborah A. and Engler, Dawson R. and Kaashoek, M. Frans",ASHs: Application-Specific Handlers for High-Performance Messaging,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248161,10.1145/248157.248161,"Application-specific safe message handlers (ASHs) are designed to provide applications with hardware-level network performance. ASHs are user-written code fragments that safely and efficiently execute in the kernel in response to message arrival. ASHs can direct message transfers (thereby eliminating copies) and send messages (thereby reducing send-response latency). In addition, the ASH system provides support for dynamic integrated layer processing (thereby eliminating duplicate message traversals) and dynamic protocol composition (thereby supporting modularity). ASHs provide this high degree of flexibility while still providing network performance as good as, or (if they exploit application-specific knowledge) even better than, hard-wired in-kernel implementations. A combination of user-level microbenchmarks and end-to-end system measurements using TCP demonstrate the benefits of the ASH system.",,40–52,13,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2024,inproceedings,"Engler, Dawson R. and Kaashoek, M. Frans","DPF: Fast, Flexible Message Demultiplexing Using Dynamic Code Generation",1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248162,10.1145/248156.248162,"Fast and flexible message demultiplexing are well-established goals in the networking community [1, 18, 22]. Currently, however, network architects have had to sacrifice one for the other. We present a new packet-filter system, DPF (Dynamic Packet Filters), that provides both the traditional flexibility of packet filters [18] and the speed of hand-crafted demultiplexing routines [3]. DPF filters run 10-50 times faster than the fastest packet filters reported in the literature [1, 17, 18, 27]. DPF's performance is either equivalent to or, when it can exploit runtime information, superior to hand-coded demultiplexors. DPF achieves high performance by using a carefully-designed declarative packet-filter language that is aggressively optimized using dynamic code generation. The contributions of this work are: (1) a detailed description of the DPF design, (2) discussion of the use of dynamic code generation and quantitative results on its performance impact, (3) quantitative results on how DPF is used in the Aegis kernel to export network devices safely and securely to user space so that UDP and TCP can be implemented efficiently as user-level libraries, and (4) the unrestricted release of DPF into the public domain.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",53–59,7,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2025,article,"Engler, Dawson R. and Kaashoek, M. Frans","DPF: Fast, Flexible Message Demultiplexing Using Dynamic Code Generation",1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248162,10.1145/248157.248162,"Fast and flexible message demultiplexing are well-established goals in the networking community [1, 18, 22]. Currently, however, network architects have had to sacrifice one for the other. We present a new packet-filter system, DPF (Dynamic Packet Filters), that provides both the traditional flexibility of packet filters [18] and the speed of hand-crafted demultiplexing routines [3]. DPF filters run 10-50 times faster than the fastest packet filters reported in the literature [1, 17, 18, 27]. DPF's performance is either equivalent to or, when it can exploit runtime information, superior to hand-coded demultiplexors. DPF achieves high performance by using a carefully-designed declarative packet-filter language that is aggressively optimized using dynamic code generation. The contributions of this work are: (1) a detailed description of the DPF design, (2) discussion of the use of dynamic code generation and quantitative results on its performance impact, (3) quantitative results on how DPF is used in the Aegis kernel to export network devices safely and securely to user space so that UDP and TCP can be implemented efficiently as user-level libraries, and (4) the unrestricted release of DPF into the public domain.",,53–59,7,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2026,inproceedings,"Dabbous, Walid and O'Malley, Sean and Castelluccia, Claude",Generating Efficient Protocol Code from an Abstract Specification,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248163,10.1145/248156.248163,"A protocol compiler takes as input an abstract specification of a protocol and generates an implementation of that protocol. Protocol compilers usually produce inefficient code both in terms of code speed and code size. In this paper, we show that the combination of two techniques makes it possible to build protocol compilers that generate efficient code. These techniques are i) the use of a compiler that generates from the specification a unique tree-shaped automaton (rather than multiple independent automata), and ii) the use of optimization techniques applied at the automaton level, i.e. on the branches of the trees.We have developed a protocol compiler that uses both these techniques. The compiler takes as input a protocol specification written in the synchronous language Esterel. The specification is compiled into a unique automaton by the Esterel front end compiler. The automaton is then optimized and converted into C code by our protocol optimizer called HIPPCO. HIPPCO improves code performance and reduces code size by simultaneously optimizing the performance of the common path and optimizing the size of the uncommon path. We evaluate the gain expected with our approach on a real-life example, namely a working subset of the TCP protocol generated from an Esterel specification. We compare the protocol code generated with our approach to that derived from the standard BSD TCP implementation. The results are very encouraging. HIPPCO-generated code executes up to 25 % fewer instructions than the BSD code for input packet processing while maintaining comparable code size.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",60–72,13,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2027,article,"Dabbous, Walid and O'Malley, Sean and Castelluccia, Claude",Generating Efficient Protocol Code from an Abstract Specification,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248163,10.1145/248157.248163,"A protocol compiler takes as input an abstract specification of a protocol and generates an implementation of that protocol. Protocol compilers usually produce inefficient code both in terms of code speed and code size. In this paper, we show that the combination of two techniques makes it possible to build protocol compilers that generate efficient code. These techniques are i) the use of a compiler that generates from the specification a unique tree-shaped automaton (rather than multiple independent automata), and ii) the use of optimization techniques applied at the automaton level, i.e. on the branches of the trees.We have developed a protocol compiler that uses both these techniques. The compiler takes as input a protocol specification written in the synchronous language Esterel. The specification is compiled into a unique automaton by the Esterel front end compiler. The automaton is then optimized and converted into C code by our protocol optimizer called HIPPCO. HIPPCO improves code performance and reduces code size by simultaneously optimizing the performance of the common path and optimizing the size of the uncommon path. We evaluate the gain expected with our approach on a real-life example, namely a working subset of the TCP protocol generated from an Esterel specification. We compare the protocol code generated with our approach to that derived from the standard BSD TCP implementation. The results are very encouraging. HIPPCO-generated code executes up to 25 % fewer instructions than the BSD code for input packet processing while maintaining comparable code size.",,60–72,13,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2028,inproceedings,"Mosberger, David and Peterson, Larry L. and Bridges, Patrick G. and O'Malley, Sean",Analysis of Techniques to Improve Protocol Processing Latency,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248164,10.1145/248156.248164,"This paper describes several techniques designed to improve protocol latency, and reports on their effectiveness when measured on a modern RISC machine employing the DEC Alpha processor. We found that the memory system---which has long been known to dominate network throughput---is also a key factor in protocol latency. As a result, improving instruction cache effectiveness can greatly reduce protocol processing overheads. An important metric in this context is the memory cycles per instructions (mCPI), which is the average number of cycles that an instruction stalls waiting for a memory access to complete. The techniques presented in this paper reduce the mCPI by a factor of 1.35 to 5.8. In analyzing the effectiveness of the techniques, we also present a detailed study of the protocol processing behavior of two protocol stacks---TCP/IP and RPC---on a modern RISC processor.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",73–84,12,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2029,article,"Mosberger, David and Peterson, Larry L. and Bridges, Patrick G. and O'Malley, Sean",Analysis of Techniques to Improve Protocol Processing Latency,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248164,10.1145/248157.248164,"This paper describes several techniques designed to improve protocol latency, and reports on their effectiveness when measured on a modern RISC machine employing the DEC Alpha processor. We found that the memory system---which has long been known to dominate network throughput---is also a key factor in protocol latency. As a result, improving instruction cache effectiveness can greatly reduce protocol processing overheads. An important metric in this context is the memory cycles per instructions (mCPI), which is the average number of cycles that an instruction stalls waiting for a memory access to complete. The techniques presented in this paper reduce the mCPI by a factor of 1.35 to 5.8. In analyzing the effectiveness of the techniques, we also present a detailed study of the protocol processing behavior of two protocol stacks---TCP/IP and RPC---on a modern RISC processor.",,73–84,12,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2030,inproceedings,"Blackwell, Trevor",Speeding up Protocols for Small Messages,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248165,10.1145/248156.248165,"Many techniques have been discovered to improve performance of bulk data transfer protocols which use large messages. This paper describes a technique that improves protocol performance for protocols that use small messages, such as signalling protocols, by reducing memory system penalties. Detailed measurements show that for TCP, most memory system costs are due to poor locality in the protocol code itself, rather than movement of data. We present a new technique, analogous to blocked matrix multiplication, for scheduling layer processing to reduce memory system costs, and analyze its performance in a synthetic environment.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",85–95,11,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2031,article,"Blackwell, Trevor",Speeding up Protocols for Small Messages,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248165,10.1145/248157.248165,"Many techniques have been discovered to improve performance of bulk data transfer protocols which use large messages. This paper describes a technique that improves protocol performance for protocols that use small messages, such as signalling protocols, by reducing memory system penalties. Detailed measurements show that for TCP, most memory system costs are due to poor locality in the protocol code itself, rather than movement of data. We present a new technique, analogous to blocked matrix multiplication, for scheduling layer processing to reduce memory system costs, and analyze its performance in a synthetic environment.",,85–95,11,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2032,inproceedings,"van Renesse, Robbert",Masking the Overhead of Protocol Layering,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248166,10.1145/248156.248166,"Protocol layering has been advocated as a way of dealing with the complexity of computer communication. It has also been criticized for its performance overhead. In this paper, we present some insights in the design of protocols, and how these insights can be used to mask the overhead of layering, in a way similar to client caching in a file system. With our techniques, we achieve an order of magnitude improvement in end-to-end message latency in the Horus communication framework. Over an ATM network, we are able to do a round-trip message exchange, of varying levels of semantics, in about 170 µseconds, using a protocol stack of four layers that were written in ML, a high-level functional language.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",96–104,9,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2033,article,"van Renesse, Robbert",Masking the Overhead of Protocol Layering,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248166,10.1145/248157.248166,"Protocol layering has been advocated as a way of dealing with the complexity of computer communication. It has also been criticized for its performance overhead. In this paper, we present some insights in the design of protocols, and how these insights can be used to mask the overhead of layering, in a way similar to client caching in a file system. With our techniques, we achieve an order of magnitude improvement in end-to-end message latency in the Horus communication framework. Over an ATM network, we are able to do a round-trip message exchange, of varying levels of semantics, in about 170 µseconds, using a protocol stack of four layers that were written in ML, a high-level functional language.",,96–104,9,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2034,inproceedings,"Georgiadis, L. and Gu\'{e",Efficient Support of Delay and Rate Guarantees in an Internet,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248167,10.1145/248156.248167,"In this paper, we investigate some issues related to the efficient provision of end-to-end delay guarantees in the context of the Guaranteed (G) Services framework [16]. First, we consider the impact of reshaping traffic within the network on the end-to-end delay, the end-to-end jitter, as well as per-hop buffer requirements. This leads us to examine a class of traffic disciplines that use reshaping at each hop, namely rate-controlled disciplines. In this case, it is known that it is advantageous to use the Earliest Deadline First (EDF) scheduling policy at the link scheduler [8]. For this service discipline, we determine the appropriate values of the parameters that have to be exported, as specified in [16]. Subsequently, with the help of an example, we illustrate how the G service traffic will typically underutilize the network, regardless of the scheduling policy used. We then define a Guaranteed Rate (GR) service, that is synergetic with the G service framework and makes use of this unutilized bandwidth to provide rate guarantees to flows. We outline some of the details of the GR service and explain how it can be supported in conjunction with the G service in an efficient manner.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",106–116,11,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2035,article,"Georgiadis, L. and Gu\'{e",Efficient Support of Delay and Rate Guarantees in an Internet,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248167,10.1145/248157.248167,"In this paper, we investigate some issues related to the efficient provision of end-to-end delay guarantees in the context of the Guaranteed (G) Services framework [16]. First, we consider the impact of reshaping traffic within the network on the end-to-end delay, the end-to-end jitter, as well as per-hop buffer requirements. This leads us to examine a class of traffic disciplines that use reshaping at each hop, namely rate-controlled disciplines. In this case, it is known that it is advantageous to use the Earliest Deadline First (EDF) scheduling policy at the link scheduler [8]. For this service discipline, we determine the appropriate values of the parameters that have to be exported, as specified in [16]. Subsequently, with the help of an example, we illustrate how the G service traffic will typically underutilize the network, regardless of the scheduling policy used. We then define a Guaranteed Rate (GR) service, that is synergetic with the G service framework and makes use of this unutilized bandwidth to provide rate guarantees to flows. We outline some of the details of the GR service and explain how it can be supported in conjunction with the G service in an efficient manner.",,106–116,11,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2036,inproceedings,"McCanne, Steven and Jacobson, Van and Vetterli, Martin",Receiver-Driven Layered Multicast,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248168,10.1145/248156.248168,"State of the art, real-time, rate-adaptive, multimedia applications adjust their transmission rate to match the available network capacity. Unfortunately, this source-based rate-adaptation performs poorly in a heterogeneous multicast environment because there is no single target rate --- the conflicting bandwidth requirements of all receivers cannot be simultaneously satisfied with one transmission rate. If the burden of rate-adaption is moved from the source to the receivers, heterogeneity is accommodated. One approach to receiver-driven adaptation is to combine a layered source coding algorithm with a layered transmission system. By selectively forwarding subsets of layers at constrained network links, each user receives the best quality signal that the network can deliver. We and others have proposed that selective-forwarding be carried out using multiple IP-Multicast groups where each receiver specifies its level of subscription by joining a subset of the groups. In this paper, we extend the multiple group framework with a rate-adaptation protocol called Receiver-driven Layered Multicast, or RLM. Under RLM, multicast receivers adapt to both the static heterogeneity of link bandwidths as well as dynamic variations in network capacity (i.e., congestion). We describe the RLM protocol and evaluate its performance with a preliminary simulation study that characterizes user-perceived quality by assessing loss rates over multiple time scales. For the configurations we simulated, RLM results in good throughput with transient short-term loss rates on the order of a few percent and long-term loss rates on the order of one percent. Finally, we discuss our implementation of a software-based Internet video codec and its integration with RLM.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",117–130,14,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2037,article,"McCanne, Steven and Jacobson, Van and Vetterli, Martin",Receiver-Driven Layered Multicast,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248168,10.1145/248157.248168,"State of the art, real-time, rate-adaptive, multimedia applications adjust their transmission rate to match the available network capacity. Unfortunately, this source-based rate-adaptation performs poorly in a heterogeneous multicast environment because there is no single target rate --- the conflicting bandwidth requirements of all receivers cannot be simultaneously satisfied with one transmission rate. If the burden of rate-adaption is moved from the source to the receivers, heterogeneity is accommodated. One approach to receiver-driven adaptation is to combine a layered source coding algorithm with a layered transmission system. By selectively forwarding subsets of layers at constrained network links, each user receives the best quality signal that the network can deliver. We and others have proposed that selective-forwarding be carried out using multiple IP-Multicast groups where each receiver specifies its level of subscription by joining a subset of the groups. In this paper, we extend the multiple group framework with a rate-adaptation protocol called Receiver-driven Layered Multicast, or RLM. Under RLM, multicast receivers adapt to both the static heterogeneity of link bandwidths as well as dynamic variations in network capacity (i.e., congestion). We describe the RLM protocol and evaluate its performance with a preliminary simulation study that characterizes user-perceived quality by assessing loss rates over multiple time scales. For the configurations we simulated, RLM results in good throughput with transient short-term loss rates on the order of a few percent and long-term loss rates on the order of one percent. Finally, we discuss our implementation of a software-based Internet video codec and its integration with RLM.",,117–130,14,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2038,inproceedings,"Adiseshu, Hari and Parulkar, Guru and Varghese, George",A Reliable and Scalable Striping Protocol,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248169,10.1145/248156.248169,"Link striping algorithms are often used to overcome transmission bottlenecks in computer networks. Traditional striping algorithms suffer from two major disadvantages. They provide inadequate load sharing in the presence of variable length packets, and may result in non-FIFO delivery of data. We describe a new family of link striping algorithms that solves both problems. Our scheme applies to any layer that can provide multiple FIFO channels.We deal with variable sized packets by showing how fair queuing algorithms can be transformed into load sharing algorithms. Our transformation results in practical load sharing protocols, and shows a theoretical connection between two seemingly different problems. The same transformation can be applied to obtain load sharing protocols for links with different capacities. We deal with the FIFO requirement for two separate cases. If a sequence number can be added to each packet, we show how to speed up packet processing by letting the receiver simulate the sender algorithm. If no header can be added, we show how to provide quasi-FIFO delivery. Quasi-FIFO is FIFO except during occasional periods of loss of synchronization. We argue that quasi-FIFO is adequate for most applications. We also describe a simple technique for speedy restoration of synchronization in the event of loss.We develop an architectural framework for transparently embedding our protocol at the network level by striping IP packets across multiple physical interfaces. The resulting strIPe protocol has been implemented within the NetBSD kernel. Our measurements and simulations show that the protocol offers scalable throughput even when striping is done over dissimilar links, and that the protocol synchronizes quickly after packet loss. Measurements show performance improvements over conventional round robin striping schemes and striping schemes that do not resequence packets.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",131–141,11,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2039,article,"Adiseshu, Hari and Parulkar, Guru and Varghese, George",A Reliable and Scalable Striping Protocol,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248169,10.1145/248157.248169,"Link striping algorithms are often used to overcome transmission bottlenecks in computer networks. Traditional striping algorithms suffer from two major disadvantages. They provide inadequate load sharing in the presence of variable length packets, and may result in non-FIFO delivery of data. We describe a new family of link striping algorithms that solves both problems. Our scheme applies to any layer that can provide multiple FIFO channels.We deal with variable sized packets by showing how fair queuing algorithms can be transformed into load sharing algorithms. Our transformation results in practical load sharing protocols, and shows a theoretical connection between two seemingly different problems. The same transformation can be applied to obtain load sharing protocols for links with different capacities. We deal with the FIFO requirement for two separate cases. If a sequence number can be added to each packet, we show how to speed up packet processing by letting the receiver simulate the sender algorithm. If no header can be added, we show how to provide quasi-FIFO delivery. Quasi-FIFO is FIFO except during occasional periods of loss of synchronization. We argue that quasi-FIFO is adequate for most applications. We also describe a simple technique for speedy restoration of synchronization in the event of loss.We develop an architectural framework for transparently embedding our protocol at the network level by striping IP packets across multiple physical interfaces. The resulting strIPe protocol has been implemented within the NetBSD kernel. Our measurements and simulations show that the protocol offers scalable throughput even when striping is done over dissimilar links, and that the protocol synchronizes quickly after packet loss. Measurements show performance improvements over conventional round robin striping schemes and striping schemes that do not resequence packets.",,131–141,11,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2040,inproceedings,"Bennett, Jon C. R. and Zhang, Hui",Hierarchical Packet Fair Queueing Algorithms,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248170,10.1145/248156.248170,"Hierarchical Packet Fair Queueing (H-PFQ) algorithms have the potential to simultaneously support guaranteed real-time service, rate-adaptive best-effort, and controlled link-sharing service. In this paper, we design practical H-PFQ algorithms by using one-level Packet Fair Queueing (PFQ) servers as basic building blocks, and develop techniques to analyze delay and fairness properties of the resulted H-PFQ servers. We demonstrate that, in order to provide tight delay bounds in a H-PFQ server, it is essential for the one-level PFQ servers to have small Worst-case Fair Indices (WFI). We propose a new one-level PFQ algorithm called WF2Q+ that is the first to have all the following three properties: (a) providing the tightest delay bound among all PFQ algorithms; (b) having the smallest WFI among all PFQ algorithms; and (c) having a relatively low implementation complexity of O(log N). We show that practical H-PFQ algorithms can be implemented by using WF2Q+ as the basic building block and prove that the resulting H-WF2Q+ algorithms provide similar delay bounds and bandwidth distribution as those provided by a H-GPS server. Simulation experiments are presented to evaluate the proposed algorithm.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",143–156,14,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2041,article,"Bennett, Jon C. R. and Zhang, Hui",Hierarchical Packet Fair Queueing Algorithms,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248170,10.1145/248157.248170,"Hierarchical Packet Fair Queueing (H-PFQ) algorithms have the potential to simultaneously support guaranteed real-time service, rate-adaptive best-effort, and controlled link-sharing service. In this paper, we design practical H-PFQ algorithms by using one-level Packet Fair Queueing (PFQ) servers as basic building blocks, and develop techniques to analyze delay and fairness properties of the resulted H-PFQ servers. We demonstrate that, in order to provide tight delay bounds in a H-PFQ server, it is essential for the one-level PFQ servers to have small Worst-case Fair Indices (WFI). We propose a new one-level PFQ algorithm called WF2Q+ that is the first to have all the following three properties: (a) providing the tightest delay bound among all PFQ algorithms; (b) having the smallest WFI among all PFQ algorithms; and (c) having a relatively low implementation complexity of O(log N). We show that practical H-PFQ algorithms can be implemented by using WF2Q+ as the basic building block and prove that the resulting H-WF2Q+ algorithms provide similar delay bounds and bandwidth distribution as those provided by a H-GPS server. Simulation experiments are presented to evaluate the proposed algorithm.",,143–156,14,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2042,inproceedings,"Goyal, Pawan and Vin, Harrick M. and Chen, Haichen",Start-Time Fair Queueing: A Scheduling Algorithm for Integrated Services Packet Switching Networks,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248171,10.1145/248156.248171,"We present Start-time Fair Queuing (SFQ) algorithm that is computationally efficient, achieves fairness regardless of variation in a server capacity, and has the smallest fairness measure among all known fair scheduling algorithms. We analyze its throughput, single server delay, and end-to-end delay guarantee for variable rate Fluctuation Constrained (FC) and Exponentially Bounded Fluctuation (EBF) servers. We show that SFQ is better suited than Weighted Fair Queuing for integrated services networks and it is strictly better than Self Clocked Fair Queuing. To support heterogeneous services and multiple protocol families in integrated services networks, we present a hierarchical SFQ scheduler and derive its performance bounds. Our analysis demonstrates that SFQ is suitable for integrated services networks since it: (1) achieves low average as well as maximum delay for low-throughput applications (e.g., interactive audio, telnet, etc.); (2) provides fairness which is desirable for VBR video; (3) provides fairness, regardless of variation in a server capacity, for throughput-intensive, flow-controlled data applications; (4) enables hierarchical link sharing which is desirable for managing heterogeneity; and (5) is computationally efficient.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",157–168,12,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2043,article,"Goyal, Pawan and Vin, Harrick M. and Chen, Haichen",Start-Time Fair Queueing: A Scheduling Algorithm for Integrated Services Packet Switching Networks,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248171,10.1145/248157.248171,"We present Start-time Fair Queuing (SFQ) algorithm that is computationally efficient, achieves fairness regardless of variation in a server capacity, and has the smallest fairness measure among all known fair scheduling algorithms. We analyze its throughput, single server delay, and end-to-end delay guarantee for variable rate Fluctuation Constrained (FC) and Exponentially Bounded Fluctuation (EBF) servers. We show that SFQ is better suited than Weighted Fair Queuing for integrated services networks and it is strictly better than Self Clocked Fair Queuing. To support heterogeneous services and multiple protocol families in integrated services networks, we present a hierarchical SFQ scheduler and derive its performance bounds. Our analysis demonstrates that SFQ is suitable for integrated services networks since it: (1) achieves low average as well as maximum delay for low-throughput applications (e.g., interactive audio, telnet, etc.); (2) provides fairness which is desirable for VBR video; (3) provides fairness, regardless of variation in a server capacity, for throughput-intensive, flow-controlled data applications; (4) enables hierarchical link sharing which is desirable for managing heterogeneity; and (5) is computationally efficient.",,157–168,12,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2044,inproceedings,"Afek, Yehuda and Mansour, Yishay and Ostfeld, Zvi",Phantom: A Simple and Effective Flow Control Scheme,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248172,10.1145/248156.248172,"This paper presents Phantom, a simple constant space algorithm for rate based flow control. As shown by our simulations, it converges fast to a fair rate allocation while generating a moderate queue length. While our approach can be easily implemented in ATM switches for managing ABR traffic, it is also suitable for flow control in TCP router based networks. Both the introduced overhead and the required modifications in TCP flow control systems are minimal. The implementation of this approach in TCP guarantees fairness and provides a unifying interconnection between TCP routers and ATM networks. The new algorithm easily inter-operates with current TCP flow control mechanisms and thus can be gradually introduced into installed based TCP networks.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",169–182,14,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2045,article,"Afek, Yehuda and Mansour, Yishay and Ostfeld, Zvi",Phantom: A Simple and Effective Flow Control Scheme,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248172,10.1145/248157.248172,"This paper presents Phantom, a simple constant space algorithm for rate based flow control. As shown by our simulations, it converges fast to a fair rate allocation while generating a moderate queue length. While our approach can be easily implemented in ATM switches for managing ABR traffic, it is also suitable for flow control in TCP router based networks. Both the introduced overhead and the required modifications in TCP flow control systems are minimal. The implementation of this approach in TCP guarantees fairness and provides a unifying interconnection between TCP routers and ATM networks. The new algorithm easily inter-operates with current TCP flow control mechanisms and thus can be gradually introduced into installed based TCP networks.",,169–182,14,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2046,inproceedings,"Gerla, Mario and Palnati, Prasasth and Walton, Simon","Multicasting Protocols for High-Speed, Wormhole-Routing Local Area Networks",1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248173,10.1145/248156.248173,"Wormhole routing LANs are emerging as an effective solution for high-bandwidth, low-latency interconnects in distributed computing and cluster computing applications. An important example is the 640 Mb/s crossbar-based Myrinet. A key property of conventional LANs, which is valuable for many distributed applications, is transparent, reliable network-level multicast. It is desirable to retain this property also in wormhole LANs. Unfortunately, efficient, reliable multicasting in wormhole LANs is problematic because of the potential for deadlocks. As a consequence, current multicasting implementations typically consist of repeated unicast or assume a priori buffer reservations. These solutions, however, tend to increase latency and do not scale well.In this paper we address the problem of providing transparent, reliable, efficient network level multicasting in the wormhole LAN. We describe several protocols for achieving deadlock-free, reliable multicasting using restricted routing and fast buffer reservation techniques. Tradeoffs involving complexity and performance of various solutions are discussed, and are illustrated using simulation. A simple multicast implementation for Myrinet has been carried out, and experimental results are presented.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",184–193,10,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2047,article,"Gerla, Mario and Palnati, Prasasth and Walton, Simon","Multicasting Protocols for High-Speed, Wormhole-Routing Local Area Networks",1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248173,10.1145/248157.248173,"Wormhole routing LANs are emerging as an effective solution for high-bandwidth, low-latency interconnects in distributed computing and cluster computing applications. An important example is the 640 Mb/s crossbar-based Myrinet. A key property of conventional LANs, which is valuable for many distributed applications, is transparent, reliable network-level multicast. It is desirable to retain this property also in wormhole LANs. Unfortunately, efficient, reliable multicasting in wormhole LANs is problematic because of the potential for deadlocks. As a consequence, current multicasting implementations typically consist of repeated unicast or assume a priori buffer reservations. These solutions, however, tend to increase latency and do not scale well.In this paper we address the problem of providing transparent, reliable, efficient network level multicasting in the wormhole LAN. We describe several protocols for achieving deadlock-free, reliable multicasting using restricted routing and fast buffer reservation techniques. Tradeoffs involving complexity and performance of various solutions are discussed, and are illustrated using simulation. A simple multicast implementation for Myrinet has been carried out, and experimental results are presented.",,184–193,10,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2048,inproceedings,"Banerjea, Anindo",Simulation Study of the Capacity Effects of Dispersity Routing for Fault Tolerant Realtime Channels,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248174,10.1145/248156.248174,"The paper presents a simulation study of the use of dispersity routing to provide fault tolerance on top of a connection oriented realtime service such as that provided by the Tenet scheme. A framework to study the dispersity schemes is presented. The simulations show that the dispersity schemes, by dividing the connection's traffic among multiple paths in the network, have a beneficent effect on the capacity of the network. Thus, for certain classes of dispersity schemes, we obtain a small improvement in fault tolerance as well as an improvement in the number of connections that the network can support. For other classes of dispersity schemes, greater improvement in service may be purchased at the cost of decrease in capacity. The paper explores the tradeoffs available through exhaustive simulations. We conclude that dispersity routing is a flexible approach to increasing the fault tolerance of realtime connections, which can provide a range of improvements in service with a corresponding range of costs.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",194–205,12,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2049,article,"Banerjea, Anindo",Simulation Study of the Capacity Effects of Dispersity Routing for Fault Tolerant Realtime Channels,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248174,10.1145/248157.248174,"The paper presents a simulation study of the use of dispersity routing to provide fault tolerance on top of a connection oriented realtime service such as that provided by the Tenet scheme. A framework to study the dispersity schemes is presented. The simulations show that the dispersity schemes, by dividing the connection's traffic among multiple paths in the network, have a beneficent effect on the capacity of the network. Thus, for certain classes of dispersity schemes, we obtain a small improvement in fault tolerance as well as an improvement in the number of connections that the network can support. For other classes of dispersity schemes, greater improvement in service may be purchased at the cost of decrease in capacity. The paper explores the tradeoffs available through exhaustive simulations. We conclude that dispersity routing is a flexible approach to increasing the fault tolerance of realtime connections, which can provide a range of improvements in service with a corresponding range of costs.",,194–205,12,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2050,inproceedings,"Ma, Qingming and Steenkiste, Peter and Zhang, Hui",Routing High-Bandwidth Traffic in Max-Min Fair Share Networks,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248175,10.1145/248156.248175,"We study how to improve the throughput of high-bandwidth traffic such as large file transfers in a network where resources are fairly shared among connections. While it is possible to devise priority or reservation-based schemes that give high-bandwidth traffic preferential treatment at the expense of other connections, we focus on the use of routing algorithms that improve resource allocation while maintaining max-min fair share semantics. In our approach, routing is closely coupled with congestion control in the sense that congestion information, such as the rates allocated to existing connections, is used by the routing algorithm. To reduce the amount of routing information that must be distributed, an abstraction of the congestion information is introduced. Using an extensive set of simulation, we identify a link-cost or cost metric for ""shortest-path"" routing that performs uniformly better than the minimal-hop routing and shortest-widest path routing algorithms. To further improve throughput without reducing the fair share of single-path connections, we propose a novel prioritized multi-path routing algorithm in which low priority paths share the bandwidth left unused by higher priority paths. This leads to a conservative extension of max-min fairness called prioritized multi-level max-min fairness. Simulation results confirm the advantages of our multi-path routing algorithm.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",206–217,12,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2051,article,"Ma, Qingming and Steenkiste, Peter and Zhang, Hui",Routing High-Bandwidth Traffic in Max-Min Fair Share Networks,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248175,10.1145/248157.248175,"We study how to improve the throughput of high-bandwidth traffic such as large file transfers in a network where resources are fairly shared among connections. While it is possible to devise priority or reservation-based schemes that give high-bandwidth traffic preferential treatment at the expense of other connections, we focus on the use of routing algorithms that improve resource allocation while maintaining max-min fair share semantics. In our approach, routing is closely coupled with congestion control in the sense that congestion information, such as the rates allocated to existing connections, is used by the routing algorithm. To reduce the amount of routing information that must be distributed, an abstraction of the congestion information is introduced. Using an extensive set of simulation, we identify a link-cost or cost metric for ""shortest-path"" routing that performs uniformly better than the minimal-hop routing and shortest-widest path routing algorithms. To further improve throughput without reducing the fair share of single-path connections, we propose a novel prioritized multi-path routing algorithm in which low priority paths share the bandwidth left unused by higher priority paths. This leads to a conservative extension of max-min fairness called prioritized multi-level max-min fairness. Simulation results confirm the advantages of our multi-path routing algorithm.",,206–217,12,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2052,inproceedings,"Shepard, Timothy J.",A Channel Access Scheme for Large Dense Packet Radio Networks,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248176,10.1145/248156.248176,"Prior work in the field of packet radio networks has often assumed a simple success-if-exclusive model of successful reception. This simple model is insufficient to model interference in large dense packet radio networks accurately. In this paper we present a model that more closely approximates communication theory and the underlying physics of radio communication. Using this model we present a decentralized channel access scheme for scalable packet radio networks that is free of packet loss due to collisions and that at each hop requires no per-packet transmissions other than the single transmission used to convey the packet to the next-hop station. We also show that with a modest fraction of the radio spectrum, pessimistic assumptions about propagation resulting in maximum-possible self-interference, and an optimistic view of future signal processing capabilities that a self-organizing packet radio network may scale to millions of stations within a metro area with raw per-station rates in the hundreds of megabits per second.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",219–230,12,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2053,article,"Shepard, Timothy J.",A Channel Access Scheme for Large Dense Packet Radio Networks,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248176,10.1145/248157.248176,"Prior work in the field of packet radio networks has often assumed a simple success-if-exclusive model of successful reception. This simple model is insufficient to model interference in large dense packet radio networks accurately. In this paper we present a model that more closely approximates communication theory and the underlying physics of radio communication. Using this model we present a decentralized channel access scheme for scalable packet radio networks that is free of packet loss due to collisions and that at each hop requires no per-packet transmissions other than the single transmission used to convey the packet to the next-hop station. We also show that with a modest fraction of the radio spectrum, pessimistic assumptions about propagation resulting in maximum-possible self-interference, and an optimistic view of future signal processing capabilities that a self-organizing packet radio network may scale to millions of stations within a metro area with raw per-station rates in the hundreds of megabits per second.",,219–230,12,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2054,inproceedings,"Lu, Songwu and Bharghavan, Vaduvur",Adaptive Resource Management Algorithms for Indoor Mobile Computing Environments,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248177,10.1145/248156.248177,"Emerging indoor mobile computing environments seek to provide a user with an advanced set of communication-intensive applications, which require sustained quality of service in the presence of wireless channel error, user mobility, and scarce available resources. In this paper, we investigate two related approaches for the management of critical networking resources in indoor mobile computing environments:• adaptively re-adjusting the quality of service within pre-negotiated bounds in order to accommodate network dynamics and user mobility.• classifying cells based on location and handoff profiles, and designing advance resource reservation algorithms specific to individual cell characteristics.Preliminary simulation results are presented in order to validate the approaches for algorithmic design. A combination of the above approaches provide the framework for resource management in an ongoing indoor mobile computing environment project at the University of Illinois.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",231–242,12,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2055,article,"Lu, Songwu and Bharghavan, Vaduvur",Adaptive Resource Management Algorithms for Indoor Mobile Computing Environments,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248177,10.1145/248157.248177,"Emerging indoor mobile computing environments seek to provide a user with an advanced set of communication-intensive applications, which require sustained quality of service in the presence of wireless channel error, user mobility, and scarce available resources. In this paper, we investigate two related approaches for the management of critical networking resources in indoor mobile computing environments:• adaptively re-adjusting the quality of service within pre-negotiated bounds in order to accommodate network dynamics and user mobility.• classifying cells based on location and handoff profiles, and designing advance resource reservation algorithms specific to individual cell characteristics.Preliminary simulation results are presented in order to validate the approaches for algorithmic design. A combination of the above approaches provide the framework for resource management in an ongoing indoor mobile computing environment project at the University of Illinois.",,231–242,12,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2056,inproceedings,"Eckhardt, David and Steenkiste, Peter",Measurement and Analysis of the Error Characteristics of an In-Building Wireless Network,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248178,10.1145/248156.248178,"There is general belief that networks based on wireless technologies have much higher error rates than those based on more traditional technologies such as optical fiber, coaxial cable, or twisted pair wiring. This difference has motivated research on new protocol suites specifically for wireless networks. While the error characteristics of wired networks have been well documented, less experimental data is available for wireless LANs.In this paper we report the results of a study characterizing the error environment provided by AT&T WaveLAN, a commercial product designed for constructing 2 Mb/s in-building wireless networks. We evaluated the effects of interfering radiation sources, and of attenuation due to distance and obstacles, on the packet loss rate and bit error rate. We found that under many conditions the error rate of this physical layer is comparable to that of wired links. We analyze the implications of our results on today's CSMA/CA based wireless LANs and on future pico-cellular shared-medium reservation-based wireless networks.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",243–254,12,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2057,article,"Eckhardt, David and Steenkiste, Peter",Measurement and Analysis of the Error Characteristics of an In-Building Wireless Network,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248178,10.1145/248157.248178,"There is general belief that networks based on wireless technologies have much higher error rates than those based on more traditional technologies such as optical fiber, coaxial cable, or twisted pair wiring. This difference has motivated research on new protocol suites specifically for wireless networks. While the error characteristics of wired networks have been well documented, less experimental data is available for wireless LANs.In this paper we report the results of a study characterizing the error environment provided by AT&T WaveLAN, a commercial product designed for constructing 2 Mb/s in-building wireless networks. We evaluated the effects of interfering radiation sources, and of attenuation due to distance and obstacles, on the packet loss rate and bit error rate. We found that under many conditions the error rate of this physical layer is comparable to that of wired links. We analyze the implications of our results on today's CSMA/CA based wireless LANs and on future pico-cellular shared-medium reservation-based wireless networks.",,243–254,12,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2058,inproceedings,"Balakrishnan, Hari and Padmanabhan, Venkata N. and Seshan, Srinivasan and Katz, Randy H.",A Comparison of Mechanisms for Improving TCP Performance over Wireless Links,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248179,10.1145/248156.248179,"Reliable transport protocols such as TCP are tuned to perform well in traditional networks where packet losses occur mostly because of congestion. However, networks with wireless and other lossy links also suffer from significant non-congestion-related losses due to reasons such as bit errors and handoffs. TCP responds to all losses by invoking congestion control and avoidance algorithms, resulting in degraded end-to-end performance in wireless and lossy systems. In this paper, we compare several schemes designed to improve the performance of TCP in such networks. These schemes are classified into three broad categories: end-to-end protocols, where the sender is aware of the wireless link; link-layer protocols, that provide local reliability; and split-connection protocols, that break the end-to-end connection into two parts at the base station. We present the results of several experiments performed in both LAN and WAN environments, using throughput and goodput as the metrics for comparison.Our results show that a reliable link-layer protocol with some knowledge of TCP provides very good performance. Furthermore, it is possible to achieve good performance without splitting the end-to-end connection at the base station. We also demonstrate that selective acknowledgments and explicit loss notifications result in significant performance improvements.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",256–269,14,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2059,article,"Balakrishnan, Hari and Padmanabhan, Venkata N. and Seshan, Srinivasan and Katz, Randy H.",A Comparison of Mechanisms for Improving TCP Performance over Wireless Links,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248179,10.1145/248157.248179,"Reliable transport protocols such as TCP are tuned to perform well in traditional networks where packet losses occur mostly because of congestion. However, networks with wireless and other lossy links also suffer from significant non-congestion-related losses due to reasons such as bit errors and handoffs. TCP responds to all losses by invoking congestion control and avoidance algorithms, resulting in degraded end-to-end performance in wireless and lossy systems. In this paper, we compare several schemes designed to improve the performance of TCP in such networks. These schemes are classified into three broad categories: end-to-end protocols, where the sender is aware of the wireless link; link-layer protocols, that provide local reliability; and split-connection protocols, that break the end-to-end connection into two parts at the base station. We present the results of several experiments performed in both LAN and WAN environments, using throughput and goodput as the metrics for comparison.Our results show that a reliable link-layer protocol with some knowledge of TCP provides very good performance. Furthermore, it is possible to achieve good performance without splitting the end-to-end connection at the base station. We also demonstrate that selective acknowledgments and explicit loss notifications result in significant performance improvements.",,256–269,14,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2060,inproceedings,"Hoe, Janey C.",Improving the Start-up Behavior of a Congestion Control Scheme for TCP,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248180,10.1145/248156.248180,"Based on experiments conducted in a network simulator and over real networks, this paper proposes changes to the congestion control scheme in current TCP implementations to improve its behavior during the start-up period of a TCP connection.The scheme, which includes Slow-start, Fast Retransmit, and Fast Recovery algorithms, uses acknowledgments from a receiver to dynamically calculate reasonable operating values for a sender's TCP parameters governing when and how much a sender can pump into the network. During the start-up period, because a TCP sender starts with default parameters, it often ends up sending too many packets and too fast, leading to multiple losses of packets from the same window. This paper shows that recovery from losses during this start-up period is often unnecessarily time-consuming.In particular, using the current Fast Retransmit algorithm, when multiple packets in the same window are lost, only one of the packet losses may be recovered by each Fast Retransmit; the rest are often recovered by Slow-start after a usually lengthy retransmission timeout. Thus, this paper proposes changes to the Fast Retransmit algorithm so that it can quickly recover from multiple packet losses without waiting unnecessarily for the timeout. These changes, tested in the simulator and on the real networks, show significant performance improvements, especially for short TCP transfers. The paper also proposes other changes to help minimize the number of packets lost during the start-up period.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",270–280,11,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2061,article,"Hoe, Janey C.",Improving the Start-up Behavior of a Congestion Control Scheme for TCP,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248180,10.1145/248157.248180,"Based on experiments conducted in a network simulator and over real networks, this paper proposes changes to the congestion control scheme in current TCP implementations to improve its behavior during the start-up period of a TCP connection.The scheme, which includes Slow-start, Fast Retransmit, and Fast Recovery algorithms, uses acknowledgments from a receiver to dynamically calculate reasonable operating values for a sender's TCP parameters governing when and how much a sender can pump into the network. During the start-up period, because a TCP sender starts with default parameters, it often ends up sending too many packets and too fast, leading to multiple losses of packets from the same window. This paper shows that recovery from losses during this start-up period is often unnecessarily time-consuming.In particular, using the current Fast Retransmit algorithm, when multiple packets in the same window are lost, only one of the packet losses may be recovered by each Fast Retransmit; the rest are often recovered by Slow-start after a usually lengthy retransmission timeout. Thus, this paper proposes changes to the Fast Retransmit algorithm so that it can quickly recover from multiple packet losses without waiting unnecessarily for the timeout. These changes, tested in the simulator and on the real networks, show significant performance improvements, especially for short TCP transfers. The paper also proposes other changes to help minimize the number of packets lost during the start-up period.",,270–280,11,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2062,inproceedings,"Mathis, Matthew and Mahdavi, Jamshid",Forward Acknowledgement: Refining TCP Congestion Control,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248181,10.1145/248156.248181,"We have developed a Forward Acknowledgment (FACK) congestion control algorithm which addresses many of the performance problems recently observed in the Internet. The FACK algorithm is based on first principles of congestion control and is designed to be used with the proposed TCP SACK option. By decoupling congestion control from other algorithms such as data recovery, it attains more precise control over the data flow in the network. We introduce two additional algorithms to improve the behavior in specific situations. Through simulations we compare FACK to both Reno and Reno with SACK. Finally, we consider the potential performance and impact of FACK in the Internet.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",281–291,11,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2063,article,"Mathis, Matthew and Mahdavi, Jamshid",Forward Acknowledgement: Refining TCP Congestion Control,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248181,10.1145/248157.248181,"We have developed a Forward Acknowledgment (FACK) congestion control algorithm which addresses many of the performance problems recently observed in the Internet. The FACK algorithm is based on first principles of congestion control and is designed to be used with the proposed TCP SACK option. By decoupling congestion control from other algorithms such as data recovery, it attains more precise control over the data flow in the network. We introduce two additional algorithms to improve the behavior in specific situations. Through simulations we compare FACK to both Reno and Reno with SACK. Finally, we consider the potential performance and impact of FACK in the Internet.",,281–291,11,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2064,inproceedings,"Abrams, Marc and Standridge, Charles R. and Abdulla, Ghaleb and Fox, Edward A. and Williams, Stephen",Removal Policies in Network Caches for World-Wide Web Documents,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248182,10.1145/248156.248182,"World-Wide Web proxy servers that cache documents can potentially reduce three quantities: the number of requests that reach popular servers, the volume of network traffic resulting from document requests, and the latency that an end-user experiences in retrieving a document. This paper examines the first two using the measures of cache hit rate and weighted hit rate (or fraction of client-requested bytes returned by the proxy). A client request for an uncached document may cause the removal of one or more cached documents. Variable document sizes and types allow a rich variety of policies to select a document for removal, in contrast to policies for CPU caches or demand paging, that manage homogeneous objects. We present a taxonomy of removal policies. Through trace-driven simulation, we determine the maximum possible hit rate and weighted hit rate that a cache could ever achieve, and the removal policy that maximizes hit rate and weighted hit rate. The experiments use five traces of 37 to 185 days of client URL requests. Surprisingly, the criteria used by several proxy-server removal policies (LRU, Hyper-G, and a proposal by Pitkow and Recker) are among the worst performing criteria in our simulation; instead, replacing documents based on size maximizes hit rate in each of the studied workloads.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",293–305,13,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2065,article,"Abrams, Marc and Standridge, Charles R. and Abdulla, Ghaleb and Fox, Edward A. and Williams, Stephen",Removal Policies in Network Caches for World-Wide Web Documents,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248182,10.1145/248157.248182,"World-Wide Web proxy servers that cache documents can potentially reduce three quantities: the number of requests that reach popular servers, the volume of network traffic resulting from document requests, and the latency that an end-user experiences in retrieving a document. This paper examines the first two using the measures of cache hit rate and weighted hit rate (or fraction of client-requested bytes returned by the proxy). A client request for an uncached document may cause the removal of one or more cached documents. Variable document sizes and types allow a rich variety of policies to select a document for removal, in contrast to policies for CPU caches or demand paging, that manage homogeneous objects. We present a taxonomy of removal policies. Through trace-driven simulation, we determine the maximum possible hit rate and weighted hit rate that a cache could ever achieve, and the removal policy that maximizes hit rate and weighted hit rate. The experiments use five traces of 37 to 185 days of client URL requests. Surprisingly, the criteria used by several proxy-server removal policies (LRU, Hyper-G, and a proposal by Pitkow and Recker) are among the worst performing criteria in our simulation; instead, replacing documents based on size maximizes hit rate in each of the studied workloads.",,293–305,13,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2066,inproceedings,"Gokhale, Aniruddha and Schmidt, Douglas C.",Measuring the Performance of Communication Middleware on High-Speed Networks,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248183,10.1145/248156.248183,"Conventional implementations of communication middleware (such as CORBA and traditional RPC toolkits) incur considerable over-head when used for performance-sensitive applications over high-speed networks. As gigabit networks become pervasive, inefficient middleware will force programmers to use lower-level mechanisms to achieve the necessary transfer rates. This is a serious problem for mission/life-critical applications (such as satellite surveillance and medical imaging).This paper compares the performance of several widely used communication middleware mechanisms on a high-speed ATM network. The middleware ranged from lower-level mechanisms (such as socket-based C interfaces and C++ wrappers for sockets) to higher-level mechanisms (such as RPC, hand-optimized RPC and two implementations of CORBA - Orbix and ORBeline). These measurements reveal that the lower-level C and C++ implementations outperform the CORBA implementations significantly (the best CORBA throughput for remote transfer was roughly 75 to 80 percent of the best C/C++ throughput for sending scalar data types and only around 33 percent for sending structs containing binary fields), and the hand-optimized RPC code performs slightly better than the CORBA implementations. Our goal in precisely pinpointing the sources of overhead for communication middleware is to develop scalable and flexible CORBA implementations that can deliver gigabit data rates to applications.","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",306–317,12,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2067,article,"Gokhale, Aniruddha and Schmidt, Douglas C.",Measuring the Performance of Communication Middleware on High-Speed Networks,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248183,10.1145/248157.248183,"Conventional implementations of communication middleware (such as CORBA and traditional RPC toolkits) incur considerable over-head when used for performance-sensitive applications over high-speed networks. As gigabit networks become pervasive, inefficient middleware will force programmers to use lower-level mechanisms to achieve the necessary transfer rates. This is a serious problem for mission/life-critical applications (such as satellite surveillance and medical imaging).This paper compares the performance of several widely used communication middleware mechanisms on a high-speed ATM network. The middleware ranged from lower-level mechanisms (such as socket-based C interfaces and C++ wrappers for sockets) to higher-level mechanisms (such as RPC, hand-optimized RPC and two implementations of CORBA - Orbix and ORBeline). These measurements reveal that the lower-level C and C++ implementations outperform the CORBA implementations significantly (the best CORBA throughput for remote transfer was roughly 75 to 80 percent of the best C/C++ throughput for sending scalar data types and only around 33 percent for sending structs containing binary fields), and the hand-optimized RPC code performs slightly better than the CORBA implementations. Our goal in precisely pinpointing the sources of overhead for communication middleware is to develop scalable and flexible CORBA implementations that can deliver gigabit data rates to applications.",,306–317,12,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2068,inproceedings,"Cheshire, Stuart and Baker, Mary",Internet Mobility 4\texttimes{,1996,0897917901,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248156.248184,10.1145/248156.248184,"Mobile IP protocols allow mobile hosts to send and receive packets addressed with their home network IP address, regardless of the IP address of their current point of attachment in the Internet.While some recent work in Mobile IP focuses on a couple of specific routing optimizations for sending packets to and from mobile hosts [Joh96] [Mon96], we show that a variety of different optimizations are appropriate in different circumstances. The best choice, which may vary on a connection-by-connection or even a packet-by-packet basis, depends on three factors: the characteristics the protocol should optimize, the permissiveness of the networks over which the packets travel and the level of mobile-awareness of the hosts with which the mobile host corresponds.Of the sixteen possible routing choices that we identify, we describe the seven that are most useful and discuss their benefits and limitations. These optimizations range from the most costly, which provides completely transparent mobility in all networks, to the most economical, which does not attempt to conceal location information. In particular, hosts should retain the option to communicate conventionally without using Mobile IP whenever appropriate.Further, we show that all optimizations can be described using a 4\texttimes{","Conference Proceedings on Applications, Technologies, Architectures, and Protocols for Computer Communications",318–329,12,,"Palo Alto, California, USA",SIGCOMM '96,,,,,,
2069,article,"Cheshire, Stuart and Baker, Mary",Internet Mobility 4\texttimes{,1996,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/248157.248184,10.1145/248157.248184,"Mobile IP protocols allow mobile hosts to send and receive packets addressed with their home network IP address, regardless of the IP address of their current point of attachment in the Internet.While some recent work in Mobile IP focuses on a couple of specific routing optimizations for sending packets to and from mobile hosts [Joh96] [Mon96], we show that a variety of different optimizations are appropriate in different circumstances. The best choice, which may vary on a connection-by-connection or even a packet-by-packet basis, depends on three factors: the characteristics the protocol should optimize, the permissiveness of the networks over which the packets travel and the level of mobile-awareness of the hosts with which the mobile host corresponds.Of the sixteen possible routing choices that we identify, we describe the seven that are most useful and discuss their benefits and limitations. These optimizations range from the most costly, which provides completely transparent mobility in all networks, to the most economical, which does not attempt to conceal location information. In particular, hosts should retain the option to communicate conventionally without using Mobile IP whenever appropriate.Further, we show that all optimizations can be described using a 4\texttimes{",,318–329,12,,,,Oct. 1996,26,4,0146-4833,SIGCOMM Comput. Commun. Rev.,aug
2070,inproceedings,"Jamin, Sugih and Danzig, Peter B. and Shenker, Scott and Zhang, Lixia",A Measurement-Based Admission Control Algorithm for Integrated Services Packet Networks,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217396,10.1145/217382.217396,"Many designs for integrated service networks offer a bounded delay packet delivery service to support real-time applications. To provide bounded delay service, networks must use admission control to regulate their load. Previous work on admission control mainly focused on algorithms that compute the worst case theoretical queueing delay to guarantee an absolute delay bound for all packets. In this paper we describe a measurement-based admission control algorithm for predictive service, which allows occasional delay violations. We have tested our algorithm through simulations on a wide variety of network topologies and driven with various source models, including some that exhibit long-range dependence, both in themselves and in their aggregation. Our simulation results suggest that, at least for the scenarios studied here, the measurement-based approach combined with the relaxed service commitment of predictive service enables us to achieve a high level of network utilization while still reliably meeting the delay bound.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",2–13,12,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2071,article,"Jamin, Sugih and Danzig, Peter B. and Shenker, Scott and Zhang, Lixia",A Measurement-Based Admission Control Algorithm for Integrated Services Packet Networks,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217396,10.1145/217391.217396,"Many designs for integrated service networks offer a bounded delay packet delivery service to support real-time applications. To provide bounded delay service, networks must use admission control to regulate their load. Previous work on admission control mainly focused on algorithms that compute the worst case theoretical queueing delay to guarantee an absolute delay bound for all packets. In this paper we describe a measurement-based admission control algorithm for predictive service, which allows occasional delay violations. We have tested our algorithm through simulations on a wide variety of network topologies and driven with various source models, including some that exhibit long-range dependence, both in themselves and in their aggregation. Our simulation results suggest that, at least for the scenarios studied here, the measurement-based approach combined with the relaxed service commitment of predictive service enables us to achieve a high level of network utilization while still reliably meeting the delay bound.",,2–13,12,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2072,inproceedings,"Shenker, Scott and Breslau, Lee",Two Issues in Reservation Establishment,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217403,10.1145/217382.217403,"This paper addresses two issues related to resource reservation establishment in packet switched networks offering real-time services. The first issue arises out of the natural tension between the local nature of reservations (i.e., they control the service provided on a particular link) and the end-to-end nature of application service requirements. How do reservation establishment protocols enable applications to receive their desired end-to-end service? We review the current one-pass and two-pass approaches, and then propose a new hybrid approach called one-pass-with-advertising. The second issue in reservation establishment we consider arises from the inevitable heterogeneity in network router capabilities. Some routers and subnets in the Internet will support real-time services and others, such as ethernets, will not. How can a reservation establishment mechanism enable applications to achieve the end-to-end service they desire in the face of this heterogeneity? We propose an approach involving replacement services and advertising to build end-to-end service out of heterogeneous per-link service offerings.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",14–26,13,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2073,article,"Shenker, Scott and Breslau, Lee",Two Issues in Reservation Establishment,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217403,10.1145/217391.217403,"This paper addresses two issues related to resource reservation establishment in packet switched networks offering real-time services. The first issue arises out of the natural tension between the local nature of reservations (i.e., they control the service provided on a particular link) and the end-to-end nature of application service requirements. How do reservation establishment protocols enable applications to receive their desired end-to-end service? We review the current one-pass and two-pass approaches, and then propose a new hybrid approach called one-pass-with-advertising. The second issue in reservation establishment we consider arises from the inevitable heterogeneity in network router capabilities. Some routers and subnets in the Internet will support real-time services and others, such as ethernets, will not. How can a reservation establishment mechanism enable applications to achieve the end-to-end service they desire in the face of this heterogeneity? We propose an approach involving replacement services and advertising to build end-to-end service out of heterogeneous per-link service offerings.",,14–26,13,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2074,inproceedings,"Venkatramani, Chitra and Chiueh, Tzi-cker","Design, Implementation, and Evaluation of a Software-Based Real-Time Ethernet Protocol",1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217404,10.1145/217382.217404,"Distributed multimedia applications require performance guarantees from the underlying network subsystem. Ethernet has been the dominant local area network architecture in the last decade, and we believe that it will remain popular because of its cost-effectiveness and the availability of higher-bandwidth Ethernets. We present the design, implementation and evaluation of a software-based timed-token protocol called RETHER that provides real-time performance guarantees to multimedia applications without requiring any modifications to existing Ethernet hardware. RETHER features a hybrid mode of operation to reduce the performance impact on non-real-time network traffic, a race-condition-free distributed admission control mechanism, and an efficient token-passing scheme that protects the network against token loss due to node failures or otherwise. To our knowledge, this is the first software implementation of a real-time protocol over existing Ethernet hardware. Performance measurements from experiments on a 10 Mbps Ethernet indicate that up to 60% of the raw bandwidth can be reserved without deteriorating the performance of non-real-time traffic. Additional simulations for high bandwidth networks and faster workstation hardware indicate that the protocol allows reservation of a greater percentage of the available bandwidth.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",27–37,11,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2075,article,"Venkatramani, Chitra and Chiueh, Tzi-cker","Design, Implementation, and Evaluation of a Software-Based Real-Time Ethernet Protocol",1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217404,10.1145/217391.217404,"Distributed multimedia applications require performance guarantees from the underlying network subsystem. Ethernet has been the dominant local area network architecture in the last decade, and we believe that it will remain popular because of its cost-effectiveness and the availability of higher-bandwidth Ethernets. We present the design, implementation and evaluation of a software-based timed-token protocol called RETHER that provides real-time performance guarantees to multimedia applications without requiring any modifications to existing Ethernet hardware. RETHER features a hybrid mode of operation to reduce the performance impact on non-real-time network traffic, a race-condition-free distributed admission control mechanism, and an efficient token-passing scheme that protects the network against token loss due to node failures or otherwise. To our knowledge, this is the first software implementation of a real-time protocol over existing Ethernet hardware. Performance measurements from experiments on a 10 Mbps Ethernet indicate that up to 60% of the raw bandwidth can be reserved without deteriorating the performance of non-real-time traffic. Additional simulations for high bandwidth networks and faster workstation hardware indicate that the protocol allows reservation of a greater percentage of the available bandwidth.",,27–37,11,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2076,inproceedings,"Katevenis, Manolis and Vatsolaki, Panagiota and Efthymiou, Aristides",Pipelined Memory Shared Buffer for VLSI Switches,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217406,10.1145/217382.217406,"Switch chips are building blocks for computer and communication systems. Switches need internal buffering, because of output contention; shared buffering is known to perform better than multiple input queues or buffers, and the VLSI implementation of the former is not more expensive than the latter. We present a new organization for a shared buffer with its associated switching and cut-through functions. It is simpler and smaller than wide or interleaved organizations, and it is particularly suitable for VLSI technologies. It is based on multiple memory banks, addressed in a pipelined fashion. The first word of a packet is transferred to/from the first bank, followed by a ""wave"" of similar operations for the remaining words in the remaining banks. An FPGA-based prototype is operational, while standard-cell and full-custom chips are being submitted for fabrication. Simulation of the full-custom version indicates that, even in a conservative 1-micron CMOS technology, a 64 Kbit central buffer for an 8\texttimes{","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",39–48,10,"shared buffering, multiport buffer, pipelined memory, gigabit VLSI switch buffer, crossbar switch, input queueing","Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2077,article,"Katevenis, Manolis and Vatsolaki, Panagiota and Efthymiou, Aristides",Pipelined Memory Shared Buffer for VLSI Switches,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217406,10.1145/217391.217406,"Switch chips are building blocks for computer and communication systems. Switches need internal buffering, because of output contention; shared buffering is known to perform better than multiple input queues or buffers, and the VLSI implementation of the former is not more expensive than the latter. We present a new organization for a shared buffer with its associated switching and cut-through functions. It is simpler and smaller than wide or interleaved organizations, and it is particularly suitable for VLSI technologies. It is based on multiple memory banks, addressed in a pipelined fashion. The first word of a packet is transferred to/from the first bank, followed by a ""wave"" of similar operations for the remaining words in the remaining banks. An FPGA-based prototype is operational, while standard-cell and full-custom chips are being submitted for fabrication. Simulation of the full-custom version indicates that, even in a conservative 1-micron CMOS technology, a 64 Kbit central buffer for an 8\texttimes{",,39–48,10,"shared buffering, input queueing, multiport buffer, crossbar switch, pipelined memory, gigabit VLSI switch buffer",,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2078,inproceedings,"Parulkar, Guru and Schmidt, Douglas C. and Turner, Jonathan S.",AItPm: A Strategy for Integrating IP with ATM,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217409,10.1145/217382.217409,"This paper describes research on new methods and architectures that enable the synergistic combination of IP and ATM technologies. We have designed a highly scalable gigabit IP router based on an ATM core and a set of tightly coupled general-purpose processors. This aItPm (pronounced ""IP on ATM"" or, if you prefer, ""ip-attem"") architecture provides flexibility in congestion control, routing, resource management, and packet scheduling.The aItPm architecture is designed to allow experimentation with, and fine tuning of, the protocols and algorithms that are expected to form the core of the next generation IP in the context of a gigabit environment. The underlying multi-CPU embedded system will ensure that there are enough CPU and memory cycles to perform all IP packet processing at gigabit rates. We believe that the aItPm architecture will not only lead to a scalable high-performance gigabit IP router technology, but will also demonstrate that IP and ATM technologies can be mutually supportive.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",49–59,11,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2079,article,"Parulkar, Guru and Schmidt, Douglas C. and Turner, Jonathan S.",AItPm: A Strategy for Integrating IP with ATM,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217409,10.1145/217391.217409,"This paper describes research on new methods and architectures that enable the synergistic combination of IP and ATM technologies. We have designed a highly scalable gigabit IP router based on an ATM core and a set of tightly coupled general-purpose processors. This aItPm (pronounced ""IP on ATM"" or, if you prefer, ""ip-attem"") architecture provides flexibility in congestion control, routing, resource management, and packet scheduling.The aItPm architecture is designed to allow experimentation with, and fine tuning of, the protocols and algorithms that are expected to form the core of the next generation IP in the context of a gigabit environment. The underlying multi-CPU embedded system will ensure that there are enough CPU and memory cycles to perform all IP packet processing at gigabit rates. We believe that the aItPm architecture will not only lead to a scalable high-performance gigabit IP router technology, but will also demonstrate that IP and ATM technologies can be mutually supportive.",,49–59,11,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2080,inproceedings,"Thyagarajan, Ajit S. and Deering, Stephen E.",Hierarchical Distance-Vector Multicast Routing for the MBone,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217411,10.1145/217382.217411,"The exponential growth of the Multicast Back-bone (MBone) has resulted in increased routing overhead and processing costs. In this paper we propose a two-level hierarchical routing model as a solution to this problem. This approach involves partitioning the MBone into non-overlapping regions using DVMRP as the inter-region routing protocol; intra-region routing may be accomplished by any of a number of existing multicast protocols. Our design is flexible enough to accommodate additional levels of hierarchy, and protocols other than DVMRP at the higher levels. The unique feature of this approach is the independence of the higher level routing protocol from the subnet addresses, which allows for easy incremental deployment with small changes to existing intra-region protocols.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",60–66,7,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2081,article,"Thyagarajan, Ajit S. and Deering, Stephen E.",Hierarchical Distance-Vector Multicast Routing for the MBone,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217411,10.1145/217391.217411,"The exponential growth of the Multicast Back-bone (MBone) has resulted in increased routing overhead and processing costs. In this paper we propose a two-level hierarchical routing model as a solution to this problem. This approach involves partitioning the MBone into non-overlapping regions using DVMRP as the inter-region routing protocol; intra-region routing may be accomplished by any of a number of existing multicast protocols. Our design is flexible enough to accommodate additional levels of hierarchy, and protocols other than DVMRP at the higher levels. The unique feature of this approach is the independence of the higher level routing protocol from the subnet addresses, which allows for easy incremental deployment with small changes to existing intra-region protocols.",,60–66,7,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2082,inproceedings,"Partridge, Craig and Hughes, Jim and Stone, Jonathan",Performance of Checksums and CRCs over Real Data,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217413,10.1145/217382.217413,"Checksum and CRC algorithms have historically been studied under the assumption that the data fed to the algorithms was entirely random. This paper examines the behavior of checksums and CRCs over real data from various UNIX® file systems. We show that, when given real data in small to modest pieces (e.g., 48 bytes), all the checksum algorithms have skewed distributions. In one dramatic case, 0.01% of the check values appeared nearly 19% of the time. These results have implications for CRCs and checksums when applied to real data. They also cause a spectacular failure rate for the both TCP and Fletcher's checksums when trying to detect certain types of packet splices.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",68–76,9,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2083,article,"Partridge, Craig and Hughes, Jim and Stone, Jonathan",Performance of Checksums and CRCs over Real Data,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217413,10.1145/217391.217413,"Checksum and CRC algorithms have historically been studied under the assumption that the data fed to the algorithms was entirely random. This paper examines the behavior of checksums and CRCs over real data from various UNIX® file systems. We show that, when given real data in small to modest pieces (e.g., 48 bytes), all the checksum algorithms have skewed distributions. In one dramatic case, 0.01% of the check values appeared nearly 19% of the time. These results have implications for CRCs and checksums when applied to real data. They also cause a spectacular failure rate for the both TCP and Fletcher's checksums when trying to detect certain types of packet splices.",,68–76,9,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2084,inproceedings,"Touch, Joseph D.",Performance Analysis of MD5,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217414,10.1145/217382.217414,"MD5 is an authentication algorithm proposed as the required implementation of the authentication option in IPv6. This paper presents an analysis of the speed at which MD5 can be implemented in software and hardware, and discusses whether its use interferes with high bandwidth networking. The analysis indicates that MD5 software currently runs at 85 Mbps on a 190 Mhz RISC architecture, a rate that cannot be improved more than 20-40%. Because MD5 processes the entire body of a packet, this data rate is insufficient for current high bandwidth networks, including HiPPI and FiberChannel. Further analysis indicates that a 300 Mhz custom VLSI CMOS hardware implementation of MD5 may run as fast as 256 Mbps. The hardware rate cannot support existing IPv4 data rates on high bandwidth links (800 Mbps HiPPI). The use of MD5 as the default required authentication algorithm in IPv6 should therefore be reconsidered, and an alternative should be proposed. This paper includes a brief description of the properties of such an alternative, including a sample alternate hash algorithm.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",77–86,10,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2085,article,"Touch, Joseph D.",Performance Analysis of MD5,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217414,10.1145/217391.217414,"MD5 is an authentication algorithm proposed as the required implementation of the authentication option in IPv6. This paper presents an analysis of the speed at which MD5 can be implemented in software and hardware, and discusses whether its use interferes with high bandwidth networking. The analysis indicates that MD5 software currently runs at 85 Mbps on a 190 Mhz RISC architecture, a rate that cannot be improved more than 20-40%. Because MD5 processes the entire body of a packet, this data rate is insufficient for current high bandwidth networks, including HiPPI and FiberChannel. Further analysis indicates that a 300 Mhz custom VLSI CMOS hardware implementation of MD5 may run as fast as 256 Mbps. The hardware rate cannot support existing IPv4 data rates on high bandwidth links (800 Mbps HiPPI). The use of MD5 as the default required authentication algorithm in IPv6 should therefore be reconsidered, and an alternative should be proposed. This paper includes a brief description of the properties of such an alternative, including a sample alternate hash algorithm.",,77–86,10,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2086,inproceedings,"Kleinpaste, Karl and Steenkiste, Peter and Zill, Brian",Software Support for Outboard Buffering and Checksumming,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217416,10.1145/217382.217416,"Data copying and checksumming are the most expensive operations when doing high-bandwidth network IO over a high-speed network. Under some conditions, outboard buffering and checksumming can eliminate accesses to the data, thus making communication less expensive and faster. One of the scenarios in which outboard buffering pays off is the common case of applications accessing the network using the Berkeley sockets interface and the Internet protocol stack. In this paper we describe the changes that were made to a BSD protocol stack to make use of a network adaptor that supports outboard buffering and checksumming. Our goal is not only to achieve ""single copy"" communication for application that use sockets, but to also have efficient communication for in-kernel applications and for applications using other networks. Performance measurements show that for large reads and writes the single-copy path through the stack is significantly more efficient than the original implementation.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",87–98,12,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2087,article,"Kleinpaste, Karl and Steenkiste, Peter and Zill, Brian",Software Support for Outboard Buffering and Checksumming,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217416,10.1145/217391.217416,"Data copying and checksumming are the most expensive operations when doing high-bandwidth network IO over a high-speed network. Under some conditions, outboard buffering and checksumming can eliminate accesses to the data, thus making communication less expensive and faster. One of the scenarios in which outboard buffering pays off is the common case of applications accessing the network using the Berkeley sockets interface and the Internet protocol stack. In this paper we describe the changes that were made to a BSD protocol stack to make use of a network adaptor that supports outboard buffering and checksumming. Our goal is not only to achieve ""single copy"" communication for application that use sockets, but to also have efficient communication for in-kernel applications and for applications using other networks. Performance measurements show that for large reads and writes the single-copy path through the stack is significantly more efficient than the original implementation.",,87–98,12,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2088,inproceedings,"Willinger, Walter and Taqqu, Murad S. and Sherman, Robert and Wilson, Daniel V.",Self-Similarity through High-Variability: Statistical Analysis of Ethernet LAN Traffic at the Source Level,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217418,10.1145/217382.217418,"A number of recent empirical studies of traffic measurements from a variety of working packet networks have convincingly demonstrated that actual network traffic is self-similar or long-range dependent in nature (i.e., bursty over a wide range of time scales) - in sharp contrast to commonly made traffic modeling assumptions. In this paper, we provide a plausible physical explanation for the occurrence of self-similarity in high-speed network traffic. Our explanation is based on convergence results for processes that exhibit high variability (i.e., infinite variance) and is supported by detailed statistical analyses of real-time traffic measurements from Ethernet LAN's at the level of individual sources.Our key mathematical result states that the superposition of many ON/OFF sources (also known as packet trains) whose ON-periods and OFF-periods exhibit the Noah Effect (i.e., have high variability or infinite variance) produces aggregate network traffic that features the Joseph Effect (i.e., is self-similar or long-range dependent). There is, moreover, a simple relation between the parameters describing the intensities of the Noah Effect (high variability) and the Joseph Effect (self-similarity). An extensive statistical analysis of two sets of high time-resolution traffic measurements from two Ethernet LAN's (involving a few hundred active source-destination pairs) confirms that the data at the level of individual sources or source-destination pairs are consistent with the Noah Effect. We also discuss implications of this simple physical explanation for the presence of self-similar traffic patterns in modern high-speed network traffic for (i) parsimonious traffic modeling (ii) efficient synthetic generation of realistic traffic patterns, and (iii) relevant network performance and protocol analysis.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",100–113,14,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2089,article,"Willinger, Walter and Taqqu, Murad S. and Sherman, Robert and Wilson, Daniel V.",Self-Similarity through High-Variability: Statistical Analysis of Ethernet LAN Traffic at the Source Level,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217418,10.1145/217391.217418,"A number of recent empirical studies of traffic measurements from a variety of working packet networks have convincingly demonstrated that actual network traffic is self-similar or long-range dependent in nature (i.e., bursty over a wide range of time scales) - in sharp contrast to commonly made traffic modeling assumptions. In this paper, we provide a plausible physical explanation for the occurrence of self-similarity in high-speed network traffic. Our explanation is based on convergence results for processes that exhibit high variability (i.e., infinite variance) and is supported by detailed statistical analyses of real-time traffic measurements from Ethernet LAN's at the level of individual sources.Our key mathematical result states that the superposition of many ON/OFF sources (also known as packet trains) whose ON-periods and OFF-periods exhibit the Noah Effect (i.e., have high variability or infinite variance) produces aggregate network traffic that features the Joseph Effect (i.e., is self-similar or long-range dependent). There is, moreover, a simple relation between the parameters describing the intensities of the Noah Effect (high variability) and the Joseph Effect (self-similarity). An extensive statistical analysis of two sets of high time-resolution traffic measurements from two Ethernet LAN's (involving a few hundred active source-destination pairs) confirms that the data at the level of individual sources or source-destination pairs are consistent with the Noah Effect. We also discuss implications of this simple physical explanation for the presence of self-similar traffic patterns in modern high-speed network traffic for (i) parsimonious traffic modeling (ii) efficient synthetic generation of realistic traffic patterns, and (iii) relevant network performance and protocol analysis.",,100–113,14,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2090,inproceedings,"Huang, Changcheng and Devetsikiotis, Michael and Lambadaris, Ioannis and Kaye, A. Roger",Modeling and Simulation of Self-Similar Variable Bit Rate Compressed Video: A Unified Approach,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217420,10.1145/217382.217420,"Variable bit rate (VBR) compressed video is expected to become one of the major loading factors in high-speed packet networks such as ATM-based B-ISDN. However, recent measurements based on long empirical traces (complete movies) revealed that VBR video traffic possesses self-similar (or fractal) characteristics, meaning that the dependence in the traffic stream lasts much longer than traditional models can capture.In this paper, we present a unified approach which, in addition to accurately modeling the marginal distribution of empirical video records, also models directly both the short and the long-term empirical autocorrelation structures. We also present simulation results using synthetic data and compare with results based on empirical video traces.Furthermore, we extend the application of efficient estimation techniques based on importance sampling that we had used before only for simple fractal processes. We use importance sampling techniques to efficiently estimate low probabilities of packet losses that occur when a multiplexer is fed with synthetic traffic from our self-similar VBR video model.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",114–125,12,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2091,article,"Huang, Changcheng and Devetsikiotis, Michael and Lambadaris, Ioannis and Kaye, A. Roger",Modeling and Simulation of Self-Similar Variable Bit Rate Compressed Video: A Unified Approach,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217420,10.1145/217391.217420,"Variable bit rate (VBR) compressed video is expected to become one of the major loading factors in high-speed packet networks such as ATM-based B-ISDN. However, recent measurements based on long empirical traces (complete movies) revealed that VBR video traffic possesses self-similar (or fractal) characteristics, meaning that the dependence in the traffic stream lasts much longer than traditional models can capture.In this paper, we present a unified approach which, in addition to accurately modeling the marginal distribution of empirical video records, also models directly both the short and the long-term empirical autocorrelation structures. We also present simulation results using synthetic data and compare with results based on empirical video traces.Furthermore, we extend the application of efficient estimation techniques based on importance sampling that we had used before only for simple fractal processes. We use importance sampling techniques to efficiently estimate low probabilities of packet losses that occur when a multiplexer is fed with synthetic traffic from our self-similar VBR video model.",,114–125,12,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2092,inproceedings,"Lee, Kam",Performance Bounds in Communication Networks with Variable-Rate Links,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217422,10.1145/217382.217422,"In most network models for quality of service support, the communication links interconnecting the switches and gateways are assumed to have fixed bandwidth and zero error rate. This assumption of steadiness, especially in a heterogeneous internet-working environment, might be invalid owing to subnetwork multiple-access mechanism, link-level flow/error control, and user mobility. Techniques are presented in this paper to characterize and analyze work-conserving communication nodes with varying output rate. In the deterministic approach, the notion of ""fluctuation constraint,"" analogous to the ""burstiness constraint"" for traffic characterization, is introduced to characterize the node. In the statistical approach, the variable-rate output is modelled as an ""exponentially bounded fluctuation"" process in a way similar to the ""exponentially bounded burstiness"" method for traffic modelling. Based on these concepts, deterministic and statistical bounds on queue size and packet delay in isolated variable-rate communication server-nodes are derived, including cases of single-input and multiple-input under first-come-first-serve queueing. Queue size bounds are shown to be useful for buffer requirement and packet loss probability estimation at individual nodes. Our formulations also facilitate the computation of end-to-end performance bounds across a feedforward network of variable-rate server-nodes. Several numerical examples of interest are given in the discussion.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",126–136,11,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2093,article,"Lee, Kam",Performance Bounds in Communication Networks with Variable-Rate Links,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217422,10.1145/217391.217422,"In most network models for quality of service support, the communication links interconnecting the switches and gateways are assumed to have fixed bandwidth and zero error rate. This assumption of steadiness, especially in a heterogeneous internet-working environment, might be invalid owing to subnetwork multiple-access mechanism, link-level flow/error control, and user mobility. Techniques are presented in this paper to characterize and analyze work-conserving communication nodes with varying output rate. In the deterministic approach, the notion of ""fluctuation constraint,"" analogous to the ""burstiness constraint"" for traffic characterization, is introduced to characterize the node. In the statistical approach, the variable-rate output is modelled as an ""exponentially bounded fluctuation"" process in a way similar to the ""exponentially bounded burstiness"" method for traffic modelling. Based on these concepts, deterministic and statistical bounds on queue size and packet delay in isolated variable-rate communication server-nodes are derived, including cases of single-input and multiple-input under first-come-first-serve queueing. Queue size bounds are shown to be useful for buffer requirement and packet loss probability estimation at individual nodes. Our formulations also facilitate the computation of end-to-end performance bounds across a feedforward network of variable-rate server-nodes. Several numerical examples of interest are given in the discussion.",,126–136,11,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2094,inproceedings,"Bhatti, Nina T. and Schlichting, Richard D.",A System for Constructing Configurable High-Level Protocols,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217424,10.1145/217382.217424,"New distributed computing applications are driving the development of more specialized protocols, as well as demanding greater control over the communication substrate. Here, a network subsystem that supports modular, fine-grained construction of high-level protocols such as atomic multicast and group RPC is described. The approach is based on extending the standard hierarchical model of the x-kernel with composite protocols in which micro-protocol objects are composed within a standard runtime framework. Each micro-protocol realizes a separate semantic property, leading to a highly modular and configurable implementation. In contrast with similar systems, this approach provides finer granularity and more flexible inter-object communication. The design and prototype implementation runing on Mach are described. Performance results are also given for a micro-protocol suite implementing variants of group RPC.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",138–150,13,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2095,article,"Bhatti, Nina T. and Schlichting, Richard D.",A System for Constructing Configurable High-Level Protocols,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217424,10.1145/217391.217424,"New distributed computing applications are driving the development of more specialized protocols, as well as demanding greater control over the communication substrate. Here, a network subsystem that supports modular, fine-grained construction of high-level protocols such as atomic multicast and group RPC is described. The approach is based on extending the standard hierarchical model of the x-kernel with composite protocols in which micro-protocol objects are composed within a standard runtime framework. Each micro-protocol realizes a separate semantic property, leading to a highly modular and configurable implementation. In contrast with similar systems, this approach provides finer granularity and more flexible inter-object communication. The design and prototype implementation runing on Mach are described. Performance results are also given for a micro-protocol suite implementing variants of group RPC.",,138–150,13,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2096,inproceedings,"Braun, Torsten and Diot, Christophe",Protocol Implementation Using Integrated Layer Processing,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217425,10.1145/217382.217425,"Integrated Layer Processing (ILP) is an implementation concept which ""permit[s] the implementor the option of performing all the [data] manipulation steps in one or two integrated processing loops"" [1]. To estimate the achievable benefits of ILP a file transfer application with an encryption function on top of a user-level TCP has been implemented and the performance of the application in terms of throughput and packet processing times has been measured. The results show that it is possible to obtain performance benefits by integrating marshalling, encryption and TCP checksum calculation. They also show that the benefits are smaller than in simple experiments, where ILP effects have not been evaluated in a complete protocol environment. Simulations of memory access and cache hit rate show that the main benefit of ILP is reduced memory accesses rather than an improved cache hit rate. The results further show that data manipulation characteristics may significantly influence the cache behavior and the achievable performance gain of ILP.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",151–161,11,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2097,article,"Braun, Torsten and Diot, Christophe",Protocol Implementation Using Integrated Layer Processing,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217425,10.1145/217391.217425,"Integrated Layer Processing (ILP) is an implementation concept which ""permit[s] the implementor the option of performing all the [data] manipulation steps in one or two integrated processing loops"" [1]. To estimate the achievable benefits of ILP a file transfer application with an encryption function on top of a user-level TCP has been implemented and the performance of the application in terms of throughput and packet processing times has been measured. The results show that it is possible to obtain performance benefits by integrating marshalling, encryption and TCP checksum calculation. They also show that the benefits are smaller than in simple experiments, where ILP effects have not been evaluated in a complete protocol environment. Simulations of memory access and cache hit rate show that the main benefit of ILP is reduced memory accesses rather than an improved cache hit rate. The results further show that data manipulation characteristics may significantly influence the cache behavior and the achievable performance gain of ILP.",,151–161,11,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2098,inproceedings,"Chandranmenon, Girish P. and Varghese, George",Trading Packet Headers for Packet Processing,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217427,10.1145/217382.217427,"In high speed networks, packet processing is relatively expensive while bandwidth is cheap. Thus it pays to add information to packet headers to make packet processing easier. While this is an old idea, we describe several specific new mechanisms based on this principle. We describe a new technique, source hashing, which can provide O(1) lookup costs at the Data Link, Routing, and Transport layers. Source hashing is especially powerful when combined with the old idea of a flow ID; the flow identifier allows packet processing information to be cached, and source hashing allows efficient cache lookups. Unlike Virtual Circuit Identifiers (VCIs), source hashing does not require a round trip delay for set up. In an experiment with the BSD Packet Filter implementation, we found that adding a flow ID and a source hash improved packet processing costs by a factor of 7. We also found a 45% improvement when we conducted a similar experiment with IP packet forwarding. We also describe two other new techniques: threaded indices, which allows fast VCI-like lookups for datagram protocols like IP; and a Data Manipulation Layer, which compiles out all the information needed for Integrated Layer Processing into an easily accessible portion of each packet.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",162–173,12,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2099,article,"Chandranmenon, Girish P. and Varghese, George",Trading Packet Headers for Packet Processing,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217427,10.1145/217391.217427,"In high speed networks, packet processing is relatively expensive while bandwidth is cheap. Thus it pays to add information to packet headers to make packet processing easier. While this is an old idea, we describe several specific new mechanisms based on this principle. We describe a new technique, source hashing, which can provide O(1) lookup costs at the Data Link, Routing, and Transport layers. Source hashing is especially powerful when combined with the old idea of a flow ID; the flow identifier allows packet processing information to be cached, and source hashing allows efficient cache lookups. Unlike Virtual Circuit Identifiers (VCIs), source hashing does not require a round trip delay for set up. In an experiment with the BSD Packet Filter implementation, we found that adding a flow ID and a source hash improved packet processing costs by a factor of 7. We also found a 45% improvement when we conducted a similar experiment with IP packet forwarding. We also describe two other new techniques: threaded indices, which allows fast VCI-like lookups for datagram protocols like IP; and a Data Manipulation Layer, which compiles out all the information needed for Integrated Layer Processing into an easily accessible portion of each packet.",,162–173,12,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2100,inproceedings,"Ezust, S. Alan and v. Bochmann, Gregor",An Automatic Trace Analysis Tool Generator for Estelle Specifications,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217428,10.1145/217382.217428,"This paper describes the development of Tango, an automatic generator of backtracking trace analysis tools for single-process specifications written in the formal description language, Estelle. A tool generated by Tango automatically checks the validity of any execution trace against the given specification, and supports a number of checking options. The approach taken was to modify an Estelle-to-C++ compiler. Discussion about nondeterministic specifications, multiple observation points, and on-line trace analysis follow. Trace analyzers for the protocols LAPD and TP0 have been tested and performance results are evaluated. Issues in the analysis of partial traces are also discussed.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",175–184,10,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2101,article,"Ezust, S. Alan and v. Bochmann, Gregor",An Automatic Trace Analysis Tool Generator for Estelle Specifications,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217428,10.1145/217391.217428,"This paper describes the development of Tango, an automatic generator of backtracking trace analysis tools for single-process specifications written in the formal description language, Estelle. A tool generated by Tango automatically checks the validity of any execution trace against the given specification, and supports a number of checking options. The approach taken was to modify an Estelle-to-C++ compiler. Discussion about nondeterministic specifications, multiple observation points, and on-line trace analysis follow. Trace analyzers for the protocols LAPD and TP0 have been tested and performance results are evaluated. Issues in the analysis of partial traces are also discussed.",,175–184,10,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2102,inproceedings,"Ahn, Jong Suk and Danzig, Peter B. and Liu, Zhen and Yan, Limin",Evaluation of TCP Vegas: Emulation and Experiment,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217431,10.1145/217382.217431,"This paper explores the claims that TCP Vegas [2] both uses network bandwidth more efficiently and achieves higher network throughput than TCP Reno [6]. It explores how link bandwidth, network buffer capacity, TCP receiver acknowledgment algorithm, and degree of network congestion affect the relative performance of Vegas and Reno.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",185–195,11,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2103,article,"Ahn, Jong Suk and Danzig, Peter B. and Liu, Zhen and Yan, Limin",Evaluation of TCP Vegas: Emulation and Experiment,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217431,10.1145/217391.217431,"This paper explores the claims that TCP Vegas [2] both uses network bandwidth more efficiently and achieves higher network throughput than TCP Reno [6]. It explores how link bandwidth, network buffer capacity, TCP receiver acknowledgment algorithm, and degree of network congestion affect the relative performance of Vegas and Reno.",,185–195,11,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2104,inproceedings,"Edwards, Aled and Muir, Steve",Experiences Implementing a High Performance TCP in User-Space,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.318122,10.1145/217382.318122,"The advantages of user-space protocols are well-known, but implementations often exhibit poor performance. This paper describes a user-space TCP implementation that outperforms a 'normal' kernel TCP and that achieves 80% of the performance of a 'single-copy' TCP. Throughput of 160 Mbit/s has been measured. We describe some of the techniques we used and some of the problems we encountered.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",196–205,10,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2105,article,"Edwards, Aled and Muir, Steve",Experiences Implementing a High Performance TCP in User-Space,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.318122,10.1145/217391.318122,"The advantages of user-space protocols are well-known, but implementations often exhibit poor performance. This paper describes a user-space TCP implementation that outperforms a 'normal' kernel TCP and that achieves 80% of the performance of a 'single-copy' TCP. Throughput of 160 Mbit/s has been measured. We describe some of the techniques we used and some of the problems we encountered.",,196–205,10,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2106,inproceedings,"Figueira, Norival R. and Pasquale, Joseph",Leave-in-Time: A New Service Discipline for Real-Time Communications in a Packet-Switching Network,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217433,10.1145/217382.217433,"Leave-in-Time is a new rate-based service discipline for packet-switching nodes in a connection-oriented data network. Leave-in-Time provides sessions with upper bounds on end-to-end delay, delay jitter, buffer space requirements, and an upper bound on the probability distribution of end-to-end delays. A Leave-in-Time session's guarantees are completely determined by the dynamic traffic behavior of that session, without influence from other sessions. This results in the desirable property that these guarantees are expressed as functions derivable simply from a single fixed-rate server (with rate equal to the session's reserved rate) serving only that session. Leave-in-Time has a non-work-conserving mode of operation for sessions desiring low end-to-end delay jitter. Finally, Leave-in-Time supports the notion of delay shifting, whereby the delay bounds of some sessions may be decreased at the expense of increasing those of other sessions. We present a set of admission control algorithms which support the ability to do delay shifting in a systematic way.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",207–218,12,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2107,article,"Figueira, Norival R. and Pasquale, Joseph",Leave-in-Time: A New Service Discipline for Real-Time Communications in a Packet-Switching Network,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217433,10.1145/217391.217433,"Leave-in-Time is a new rate-based service discipline for packet-switching nodes in a connection-oriented data network. Leave-in-Time provides sessions with upper bounds on end-to-end delay, delay jitter, buffer space requirements, and an upper bound on the probability distribution of end-to-end delays. A Leave-in-Time session's guarantees are completely determined by the dynamic traffic behavior of that session, without influence from other sessions. This results in the desirable property that these guarantees are expressed as functions derivable simply from a single fixed-rate server (with rate equal to the session's reserved rate) serving only that session. Leave-in-Time has a non-work-conserving mode of operation for sessions desiring low end-to-end delay jitter. Finally, Leave-in-Time supports the notion of delay shifting, whereby the delay bounds of some sessions may be decreased at the expense of increasing those of other sessions. We present a set of admission control algorithms which support the ability to do delay shifting in a systematic way.",,207–218,12,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2108,inproceedings,"Grossglauser, M. and Keshav, S. and Tse, D.",RCBR: A Simple and Efficient Service for Multiple Time-Scale Traffic,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217436,10.1145/217382.217436,"Compressed video traffic is expected to be a significant component of the traffic mix in integrated services networks. This traffic is hard to manage, since it has strict delay and loss requirements, but at the same time, exhibits burstiness at multiple time-scales. In this paper, we observe that slow time-scale variations can cause sustained peaks in the source rate, substantially degrading performance. We use large deviation theory to study this problem and to motivate the design of Renegotiated Constant Bit Rate Service (RCBR), that adds renegotiation and buffer monitoring to traditional CBR service. We argue the the load placed on signalling by RCBR can be handled by current technology. We present a) an algorithm to compute the optimal renegotiation schedule for stored (off-line) traffic, and b) a heuristic to approximate the optimal schedule for online traffic. Simulation experiments show that RCBR is able to extract almost all of the statistical multiplexing gain available by exploiting slow time-scale variations in traffic. In more general terms, we believe that a clean system design must match control time-scales to the time scales over which the workload varies. RCBR works well because it makes intelligent use of this time-scale separation.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",219–230,12,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2109,article,"Grossglauser, M. and Keshav, S. and Tse, D.",RCBR: A Simple and Efficient Service for Multiple Time-Scale Traffic,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217436,10.1145/217391.217436,"Compressed video traffic is expected to be a significant component of the traffic mix in integrated services networks. This traffic is hard to manage, since it has strict delay and loss requirements, but at the same time, exhibits burstiness at multiple time-scales. In this paper, we observe that slow time-scale variations can cause sustained peaks in the source rate, substantially degrading performance. We use large deviation theory to study this problem and to motivate the design of Renegotiated Constant Bit Rate Service (RCBR), that adds renegotiation and buffer monitoring to traditional CBR service. We argue the the load placed on signalling by RCBR can be handled by current technology. We present a) an algorithm to compute the optimal renegotiation schedule for stored (off-line) traffic, and b) a heuristic to approximate the optimal schedule for online traffic. Simulation experiments show that RCBR is able to extract almost all of the statistical multiplexing gain available by exploiting slow time-scale variations in traffic. In more general terms, we believe that a clean system design must match control time-scales to the time scales over which the workload varies. RCBR works well because it makes intelligent use of this time-scale separation.",,219–230,12,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2110,inproceedings,"Shreedhar, M. and Varghese, George",Efficient Fair Queueing Using Deficit Round Robin,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217453,10.1145/217382.217453,"Fair queuing is a technique that allows each flow passing through a network device to have a fair share of network resources. Previous schemes for fair queuing that achieved nearly perfect fairness were expensive to implement: specifically, the work required to process a packet in these schemes was O(log(n)), where n is the number of active flows. This is expensive at high speeds. On the other hand, cheaper approximations of fair queuing that have been reported in the literature exhibit unfair behavior. In this paper, we describe a new approximation of fair queuing, that we call Deficit Round Robin. Our scheme achieves nearly perfect fairness in terms of throughput, requires only O(1) work to process a packet, and is simple enough to implement in hardware. Deficit Round Robin is also applicable to other scheduling problems where servicing cannot be broken up into smaller units, and to distributed queues.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",231–242,12,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2111,article,"Shreedhar, M. and Varghese, George",Efficient Fair Queueing Using Deficit Round Robin,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217453,10.1145/217391.217453,"Fair queuing is a technique that allows each flow passing through a network device to have a fair share of network resources. Previous schemes for fair queuing that achieved nearly perfect fairness were expensive to implement: specifically, the work required to process a packet in these schemes was O(log(n)), where n is the number of active flows. This is expensive at high speeds. On the other hand, cheaper approximations of fair queuing that have been reported in the literature exhibit unfair behavior. In this paper, we describe a new approximation of fair queuing, that we call Deficit Round Robin. Our scheme achieves nearly perfect fairness in terms of throughput, requires only O(1) work to process a packet, and is simple enough to implement in hardware. Deficit Round Robin is also applicable to other scheduling problems where servicing cannot be broken up into smaller units, and to distributed queues.",,231–242,12,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2112,inproceedings,"Akyildiz, Ian F. and Ho, Joseph S. M.",A Mobile User Location Update and Paging Mechanism under Delay Constraints,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217455,10.1145/217382.217455,"A mobile user location management mechanism is introduced that incorporates a distance based location update scheme and a paging mechanism that satisfies predefined delay requirements. An analytical model is developed which captures the mobility and call arrival pattern of a terminal. Given the respective costs for location update and terminal paging, the average total location update and terminal paging cost is determined. An iterative algorithm is then used to determine the optimal location update threshold distance that results in the minimum cost. Analytical results are also obtained to demonstrate the relative cost incurred by the proposed mechanism under various delay requirements.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",244–255,12,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2113,article,"Akyildiz, Ian F. and Ho, Joseph S. M.",A Mobile User Location Update and Paging Mechanism under Delay Constraints,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217455,10.1145/217391.217455,"A mobile user location management mechanism is introduced that incorporates a distance based location update scheme and a paging mechanism that satisfies predefined delay requirements. An analytical model is developed which captures the mobility and call arrival pattern of a terminal. Given the respective costs for location update and terminal paging, the average total location update and terminal paging cost is determined. An iterative algorithm is then used to determine the optimal location update threshold distance that results in the minimum cost. Analytical results are also obtained to demonstrate the relative cost incurred by the proposed mechanism under various delay requirements.",,244–255,12,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2114,inproceedings,"Lin, Hung-Yu and Harn, Lein",Authentication Protocols for Personal Communication Systems,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217456,10.1145/217382.217456,"Masquerading and eavesdropping are major threats to the security of wireless communications. To provide proper protection for the communication of the wireless link, contents of the communication should be enciphered and mutual authentication should be conducted between the subscriber and the serving network. Several protocols have been proposed by standards bodies and independent researchers in recent years to counteract these threats. However, the strength of these protocols is usually weakened in the roaming environment where the security breach of a visited network could lead to persistent damages to subscribers who visit. The subscriber's identity is not well protected in most protocols, and appropriate mechanisms solving disputes on roaming bills are not supported either. To solve these problems, new authentication protocols are proposed in this paper with new security features that have not been fully explored before.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",256–261,6,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2115,article,"Lin, Hung-Yu and Harn, Lein",Authentication Protocols for Personal Communication Systems,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217456,10.1145/217391.217456,"Masquerading and eavesdropping are major threats to the security of wireless communications. To provide proper protection for the communication of the wireless link, contents of the communication should be enciphered and mutual authentication should be conducted between the subscriber and the serving network. Several protocols have been proposed by standards bodies and independent researchers in recent years to counteract these threats. However, the strength of these protocols is usually weakened in the roaming environment where the security breach of a visited network could lead to persistent damages to subscribers who visit. The subscriber's identity is not well protected in most protocols, and appropriate mechanisms solving disputes on roaming bills are not supported either. To solve these problems, new authentication protocols are proposed in this paper with new security features that have not been fully explored before.",,256–261,6,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2116,inproceedings,"Fullmer, Chane L. and Garcia-Luna-Aceves, J. J.",Floor Acquisition Multiple Access (FAMA) for Packet-Radio Networks,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217458,10.1145/217382.217458,"A family of medium access control protocols for single-channel packet radio networks is specified and analyzed. These protocols are based on a new channel access discipline called floor acquisition multiple access (FAMA), which consists of both carrier sensing and a collision-avoidance dialogue between a source and the intended receiver of a packet. Control of the channel (the floor) is assigned to at most one station in the network at any given time, and this station is guaranteed to be able to transmit one or more data packets to different destinations with no collision with transmissions from other stations. The minimum length needed in control packets to acquire the floor is specified as a function of the channel propagation time. The medium access collision avoidance (MACA) protocol proposed by Karn and variants of CSMA based on collision avoidance are shown to be variants of FAMA protocols when control packets last long enough compared to the channel propagation delay. The throughput of FAMA protocols is analyzed and compared with the throughput of non-persistent CSMA. This analysis shows that using carrier sensing as an integral part of the floor acquisition strategy provides the benefits of MACA in the presence of hidden terminals, and can provide a throughput comparable to, or better than, that of non-persistent CSMA when no hidden terminals exist.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",262–273,12,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2117,article,"Fullmer, Chane L. and Garcia-Luna-Aceves, J. J.",Floor Acquisition Multiple Access (FAMA) for Packet-Radio Networks,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217458,10.1145/217391.217458,"A family of medium access control protocols for single-channel packet radio networks is specified and analyzed. These protocols are based on a new channel access discipline called floor acquisition multiple access (FAMA), which consists of both carrier sensing and a collision-avoidance dialogue between a source and the intended receiver of a packet. Control of the channel (the floor) is assigned to at most one station in the network at any given time, and this station is guaranteed to be able to transmit one or more data packets to different destinations with no collision with transmissions from other stations. The minimum length needed in control packets to acquire the floor is specified as a function of the channel propagation time. The medium access collision avoidance (MACA) protocol proposed by Karn and variants of CSMA based on collision avoidance are shown to be variants of FAMA protocols when control packets last long enough compared to the channel propagation delay. The throughput of FAMA protocols is analyzed and compared with the throughput of non-persistent CSMA. This analysis shows that using carrier sensing as an integral part of the floor acquisition strategy provides the benefits of MACA in the presence of hidden terminals, and can provide a throughput comparable to, or better than, that of non-persistent CSMA when no hidden terminals exist.",,262–273,12,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2118,inproceedings,"Handley, Mark and Wakeman, Ian and Crowcroft, Jon",The Conference Control Channel Protocol (CCCP): A Scalable Base for Building Conference Control Applications,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217461,10.1145/217382.217461,"This paper presents the Conference Control Channel Protocol (CCCP), a new scheme intended for controlling conferences ranging from small, tightly coupled meetings, to extremely large loosely coupled seminars. We describe the requirements of such a scheme, and present a framework for building systems that connect together new and existing applications.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",275–287,13,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2119,article,"Handley, Mark and Wakeman, Ian and Crowcroft, Jon",The Conference Control Channel Protocol (CCCP): A Scalable Base for Building Conference Control Applications,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217461,10.1145/217391.217461,"This paper presents the Conference Control Channel Protocol (CCCP), a new scheme intended for controlling conferences ranging from small, tightly coupled meetings, to extremely large loosely coupled seminars. We describe the requirements of such a scheme, and present a framework for building systems that connect together new and existing applications.",,275–287,13,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2120,inproceedings,"Guyton, James D. and Schwartz, Michael F.",Locating Nearby Copies of Replicated Internet Servers,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217463,10.1145/217382.217463,"In this paper we consider the problem of choosing among a collection of replicated servers, focusing on the question of how to make choices that segregate client/server traffic according to network topology. We explore the cost and effectiveness of a variety of approaches, ranging from those requiring routing layer support (e.g., anycast) to those that build location databases using application-level probe tools like traceroute. We uncover a number of tradeoffs between effectiveness, network cost, ease of deployment, and portability across different types of networks. We performed our experiments using a simulation parameterized by a topology collected from 7 survey sites across the United States, exploring a global collection of Network Time Protocol servers.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",288–298,11,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2121,article,"Guyton, James D. and Schwartz, Michael F.",Locating Nearby Copies of Replicated Internet Servers,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217463,10.1145/217391.217463,"In this paper we consider the problem of choosing among a collection of replicated servers, focusing on the question of how to make choices that segregate client/server traffic according to network topology. We explore the cost and effectiveness of a variety of approaches, ranging from those requiring routing layer support (e.g., anycast) to those that build location databases using application-level probe tools like traceroute. We uncover a number of tradeoffs between effectiveness, network cost, ease of deployment, and portability across different types of networks. We performed our experiments using a simulation parameterized by a topology collected from 7 survey sites across the United States, exploring a global collection of Network Time Protocol servers.",,288–298,11,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2122,inproceedings,"Mogul, Jeffrey C.",The Case for Persistent-Connection HTTP,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217465,10.1145/217382.217465,"The success of the World-Wide Web is largely due to the simplicity, hence ease of implementation, of the Hypertext Transfer Protocol (HTTP). HTTP, however, makes inefficient use of network and server resources, and adds unnecessary latencies, by creating a new TCP connection for each request. Modifications to HTTP have been proposed that would transport multiple requests over each TCP connection. These modifications have led to debate over their actual impact on users, on servers, and on the network. This paper reports the results of log-driven simulations of several variants of the proposed modifications, which demonstrate the value of persistent connections.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",299–313,15,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2123,article,"Mogul, Jeffrey C.",The Case for Persistent-Connection HTTP,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217465,10.1145/217391.217465,"The success of the World-Wide Web is largely due to the simplicity, hence ease of implementation, of the Hypertext Transfer Protocol (HTTP). HTTP, however, makes inefficient use of network and server resources, and adds unnecessary latencies, by creating a new TCP connection for each request. Modifications to HTTP have been proposed that would transport multiple requests over each TCP connection. These modifications have led to debate over their actual impact on users, on servers, and on the network. This paper reports the results of log-driven simulations of several variants of the proposed modifications, which demonstrate the value of persistent connections.",,299–313,15,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2124,inproceedings,"Herzog, Shai and Shenker, Scott and Estrin, Deborah",Sharing the “Cost” of Multicast Trees: An Axiomatic Analysis,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217466,10.1145/217382.217466,"Given the need to provide users with reasonable feedback about the ""costs"" their network usage incurs, and the increasingly commercial nature of the Internet, we believe that the allocation of cost among users will play an important role in future networks. This paper discusses cost allocation in the context of multicast flows. The question we discuss is this: when a single data flow is shared among many receivers, how does one split the cost of that flow among the receivers? Multicast routing increases network efficiency by using a single shared delivery tree. We address the issue of how these savings are allocated among the various members of the multicast group. We first consider an axiomatic approach to the problem, analyzing the implications of different distributive notions on the resulting allocations. We then consider a one-pass mechanism to implement such allocation schemes and investigate the family of allocation schemes such mechanisms can support.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",315–327,13,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2125,article,"Herzog, Shai and Shenker, Scott and Estrin, Deborah",Sharing the “Cost” of Multicast Trees: An Axiomatic Analysis,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217466,10.1145/217391.217466,"Given the need to provide users with reasonable feedback about the ""costs"" their network usage incurs, and the increasingly commercial nature of the Internet, we believe that the allocation of cost among users will play an important role in future networks. This paper discusses cost allocation in the context of multicast flows. The question we discuss is this: when a single data flow is shared among many receivers, how does one split the cost of that flow among the receivers? Multicast routing increases network efficiency by using a single shared delivery tree. We address the issue of how these savings are allocated among the various members of the multicast group. We first consider an axiomatic approach to the problem, analyzing the implications of different distributive notions on the resulting allocations. We then consider a one-pass mechanism to implement such allocation schemes and investigate the family of allocation schemes such mechanisms can support.",,315–327,13,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2126,inproceedings,"Holbrook, Hugh W. and Singhal, Sandeep K. and Cheriton, David R.",Log-Based Receiver-Reliable Multicast for Distributed Interactive Simulation,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217468,10.1145/217382.217468,"Reliable multicast communication is important in large-scale distributed applications. For example, reliable multicast is used to transmit terrain and environmental updates in distributed simulations. To date, proposed protocols have not supported these applications' requirements, which include wide-area data distribution, low-latency packet loss detection and recovery, and minimal data and management over-head within fine-grained multicast groups, each containing a single data source.In this paper, we introduce the notion of Log-Based Receiver-reliable Multicast (LBRM) communication, and we describe and evaluate a collection of log-based receiver reliable multicast optimizations that provide an efficient, scalable protocol for high-performance simulation applications. We argue that these techniques provide value to a broader range of applications and that the receiver-reliable model is an appropriate one for communication in general.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",328–341,14,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2127,article,"Holbrook, Hugh W. and Singhal, Sandeep K. and Cheriton, David R.",Log-Based Receiver-Reliable Multicast for Distributed Interactive Simulation,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217468,10.1145/217391.217468,"Reliable multicast communication is important in large-scale distributed applications. For example, reliable multicast is used to transmit terrain and environmental updates in distributed simulations. To date, proposed protocols have not supported these applications' requirements, which include wide-area data distribution, low-latency packet loss detection and recovery, and minimal data and management over-head within fine-grained multicast groups, each containing a single data source.In this paper, we introduce the notion of Log-Based Receiver-reliable Multicast (LBRM) communication, and we describe and evaluate a collection of log-based receiver reliable multicast optimizations that provide an efficient, scalable protocol for high-performance simulation applications. We argue that these techniques provide value to a broader range of applications and that the receiver-reliable model is an appropriate one for communication in general.",,328–341,14,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2128,inproceedings,"Floyd, Sally and Jacobson, Van and McCanne, Steve and Liu, Ching-Gung and Zhang, Lixia",A Reliable Multicast Framework for Light-Weight Sessions and Application Level Framing,1995,0897917111,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217382.217470,10.1145/217382.217470,"This paper describes SRM (Scalable Reliable Multicast), a reliable multicast framework for application level framing and light-weight sessions. The algorithms of this framework are efficient, robust, and scale well to both very large networks and very large sessions. The framework has been prototyped in wb, a distributed whiteboard application, and has been extensively tested on a global scale with sessions ranging from a few to more than 1000 participants. The paper describes the principles that have guided our design, including the IP multicast group delivery model, an end-to-end, receiver-based model of reliability, and the application level framing protocol model. As with unicast communications, the performance of a reliable multicast delivery algorithm depends on the underlying topology and operational environment. We investigate that dependence via analysis and simulation, and demonstrate an adaptive algorithm that uses the results of previous loss recovery events to adapt the control parameters used for future loss recovery. With the adaptive algorithm, our reliable multicast delivery algorithm provides good performance over a wide range of underlying topologies.","Proceedings of the Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",342–356,15,,"Cambridge, Massachusetts, USA",SIGCOMM '95,,,,,,
2129,article,"Floyd, Sally and Jacobson, Van and McCanne, Steve and Liu, Ching-Gung and Zhang, Lixia",A Reliable Multicast Framework for Light-Weight Sessions and Application Level Framing,1995,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/217391.217470,10.1145/217391.217470,"This paper describes SRM (Scalable Reliable Multicast), a reliable multicast framework for application level framing and light-weight sessions. The algorithms of this framework are efficient, robust, and scale well to both very large networks and very large sessions. The framework has been prototyped in wb, a distributed whiteboard application, and has been extensively tested on a global scale with sessions ranging from a few to more than 1000 participants. The paper describes the principles that have guided our design, including the IP multicast group delivery model, an end-to-end, receiver-based model of reliability, and the application level framing protocol model. As with unicast communications, the performance of a reliable multicast delivery algorithm depends on the underlying topology and operational environment. We investigate that dependence via analysis and simulation, and demonstrate an adaptive algorithm that uses the results of previous loss recovery events to adapt the control parameters used for future loss recovery. With the adaptive algorithm, our reliable multicast delivery algorithm provides good performance over a wide range of underlying topologies.",,342–356,15,,,,Oct. 1995,25,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2130,inproceedings,"Druschel, Peter and Peterson, Larry L. and Davie, Bruce S.",Experiences with a High-Speed Network Adaptor: A Software Perspective,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190315,10.1145/190314.190315,"This paper describes our experiences, from a software perspective, with the OSIRIS network adaptor. It first identifies the problems we encountered while programming OSIRIS and optimizing network performance, and outlines how we either addressed them in the software, or had to modify the hardware. It then describes the opportunities provided by OSIRIS that we were able to exploit in the host operating system (OS); opportunities that suggested techniques for making the OS more effective in delivering network data to application programs. The most novel of these techniques, called application device channels, gives application programs running in user space direct access to the adaptor. The paper concludes with the lessons drawn from this work, which we believe will benefit the designers of future network adaptors.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",2–13,12,,"London, United Kingdom",SIGCOMM '94,,,,,,
2131,article,"Druschel, Peter and Peterson, Larry L. and Davie, Bruce S.",Experiences with a High-Speed Network Adaptor: A Software Perspective,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190315,10.1145/190809.190315,"This paper describes our experiences, from a software perspective, with the OSIRIS network adaptor. It first identifies the problems we encountered while programming OSIRIS and optimizing network performance, and outlines how we either addressed them in the software, or had to modify the hardware. It then describes the opportunities provided by OSIRIS that we were able to exploit in the host operating system (OS); opportunities that suggested techniques for making the OS more effective in delivering network data to application programs. The most novel of these techniques, called application device channels, gives application programs running in user space direct access to the adaptor. The paper concludes with the lessons drawn from this work, which we believe will benefit the designers of future network adaptors.",,2–13,12,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2132,inproceedings,"Edwards, Aled and Watson, Greg and Lumley, John and Banks, David and Calamvokis, Costas and Dalton, C.",User-Space Protocols Deliver High Performance to Applications on a Low-Cost Gb/s LAN,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190316,10.1145/190314.190316,"Two important questions in high-speed networking are firstly, how to provide Gbit/s networking at low cost and secondly, how to provide a flexible low-level network interface so that applications can control their data from the instant it arrives.We describe some work that addresses both of these questions. The Jetstream Gbit/s LAN is an experimental, low-cost network interface that provides the services required by delay-sensitive traffic as well as meeting the performance needs of current applications. Jetstream is a combination of traditional shared-medium LAN technology and more recent ATM cell- and switch-based technology.Jetstream frames contain a channel identifier so that the network driver can immediately associate an incoming frame with its application. We have developed such a driver that enables applications to control how their data should be managed without the need to first move the data into the application's address space. Consequently, applications can elect to read just a part of a frame and then instruct the driver to move the remainder directly to its destination. Individual channels can elect to receive frames that have failed their CRC, while applications can specify frame-drop policies on a per-channel basis.Measured results show that both kernel- and user-space protocols can achieve very good throughput: applications using both TCP and our own reliable byte-stream protocol have demonstrated throughputs in excess of 200 Mbit/s. The benefits of running protocols in user-space are well known- the drawback has often been a severe penalty in the performance achieved. In this paper we show that it is possible to have the best of both worlds.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",14–23,10,,"London, United Kingdom",SIGCOMM '94,,,,,,
2133,article,"Edwards, Aled and Watson, Greg and Lumley, John and Banks, David and Calamvokis, Costas and Dalton, C.",User-Space Protocols Deliver High Performance to Applications on a Low-Cost Gb/s LAN,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190316,10.1145/190809.190316,"Two important questions in high-speed networking are firstly, how to provide Gbit/s networking at low cost and secondly, how to provide a flexible low-level network interface so that applications can control their data from the instant it arrives.We describe some work that addresses both of these questions. The Jetstream Gbit/s LAN is an experimental, low-cost network interface that provides the services required by delay-sensitive traffic as well as meeting the performance needs of current applications. Jetstream is a combination of traditional shared-medium LAN technology and more recent ATM cell- and switch-based technology.Jetstream frames contain a channel identifier so that the network driver can immediately associate an incoming frame with its application. We have developed such a driver that enables applications to control how their data should be managed without the need to first move the data into the application's address space. Consequently, applications can elect to read just a part of a frame and then instruct the driver to move the remainder directly to its destination. Individual channels can elect to receive frames that have failed their CRC, while applications can specify frame-drop policies on a per-channel basis.Measured results show that both kernel- and user-space protocols can achieve very good throughput: applications using both TCP and our own reliable byte-stream protocol have demonstrated throughputs in excess of 200 Mbit/s. The benefits of running protocols in user-space are well known- the drawback has often been a severe penalty in the performance achieved. In this paper we show that it is possible to have the best of both worlds.",,14–23,10,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2134,inproceedings,"Brakmo, Lawrence S. and O'Malley, Sean W. and Peterson, Larry L.",TCP Vegas: New Techniques for Congestion Detection and Avoidance,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190317,10.1145/190314.190317,"Vegas is a new implementation of TCP that achieves between 40 and 70% better throughput, with one-fifth to one-half the losses, as compared to the implementation of TCP in the Reno distribution of BSD Unix. This paper motivates and describes the three key techniques employed by Vegas, and presents the results of a comprehensive experimental performance study—using both simulations and measurements on the Internet—of the Vegas and Reno implementations of TCP.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",24–35,12,,"London, United Kingdom",SIGCOMM '94,,,,,,
2135,article,"Brakmo, Lawrence S. and O'Malley, Sean W. and Peterson, Larry L.",TCP Vegas: New Techniques for Congestion Detection and Avoidance,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190317,10.1145/190809.190317,"Vegas is a new implementation of TCP that achieves between 40 and 70% better throughput, with one-fifth to one-half the losses, as compared to the implementation of TCP in the Reno distribution of BSD Unix. This paper motivates and describes the three key techniques employed by Vegas, and presents the results of a comprehensive experimental performance study—using both simulations and measurements on the Internet—of the Vegas and Reno implementations of TCP.",,24–35,12,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2136,inproceedings,"Biagioni, Edoardo",A Structured TCP in Standard ML.,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190318,10.1145/190314.190318,"This paper describes the design of an implementation of the Transmission Control Protocol using an extension of the Standard ML (SML) language. SML supports higher-order functions, modularity, and type-safe module composition. We find that by using SML we can achieve good structure and good performance simultaneously. Good structure includes a modular decomposition of the protocol stack and of the TCP implementation, a control structure that imposes a total ordering on all events and processes them synchronously, and a test structure that allows component testing to catch problems before system integration. Strategies that help achieve good performance include using fast algorithms, using language constructs that make it easy to stage function evaluation, and language implementation features such as compacting garbage collection.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",36–45,10,,"London, United Kingdom",SIGCOMM '94,,,,,,
2137,article,"Biagioni, Edoardo",A Structured TCP in Standard ML.,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190318,10.1145/190809.190318,"This paper describes the design of an implementation of the Transmission Control Protocol using an extension of the Standard ML (SML) language. SML supports higher-order functions, modularity, and type-safe module composition. We find that by using SML we can achieve good structure and good performance simultaneously. Good structure includes a modular decomposition of the protocol stack and of the TCP implementation, a control structure that imposes a total ordering on all events and processes them synchronously, and a test structure that allows component testing to catch problems before system integration. Strategies that help achieve good performance include using fast algorithms, using language constructs that make it easy to stage function evaluation, and language implementation features such as compacting garbage collection.",,36–45,10,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2138,inproceedings,"Shenker, Scott",Making Greed Work in Networks: A Game-Theoretic Analysis of Switch Service Disciplines,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190319,10.1145/190314.190319,"This paper discusses congestion control from a game-theoretic perspective. There are two basic premises: (1) users are assumed to be independent and selfish, and (2) central administrative control is exercised only at the network switches. The operating points resulting from selfish user behavior depend crucially on the service disciplines implemented in network switches. This effect is investigated in a simple model consisting of a single exponential server shared by many Poisson sources. We discuss the extent to which one can guarantee, through the choice of switch service disciplines, that these selfish operating points will be efficient and fair. We also discuss to what extent the choice of switch service disciplines can ensure that these selfish operating points are unique and are easily and rapidly accessible by simple self-optimization techniques. We show that no service discipline can guarantee optimal efficiency. As for the other properties, we show that the traditional FIFO service discipline guarantees none of these properties, but that a service discipline called Fair Share guarantees all of them. While the treatment utilizes game-theoretic concepts, no previous knowledge of game theory is assumed.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",47–57,11,,"London, United Kingdom",SIGCOMM '94,,,,,,
2139,article,"Shenker, Scott",Making Greed Work in Networks: A Game-Theoretic Analysis of Switch Service Disciplines,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190319,10.1145/190809.190319,"This paper discusses congestion control from a game-theoretic perspective. There are two basic premises: (1) users are assumed to be independent and selfish, and (2) central administrative control is exercised only at the network switches. The operating points resulting from selfish user behavior depend crucially on the service disciplines implemented in network switches. This effect is investigated in a simple model consisting of a single exponential server shared by many Poisson sources. We discuss the extent to which one can guarantee, through the choice of switch service disciplines, that these selfish operating points will be efficient and fair. We also discuss to what extent the choice of switch service disciplines can ensure that these selfish operating points are unique and are easily and rapidly accessible by simple self-optimization techniques. We show that no service discipline can guarantee optimal efficiency. As for the other properties, we show that the traditional FIFO service discipline guarantees none of these properties, but that a service discipline called Fair Share guarantees all of them. While the treatment utilizes game-theoretic concepts, no previous knowledge of game theory is assumed.",,47–57,11,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2140,inproceedings,"Bolot, Jean-Chrysostome and Turletti, Thierry and Wakeman, Ian",Scalable Feedback Control for Multicast Video Distribution in the Internet,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190320,10.1145/190314.190320,"We describe a mechanism for scalable control of multicast continuous media streams. The mechanism uses a novel probing mechanism to solicit feedback information in a scalable manner and to estimate the number of receivers. In addition, it separates the congestion signal from the congestion control algorithm, so as to cope with heterogeneous networks.This mechanism has been implemented in the IVS video conference system using options within RTP to elicit information about the quality of the video delivered to the receivers. The H.261 coder of IVS then uses this information to adjust its output rate, the goal being to maximize the perceptual quality of the image received at the destinations while minimizing the bandwidth used by the video transmission. We find that our prototype control mechanism is well suited to the Internet environment. Furthermore, it prevents video sources from creating congestion in the Internet. Experiments are underway to investigate how the scalable proving mechanism can be used to facilitate multicast video distribution to large number of participants.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",58–67,10,,"London, United Kingdom",SIGCOMM '94,,,,,,
2141,article,"Bolot, Jean-Chrysostome and Turletti, Thierry and Wakeman, Ian",Scalable Feedback Control for Multicast Video Distribution in the Internet,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190320,10.1145/190809.190320,"We describe a mechanism for scalable control of multicast continuous media streams. The mechanism uses a novel probing mechanism to solicit feedback information in a scalable manner and to estimate the number of receivers. In addition, it separates the congestion signal from the congestion control algorithm, so as to cope with heterogeneous networks.This mechanism has been implemented in the IVS video conference system using options within RTP to elicit information about the quality of the video delivered to the receivers. The H.261 coder of IVS then uses this information to adjust its output rate, the goal being to maximize the perceptual quality of the image received at the destinations while minimizing the bandwidth used by the video transmission. We find that our prototype control mechanism is well suited to the Internet environment. Furthermore, it prevents video sources from creating congestion in the Internet. Experiments are underway to investigate how the scalable proving mechanism can be used to facilitate multicast video distribution to large number of participants.",,58–67,10,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2142,inproceedings,"Zhang, Zhi-Li and Towsley, Don and Kurose, Jim",Statistical Analysis of Generalized Processor Sharing Scheduling Discipline,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190321,10.1145/190314.190321,"In this paper, we consider the problem of providing statistical guarantees (for example, on the tail distribution of delay) under the Generalized Processor Sharing (GPS) scheduling discipline. This work is motivated by, and is an extension of, Parekh and Gallager's deterministic study of GPS scheduling discipline with leaky-bucket token controlled sessions [PG93a,b, Parekh92]. Using the exponentially bounded burstiness (E.B.B.) process model introduced in [YaSi93a] as a source traffic characterization, we establish results that extend the deterministic study of GPS: for a single GPS server in isolation, we present statistical bounds on the tail distributions of backlog and delay for each session. In the network setting, we show that networks belonging to a broad class of GPS assignments, the so-called Consistent Relative Session Treatment (CRST) GPS assignments, are stable in a stochastic sense. In particular, we establish simple bounds on the tail distribution of backlog and delay for each session in a Rate Proportional Processor Sharing (RPPS) GPS network with arbitrary topology.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",68–77,10,,"London, United Kingdom",SIGCOMM '94,,,,,,
2143,article,"Zhang, Zhi-Li and Towsley, Don and Kurose, Jim",Statistical Analysis of Generalized Processor Sharing Scheduling Discipline,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190321,10.1145/190809.190321,"In this paper, we consider the problem of providing statistical guarantees (for example, on the tail distribution of delay) under the Generalized Processor Sharing (GPS) scheduling discipline. This work is motivated by, and is an extension of, Parekh and Gallager's deterministic study of GPS scheduling discipline with leaky-bucket token controlled sessions [PG93a,b, Parekh92]. Using the exponentially bounded burstiness (E.B.B.) process model introduced in [YaSi93a] as a source traffic characterization, we establish results that extend the deterministic study of GPS: for a single GPS server in isolation, we present statistical bounds on the tail distributions of backlog and delay for each session. In the network setting, we show that networks belonging to a broad class of GPS assignments, the so-called Consistent Relative Session Treatment (CRST) GPS assignments, are stable in a stochastic sense. In particular, we establish simple bounds on the tail distribution of backlog and delay for each session in a Rate Proportional Processor Sharing (RPPS) GPS network with arbitrary topology.",,68–77,10,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2144,inproceedings,"Romanow, Allyn and Floyd, Sally",Dynamics of TCP Traffic over ATM Networks,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190322,10.1145/190314.190322,"We investigate the performance of TCP connections over ATM networks without ATM-level congestion control, and compare it to the performance of TCP over packet-based networks. For simulations of congested networks, the effective throughput of TCP over ATM can be quite low when cells are dropped at the congested ATM switch. The low throughput is due to wasted bandwidth as the congested link transmits cells from “corrupted” packets, i.e., packets in which at least one cell is dropped by the switch. This fragmentation effect can be corrected and high throughput can be achieved if the switch drops whole packets prior to buffer overflow; we call this strategy Early Packet Discard. We also discuss general issues of congestion avoidance for best-effort traffic in ATM networks.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",79–88,10,,"London, United Kingdom",SIGCOMM '94,,,,,,
2145,article,"Romanow, Allyn and Floyd, Sally",Dynamics of TCP Traffic over ATM Networks,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190322,10.1145/190809.190322,"We investigate the performance of TCP connections over ATM networks without ATM-level congestion control, and compare it to the performance of TCP over packet-based networks. For simulations of congested networks, the effective throughput of TCP over ATM can be quite low when cells are dropped at the congested ATM switch. The low throughput is due to wasted bandwidth as the congested link transmits cells from “corrupted” packets, i.e., packets in which at least one cell is dropped by the switch. This fragmentation effect can be corrected and high throughput can be achieved if the switch drops whole packets prior to buffer overflow; we call this strategy Early Packet Discard. We also discuss general issues of congestion avoidance for best-effort traffic in ATM networks.",,79–88,10,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2146,inproceedings,"\""{O",Reliable and Efficient Hop-by-Hop Flow Control,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190323,10.1145/190314.190323,"Hop-by-hop flow control can be used to fairly share the bandwidth of a network among competing flows. No data is lost even in overload conditions; yet each flow gets access to the maximum throughput when the network is lightly loaded. However, some schemes for hop-by-hop flow control require too much memory; some of them are not resilient to errors. We propose a scheme for making hop-by-hop flow control resilient and show that it has advantages over schemes proposed by Kung. We also describe a novel method for sharing the available buffers among the flows on a link; our scheme allows us to potentially reduce the memory requirement (or increase the number of flows that can be supported) by an order of magnitude. Most of the work is described in the context of an ATM network that uses credit based flow control. However our ideas extend to networks in which flows can be distinguished, and to rate based flow control schemes.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",89–100,12,,"London, United Kingdom",SIGCOMM '94,,,,,,
2147,article,"\""{O",Reliable and Efficient Hop-by-Hop Flow Control,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190323,10.1145/190809.190323,"Hop-by-hop flow control can be used to fairly share the bandwidth of a network among competing flows. No data is lost even in overload conditions; yet each flow gets access to the maximum throughput when the network is lightly loaded. However, some schemes for hop-by-hop flow control require too much memory; some of them are not resilient to errors. We propose a scheme for making hop-by-hop flow control resilient and show that it has advantages over schemes proposed by Kung. We also describe a novel method for sharing the available buffers among the flows on a link; our scheme allows us to potentially reduce the memory requirement (or increase the number of flows that can be supported) by an order of magnitude. Most of the work is described in the context of an ATM network that uses credit based flow control. However our ideas extend to networks in which flows can be distinguished, and to rate based flow control schemes.",,89–100,12,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2148,inproceedings,"Kung, H. T. and Blackwell, Trevor and Chapman, Alan","Credit-Based Flow Control for ATM Networks: Credit Update Protocol, Adaptive Credit Allocation and Statistical Multiplexing",1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190324,10.1145/190314.190324,"This paper presents three new results concerning credit-based flow control for ATM networks: (1) a simple and robust credit update protocol (CUP) suited for relatively inexpensive hardware/software implementation; (2) automatic adaptation of credit buffer allocation for virtual circuits (VCs) sharing the same buffer pool; (3) use of credit-based flow control to improve the effectiveness of statistical multiplexing in minimizing switch memory. These results have been substantiated by analysis, simulation and implementation.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",101–114,14,,"London, United Kingdom",SIGCOMM '94,,,,,,
2149,article,"Kung, H. T. and Blackwell, Trevor and Chapman, Alan","Credit-Based Flow Control for ATM Networks: Credit Update Protocol, Adaptive Credit Allocation and Statistical Multiplexing",1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190324,10.1145/190809.190324,"This paper presents three new results concerning credit-based flow control for ATM networks: (1) a simple and robust credit update protocol (CUP) suited for relatively inexpensive hardware/software implementation; (2) automatic adaptation of credit buffer allocation for virtual circuits (VCs) sharing the same buffer pool; (3) use of credit-based flow control to improve the effectiveness of statistical multiplexing in minimizing switch memory. These results have been substantiated by analysis, simulation and implementation.",,101–114,14,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2150,inproceedings,"Francis, Paul and Govindan, Ramesh",Flexible Routing and Addressing for a next Generation IP,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190325,10.1145/190314.190325,"Due to a limited address space and poor scaling of backbone routing information, the Internet Protocol (IP) is rapidly reaching the end of its useful lifetime. The Simple Internet Protocol Plus (SIPP), a proposed next generation Internet Protocol, solves these problems with larger internet layer addresses. In addition, SIPP provides a number of advanced routing and addressing capabilities including mobility, extended (variable-length) addressing, provider selection, and certain forms of multicast. These capabilities are all achieved through a single mechanism, a generalization of the IP loose source route. We argue that, for reasons of simplicity and evolvability, a single powerful mechanism to achieve a wide range of routing and addressing functions is preferable to having multiple specific mechanisms, one for each function.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",116–125,10,,"London, United Kingdom",SIGCOMM '94,,,,,,
2151,article,"Francis, Paul and Govindan, Ramesh",Flexible Routing and Addressing for a next Generation IP,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190325,10.1145/190809.190325,"Due to a limited address space and poor scaling of backbone routing information, the Internet Protocol (IP) is rapidly reaching the end of its useful lifetime. The Simple Internet Protocol Plus (SIPP), a proposed next generation Internet Protocol, solves these problems with larger internet layer addresses. In addition, SIPP provides a number of advanced routing and addressing capabilities including mobility, extended (variable-length) addressing, provider selection, and certain forms of multicast. These capabilities are all achieved through a single mechanism, a generalization of the IP loose source route. We argue that, for reasons of simplicity and evolvability, a single powerful mechanism to achieve a wide range of routing and addressing functions is preferable to having multiple specific mechanisms, one for each function.",,116–125,10,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2152,inproceedings,"Deering, Stephen and Estrin, Deborah and Farinacci, Dino and Jacobson, Van and Liu, Ching-Gung and Wei, Liming",An Architecture for Wide-Area Multicast Routing,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190326,10.1145/190314.190326,"Existing multicast routing mechanisms were intended for use within regions where a group is widely represented or bandwidth is universally plentiful. When group members, and senders to those group members, are distributed sparsely across a wide area, these schemes are not efficient; data packets or membership report information are occasionally sent over many links that do not lead to receivers or senders, respectively. We have developed a multicast routing architecture that efficiently establishes distribution trees across wide area internets, where many groups will be sparsely represented. Efficiency is measured in terms of the state, control message processing, and data packet processing, required across the entire network in order to deliver data packets to the members of the group.Our Protocol Independent Multicast (PIM) architecture: (a) maintains the traditional IP multicast service model of receiver-initiated membership; (b) can be configured to adapt to different multicast group and network characteristics; (c) is not dependent on a specific unicast routing protocol; and (d) uses soft-state mechanisms to adapt to underlying network conditions and group dynamics. The robustness, flexibility, and scaling properties of this architecture make it well suited to large heterogeneous inter-networks.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",126–135,10,,"London, United Kingdom",SIGCOMM '94,,,,,,
2153,article,"Deering, Stephen and Estrin, Deborah and Farinacci, Dino and Jacobson, Van and Liu, Ching-Gung and Wei, Liming",An Architecture for Wide-Area Multicast Routing,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190326,10.1145/190809.190326,"Existing multicast routing mechanisms were intended for use within regions where a group is widely represented or bandwidth is universally plentiful. When group members, and senders to those group members, are distributed sparsely across a wide area, these schemes are not efficient; data packets or membership report information are occasionally sent over many links that do not lead to receivers or senders, respectively. We have developed a multicast routing architecture that efficiently establishes distribution trees across wide area internets, where many groups will be sparsely represented. Efficiency is measured in terms of the state, control message processing, and data packet processing, required across the entire network in order to deliver data packets to the members of the group.Our Protocol Independent Multicast (PIM) architecture: (a) maintains the traditional IP multicast service model of receiver-initiated membership; (b) can be configured to adapt to different multicast group and network characteristics; (c) is not dependent on a specific unicast routing protocol; and (d) uses soft-state mechanisms to adapt to underlying network conditions and group dynamics. The robustness, flexibility, and scaling properties of this architecture make it well suited to large heterogeneous inter-networks.",,126–135,10,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2154,inproceedings,"Behrens, Jochen and Garcia-Luna-Aceves, J. J.","Distributed, Scalable Routing Based on Link-State Vectors",1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190327,10.1145/190314.190327,"A new family of routing algorithms for the distributed maintenance of routing information in large networks and internets is introduced. This family is called link vector algorithms (LVA), and is based on the selective diffusion of link-state information based on the distributed computation of preferred paths, rather than on the flooding of complete link-state information based on the distributed computation of preferred paths, rather than on the flooding of complete link-state information to all routers. According to LVA, each router maintains a subset of the topology that corresponds to the links used by its neighbor routers in their preferred paths to known destinations. Based on that subset of topology information, the router derives its own preferred paths and communicates the corresponding link-state information to its neighbors. An update message contains a vector of updates; each such update specifies a link and its parameters. LVAs can be used for different types of routing. The correctness of LVA is verified for arbitrary types of routing when correct and deterministic algorithms are used to select preferred paths at each router. LVA is shown to have smaller complexity than link-state and distance-vector algorithms, and to have better average performance than the ideal topology-broadcast algorithm and the distributed Bellman-Ford algorithm.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",136–147,12,,"London, United Kingdom",SIGCOMM '94,,,,,,
2155,article,"Behrens, Jochen and Garcia-Luna-Aceves, J. J.","Distributed, Scalable Routing Based on Link-State Vectors",1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190327,10.1145/190809.190327,"A new family of routing algorithms for the distributed maintenance of routing information in large networks and internets is introduced. This family is called link vector algorithms (LVA), and is based on the selective diffusion of link-state information based on the distributed computation of preferred paths, rather than on the flooding of complete link-state information based on the distributed computation of preferred paths, rather than on the flooding of complete link-state information to all routers. According to LVA, each router maintains a subset of the topology that corresponds to the links used by its neighbor routers in their preferred paths to known destinations. Based on that subset of topology information, the router derives its own preferred paths and communicates the corresponding link-state information to its neighbors. An update message contains a vector of updates; each such update specifies a link and its parameters. LVAs can be used for different types of routing. The correctness of LVA is verified for arbitrary types of routing when correct and deterministic algorithms are used to select preferred paths at each router. LVA is shown to have smaller complexity than link-state and distance-vector algorithms, and to have better average performance than the ideal topology-broadcast algorithm and the distributed Bellman-Ford algorithm.",,136–147,12,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2156,inproceedings,"Sharma, R. and Keshav, S.",Signaling and Operating System Support for Native-Mode ATM Applications,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190328,10.1145/190314.190328,"Applications communicating over connectionless networks, such as IP, cannot obtain per-connection Quality of Service (QoS) guarantees. In contrast, the connection-oriented nature of the ATM layer and its per-virtual-circuit QoS guarantees are visible to a native-mode ATM application. We describe the design and implementation of operating system and signaling support for native-mode applications, independent of the semantics of the protocol layers or of the signaling protocol. The work was done in the context of a Unix-like operating system and the Xunet 2 wide-area high-speed ATM network. The IPC-based interface between an application and the signaling entity allows processes to request parameterized virtual circuits, and the signaling-kernel interface allows resources to be reclaimed from prematurely terminating processes. We also built a simple encapsulation layer over raw IP that allows any host with IP access to send AAL frames into the wide-area network with little performance degradation. Our design makes it simple to port existing TCP/IP socket applications to a native-mode ATM protocol stack and also enables interoperation of existing IP networks with our ATM network. Our experience has been positive - the design is robust, easily extendible and scales well with the number of open connections.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",149–157,9,,"London, United Kingdom",SIGCOMM '94,,,,,,
2157,article,"Sharma, R. and Keshav, S.",Signaling and Operating System Support for Native-Mode ATM Applications,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190328,10.1145/190809.190328,"Applications communicating over connectionless networks, such as IP, cannot obtain per-connection Quality of Service (QoS) guarantees. In contrast, the connection-oriented nature of the ATM layer and its per-virtual-circuit QoS guarantees are visible to a native-mode ATM application. We describe the design and implementation of operating system and signaling support for native-mode applications, independent of the semantics of the protocol layers or of the signaling protocol. The work was done in the context of a Unix-like operating system and the Xunet 2 wide-area high-speed ATM network. The IPC-based interface between an application and the signaling entity allows processes to request parameterized virtual circuits, and the signaling-kernel interface allows resources to be reclaimed from prematurely terminating processes. We also built a simple encapsulation layer over raw IP that allows any host with IP access to send AAL frames into the wide-area network with little performance degradation. Our design makes it simple to port existing TCP/IP socket applications to a native-mode ATM protocol stack and also enables interoperation of existing IP networks with our ATM network. Our experience has been positive - the design is robust, easily extendible and scales well with the number of open connections.",,149–157,9,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2158,inproceedings,"Black, Richard and Leslie, Ian and McAuley, Derek",Experiences of Building an ATM Switch for the Local Area,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190329,10.1145/190314.190329,"The Fairisle project was concerned with ATM in the local area. An earlier paper [9] described the preliminary work and plans for the project. Here we present the experiences we have had with the Fairisle network, describing how implementation has changed over the life of the project, the lessons learned, and some conclusions about the work so far.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",158–167,10,,"London, United Kingdom",SIGCOMM '94,,,,,,
2159,article,"Black, Richard and Leslie, Ian and McAuley, Derek",Experiences of Building an ATM Switch for the Local Area,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190329,10.1145/190809.190329,"The Fairisle project was concerned with ATM in the local area. An earlier paper [9] described the preliminary work and plans for the project. Here we present the experiences we have had with the Fairisle network, describing how implementation has changed over the life of the project, the lessons learned, and some conclusions about the work so far.",,158–167,10,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2160,inproceedings,"Sibal, Sandeep and DeSimone, Antonio",Controlling Alternate Routing in General-Mesh Packet Flow Networks,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190330,10.1145/190314.190330,"High-speed packet networks will begin to support services that need Quality-of-Service (QoS) guarantees. Guaranteeing QoS typically translates to reserving resources for the duration of a call. We propose a state-dependent routing scheme that builds on any base state-independent routing scheme, by routing flows which are blocked on their primary paths (as selected by the state-independent scheme) onto alternate paths in a manner that is guaranteed—under certain Poisson assumptions— to improve on the performance of the base state-independent scheme. Our scheme only requires each node to have state information of those links that are incident on it. Such a scheme is of value when either the base state-independent scheme is already in place and a complete overhaul of the routing algorithm is undesirable, or when the state (reserved flows) of a link changes fast enough that the timely update of state information is infeasible to all possible call-originators. The performance improvements due to our controlled alternate routing scheme are borne out from simulations conducted on a fully-connected 4-node network, as well as on a sparsely-connected 12-node network modeled on the NSFNet T3 Backbone.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",168–179,12,,"London, United Kingdom",SIGCOMM '94,,,,,,
2161,article,"Sibal, Sandeep and DeSimone, Antonio",Controlling Alternate Routing in General-Mesh Packet Flow Networks,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190330,10.1145/190809.190330,"High-speed packet networks will begin to support services that need Quality-of-Service (QoS) guarantees. Guaranteeing QoS typically translates to reserving resources for the duration of a call. We propose a state-dependent routing scheme that builds on any base state-independent routing scheme, by routing flows which are blocked on their primary paths (as selected by the state-independent scheme) onto alternate paths in a manner that is guaranteed—under certain Poisson assumptions— to improve on the performance of the base state-independent scheme. Our scheme only requires each node to have state information of those links that are incident on it. Such a scheme is of value when either the base state-independent scheme is already in place and a complete overhaul of the routing algorithm is undesirable, or when the state (reserved flows) of a link changes fast enough that the timely update of state information is infeasible to all possible call-originators. The performance improvements due to our controlled alternate routing scheme are borne out from simulations conducted on a fully-connected 4-node network, as well as on a sparsely-connected 12-node network modeled on the NSFNet T3 Backbone.",,168–179,12,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2162,inproceedings,"Matsumoto, Yutaka",On Optimization of Polling Policy Represented by Neural Network,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190331,10.1145/190314.190331,"This paper deals with the problem of scheduling a server in a polling system with multiple queues and complete information. We represent the polling policy by a neural network; namely, given the number of waiting customers in each queue, the server determines next queue he should visit according to the output of the neural network. By using the simulated annealing method, we improve the neural polling policy in such a way that the mean delay of customers is minimized. Numerical results show that the present approach is especially valid for asymmetric polling systems whose analytical optimization is considered intractable.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",181–190,10,,"London, United Kingdom",SIGCOMM '94,,,,,,
2163,article,"Matsumoto, Yutaka",On Optimization of Polling Policy Represented by Neural Network,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190331,10.1145/190809.190331,"This paper deals with the problem of scheduling a server in a polling system with multiple queues and complete information. We represent the polling policy by a neural network; namely, given the number of waiting customers in each queue, the server determines next queue he should visit according to the output of the neural network. By using the simulated annealing method, we improve the neural polling policy in such a way that the mean delay of customers is minimized. Numerical results show that the present approach is especially valid for asymmetric polling systems whose analytical optimization is considered intractable.",,181–190,10,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2164,inproceedings,"Feehrer, John and Sauer, Jon and Ramfelt, Lars",Design and Implementation of a Prototype Optical Deflection Network,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190332,10.1145/190314.190332,"We describe the design and implementation of a packet-switched fiber optic interconnect prototype with a ShuffleNet topology, intended for use in shared-memory multiprocessors. Coupled with existing latency-hiding mechanisms, it can reduce latency to remote memory locations. Nodes use deflection routing to resolve contention. Each node contains a processor, memory, photonic switch, and packet routing processor. Payload remains in optical form from source to final destination. Each host processor is a commercial workstation with FIFO interfaces between its bus and the photonic switch. A global clock is distributed optically to each node to minimize skew. Component costs and network performance figures are presented for various node configurations including bit-per-wavelength and fiber-parallel packet formats. Our efforts to implement and test a practical interconnect including real host computers distinguishes our work from previous theoretical and experimental work. We summarize obstacles we encountered and discuss future work.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",191–200,10,,"London, United Kingdom",SIGCOMM '94,,,,,,
2165,article,"Feehrer, John and Sauer, Jon and Ramfelt, Lars",Design and Implementation of a Prototype Optical Deflection Network,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190332,10.1145/190809.190332,"We describe the design and implementation of a packet-switched fiber optic interconnect prototype with a ShuffleNet topology, intended for use in shared-memory multiprocessors. Coupled with existing latency-hiding mechanisms, it can reduce latency to remote memory locations. Nodes use deflection routing to resolve contention. Each node contains a processor, memory, photonic switch, and packet routing processor. Payload remains in optical form from source to final destination. Each host processor is a commercial workstation with FIFO interfaces between its bus and the photonic switch. A global clock is distributed optically to each node to minimize skew. Component costs and network performance figures are presented for various node configurations including bit-per-wavelength and fiber-parallel packet formats. Our efforts to implement and test a practical interconnect including real host computers distinguishes our work from previous theoretical and experimental work. We summarize obstacles we encountered and discuss future work.",,191–200,10,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2166,inproceedings,"Aly, Khaled A.",Conflict-Free Channel Assignment for an Optical Cluster-Based Shuffle Network Configuration,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190333,10.1145/190314.190333,"A passive optical realization of large expandable shuffle networks is considered, where the general (p, k) shuffle function interconnects star-coupled clusters of time and/or wavelength multiplexed nodes. This configuration enables network partitioning into independent subnetworks that can emulate various indirect cube topologies on a virtual point-to-point basis. Node transmitters are assigned fixed channels and reconfiguring the partition and/or the individual sub-network topologies is achieved by monitoring the appropriate channels. The considered network configuration can function as a general-purpose optical interconnect for a variety of heterogeneous traffic sources, such as multicomputers and ATM network interface units, communicating independently within logically defined subnetworks. The paper derives a conflict-free channel partition and assignment map for a general (p, k) cluster shuffle using a minimal number of channel sets.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",201–210,10,,"London, United Kingdom",SIGCOMM '94,,,,,,
2167,article,"Aly, Khaled A.",Conflict-Free Channel Assignment for an Optical Cluster-Based Shuffle Network Configuration,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190333,10.1145/190809.190333,"A passive optical realization of large expandable shuffle networks is considered, where the general (p, k) shuffle function interconnects star-coupled clusters of time and/or wavelength multiplexed nodes. This configuration enables network partitioning into independent subnetworks that can emulate various indirect cube topologies on a virtual point-to-point basis. Node transmitters are assigned fixed channels and reconfiguring the partition and/or the individual sub-network topologies is achieved by monitoring the appropriate channels. The considered network configuration can function as a general-purpose optical interconnect for a variety of heterogeneous traffic sources, such as multicomputers and ATM network interface units, communicating independently within logically defined subnetworks. The paper derives a conflict-free channel partition and assignment map for a general (p, k) cluster shuffle using a minimal number of channel sets.",,201–210,10,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2168,inproceedings,"Bharghavan, Vaduvur and Demers, Alan and Shenker, Scott and Zhang, Lixia",MACAW: A Media Access Protocol for Wireless LAN's,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190334,10.1145/190314.190334,"In recent years, a wide variety of mobile computing devices has emerged, including portables, palmtops, and personal digital assistants. Providing adequate network connectivity for these devices will require a new generation of wireless LAN technology. In this paper we study media access protocols for a single channel wireless LAN being developed at Xerox Corporation's Palo Alto Research Center. We start with the MACA media access protocol first proposed by Karn [9] and later refined by Biba [3] which uses an RTS-CTS-DATA packet exchange and binary exponential back-off. Using packet-level simulations, we examine various performance and design issues in such protocols. Our analysis leads to a new protocol, MACAW, which uses an RTS-CTS-DS-DATA-ACK message exchange and includes a significantly different backoff algorithm.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",212–225,14,,"London, United Kingdom",SIGCOMM '94,,,,,,
2169,article,"Bharghavan, Vaduvur and Demers, Alan and Shenker, Scott and Zhang, Lixia",MACAW: A Media Access Protocol for Wireless LAN's,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190334,10.1145/190809.190334,"In recent years, a wide variety of mobile computing devices has emerged, including portables, palmtops, and personal digital assistants. Providing adequate network connectivity for these devices will require a new generation of wireless LAN technology. In this paper we study media access protocols for a single channel wireless LAN being developed at Xerox Corporation's Palo Alto Research Center. We start with the MACA media access protocol first proposed by Karn [9] and later refined by Biba [3] which uses an RTS-CTS-DATA packet exchange and binary exponential back-off. Using packet-level simulations, we examine various performance and design issues in such protocols. Our analysis leads to a new protocol, MACAW, which uses an RTS-CTS-DS-DATA-ACK message exchange and includes a significantly different backoff algorithm.",,212–225,14,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2170,inproceedings,"Mitzel, Danny J. and Shenker, Scott",Asymptotic Resource Consumption in Multicast Reservation Styles,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190335,10.1145/190314.190335,"The goal of network design is to meet the needs of resident applications in an efficient manner. Adding real-time service and point-to-multipoint multicast routing to the Internet's traditional point-to-point best effort service model will greatly increase the Internet's efficiency in handling point-to-multipoint real-time applications. Recently, the RSVP resource reservation protocol has introduced the concept of “reservation styles”, which control how reservations are aggregated in multipoint-to-multipoint real-time applications. In this paper, which is an extension of [9], we analytically evaluate the efficiency gains offered by this new paradigm on three simple network topologies: linear, m-tree, and star. We compare the resource utilization of more traditional reservation approaches to the RSVP reservation styles in the asymptotic limit of large multipoint applications. We find that in several cases the efficiency improvements scale linearly in the number of hosts.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",226–233,8,,"London, United Kingdom",SIGCOMM '94,,,,,,
2171,article,"Mitzel, Danny J. and Shenker, Scott",Asymptotic Resource Consumption in Multicast Reservation Styles,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190335,10.1145/190809.190335,"The goal of network design is to meet the needs of resident applications in an efficient manner. Adding real-time service and point-to-multipoint multicast routing to the Internet's traditional point-to-point best effort service model will greatly increase the Internet's efficiency in handling point-to-multipoint real-time applications. Recently, the RSVP resource reservation protocol has introduced the concept of “reservation styles”, which control how reservations are aggregated in multipoint-to-multipoint real-time applications. In this paper, which is an extension of [9], we analytically evaluate the efficiency gains offered by this new paradigm on three simple network topologies: linear, m-tree, and star. We compare the resource utilization of more traditional reservation approaches to the RSVP reservation styles in the asymptotic limit of large multipoint applications. We find that in several cases the efficiency improvements scale linearly in the number of hosts.",,226–233,8,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2172,inproceedings,"Perkins, Charles E. and Bhagwat, Pravin",Highly Dynamic Destination-Sequenced Distance-Vector Routing (DSDV) for Mobile Computers,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190336,10.1145/190314.190336,"An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize ad hoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",234–244,11,,"London, United Kingdom",SIGCOMM '94,,,,,,
2173,article,"Perkins, Charles E. and Bhagwat, Pravin",Highly Dynamic Destination-Sequenced Distance-Vector Routing (DSDV) for Mobile Computers,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190336,10.1145/190809.190336,"An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize ad hoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.",,234–244,11,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2174,inproceedings,"Singh, Gurdip",A Methodology for Designing Communication Protocols,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190337,10.1145/190314.190337,"We propose a compositional technique for designing protocols. The technique involves specifying constraints between the events of the component protocols. A constraint may either require synchronization between certain events of the component protocols or may require inhibiting an event in one protocol on the occurrence of an event in another component protocol. We find both types of constraints useful in composing protocols. We demonstrate the applicability of the technique by deriving several protocols. The technique facilitates modular design and verification. Our technique, in conjunction with the sequential composition technique, can be used to design complex protocols.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",245–255,11,,"London, United Kingdom",SIGCOMM '94,,,,,,
2175,article,"Singh, Gurdip",A Methodology for Designing Communication Protocols,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190337,10.1145/190809.190337,"We propose a compositional technique for designing protocols. The technique involves specifying constraints between the events of the component protocols. A constraint may either require synchronization between certain events of the component protocols or may require inhibiting an event in one protocol on the occurrence of an event in another component protocol. We find both types of constraints useful in composing protocols. We demonstrate the applicability of the technique by deriving several protocols. The technique facilitates modular design and verification. Our technique, in conjunction with the sequential composition technique, can be used to design complex protocols.",,245–255,11,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2176,inproceedings,"Paxson, Vern and Floyd, Sally",Wide-Area Traffic: The Failure of Poisson Modeling,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190338,10.1145/190314.190338,"Network arrivals are often modeled as Poisson processes for analytic simplicity, even though a number of traffic studies have shown that packet interarrivals are not exponentially distributed. We evaluate 21 wide-area traces, investigating a number of wide-area TCP arrival processes (session and connection arrivals, FTPDATA connection arrivals within FTP sessions, and TELNET packet arrivals) to determine the error introduced by modeling them using Poisson processes. We find that user-initiated TCP session arrivals, such as remote-login and file-transfer, are well-modeled as Poisson processes with fixed hourly rates, but that other connection arrivals deviate considerably from Poisson; that modeling TELNET packet interarrivals as exponential grievously underestimates the burstiness of TELNET traffic, but using the empirical Tcplib[DJCME92] interarrivals preserves burstiness over many time scales; and that FTPDATA connection arrivals within FTP sessions come bunched into “connection burst”, the largest of which are so large that they completely dominate FTPDATA traffic. Finally, we offer some preliminary results regarding how our findings relate to the possible self-similarity of wide-area traffic.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",257–268,12,,"London, United Kingdom",SIGCOMM '94,,,,,,
2177,article,"Paxson, Vern and Floyd, Sally",Wide-Area Traffic: The Failure of Poisson Modeling,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190338,10.1145/190809.190338,"Network arrivals are often modeled as Poisson processes for analytic simplicity, even though a number of traffic studies have shown that packet interarrivals are not exponentially distributed. We evaluate 21 wide-area traces, investigating a number of wide-area TCP arrival processes (session and connection arrivals, FTPDATA connection arrivals within FTP sessions, and TELNET packet arrivals) to determine the error introduced by modeling them using Poisson processes. We find that user-initiated TCP session arrivals, such as remote-login and file-transfer, are well-modeled as Poisson processes with fixed hourly rates, but that other connection arrivals deviate considerably from Poisson; that modeling TELNET packet interarrivals as exponential grievously underestimates the burstiness of TELNET traffic, but using the empirical Tcplib[DJCME92] interarrivals preserves burstiness over many time scales; and that FTPDATA connection arrivals within FTP sessions come bunched into “connection burst”, the largest of which are so large that they completely dominate FTPDATA traffic. Finally, we offer some preliminary results regarding how our findings relate to the possible self-similarity of wide-area traffic.",,257–268,12,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2178,inproceedings,"Garrett, Mark W. and Willinger, Walter","Analysis, Modeling and Generation of Self-Similar VBR Video Traffic",1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190339,10.1145/190314.190339,"We present a detailed statistical analysis of a 2-hour long empirical sample of VBR video. The sample was obtained by applying a simple intraframe video compression code to an action movie. The main findings of our analysis are (1) the tail behavior of the marginal bandwidth distribution can be accurately described using “heavy-tailed” distributions (e.g., Pareto); (2) the autocorrelation of the VBR video sequence decays hyperbolically (equivalent to long-range dependence) and can be modeled using self-similar processes. We combine our findings in a new (non-Markovian) source model for VBR video and present an algorithm for generating synthetic traffic. Trace-driven simulations show that statistical multiplexing results in significant bandwidth efficiency even when long-range dependence is present. Simulations of our source model show long-range dependence and heavy-tailed marginals to be important components which are not accounted for in currently used VBR video traffic models.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",269–280,12,,"London, United Kingdom",SIGCOMM '94,,,,,,
2179,article,"Garrett, Mark W. and Willinger, Walter","Analysis, Modeling and Generation of Self-Similar VBR Video Traffic",1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190339,10.1145/190809.190339,"We present a detailed statistical analysis of a 2-hour long empirical sample of VBR video. The sample was obtained by applying a simple intraframe video compression code to an action movie. The main findings of our analysis are (1) the tail behavior of the marginal bandwidth distribution can be accurately described using “heavy-tailed” distributions (e.g., Pareto); (2) the autocorrelation of the VBR video sequence decays hyperbolically (equivalent to long-range dependence) and can be modeled using self-similar processes. We combine our findings in a new (non-Markovian) source model for VBR video and present an algorithm for generating synthetic traffic. Trace-driven simulations show that statistical multiplexing results in significant bandwidth efficiency even when long-range dependence is present. Simulations of our source model show long-range dependence and heavy-tailed marginals to be important components which are not accounted for in currently used VBR video traffic models.",,269–280,12,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2180,inproceedings,"Lam, Simon S. and Chow, Simon and Yau, David K. Y.",An Algorithm for Lossless Smoothing of MPEG Video,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190340,10.1145/190314.190340,"Interframe compression techniques, such as those used in MPEG video, give rise to a coded bit stream where picture sizes differ by a factor of 10 or more. As a result, buffering is needed to reduce (smooth) rate fluctuations of encoder output from one picture to the next; without smoothing, the performance of networks that carry such video traffic would be adversely affected. Various techniques have been suggested for controlling the output rate of a VBR encoder to alleviate network congestion or prevent buffer overflow. Most of these techniques, however, are lossy, and should be used only as a last resort. In this paper, we design and specify an algorithm for lossless smoothing. The algorithm is characterized by three parameters: D (delay bound), K (number of pictures with known sizes), and H (lookahead interval). We present a theorem which guarantees that, if K ≥ 1, the algorithm finds a solution that satisfies the delay bound. (Although the algorithm and theorem were motivated by MPEG video, they are applicable to the smoothing of compressed video in general). To study performance characteristics of the algorithm, we conducted a large number of experiments using statistics from four MPEG video sequences.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",281–293,13,,"London, United Kingdom",SIGCOMM '94,,,,,,
2181,article,"Lam, Simon S. and Chow, Simon and Yau, David K. Y.",An Algorithm for Lossless Smoothing of MPEG Video,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190340,10.1145/190809.190340,"Interframe compression techniques, such as those used in MPEG video, give rise to a coded bit stream where picture sizes differ by a factor of 10 or more. As a result, buffering is needed to reduce (smooth) rate fluctuations of encoder output from one picture to the next; without smoothing, the performance of networks that carry such video traffic would be adversely affected. Various techniques have been suggested for controlling the output rate of a VBR encoder to alleviate network congestion or prevent buffer overflow. Most of these techniques, however, are lossy, and should be used only as a last resort. In this paper, we design and specify an algorithm for lossless smoothing. The algorithm is characterized by three parameters: D (delay bound), K (number of pictures with known sizes), and H (lookahead interval). We present a theorem which guarantees that, if K ≥ 1, the algorithm finds a solution that satisfies the delay bound. (Although the algorithm and theorem were motivated by MPEG video, they are applicable to the smoothing of compressed video in general). To study performance characteristics of the algorithm, we conducted a large number of experiments using statistics from four MPEG video sequences.",,281–293,13,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2182,inproceedings,"O'Malley, Sean and Proebsting, Todd and Montz, Allen Brady",USC: A Universal Stub Compiler,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190341,10.1145/190314.190341,USC is a new stub compiler that generates stubs that perform many data conversion operations. USC is flexible and can be used in situations where previously only manual code generation was possible. USC generated code is up to 20 times faster than code generated by traditional argument marshaling schemes such as ASN.1 and Sun XDR. This paper presents the design of USC and a comprehensive set of experiments that compares USC performance with the best manually generated code and traditional stub compilers.,"Proceedings of the Conference on Communications Architectures, Protocols and Applications",295–306,12,,"London, United Kingdom",SIGCOMM '94,,,,,,
2183,article,"O'Malley, Sean and Proebsting, Todd and Montz, Allen Brady",USC: A Universal Stub Compiler,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190341,10.1145/190809.190341,USC is a new stub compiler that generates stubs that perform many data conversion operations. USC is flexible and can be used in situations where previously only manual code generation was possible. USC generated code is up to 20 times faster than code generated by traditional argument marshaling schemes such as ASN.1 and Sun XDR. This paper presents the design of USC and a comprehensive set of experiments that compares USC performance with the best manually generated code and traditional stub compilers.,,295–306,12,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2184,inproceedings,"Liu, Chung-Shyan",An Object-Based Approach to Protocol Software Implementation,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190342,10.1145/190314.190342,"In this paper, an object-based approach to protocol software implementation is presented. A protocol is specified by an FSM, then the FSM is implemented by a group of related objects. In our method, each state is implemented by an object. The member functions of an object are the interface vents that trigger state transitions, and actions associated with state transitions constitute the body of the member functions. An object becomes another object if a state transition is enabled. A real example is given for illustration. We also present a software tool that lets a designer edit a state machine graphically, and generates C++ class definitions automatically. We also discuss some implementation related issues and present an organization model for protocol layers.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",307–316,10,,"London, United Kingdom",SIGCOMM '94,,,,,,
2185,article,"Liu, Chung-Shyan",An Object-Based Approach to Protocol Software Implementation,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190342,10.1145/190809.190342,"In this paper, an object-based approach to protocol software implementation is presented. A protocol is specified by an FSM, then the FSM is implemented by a group of related objects. In our method, each state is implemented by an object. The member functions of an object are the interface vents that trigger state transitions, and actions associated with state transitions constitute the body of the member functions. An object becomes another object if a state transition is enabled. A real example is given for illustration. We also present a software tool that lets a designer edit a state machine graphically, and generates C++ class definitions automatically. We also discuss some implementation related issues and present an organization model for protocol layers.",,307–316,10,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
2186,inproceedings,"Mills, David L.",Improved Algorithms for Synchronizing Computer Network Clocks,1994,0897916824,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190314.190343,10.1145/190314.190343,"The Network Time Protocol (NTP) is widely deployed in the Internet to synchronize computer clocks to each other and to international standards via telephone modem, radio and satellite. The protocols and algorithms have evolved over more than a decade to produce the present NTP Version 3 specification and implementations. Most of the estimated deployment of 100,000 NTP servers and clients enjoy synchronization to within a few tens of milliseconds in the Internet of today.This paper describes specific improvements developed for NTP Version 3 which have resulted in increased accuracy, stability and reliability in both local-area and wide-area networks. These include engineered refinements of several algorithms used to measure time differences between a local clock and a number of peer clocks in the network, as well as to select the best ensemble from among a set of peer clocks and combine their differences to produce a clock accuracy better than any in the ensemble.This paper also describes engineered refinements of the algorithms used to adjust the time and frequency of the local clock, which functions as a disciplined oscillator. The refinements provide automatic adjustment of message-exchange intervals in order to minimize network traffic between clients and busy servers while maintaining the best accuracy. Finally, this paper describes certain enhancements to the Unix operating system software in order to realize submillisecond accuracies with fast workstations and networks.","Proceedings of the Conference on Communications Architectures, Protocols and Applications",317–327,11,,"London, United Kingdom",SIGCOMM '94,,,,,,
2187,article,"Mills, David L.",Improved Algorithms for Synchronizing Computer Network Clocks,1994,,Association for Computing Machinery,"New York, NY, USA",https://doi.org/10.1145/190809.190343,10.1145/190809.190343,"The Network Time Protocol (NTP) is widely deployed in the Internet to synchronize computer clocks to each other and to international standards via telephone modem, radio and satellite. The protocols and algorithms have evolved over more than a decade to produce the present NTP Version 3 specification and implementations. Most of the estimated deployment of 100,000 NTP servers and clients enjoy synchronization to within a few tens of milliseconds in the Internet of today.This paper describes specific improvements developed for NTP Version 3 which have resulted in increased accuracy, stability and reliability in both local-area and wide-area networks. These include engineered refinements of several algorithms used to measure time differences between a local clock and a number of peer clocks in the network, as well as to select the best ensemble from among a set of peer clocks and combine their differences to produce a clock accuracy better than any in the ensemble.This paper also describes engineered refinements of the algorithms used to adjust the time and frequency of the local clock, which functions as a disciplined oscillator. The refinements provide automatic adjustment of message-exchange intervals in order to minimize network traffic between clients and busy servers while maintaining the best accuracy. Finally, this paper describes certain enhancements to the Unix operating system software in order to realize submillisecond accuracies with fast workstations and networks.",,317–327,11,,,,Oct. 1994,24,4,0146-4833,SIGCOMM Comput. Commun. Rev.,oct
